{
  "course_name": "Linear Partial Differential Equations: Analysis and Numerics",
  "course_description": "This course provides students with the basic analytical and computational tools of linear partial differential equations (PDEs) for practical applications in science engineering, including heat / diffusion, wave, and Poisson equations. Analytics emphasize the viewpoint of linear algebra and the analogy with finite matrix problems. Numerics focus on finite-difference and finite-element techniques to reduce PDEs to matrix problems. The Julia Language (a free, open-source environment) is introduced and used in homework for simple examples.",
  "topics": [
    "Mathematics",
    "Applied Mathematics",
    "Differential Equations",
    "Linear Algebra",
    "Mathematics",
    "Applied Mathematics",
    "Differential Equations",
    "Linear Algebra"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nPrerequisites\n\n18.06 Linear Algebra\n,\n18.700 Linear Algebra\nor equivalent.\n\nDescription\n\nThis course provides students with the basic analytical and computational tools of linear partial differential equations (PDEs) for practical applications in science engineering, including heat/diffusion, wave, and Poisson equations.\n\nAnalytics emphasize the viewpoint of linear algebra and the analogy with finite matrix problems including operator adjoints and eigenproblems, series solutions, Green's functions, and separation of variables.\n\nNumerics focus on finite-difference and finite-element techniques to reduce PDEs to matrix problems, including stability and convergence analysis and implicit/explicit time-stepping.\n\nJulia programming language\n(a MATLAB(r)-like environment) is introduced and used in homework for simple examples. Julia is a high-level, high-performance dynamic language for technical computing, with syntax that is familiar to users of other technical computing environments. It provides a sophisticated compiler, distributed parallel execution, numerical accuracy, and an extensive mathematical function library.\n\nTextbook\n\nThere is no required text for this course, though the following books are recommended:\n\nStrang, Gilbert.\nComputational Science and Engineering\n. Wellesley-Cambridge Press, 2007. ISBN: 9780961408817.\n\n(emphasizing more the numerical part of the course). More information, including online chapters, can be found on\nProf. Strang's CSE website\n.\n\nOlver, Peter.\nIntroduction to Partial Differential Equations\n. Springer, 2013. ISBN: 9783319020983. [Preview with\nGoogle Books\n] (free online book)\n\nRequirements\n\nThere will be five problem sets and a mid-term exam. There is a final project instead of a final exam. Late problem sets are not accepted, however the lowest problem set score will be dropped at the end of the term.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nAssignments\n\n45%\n\nMidterm exam\n\n25%\n\nFinal project\n\n30%\n\nCalendar\n\nSES #\n\nTOPICS\n\nKEY DATES\n\nL1\n\nOverview of linear PDEs and analogies with matrix algebra\n\nL2\n\nPoisson's equation and eigenfunctions in 1d: Fourier sine series\n\nL3\n\nFinite-difference methods and accuracy\n\nL4\n\nDiscrete vs. continuous Laplacians: Symmetry and dot products\n\nOptional\n\nJulia Tutorial\n\nL5\n\nDiagonalizability of infinite-dimensional Hermitian operators\n\nProblem set 1 due\n\nL6\n\nStart with a truly discrete (finite-dimensional) system, and then derive the continuum PDE model as a limit or approximation\n\nL7\n\nStart in 1d with the \"Sturm-Liouville operator\", generalize Sturm-Liouville operators to multiple dimensions\n\nL8\n\nMusic and wave equations, Separation of variables, in time and space\n\nProblem set 2 due\n\nL9\n\nSeparation of variables in cylindrical geometries: Bessel functions\n\nL10\n\nGeneral Dirichlet and Neumann boundary conditions\n\nL11\n\nMultidimensional finite differences\n\nL12\n\nKronecker products\n\nProblem set 3 due\n\nL13\n\nThe min-max theorem\n\nL14\n\nGreen's functions with Dirichlet boundaries\n\nL15\n\nReciprocity and positivity of Green's functions\n\nL16\n\nDelta functions and distributions\n\nL17\n\nGreen's function of ∇\nin 3d for infinite space, the method of images\n\nProblem set 4 due\n\nL18\n\nThe method of images, interfaces, and surface integral equations\n\nL19\n\nGreen's functions in inhomogeneous media: Integral equations and Born approximations\n\nL20\n\nDipole sources and approximations, Overview of time-dependent problems\n\nL21\n\nTime-stepping and stability: Definitions, Lax equivalence\n\nL22\n\nVon Neumann analysis and the heat equation\n\nProblem set 5 due\n\nL23\n\nAlgebraic properties of wave equations and unitary time evolution, Conservation of energy in a stretched string\n\nL24\n\nStaggered discretizations of wave equations\n\nL25\n\nTraveling waves: D'Alembert's solution\n\nL26\n\nGroup-velocity derivation, Dispersion\n\nMidterm Exam\n\nL27\n\nMaterial dispersion and convolutions\n\nL28\n\nGeneral topic of waveguides, Superposition of modes, Evanescent modes\n\nL29\n\nWaveguide modes, Reduced eigenproblem\n\nL30\n\nGuidance, reflection, and refraction at interfaces between regions with different wave speeds\n\nL31\n\nNumerical examples of total internal reflection\n\nL32\n\nPerfectly matched layers (PML)\n\nL33\n\nPerturbation theory and Hellman-Feynman theorem\n\nL34\n\nFinite element methods: Introduction\n\nL35\n\nGalerkin discretization\n\nL36\n\nConvergence proof for the finite-element method, Boundary conditions and the finite-element method\n\nL37\n\nFinite-element software\n\nL38\n\nSymmetry and linear PDEs\n\nFinal project due",
  "files": [
    {
      "category": "Resource",
      "title": "Linear Partial Differential Equations, Problem 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/59217fe4512ea9f506b08f4e0376da44_MIT18_303F14_pset1.pdf",
      "content": "18.303 Problem Set 1\nDue Friday, 12 September 2014.\nNote: For computational (Julia-based) homework problems in 18.303, turn in with your solu\ntions a printout of any commands used and their results (please edit out extraneous/irrelevant stuff),\nand a printout of any graphs requested; alternatively, you can email your notebook (.ipynb) file\nto the grader . Always label the axes of your graphs (with the xlabel and ylabel commands),\nadd a title with the title command, and add a legend (if there are multiple curves) with the\nlegend command. (Labelling graphs is a good habit to acquire.) Because IJulia notebooks let\nyou combine code, plots, headings, and formatted text, it should be straighforward to turn in\nwell-documented solutions.\nProblem 1: 18.06 warmup\nHere are a few questions that you should be able to answer based only on 18.06:\n(a) Suppose that B is a Hermitian positive-definite matrix. Show that there is a unique matrix\n√\n√\nB which is Hermitian positive-definite and has the property (\nB)2 = B. (Hint: use the\ndiagonalization of B.)\n(b) Suppose that A and B are Hermitian matrices and that B is positive-definite.\n(i) Show that B-1A is similar (in the 18.06 sense) to a Hermitian matrix. (Hint: use your\nanswer from above.)\n(ii) What does this tell you about the eigenvalues λ of B-1A , i.e. the solutions of B-1Ax =\nλx?\n(iii) Are the eigenvectors x orthogonal?\n(iv) In Julia, make a random 5 × 5 real-symmetric matrix via A=rand(5,5); A = A+A'\nand a random 5 × 5 positive-definite matrix via B = rand(5,5); B = B'*B ...\nthen\ncheck that the eigenvalues of B-1A match your expectations from above via lambda,X\n= eigvals(B\\A) (this will give an array lambda of the eigenvalues and a matrix X whose\ncolumns are the eigenvectors).\n(v) Using your Julia result, what happens if you compute C = XT BX via C=X'*B*X? You\nshould notice that the matrix C is very special in some way. Show that the elements Cij\nof C are a kind of \"dot product\" of the eigenvectors i and j, but with a factor of B in\nthe middle of the dot product.\n√\n′′\n′\n(1+ 1+c)t\n(c) The solutions y(t) of the ODE y\n- 2y - cy = 0 are of the form y(t) = C1e\n+\n√\nC2e(1-\n1+c)t for some constants C1 and C2 determined by the initial conditions. Suppose that\nA is a real-symmetric 4×4 matrix with eigenvalues 3, 8, 15, 24 and corresponding eigenvectors\nx1, x2, . . . , x4, respectively.\nd\n(i) If x(t) solves the system of ODEs d2\nx = Ax with initial conditions x(0) = a0 and\ndt2 x-2 dt\nx ′ (0) = b0, write down the solution x(t) as a closed-form expression (no matrix inverses\nor exponentials) in terms of the eigenvectors x1, x2, . . . , x4 and a0 and b0. [Hint: expand\nx(t) in the basis of the eigenvectors with unknown coefficients c1(t), . . . , c4(t), then plug\ninto the ODE and solve for each coefficient using the fact that the eigenvectors are\n_________.]\n(ii) After a long time t ≫ 0, what do you expect the approximate form of the solution to\nbe?\n\nProblem 2: Les Poisson, les Poisson\nIn class, we considered the 1d Poisson equation\nd2\n= f(x) for the vector space of functions\ndx2 u(x)\nu(x) on x ∈ [0, L] with the \"Dirichlet\" boundary conditions u(0) = u(L) = 0, and solved it in terms\nd2\nof the eigenfunctions of dx2 (giving a Fourier sine series). Here, we will consider a couple of small\nvariations on this:\n(a) Suppose that we we change the boundary conditions to the periodic boundary condition\nu(0) = u(L).\n(i) What are the eigenfunctions of\nd2 now?\ndx2\n(ii) Will Poisson's equation have unique solutions? Why or why not?\n(iii) Under what conditions (if any) on f(x) would a solution exist? (You can restrict yourself\nto f with a convergent Fourier series.)\nd2\n(b) If we instead consider\n= g(x) for functions v(x) with the boundary conditions v(0) =\ndx2 v(x)\nv(L) + 1, do these functions form a vector space? Why or why not?\n(c) Explain how we can transform the v(x) problem of the previous part back into the original\nd2\n= f(x) problem with u(0) = u(L), by writing u(x) = v(x) + q(x) and f(x) =\ndx2 u(x)\ng(x) + r(x) for some functions q and r. (Transforming a new problem into an old, solved one\nis always a useful thing to do!)\nProblem 3: Finite-difference approximations\nFor this question, you may find it helpful to refer to the notes and reading from lecture 3. Consider\na finite-difference approximation of the form:\n-u(x + 2Δx) + c · u(x + Δx) - c · u(x - Δx) + u(x - 2Δx)\nu ′ (x) ≈\n.\nd · Δx\n(a) Substituting the Taylor series for u(x+Δx) etcetera (assuming u is a smooth function with a\nconvergent Taylor series, blah blah), show that by an appropriate choice of the constants c and\nd you can make this approximation fourth-order accurate: that is, the errors are proportional\nto (Δx)4 for small Δx.\n(b) Check your answer to the previous part by numerically computing u ′(1) for u(x) = sin(x), as\na function of Δx, exactly as in the handout from class (refer to the notebook posted in lecture\n3 for the relevant Julia commands, and adapt them as needed). Verify from your log-log plot\nof the |errors| versus Δx that you obtained the expected fourth-order accuracy.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Linear Partial Differential Equations, Solution to Problem 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/4ddac74f26070aa38bf2b99e00940db3_MIT18_303F14_pset1sol.pdf",
      "content": "18.303 Problem Set 1 Solutions\nProblem 1: (5+(2+2+2+2+2)+(10+5) points)\nNote that I don't expect you to rederive basic linear-algebra facts. You can use things derived in\n18.06, like the existence of an orthonormal diagonalization of Hermitian matrices.\n(a) Since it is Hermitian, B can be diagonalized: B = QΛQ∗, where Q is the matrix whose\ncolumns are the eigenvectors (chosen orthonormal so that Q-1 = Q∗) and Λ is the diagonal\n√\nmatrix of eigenvalues. Define Λ as the diagonal matrix of the (positive) square roots of the\neigenvalues, which is possible because the eigenvalues are > 0 (since B is positive-definite).\n√\n√\n√\n√\nThen define B = Q ΛQ∗ , and by inspection we obtain ( B)2 = B. By construction, B\nis positive-definite and Hermitian.\n√\nIt is easy to see that this B is unique, even though the eigenvectors X are not unique, because\n√\nany acceptable transformation of Q must commute with Λ and hence with Λ. Consider for\nsimplicity the case of distinct eigenvalues: in this case, we can only scale the eigenvectors by\n(nonzero) constants, corresponding to multiplying Q on the right by a diagonal (nonsingular)\nmatrix D. This gives the same B for any D, since QDΛ(QD)-1 = QΛDD-1Q-1 = QΛQ-1\n√\n(diagonal matrices commute), and for the same reason it gives the same B. For repeated\neigenvalues λ, D can have off-diagonal elements that mix eigenvectors of the same eigenvalue,\nbut D still commutes with Λ because these off-diagonal elements only appear in blocks where\nΛ is a multiple λI of the identity (which commutes with anything).\n(b) Solutions:\n(i) From 18.06, B-1A is similar to C = MB-1AM -1 for any invertible M. Let M = B1/2\nfrom above. Then C = B-1/2AB-1/2, which is clearly Hermitian since A and B-1/2 are\nHermitian. (Why is B-1/2 Hermitian? Because B1/2 is Hermitian from above, and the\ninverse of a Hermitian matrix is Hermitian.)\n(ii) From 18.06, similarity means that B-1A has the same eigenvalues as C, and since C is\nHermitian these eigenvalues are real.\n(iii) No, they are not (in general) orthogonal. The eigenvectors Q of C are (or can be chosen)\nto be orthonormal (Q∗Q = I), but the eigenvectors of B-1A are X = M -1Q = B-1/2Q,\nand hence X∗X = Q∗B-1Q\n= I.\n= I unless B\n(iv) Note that there was a typo in the pset. The eigvals function returns only the eigenval\nues; you should use the eig function instead to get both eigenvalues and eigenvectors,\nas explained in the Julia handout.\nThe array lambda that you obtain in Julia should be purely real, as expected. (You might\nnotice that the eigenvalues are in somewhat random order, e.g. I got -8.11,3.73,1.65,\n1.502,0.443. This is a side effect of how eigenvalues of non-symmetric matrices are com\nputed in standard linear-algebra libraries like LAPACK.) You can check orthogonality\nby computing X∗X via X'*X, and the result is not a diagonal matrix (or even close to\none), hence the vectors are not orthogonal.\n(v) When you compute C = X∗BX via C=X'*B*X, you should find that C is nearly diago\nnal: the off-diagonal entries are all very close to zero (around 10-15 or less). They would\nbe exactly zero except for roundoff errors (as mentioned in class, computers keep only\naround 15 significant digits). From the definition of matrix multiplication, the entry Cij\nis given by the i-th row of X∗ multiplied by B, multiplied by the j-th column of X.\n∗\nBut the j-th column X is the j-th eigenvector xj , and the i-th row of X∗ is x . Hence\ni\n∗\nCij = xi Bxj, which looks like a dot product but with B in the middle. The fact that C\n\n∗\nis diagonal means that x Bxj = 0 for i = j, which is a kind of orthogonality relation.\ni\n[In fact, if we define the inner product (x, y) = x ∗By, this is a perfectly good inner\nproduct (it satisifies all the inner-product criteria because B is positive-definite), and\nwe will see in the next pset that B-1A is actually self-adjoint under this inner product.\nHence it is no surprise that we get real eigenvalues and orthogonal eigenvectors with\nrespect to this inner product.]\n(c) Solutions:\n(i) If we write x(t) =\n(t)xn, then plugging it into the ODE and using the eigenvalue\nn=1 cn\nequation yields\n[ cn - 2 cn - λnc] xn = 0.\nn=1\nUsing the fact that the xn are necessarily orthogonal (they are eigenvectors of a Her\nmitian matrix for distinct eigenvalues), we can take the dot product of both sides with\nxm to find that c n - 2 cn - λnc = 0 for each n, and hence\n√\n√\n(1+ 1+λn)t\n(1- 1+λn)t\ncn(t) = αne\n+ βne\nfor constants αn and βn to be determined from the initial conditions. Plugging in the\ninitial conditions x(0) = a0 and x'(0) = b0, we obtain the equations:\n(αn + βn)xn = a0,\nn=1\n\n([αn + βn] +\n1 + λn[αn - βn])xn = b0.\nn=1\nAgain using orthogonality to pull out the n-th term, we find\n∗\nx a0\nn\nαn + βn =\n1xn\n\n∗\n∗\nx b0\nx (b0 - a0)\nn\nn\n[αn + βn] +\n1 + λn[αn - βn] =\n=⇒ αn - βn =\n√\n1xn\n1xn\n1 + λn\n(note that we were not given that xn were normalized to unit length, and this is not\nautomatic) and hence we can solve for αn and βn to obtain:\n\n∗\n∗\nx (b0 - a0)\n√\nx (b0 - a0)\n√\nxn\n∗\nn\n(1+ 1+λn)t\n∗\nn\n(1- 1+λn )t\nx(t) =\nx a0 +\n√\ne\n+ x a0 +\n√\ne\n.\nn\nn\n1 + λn\n1 + λn\n21xn12\nn=1\n(ii) After a long time, this expression will be dominated by the fastest growing term, which\n√\n(1+ 1+λn)t\nis the e\nterm for λ4 = 24, hence:\n\n∗\nx4(b0 - a0)\nx4\n∗\n6t\nx(t) ≈\nx4a0 +\ne\n.\n21x412\n=\n\nProblem 2: ((5+5+10)+5+5 points)\n(a) Suppose that we we change the boundary conditions to the periodic boundary condition\nu(0) = u(L).\n(i) As in class, the eigenfunctions are sines, cosines, and exponentials, and it only remains to\n2πn\napply the boundary conditions. sin(kx) is periodic if k =\nfor n = 1, 2, . . . (excluding\nL\nn = 0 because we do not allow zero eigenfunctions and excluding n < 0 because they\nare not linearly independent), and cos(kx) is periodic if n = 0, 1, 2, . . . (excluding n < 0\nsince they are the same functions). The eigenvalues are -k2 = -(2πn/L)2 .\ni 2πn\nkx\ni 2πn\nx\nL\ne\nis periodic only for imaginary k =\n, but in this case we obtain e\n=\nL\ncos(2πnx/L) + i sin(2πnx/L), which is not linearly independent of the sin and cos eigen\nfunctions above. Recall from 18.06 that the eigenvectors for a given eigenvalue form a\nvector space (the null space of A - λI), and when asked for eigenvectors we only want a\nbasis of this vector space. Alternatively, it is acceptable to start with exponentials and\ni 2πn x\nL\ncall our eigenfunctions e\nfor all integers n, in which case we wouldn't give sin and\ncos eigenfunctions separately.\nSimilarly, sin(φ + 2πnx/L) is periodic for any φ, but this is not linearly independent\nsince sin(φ + 2πnx/L) = sin φ cos(2πnx/L) + cos φ sin(2πnx/L).\n[Several of you were tempted to also allow sin(mπx/L) for odd m (not just the even\nm considered above). At first glance, this seems like it satisfies the PDE and also has\nu(0) = u(L) (= 0). Consider, for example, m = 1, i.e. sin(πx/L) solutions. This can't be\nright, however; e.g. it is not orthogonal to 1 = cos(0x), as required for self-adjoint prob\nlems. The basic problem here is that if you consider the periodic extension of sin(πx/L),\nthen it doesn't actually satisfy the PDE, because it has a slope discontinuity at the\nendpoints. Another way of thinking about it is that periodic boundary conditions arise\nbecause we have a PDE defined on a torus, e.g. diffusion around a circular tube, and in\nthis case the choice of endpoints is not unique--we can easily redefine our endpoints so\nthat x = 0 is in the \"middle\" of the domain, making it clearer that we can't have a kink\nthere. (This is one of those cases where to be completely rigorous we would need to be\na bit more careful about defining the domain of our operator.)]\n(ii) No, any solution will not be unique, because we now have a nonzero nullspace spanned\nby the constant function u(x) = 1 (which is periodic): d2 1 = 0. Equivalently, we have\ndx2\na 0 eigenvalue corresponding to cos(2πnx/L) for n = 0 above.\n(iii) As suggested, let us restrict ourselves to f(x) with a convergent Fourier series. That is,\nas in class, we are expanding f(x) in terms of the eigenfunctions:\ninf\ni 2πn x\nL\nf(x) =\ncne\n.\nn=-inf\n(You could also write out the Fourier series in terms of sines and cosines, but the complex\nexponential form is more compact so I will use it here.) Here, the coefficients cn, by the\nusual orthogonality properties of the Fourier series, or equivalently by self-adjointness of\nL - 2πn\nL\nAˆ, are cn =\ne\nxf(x)dx.\nL\nu\nIn order to solve d2\n= f, as in class we would divide each term by its eigenvalue\ndx2\n-(2πn/L)2, but we can only do this for n = 0. Hence, we can only solve the equation if\nthe n = 0 term is absent, i.e. c0 = 0. Appling the explicit formula for c0, the equation is\n=\n\nsolvable (for f with a Fourier series) if and only if:\nL\nf(x)dx = 0 .\nThere are other ways to come to the same conclusion. For example, we could expand\nu(x) in a Fourier series (i.e. in the eigenfunction basis), apply d2/dx2, and ask what is\nthe column space of d2/dx2? Again, we would find that upon taking the second derivative\nthe n = 0 (constant) term vanishes, and so the column space consist of Fourier series\nmissing a constant term.\nThe same reasoning works if you write out the Fourier series in terms of sin and cos\nsums separately, in which case you find that f must be missing the n = 0 cosine term,\ngiving the same result.\n(b) No. For example, the function 0 (which must be in any vector space) does not satisy\nthose boundary conditions. (Also adding functions doesn't work, scaling them by constants,\netcetera.)\n(c) We merely pick any twice-differentiable function q(x) with q(L) - q(0) = -1, in which case\nu(L) - u(0) = [v(L) - v(0)] + [q(L) - q(0)] = 1 - 1 = 0 and u is periodic. Then, plugging\nv = u - q into d2\ndx2 v(x) = f(x), we obtain\nd2\nd2q\nu(x) = f(x) +\n,\ndx2\ndx2\nwhich is the (periodic-u) Poisson equation for u with a (possibly) modified right-hand side.\nFor example, the simplest such q is probably q(x) = x/L, in which case d2q/dx2 = 0 and u\nsolves the Poisson equation with an unmodified right-hand side.\nProblem 3: (10+10 points)\nWe are using a difference approximation of the form:\n-u(x + 2Δx) + c · u(x + Δx) - c · u(x - Δx) + u(x - 2Δx)\nu ' (x) ≈\n.\nd · Δx\n(a) First, we Taylor expand:\ninf\n4 u(n)(x)\nn\nu(x + Δx) =\nΔx .\nn!\nn=0\nThe numerator of the difference formula flips sign if Δx →-Δx, which means that when you\nplug in the Taylor series all of the even powers of Δx must cancel! To get 4th-order accuracy,\nthe Δx3 term in the numerator (which would give an error ∼ Δx2) must cancel as well, and\nthis determines our choice of c: the Δx3 term in the numerator is\n''' (x)\nu\n\nΔx -23 + c + c - 23 ,\n3!\nand hence we must have c = 23 = 8 . The remaining terms in the numerator are the Δx term\nand the Δx5 term:\n(5)(x)\nu\n\n2 (5)(x)Δx 5\nu ' (x)Δx [-2 + c + c - 2] +\nΔx -25 + c + c - 25 = 12u ' (x)Δx - u\n+ · · · .\n5!\nClearly, to get the correct u ' (x) as Δx → 0, we must have\n(5)(x)Δx\napproximately - 1 u\n4, which is ∼ Δx4 as desired.\nd = 12 . Hence, the error is\n\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n-15\n-14\n-13\n-12\n-11\n-10\n-9\n-8\n-7\n-6\n-5\n∆x\nerror in d(sin)/dx at x=1\n\nerror in du/dx\npredicted ∼ ∆x4\nFigure 1: Actual vs. predicted error for problem 1(b), using fourth-order difference approximation\nfor u ' (x) with u(x) = sin(x), at x = 1.\n(b) The Julia code is the same as in the handout, except now we compute our difference approxi\nmation by the command: d = (-sin(x+2*dx) + 8*sin(x+dx) - 8*sin(x+dx) + sin(x-2*dx))\n./ (12 * dx); the result is plotted in Fig. 1. Note that the error falls as a straight line (a\npower law), until it reaches ∼ 10-15, when it starts becoming dominated by roundoff errors\n(and actually gets worse). To verify the order of accuracy, it would be sufficient to check the\nslope of the straight-line region, but it is more fun to plot the actual predicted error from\nthe previous part, where d5 sin(x) = - cos(x). Clearly the predicted error is almost exactly\ndx5\nright (until roundoff errors take over).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Linear Partial Differential Equations, Problem 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/5f8e4bdfb93d1f0f021409582c216df4_MIT18_303F14_pset2.pdf",
      "content": "18.303 Problem Set 2\nDue Monday, 22 September 2014.\nProblem 2: Modified inner products for column vectors\nConsider the inner product (x, y) = x ∗By from class (lecture 5.5 notes), where the vectors are in\nCN and B is an N × N Hermitian positive-definite matrix.\n(a) Show that this inner product satisfies the required properties of inner products from class:\n(x, y) = (y, x), (x, x) > 0 except for x = 0. (Linearity (x, αy +z) = α(x, y)+(x, z) is obvious\nfrom linearity the of matrix operations; you need not show it.)\n(b) If M is an arbitrary (possibly complex) N × N matrix, define the adjoint M + by (x, My) =\n(M + x, y) (for all x, y). (In this problem, we use + instead of ∗ for the adjoint in order to\navoid confusion with the conjugate transpose: for this inner product, the adjoint M + is not\nthe conjugate transpose M ∗ = M T .) Give an explicit formula for M + in terms of M and B .\n(c) Using your formula from above, show that M + = M (i.e., M is self-adjoint/Hermitian for this\ninner product) if M = B-1A for some A = A∗ .\nProblem 2: Finite-difference approximations\nFor this question you may find it helpful to refer to the notes and readings from lecture 3. Suppose\nthat we want to compute the operation\nd\ndu\nˆAu =\nc\ndx\ndx\nfor some smooth function c(x) (you can assume c has a convergent Taylor series everywhere). Now,\nwe want to construct a finite-difference approximation for Aˆ with u(x) on Ω = [0, L] and Dirichlet\nboundary conditions u(0) = u(L) = 0, similar to class, approximating u(mΔx) ≈ um for M equally\nL\nspaced points m = 1, 2, . . ., M, u0 = uM+1 = 0, and Δx =\n.\nM+1\n(a) Using center-difference operations, construct a finite-difference approximation for ˆAu evalu\nated at mΔx. (Hint: use a centered first-derivative evaluated at grid points m + 0.5, as in\nclass, followed by multiplication by c, followed by another centered first derivative. Do not\n′\n′\n′′\nseparate Au\nˆ\nby the product rule into c u + cu\nfirst, as that will make the factorization in\npart (d) more difficult.)\n(b) Show that your finite-difference expressions correspond to approximating ˆAu by Au where\nu is the column vector of the M points um and A is a real-symmetric matrix of the form\nA = -DTCD (give C, and show that D is the same as the 1st-derivative matrix from lecture).\n(c) In Julia, the diagm(c) command will create a diagonal matrix from a vector c. The function\ndiff1(M) = [ [1.0 zeros(1,M-1)]; diagm(ones(M-1),1) - eye(M) ]\nwill allow you to create the (M +1)×M matrix D from class via D = diff1(M) for any given\nvalue of M. Using these two commands, construct the matrix A from part (d) for M = 100\nand L = 1 and c(x) = e3x via\nL = 1\nM = 100\nD = diff1(M)\ndx = L / (M+1)\nx = dx*0.5:dx:L # sequence of x values from 0.5*dx to <= L in steps of dx\nC = ....something from c(x)...hint: use diagm...\n\nA = -D' * C * D / dx^2\nYou can now get the eigenvalues and eigenvectors by λ, U = eig(A), where λ is an array of\neigenvalues and U is a matrix whose columns are the corresponding eigenvectors (notice that\nall the λ are < 0 since A is negative-definite).\n(i) Plot the eigenvectors for the smallest-magnitude four eigenvalues. Since the eigenvalues\nare negative and are sorted in increasing order, these are the last four columns of U.\nYou can plot them with:\nusing PyPlot\nplot(dx:dx:L-dx, U[:,end-3:end])\nxlabel(\"x\"); ylabel(\"eigenfunctions\")\nlegend([\"fourth\", \"third\", \"second\", \"first\"])\n(ii) Verify that the first two eigenfunctions are indeed orthogonal with dot(U[:,end],\nU[:,end-1]) in Julia, which should be zero up to roundoff errors ; 10-15 .\n(iii) Verify that you are getting second-order convergence of the eigenvalues: compute the\nsmallest-magnitude eigenvalue λM[end] for M = 100, 200, 400, 800 and check that the\ndifferences are decreasing by roughly a factor of 4 (i.e. |λ100 -λ200| should be about 4\ntimes larger than |λ200 -λ400|, and so on), since doubling the resolution should multiply\nerrors by 1/4.\n(d) For c(x) = 1, we saw in class that the eigenfunctions are sin(nπx/L). How do these compare\nto the eigenvectors you plotted in the previous part? Try changing c(x) to some other function\n(note: still needs to be real and > 0), and see how different you can make the eigenfunctions\nfrom sin(nπx/L). Is there some feature that always remains similar, no matter how much\nyou change c?\nProblem 3: Discrete diffusion\nIn this problem, you will examine thermal conduction in a system of a finite number N of pieces,\nand then take the N →inf limit to recover the heat equation. In particular:\n- You have a metal bar of length L and cross-sectional area a (hence a volume La), with a\nvarying temperature T along the rod. We conceptually subdivide the rod into N (touching)\npieces of length Δx = L/N.\n- If Δx is small, we can approximate each piece as having a uniform temperature Tn within the\npiece (n = 1, 2, . . ., N), giving a vector T of N temperatures.\n- Suppose that the rate q (in units of W) at which heat flows across the boundary from piece\nκa\nn to piece n + 1 is given by q =\n(Tn -Tn+1), where κ is the metal's thermal conductivity\nΔx\n(in units of W/m·K). That is, piece n loses energy at a rate q, and piece n + 1 gains energy\nat the same rate, and the heat flows faster across bigger areas, over shorter distances, or for\nlarger temperature differences. Note that q > 0 if Tn > Tn+1 and q < 0 if Tn < Tn+1: heat\nflows from the hotter piece to the cooler piece.\n- If an amount of heat ΔQ (in J) flows into a piece, its temperature changes by ΔT =\nΔQ/(cρaΔx), where c is the specific heat capacity (in J/kg·K) and ρ is the density (kg/m3)\nof the metal.\n- The rod is insulated: no heat flows out the sides or through the ends.\nGiven these assumptions, you should be able to answer the following:\n\n(a) \"Newton's law of cooling\" says that that the temperature of an object changes at a rate (K/s)\nproportional to the temperature difference with its surroundings. Derive the equivalent here:\nn\nshow that our assumptions above imply that dT\n= α(Tn+1 -Tn) + α(Tn-1 -Tn) for some\ndt\nconstant α, for 1 < n < N. Also give the (slightly different) equations for n = 1 and n = N.\n(b) Write your equation from the previous part in matrix form: dT = AT for some matrix A.\ndt\n(c) Let T (x, t) be the temperature along the rod, and suppose Tn(t) = T ([n - 0.5]Δx, t) (the\ntemperature at the center of the n-th piece). Take the limit N →inf (with L fixed, so that\n∂T\nˆ\nΔx = L/N → 0), and derive a partial differential equation\n= AT . What is Aˆ? (Don't\n∂t\nworry about the x = 0, L ends until the next part.)\n(d) What are the boundary conditions on T (x, t) at x = 0 and L? Check that if you go backwards,\nand form a center-difference approximation of Aˆ with these boundary conditions, that you\nrecover the matrix A from above.\n(e) How does your Aˆ change in the N →inf limit if the conductivity is a function κ(x) of x?\n(f) Suppose that instead of a thin metal bar (1d), you have an L × L thin metal plate (2d), with\na temperature T (x, y, t) and a constant conductivity κ. If you go through the steps above\ndividing it into N × N little squares of size Δx × Δy, what PDE do you get for T in the limit\nN →inf? (Many of the steps should be similar to above.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Linear Partial Differential Equations, Solution to Problem 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/9e71970eef48268d82c1a0b29999f296_MIT18_303F14_pset2sol.pdf",
      "content": "18.303 Problem Set 2 Solutions\nProblem 1 (5+5+5 points)\n(a) We have (x, x) = x ∗Bx > 0 for x = 0\n\nby definition of positive-definiteness. We have (x, y) =\n∗\n∗\nx ∗By = (B∗ x) y = (Bx) y = y ∗(Bx) = (y, x) by B = B∗ .\n(b) (x, My) = x ∗BMy = (M +x, y) = x ∗M +∗By for all x, y, and hence we must have BM =\n∗\nM +∗B, or M +∗ = BMB-1 =⇒ M + = (BMB-1)\n= (B-1)∗M ∗B∗ . Using the fact that\n∗\nM + = B-1M ∗ B .\nB∗ = B (and hence (B-1) = B-1), we have\n(c) If M = B-1A where A = A∗, then M + = B-1AB-1B = B-1A = M. Q.E.D.\nProblem 2: (5+5+(3+3+3)+5 points)\n'\num+1-um\n(a) As in class, let u'([m + 0.5]Δx) ≈ u\n=\n. Define cm+0.5 = c([m + 0.5]Δx). Now\nm+0.5\nΔx\n'\nˆ\nwe want to take the derivative of cm+0.5u\nin order to approximate Au at m by a center\nm+0.5\ndifference:\n\num+1-um\num -um-1\ncm+0.5\n- cm-0.5\nΔx\nΔx\n.\nΔx\nˆAu\n\n≈\nmΔx\nThere are other ways to solve this problem of course, that are also second-order accurate.\nˆ\n(b) In order to approximate Au, we did three things: compute u' by a center-difference as in\nclass, multiply by cm+0.5 at each point m + 0.5, then compute the derivative by another\ncenter-difference. The first and last steps are exactly the same center-difference steps as in\nclass, so they correspond as in class to multiplying by D and -DT , respectively, where D is\nthe (M + 1) × M matrix\n⎞\n⎛ 1\n-1\n-1\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\n.\nD =\n.\n.\nΔ\n.\n.\nx\n.\n.\n-1\n-1\nThe middle step, multiplying the (M +1)-component vector u' by cm+0.5 at each point is just\nmultiplication by a diagonal (M + 1) × (M + 1) matrix\n⎞\n⎛ c0.5\nc1.5\nC =\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎠ .\n...\ncM +0.5\nPutting these steps together in sequence, from right to left, means that A = -DT CD\n(c) In Julia, the diagm(c) command will create a diagonal matrix from a vector c. The function\ndiff1(M) = [ [1.0 zeros(1,M-1)]; diagm(ones(M-1),1) - eye(M) ]\nwill allow you to create the (M + 1) × M matrix D from class via D = diff1(M) for any\ngiven value of M. Using these two commands, we construct the matrix A from part (d) for\n3x\nM = 100 and L = 1 and c(x) = e\nvia\nL = 1\nM = 100\nD = diff1(M)\n\n0.20\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\n0.20\n\nd\n3x\nFigure 1: Smallest-|λ| eigenfunctions of Aˆ =\nc(x) d\nfor c(x) = e .\ndx\ndx\ndx = L / (M+1)\nx = dx*0.5:dx:L # sequence of x values from 0.5*dx to <= L in steps of dx\nc(x) = exp(3x)\nC = diagm(c(x))\nA = -D' * C * D / dx^2\nYou can now get the eigenvalues and eigenvectors by λ, U = eig(A), where λ is an array of\neigenvalues and U is a matrix whose columns are the corresponding eigenvectors (notice that\nall the λ are < 0 since A is negative-definite).\n(i) The plot is shown in Figure 1. The eigenfunctions look vaguely \"sine-like\"--they have\nthe same number of oscillations as sin(nπx/L) for n = 1, 2, 3, 4--but are \"squeezed\" to\nthe left-hand side.\n(ii) We find that the dot product is ≈ 4.3 × 10-16, which is zero up to roundoff errors (your\nexact value may differ, but should be of the same order of magnitude).\n(iii) In the posted IJulia notebook for the solutions, we show a plot of |λ2M -λM | as a function\nof M on a log-log scale, and verify that it indeed decreases ∼ 1/M 2 . You can also just\nlook at the numbers instead of plotting, and we find that this difference decreases by a\nfactor of ≈ 3.95 from M = 100 to M = 200 and by a factor of ≈ 3.98 from M = 200 to\nM = 400, almost exactly the expected factor of 4. (For fun, in the solutions I went to\nM = 1600, but you only needed to go to M = 800.)\n(d) In general, the eigenfunctions have the same number of nodes (sign oscillations) as sin(nπx/L),\nbut the oscillations pushed towards the region of high c(x). This is even more dramatic if we\nincrease the c(x) contrast. In Figure xxx, we show two examples. First, c(x) = e20x, in which\nall of the functions are squished to the left where c is small. Second c(x) = 1 for x < 0.3\nand 100 otherwise--in this case, the oscillations are at the left 1/3 where c is small, but the\nfunction is not zero in the right 2/3. Instead, the function is nearly constant where c is large.\nThe reason for this has to do with the continuity of u: it is easy to see from the operator that\n'\n' ) '\n'\ncu must be continuous for (cu\nto exist, and hence the slope u must decrease by a factor of\neigenfunctions\nfourth\nthird\nsecond\nfirst\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx\n\nˆ\n' ) '\nFigure 2: First four eigenfunctions of Au = (cu\nfor two different choices of c(x).\n100 for x > 0.3, leading to a u that is nearly constant. (We will explore some of these issues\nfurther later in the semester.)\nProblem 3: (5+5+5+5+5+5 points)\ndQn\n(a) The heat capacity equation tells us that dTn =\n, where dQn/dt is the rate of change\ndt\ncρaΔx\ndt\n\nof the heat in the n-th piece. The thermal conductivity equation tells us that dQn/dt, in\nturn, is equal to the sum of the rates q at which heat flows from n + 1 and n - 1 into n:\ndTn\ndQn\nκa\n=\n=\n[(Tn+1 - Tn) + (Tn-1 - Tn)] = α(Tn+1 -Tn)+α(Tn-1 -Tn)\ndt\ncρaΔx dt\ncρaΔx Δx\nκ\nwhere α =\n. The only difference for T1 and TN is that they have no heat flow\ncρ(Δx)\ndT1\ndTN\nn - 1 and n + 1, respectively, since the ends are insulated:\n= α(T2 - T1) and\n=\ndt\ndt\nα(TN -1 - TN ).\n(b) We can obtain A in two ways. First, we can simply look directly at our equations above,\ndTn\nwhich give\ndt = α(Tn+1 - 2Tn + Tn-1) for every n except T1 and TN , and read off the\n\n__\n__\ncorresponding rows of the matrix\n-1\n-2\nA = α\n..\n..\n..\n.\n.\n.\n.\n-2\n-1\n⎞\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\n⎛\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\nAlternatively, we can write each of the above steps--differentiating T to get the rate of heat\nflow q to the left at each of the N - 1 interfaces between the pieces, then taking the difference\nof the q's to get dT/dt, in matrix form, to write:\n⎞\n⎛ 1\n⎞\n⎛ -1\n-1\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\n-1\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\n-1\n\nκ DDT\n.\n.\n= -\nA =\nκa\n.\n.\n,\n.\n.\n.\n.\ncρa Δ\nΔ\ncρ\n.\n.\nx\nx\n.\n.\n-1\n-1\n-1\n\n-1\n_\n\n_\n-DT : (N-1)×N\nD: N×(N-1)\nin terms of the D matrix from class (except with N reduced by 1), which gives the same A\nas above. As we will see in the parts below, this is indeed a second-derivative approximation,\nbut with different boundary conditions--Neumann conditions--than the Dirichlet conditions\nin class.\nBy the way, it is interesting to consider -DDT , compared to the -DT D we had in class.\nClearly, -DDT is real-symmetric and negative semidefinite. It is not, however, negative def\ninite, since DT does not (and cannot) have full column rank (its rank must be ≤ the number\nof rows N - 1, and in fact in class we showed that it has rank N - 1).\nκ Tn+1-2Tn+Tn-1\n(c) Ignoring the ends for the moment, for all the interior points we have dTn =\n,\nκ ∂2T\ndt\ncρ\nΔx\nwhich is exactly our familiar center-difference approximation for\nat the point n (x =\ncρ ∂x2\n∂T\nκ\nT\n[n - 0.5]Δx). Hence, everywhere in the interior our equations converge to\n=\n∂2 , and\n∂T\ncρ ∂x2\nthus\nκ ∂2\nˆA =\n.\ncρ ∂x2\n(d) The boundary conditions are ∂T = 0\n∂x\nat x = 0, L. The easiest way to see this is to\nobserve that our heat flow q is really a first derivative, and zero heat flow at the ends\nTn+1-Tn\nmeans zero derivatives. That is, qn+0.5 = κa\nis really an approximate derivative:\nΔx\nqn+0.5 ≈ κa ∂T\n= κa ∂T\n, while the flows q0.5 and qN+0.5 to/from n = 0 and\n∂x n+0.5\n∂x nΔx\nn = N + 1 is zero, and hence q0.5 = qN+0.5 = 0 ≈ κa ∂T\n.\n∂x 0,L\nˆ\nT\n''\nκ\nWorking backwards, consider AT = ∂2\n= T\n(setting\n= 1 for convenience) with these\n∂x2\ncρ\nboundary conditions and center-difference approximations.\nWe are given Tn = T([n -\n∂T\n'\nTn+1-Tn\n0.5]Δx, t) for n = 1, . . . , N. First, we compute\n≈ T\n=\nfor n =\n∂x nΔx\nn+0.5\nΔx\n1, . . . , N - 1 (-DT T using the D above). Unlike the Dirichlet case in class, we don't com\n'\n'\npute T\nand T\n, since these correspond to ∂T/∂x at x = 0, L, which are zero by the\n0.5\nN+0.5\nT ′\n-T ′\n''\nn+0.5\nn-0.5\nboundary conditions. Then, we compute our approximate 2nd derivatives T =\nn\nΔx\n\n'\n'\nfor n = 1, . . . , N, where we let T\n= T\n= 0 (DT ' using the D from above). This\n0.5\nN+0.5\n''\nT1.5 -0\nT2-T1\n''\n0-TN -0.5\n-TN +TN -1\n''\ngives T\n=\n=\n, T\n=\n=\nat the endpoints, and T\n=\nΔx\nΔx2\nN\nΔx\nΔx2\nn\n(Tn+1-Tn)-(Tn-Tn-1)\nTn+1 -2Tn+Tn-1\n=\nfor 1 < n < N, which are precisely the rows of our A\nΔx\nΔx\nmatrix above.\n(e) If κ(x), then we get a different κ and α factor for each Tn+1 -Tn difference:\ndTn = αn+1/2(Tn+1 -Tn) + αn-1/2(Tn-1 -Tn),\ndt\nκn+1/2\nwhere αn+1/2 =\nand κn+1/2 = κ([n + 1/2]Δx). In the N →inf limit, this gives\ncρ(Δx)2\n1 ∂\n∂\nAˆ =\nκ\n: we differentiated, multiplied by κ, differentiated again, and then divided\ncρ ∂x ∂x\nby cρ. (You weren't asked to handle the case where cρ is not a constant, so it's okay if you\ncommuted cρ with the derivatives.)\n(f) If we discretize to Tm,n = T (mΔx, nΔy), the steps are basically the same except that we have\nto consider the heat flow in both the x and y directions, and hence we have to take differences\nin both x and y. In particular, suppose the thickness of the block is h. In this case, heat\nκhΔy\nwill flow from Tm,n to Tm+1,n at a rate\nΔx (Tm,n -Tm+1,n) where hΔy is the area of the\ninterface between the two blocks. Then, to convert into a rate of temperature change, we will\ndivide by cρhΔxΔy, where hΔxΔy is the volume of the block. Putting this all together, we\nobtain:\n\ndTm,n\nκ\nTm+1,n -2Tm,n + Tm-1,n\nTm,n+1 -2Tm,n + Tm,n+1\n=\n+\n,\ndt\ncρ\nΔx2\nΔy2\nwhere the thing in [· · · ] is precisely the five-point stencil approximation for ∇2 from class.\nHence, we obtain\nˆA =\n∇· κ∇,\ncρ\nwhere for fun I have put the κ in the middle, which is the right place if κ is not a constant\n(you were not required to do this).\n′\n′\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Linear Partial Differential Equations, Problem 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/aea96a46126e9a1a7e7c2669265707fd_MIT18_303F14_pset3.pdf",
      "content": "18.303 Problem Set 3\nDue Wednesday, 1 October 2014.\nProblem 1:\nConsider the space of three-component vector fields u(x) on some finite-volume 3d domain Ω ⊂ R3 .\nOne linear operator on these fields is the curl ∇×, which is important in electromagnetism (which\nwe will study in more detail later in 18.303). Define the inner product of two vector fields u and v\n\nby the volume integral (u, v) =\nu · v.\nΩ\n(a) An 18.02 exercise: derive the identity ∇· (u × v) = (∇× u) · v -u · (∇× v).\n(b) Figure out how to do integration by parts with the curl: show that (u, ∇× v) = (∇× u, v) +\n‚\nw · dS, where dS is the usual outward surface-normal area element, and the w appearing\n∂Ω\nin the surface integral over the boundary (∂Ω) is some vector field to be determined. (Hint:\nuse the identity you derived in the previous part, combined with the divergence theorem.)\n(c) Give a possible boundary condition on our space of vector fields such that ∇× is self-adjoint\nwith this inner product. (Boundary conditions can only involve one vector field at a time! No\nfair giving an equation that relates u to v!) You should not have to specify all the components\nof u on the boundary. It may be convenient to define a vector field n(x) on ∂Ω to denote the\noutward normal vector at each point on the boundary.1\n(d) Show that ∇×∇× is self-adjoint for this inner product under either some boundary condition\non u (similar to above) or some boundary condition on derivatives of u. Is it positive or\nnegative definite or semidefinite?\n-∂B\n1 ∂E\n(e) Two of Maxwell's equations in vacuum are ∇× E =\nand ∇× B =\nwhere c is the\n∂t\nc2 ∂t\nspeed of light. Take the curl of both sides of the second equation to obtain a PDE in E alone.\nSuppose that Ω is the interior of a hollow metal container, where the boundary conditions are\nthat E is perpendicular to the metal at the surface (i.e. E × n|\n= 0).2 Combining these\n∂Ω\nfacts with the previous parts, explain why you would expect to obtain oscillating solutions to\nMaxwell's equations (standing electromagnetic waves, essentially light bouncing around inside\nthe container).\n(This kind of system exists, for example, in the microwave regime where metals have high\nconductivity, and such containers are called microwave resonant cavities.)\nProblem 2:\nIn class, we solved for the eigenfunctions of ∇2 in two dimensions, in a cylindrical region r ∈\n[0, R], θ ∈ [0, 2π] using separation of variables, and obtained Bessel's equation and Bessel-function\nsolutions. Although Bessel's equation has two solutions Jm(kr) and Ym(kr) (the Bessel functions),\nthe second solution (Ym) blows up as r → 0 and so for that problem we could only have Jm(kr)\nsolutions (although we still needed to solve a transcendental equation to obtain k).\nIn this problem, you will solve for the 2d eigenfunctions of ∇2 in an annular region Ω that\ndoes not contain the origin, as depicted schematically in Fig. 1, between radii R1 and R2, so that\n1Technically, we are assuming here that the boundary is a differentiable surface so that the normal vector is well\ndefined. Actually, it is sufficient to assume that it is differentiable except for isolated corners and edges (a \"set of\n\"\nmeasure zero\") since those isolated kinks won't contribute anything to the\nsurface integral.\n∂Ω\n2In a perfect conductor, any nonzero component of E parallel to the surface would generate an infinite current\nparallel to the surface, in which charges instantaneously rearrang to cancel the field. For a real conductor with finite\nconductivity, matters are more complicated because we must consider what the fields do inside the conductor, but\na perfect conductor is a good approximation at microwave and lower frequencies where the penetration of the field\ninto the conductor (the skin depth) is much smaller than the wavelength of light.\n\nR1\nR2\nE\ndomain N\nFigure 1: Schematic of the domain Ω for problem 3: an annular region in two dimensions, with\nradii r ∈ [R1, R2] and angles θ ∈ [0, 2π].\nyou will need both the Jm and Ym solutions. Exactly as in class, the separation of variables ansatz\nu(r, θ) = ρ(r)τ(θ) leads to functions τ(θ) spanned by sin(mθ) and cos(mθ) for integers m, and\nfunctions ρ(r) that satisfy Bessel's equation. Thus, the eigenfunctions are of the form:\nu(r, θ) = [αJm(kr) + βYm(kr)] × [A cos(mθ) + B sin(mθ)]\nfor arbitrary constants A and B, for integers m = 0, 1, 2, . . ., and for constants α, β, and k to be\ndetermined.\nFor fun, we will also change the boundary conditions somewhat. We will impose \"Neumann\"\nboundary condition ∂u = 0 at R1 and R2. That is, for a function u(r, θ) in cylindrical coordinates,\n∂r\n∂u |r=R1 = 0 and ∂u |r=R2 = 0. The following exact identities for the derivatives of the Bessel\n∂r\n∂r\nfunctions will be helpful:\n′\nJm-1(x) -Jm+1(x)\n′\nYm-1(x) -Ym+1(x)\nJ (x) =\n,\nY (x) =\nm\nm\n(a) Using the boundary conditions, write down two equations for α, β and k, of the form\n(\n)\nα\nE\n= 0 for some 2 × 2 matrix E.\nThis only has a solution when det E = 0, and\nβ\nfrom this fact obtain a single equation for k of the form fm(k) = 0 for some function fm that\ndepends on m. This is a transcendental equation; you can't solve it by hand for k. In terms\nof k (which is still unknown), write down a possible expression for α and β, i.e. a basis for\nN(E).\n(b) Assuming R1 = 1, R2 = 2, plot your function fm(k) versus k ∈ [0, 20] for m = 0, 1, 2.\nNote that Julia provides the Bessel functions built-in: Jm(x) is besselj(m,x) and Ym(x) is\nbessely(m,x). You can plot a function with the plot command. See the IJulia notebook\nposted on the course web page for lecture 8 for some examples of plotting and finding roots\nin Julia.\n(c) For m = 0, find the first three (smallest k > 0) solutions k1, k2, and k3 to f0(k) = 0. Get a\nrough estimate first from your graph (zooming if necessary), and then get an accurate answer\n\nby calling the scipy.optimize.newton function as in pset 1, and also as illustrated in the lecture\n8 IJulia notebook. (Note that there is also a k = 0 eigenfunction for m = 0, corresponding to\nthe constant function: the nullspace of Aˆ with Neumann boundary conditions, as in class.)\n\n(d) Because ∇2 is self-adjoint under (u, v) =\n\nΩ, that this is\nuv (we showed in class, in general\nΩ\nstill true with these boundary conditions), we know that the eigenfunctions must be orthogo\nnal. From class, this implies that the radial parts must also be orthogonal when integrated via\n\nr dr. Check that your Bessel solutions for k1 and k2 are indeed orthogonal, by numerically\nintegrating their product via the quadgk function in Julia as in pset 2 and as in the lecture-8\nIJulia notebook.\n(e) Let's change the problem. Suppose that the domain is now 0 ≤ r ≤ R2, and the operator is\nAˆ = c(r)∇2 with c(r) = 2 for r < R1 and c(r) = 1 for r ≥ R1. Suppose we impose Dirichlet\nboundary conditions u(R2) = 0.\n(i) What is the form of the eigenfunctions? (Define them in terms of Jm(kr) and Ym(kr)\nwith unknown coefficients in the r < R1 and r ≥ R1 regions; don't try to solve for the\ncoefficients.)\n(ii) If we solve for eigenfunctions ˆ\n=\nAu\nλu, and u is everywhere finite, then what continuity\nconditions must u satisfy at r = R1 in order for ˆAu to be well defined and finite? If\nyou combine these continuity conditions with the boundary condition at R2, you should\nfind that the number of equations that u must satisfy matches the number of unknown\ncoefficients in the previous part.\n(iii) As before, write down a condition fm(k) = 0 that must be satisfied in order for the above\nequations to have a solution. The roots of this function then give the eigenvalues. (You\nwould have to solve it numerically as above, but you need not do this here; just write\ndown fm, which you can leave in the form of a determinant.)\nProblem 3:\nThe Bessel functions u(x) = Jm(kx), from class, solve the eigenproblem:\n′\nu\nm\nˆ\n′′\nAu = u +\n-\nu = -k2 u = λu\nr\nr2\non [0, R] where u(R) = 0 and u(0) = 0 for m > 0.\n(a) Show that this operator Aˆ is of the form of a Sturm-Louville operator (from the class notes),\nand is therefore self-adjoint for an appropriate inner product (which?). (Hint: rewrite the\nˆ\nfirst two terms of A as a single term.)\n(b) Show that Aˆ is negative definite, hence λ < 0 and we are entitled to write λ = -k2 for a real\nk.\n(c) Write down a center-difference discretization of this operator Aˆ for un = u(nΔx) with n =\n1, . . . , R Δx = R/(N +1). Be careful of where you evaluate the 1/r factors (both to maintain\nsecond-order accuracy, and to avoid dividing by zero.)\nˆ\n(d) In Julia, form the matrix approximation A of A for m = 1 (with N = 100 and R = 1)\nusing code similar to problem 2 from pset 2. Compare its smallest-magnitude eigenfunction\nto J1(k1,1r/R) (where k1,1 is the first root of J1), evaluated with the help of the Julia code\nposted in Lecture 9. They should be the same up to some overall scale factor, within the\ndiscretization accuracy.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Linear Partial Differential Equations, Solution to Problem 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/b45c1232530dd3ab522dea9e13c31fc5_MIT18_303F14_pset3sol.pdf",
      "content": "18.303 Problem Set 3 Solutions\nProblem 2: (5+5+5+10+5)\n(a) Writing out the derivation in 18.02 fashion, this is tedious but straightforward:\n⎛\n⎞\n\nuyvz - uzvy\n∂\n∂\n∂\n⎝\n⎠\n\\ · (u × v) =\nuz vx - uxvz\n∂x\n∂y\n∂z\nuxvy - uyvx\n∂(uyvz - uz vy)\n∂(uz vx - uxvz )\n∂(uxvy - uyvx)\n=\n+\n+\n∂x\n∂y\n∂z\n∂uz\n∂uy\n∂ux\n∂uz\n∂uy\n∂ux\n=\n-\nvx +\n-\nvy +\n-\nvz\n∂y\n∂z\n∂z\n∂x\n∂x\n∂y\n∂vy\n∂vz\n∂vz\n∂vx\n∂vx\n∂vy\nux\n-\n+ uy\n-\n+ uz\n-\n∂z\n∂y\n∂x\n∂z\n∂y\n∂x\n= (\\ × u) · v - u · (\\ × v).\n(A much more compact derivation is possible using Einstein notation and the Levi-Civita\ntensor, but probably most of you haven't seen this notation.)\n(b) Given the above identity, integration by parts is straightforwards:\nˆ\nˆ\n\n(u, \\ × v) =\nu · (\\ × v) =\n\\ · (u × v) + \\ × u · v\nΩ\nΩ\n‹\n=\n(u × v) · dS + (\\ × u, v),\n∂Ω\n‚\napplying the divergence theorem in the second line. So, the surface term\n∂Ω w · dS is for\nw = u × v .\n(c) We must have (u × v) · dS = 0. Let dS = n dS, where n is the outward unit normal vector\nat each point on ∂Ω. Then we must have u × v ⊥ n, which is true if, for example, both u\nand v are parallel to n at the boundary. i.e. if u × n|\n= 0 . It is not necessary to require\n∂Ω\nu|\n= 0 on the boundary, and that answer will not be accepted as I specifically requested\n∂Ω\nthat you not constrain all the components of u on the boundary.\nAnother way to see this is to write (u ×v) ·dS = n ·(u ×v)dS = v ·(n ×u )dS = u ·(v ×n)dS\nby elementary triple-product identities, and hence we again see that it is sufficient to have\nu × n = 0 on the boundary.\nAlthough I will accept the above answer, it is actually possible to contrive a slightly weaker\ncondition: u and v can have components ⊥ to n on the boundary, as long as those surface-\nparallel components are in the same direction for both u and v (to obtain zero cross product).\nThat is, suppose p(x) is some surface-parallel (p ⊥ n) vector field on ∂Ω. Then it is suf\nficient for the surface-parallel component of u to be ⊥ to p everywhere on the boundary, or\nequivalently u · p|\n= 0.\n∂Ω\n[Actually, there are other possible conditions on u if we allow non-local boundary condi\ntions, where u at one point on the boundary is related to u at another point. For example,\nif Ω is a cube and u is periodic (i.e. u on one face equals u on the opposite face), then the\n1By the \"hairy ball theorem\" of topology, p must vanish somewhere on ∂Ω if Ω is simply connected (no holes), at\nwhich point u must be parallel to n.\n\n‚\n∂Ω vanishes because each face of the cube cancels the opposite face, without requiring any\ncomponent of u to be zero.]\n(d) We just \"integrate by parts\" twice:\nˆ\n‹\nˆ\n\n(u, \\ × \\ × v) =\nu · (\\ × \\ × v) =\n[u × (\\ × v)] · dS +\n(\\ × u ) · (\\ × v)\n\nΩ\n∂Ω\nΩ\n‹\nˆ\n\n=\n[(\\ × u ) × v] · dS +\n(\\ × \\ × u ) · v = (\\ × \\ × u, v),\n\n∂Ω\nΩ\nwhere the surface terms cancel if either u × n|\n= 0 or (\\ × u) × n|\n= 0 , that is if\n∂Ω\n∂Ω\neither u or its curl are normal to the surface. (As in the previous part, one can actually\nweaken this slightly, but this is sufficient for our purposes.)\nTo check definiteness, carry integration by parts \"halfway\" through:\nˆ\n(u, \\ × \\ × u) =\n|\\ × u|2 ≥ 0,\nΩ\nso it is positive semidefinite. It is not positive-definite since \\ × u = 0 for u = \\φ\n=\nfor any non-constant scalar field φ , and we can easily choose such a φ such that \\φ satisfies\nthe boundary conditions (e.g., choose φ so that it is constant in a neighborhood of ∂Ω).\n(e) Taking the curl of both sides of \\×E = - ∂B , we obtain \\×\\×E = -\\× ∂B = - ∂ \\×B =\n∂t\n∂t\n∂t\n- ∂2 E\n∂t2 . That is, we have\n∂2E\nˆ\n= AE\n∂t2\nwhere Aˆ = -\\×\\×. From the previous parts, for E ⊥ n at the surface, this Aˆ is self-adjoint\nand negative semidefinite, and hence we have a hyperbolic equation.\nAs in class, we therefore expect orthogonal eigenfunctions and real λ ≤ 0, and hence os\n√\ncillating \"normal mode\" solutions with eigenfrequencies ω =\n-λ.\n[Technically, we also obtain λ = 0 solutions which are non-oscillatory--from above, these are\nnullspace solutions E = \\φ for some φ, which physically correspond to the time-independent\nfields of fix charge distributions, where -φ is the potential and \\ · E = \\2φ is the charge\ndensity. Everything else, all of the other eigenfunctions, are oscillating solutions.]\nProblem 2: (5+5+5+5+10)\nSee also the IJulia notebook posted with the solutions.\n(a) Setting the slopes to be zero at R1 and R2 simply gives\nαJ: (kr) + βY : (kr) = 0\nm\nm\nα\nat the two radii, or E\n= 0 where\nβ\nJ: (kR1) Y : (kR1)\nm\nm\nE =\n.\nJ: (kR2) Y : (kR2)\nm\nm\nHence, writing fm(k) = det E, we get\nfm(k) = J: (kR1)Y : (kR2) - J: (kR2)Y : (kR1) .\nm\nm\nm\nm\n\nk\n0.4\n0.2\n0.0\n0.2\n0.4\nfm (k)\nf0(k)\nf1(k)\nf2(k)\nFigure 1: Plot of fm(k) for m = 0, 1, 2.\nGiven a k for which fm(k) = 0, then we can solve for the nullspace of E by arbitrarily choosing\nα\na scaling such that α = 1 and solving for β from the first or second rows of E\n= 0:\nβ\nJ: (kR1)\nJ: (kR2)\nm\nm\nβ = -\n= -\n.\nY : (kR1)\nY : (kR2)\nm\nm\n(b) The plot is shown in Figure 1. Note that fm(k) for m > 0 has a divergence as k → 0, so we\nused the ylim command to rescale the vertical axis (otherwise it would be hard to read the\nplot!); see the solution IJulia notebook.\n(c) We'll use the Scilab newton function, similar to class, to find the roots, with initial guesses\nprovided by our plot in Figure 1. We find k1 ≈ 3.196578, k2 ≈ 6.31234951, and k3 ≈\n9.4444649. See the solutions notebook.\n(d) See the IJulia notebook. Using our k1 and k2 from part (c) and our α and β from part (a),\nR2\nwe find that\nru0,1(r)u0,2(r)dr ≈ 10-15, which is zero up to roundoff errors.\nR1\n(e) Here, we have Aˆ = c\\2, and obtain:\n(i) At the origin, we can't blow up, and therefore we only have J for r < R1, but we have\nboth J and Y outside this. Hence\n\nαJm(k1r)\nr < R1\nu(r, θ) = [A cos(mθ) + B sin(mθ)] ×\n.\nβJm(k2r) + γYm(k2r) r > R1\nNote that the angular dependence must be the same for all r in order to have continuity\nat r = R1. However, we are not guaranteed that k1 = k2! In particular, we want\nˆAu = λu for some λ, and plugging in the Bessel functions and the values of c we find\n√\nλ = -2k2 = -k2. Hence, we let k2 = k and k1 = k/ 2.\n\n(ii) To get a finite Au\nˆ , we must have u and ∂u/∂r continuous at r = R1. Combined with\nu = 0 at r = R2, this gives the equations\n⎡\n√\n⎤ ⎛\n⎞\n⎛\n⎞\n⎣\n-Jm(kR1/ 2)\n-J:\nm(kR1/\n√\n2)\nJm(kR1)\nJ:\nm(kR1)\nYm(kR1)\nY :\nm(kR1) ⎦ ⎝\nα\nβ ⎠ = Em(k) ⎝\nα\nβ ⎠ = 0,\nJm(kR2) Ym(kR2)\nγ\nγ\nwhere the first two rows are the continuity conditions and the last row is the Dirichlet\ncondition, and we have defined a matrix Em(k).\n(iii) We simply need fm(k) = det Em(k) to solve for the solutions k and hence the eigenvalues.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Linear Partial Differential Equations, Problem 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/c7e3e7af8a9b6425c3e840dfc4ccdc3c_MIT18_303F14_pset4.pdf",
      "content": "18.303 Problem Set 4\nDue Wednesday, 15 October 2014.\nProblem 1:\nConsider the operator Aˆ = -c(x)Y2 in some 2d region Ω ⊆ R2 with Dirichlet boundaries (u|∂Ω = 0),\nwhere c(x) > 0. Suppose the eigenfunctions of Aˆ are un(x) with eigenvalues λn [that is, ˆ\n=\nAun\nλnun] for n = 1, 2, . . ., numbered in order λ1 < λ2 < λ3 < · · · . Let G(x, x') be the Green's function\nof Aˆ.\n\n(a) If f(x) =\nαnun(x) for some coefficients αn =_________________ (expression\nn\no\n'\nin terms of f and un), then\nG(x, x')f(x')d2x =__________________ (in terms\nΩ\nof αn and un).\n(b) The maximum possible value of\no o\n'\nu(x)G(x, x')u(x') d2x d2x\nΩ\nΩ c(x) o\n|u(x00 )|2\n,\nd2x''\nΩ\nc(x00)\nfor any possible u(x), is _____________________ (in terms of quantities men\ntioned above). [Hint: min-max. Use the fact, from the handout, that if Aˆ is self-adjoint then\nAˆ-1 is also self-adjoint.]\nProblem 2:\nIn this problem, we will solve the Laplacian eigenproblem -Y2u = λu in a 2d radius-1 cylinder\nr ≤ 1 with Dirichlet boundary conditions u|r=1Ω = 0 by \"brute force\" in Julia with a 2d finite-\ndifference discretization, and compare to the analytical Bessel solutions. You will find the IJulia\nnotebooks posted on the 18.303 website for Lecture 9 and Lecture 11 extremely useful! (Note: when\nyou open the notebook, you can choose \"Run All\" from the Cell menu to load all the commands in\nit.)\n(a) Using the notebook for a 100 × 100 grid, compute the 6 smallest-magnitude eigenvalues and\neigenfunctions of A with λi, Ui=eigs(Ai,nev=6,which=\"SM\"). The eigenvalues are given by\nλi. The notebook also shows how to compute the exact eigenvalue from the square of the root\nof the Bessel function. Compared with the high-accuracy λ1 value, compute the error Δλ1 in\nthe corresponding finite-difference eigenvalue from the previous part. Also compute Δλ1 for\nNx = Ny = 200 and 400. How fast is the convergence rate with Δx? Can you explain your\nresults, in light of the fact that the center-difference approximation we are using has an error\nthat is supposed to be ∼ Δx2? (Hint: think about how accurately the boundary condition\non ∂Ω is described in this finite-difference approximation.)\n(b) Modify the above code to instead discretize Y·cY, by writing A0 as -GT CgG for some G ma\ntrix that implements Y and for some Cg matrix that multiplies the gradient by c(r) = r2 + 1.\nDraw a sketch of the grid points at which the components of Y are discretized--these will\nnot be the same as the (nx, ny) where u is discretized, because of the centered differences. Be\ncareful that you need to evaluate c at the Y grid points now! Hint: you can make the matrix\nc\nt\nM1\nin Julia by the syntax [M1;M2].\nM2\nHint: Notice in the IJulia notebook from Lecture 11 how a matrix r is created from a column-\nvector of x values and a row-vector of y values. You will need to modify these x and/or y\nvalues to evaluate r on a new grid(s). Given the r matrix rc on this new grid, you can evaluate\n\nc(r) on the grid by c = rc.^2 + 1, and then make a diagonal sparse matrix of these values\nby spdiagm(reshape(c, prod(size(c)))).\n(c) Using this A ≈ Y · cY, compute the smallest-|λ| eigensolution and plot it. Given the eigen\nfunction converted to a 2d Nx × Ny array u, as in the Lecture 11 notebook, plot u(r) as a\nfunction of r, along with a plot of the exact Bessel eigenfunction J0(k0r) from the c = 1 case\nfor comparison.\nplot(r[Nx/2:end,Ny/2], u[Nx/2:end,Ny/2])\nk0 = so.newton(x -> besselj(0,x), 2.0)\nplot(0:0.01:1, besselj(0, k0 * (0:0.01:1))/50)\nHere, I scaled J0(k0r) by 1/50, but you should change this scale factor as needed to make the\nplots of comparable magnitudes. Note also that the r array here is the radius evaluated on\nthe original u grid, as in the Lecture 11 notebook.\nCan you qualitatively explain the differences?\nProblem 3:\nRecall that the displacement u(x, t) of a stretched string [with fixed ends: u(0, t) = u(L, t) = 0]\nu\nu\nsatisfies the wave equation ∂2 + f(x, t) = ∂2 , where f(x, t) is an external force density (pressure)\n∂x2\n∂t2\non the string.\n(a) Suppose that f(x, t) = <[g(x)e-iωt], an oscillating force with a frequency ω. Show that,\ninstead of solving the wave equation with this f(x, t), we can instead use a complex force\nf (x, t) = g(x)e-iωt, solve for a complex u (x, t), and then take u = <u to obtain the solution\nfor the original f(x, t).\n(b) Suppose that f(x, t) = g(x)e-iωt, and we want to find a steady-state solution u(x, t) =\n-iωt\nv(x)e\nthat is oscillating everywhere at the same frequency as the input force. (This\nwill be the solution after a long time if there is any dissipation in the system to allow the\ninitial transients to die away.) Write an equation ˆ\n=\nA self-adjoint?\nAv\ng that v solves. Is ˆ\nPositive/negative definite/semidefinite?\n(c) Solve for the Green's function G(x, x ' ) of this ˆ\n\nA, assuming that ω = nπ/L for any integer n\n(i.e. assume ω is not an eigenfrequency [why?]). [Write down the continuity conditions that\nG must satisfy at x = x ', solve for x =\nx ', and then use the continuity conditions to eliminate\nunknowns.]\n(d) Form a finite-difference approximation A of your Aˆ. Compute an approximate G(x, x ' ) in\nMatlab by A \\ dk, where dk is the unit vector of all 0's except for one 1/Δx at index\nk = x ' /Δx, and compare (by plotting both) to your analytical solution from the previous\n'\npart for a couple values of x and a couple of different frequencies ω (one < π/L and one\n> π/L) with L = 1.\n(e) Show the limit ω → 0 of your G relates in some expected way to the Green's function of - d2\ndx2\nfrom class.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Linear Partial Differential Equations, Solution to Problem 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/0ed71068e504a2f481dc01a5d9a2b671_MIT18_303F14_pset4sol.pdf",
      "content": "18.303 Problem Set 4 Solutions\nProblem 1: (10+10)\nn\n(a) Aˆ is self-adjoint under the inner product (u, v)\n\n=\nuv/c, from class (and the 1d version\nΩ\nin homework), and hence the eigenfunctions are orthogonal for distinct eigenvalues (but not\nnecessarily normalized to IunI = 1), hence αn = (un, f)/(un, un). The expression u(x) =\nn\ne\nG(x, x')f(x\nx' is just the solution to ˆ\nˆ\n=\nαn un\n')d2\nAu = f, i.e. it is u = A-1f\n(x).\nΩ\nn λn\n(b) This is just (u, Aˆ-1u)/(u, u), which is a Rayleigh quotient for the self-adjoint operator Aˆ-1 ,\nand hence (thanks to the min-max theorem) it is bounded above by the largest eigenvalue\nof Aˆ-1, which is 1/λ1 (the inverse of the smallest eigenvalue of Aˆ). (Note that by positive-\ndefiniteness 0 < λ1 < λ2 < · · · .)\nProblem 2:\nIn this problem, we will solve the Laplacian eigenproblem -'2u = λu in a 2d radius-1 cylinder\nr ≤ 1 with Dirichlet boundary conditions u|r=1Ω = 0 by \"brute force\" in Julia with a 2d finite-\ndifference discretization, and compare to the analytical Bessel solutions. You will find the IJulia\nnotebooks posted on the 18.303 website for Lecture 9 and Lecture 11 extremely useful! (Note: when\nyou open the notebook, you can choose \"Run All\" from the Cell menu to load all the commands in\nit.)\n(a) Using the notebook for a 100 × 100 grid, compute the 6 smallest-magnitude eigenvalues and\neigenfunctions of A with λi, Ui=eigs(Ai,nev=6,which=\"SM\"). The eigenvalues are given by\nλi. The notebook also shows how to compute the exact eigenvalue from the square of the root\nof the Bessel function. Compared with the high-accuracy λ1 value, compute the error Δλ1 in\nthe corresponding finite-difference eigenvalue from the previous part. Also compute Δλ1 for\nNx = Ny = 200 and 400. How fast is the convergence rate with Δx? Can you explain your\nresults, in light of the fact that the center-difference approximation we are using has an error\nthat is supposed to be ∼ Δx2? (Hint: think about how accurately the boundary condition\non ∂Ω is described in this finite-difference approximation.)\n(b) Modify the above code to instead discretize '·c', by writing A0 as -GT CgG for some G ma\ntrix that implements ' and for some Cg matrix that multiplies the gradient by c(r) = r2 + 1.\nDraw a sketch of the grid points at which the components of ' are discretized--these will\nnot be the same as the (nx, ny) where u is discretized, because of the centered differences. Be\ncareful that you need to evaluate c at the ' grid points now! Hint: you can make the matrix\nc\nt\nM1\nin Julia by the syntax [M1;M2].\nM2\nHint: Notice in the IJulia notebook from Lecture 11 how a matrix r is created from a column-\nvector of x values and a row-vector of y values. You will need to modify these x and/or y\nvalues to evaluate r on a new grid(s). Given the r matrix rc on this new grid, you can evaluate\nc(r) on the grid by c = rc.^2 + 1, and then make a diagonal sparse matrix of these values\nby spdiagm(reshape(c, prod(size(c)))).\n(c) Using this A ≈' · c', compute the smallest-|λ| eigensolution and plot it. Given the eigen\nfunction converted to a 2d Nx × Ny array u, as in the Lecture 11 notebook, plot u(r) as a\nfunction of r, along with a plot of the exact Bessel eigenfunction J0(k0r) from the c = 1 case\nfor comparison.\nplot(r[Nx/2:end,Ny/2], u[Nx/2:end,Ny/2])\nk0 = so.newton(x -> besselj(0,x), 2.0)\nplot(0:0.01:1, besselj(0, k0 * (0:0.01:1))/50)\n\nHere, I scaled J0(k0r) by 1/50, but you should change this scale factor as needed to make the\nplots of comparable magnitudes. Note also that the r array here is the radius evaluated on\nthe original u grid, as in the Lecture 11 notebook.\nCan you qualitatively explain the differences?\nProblem 3: (5+5+10+10+5 points)\nRecall that the displacement u(x, t) of a stretched string [with fixed ends: u(0, t) = u(L, t) = 0]\nu\nu\nsatisfies the wave equation ∂2 + f(x, t) = ∂2 , where f(x, t) is an external force density (pressure)\n∂x2\n∂t2\non the string.\n∂2\nu\nu\n(a) Suppose that u solves ∂2 + f (x, t) =\nand satisfies u (0, t) = u (L, t) = 0. Now, consider\n∂x2\n∂t2\nu +u\nu = <u =\n. Clearly, u(0, t) = u(L, t) = 0, so u satisfies the same boundary conditions. It\nalso satisfies the PDE:\n\n∂2u\n1 ∂2u\n∂2u\n=\n+\n∂t2\n∂t2\n∂t2\n\n1 ∂2u\n∂2u\n=\n+ f +\n+ f\n∂x2\n∂x2\n∂2u\n=\n+ f,\n∂x2\nf+f\nsince f =\n\n= <f . The key factors that allowed us to do this are (i) linearity, and (ii) the\nreal-ness of the PDE (the PDE itself contains no i factors or other complex coefficients).\n(b) Plugging u(x, t) = v(x)e-iωt and f(x, t) = g(x)e-iωt into the PDE, we obtain\n∂2v\n\n-iωt\n-iωt\n-iωt\n\n+ g = -ω2 v\ne\ne\ne\n,\n∂x2\nand hence\nc\nt\n∂2\n-\n- ω2 v = g\n∂x2\n∂2\nand Aˆ = -\n- ω2 . The boundary conditions are v(0) = v(L) = 0, from the boundary\n∂x2\nconditions on u.\nSince ω2 is real, this is in the general Sturm-Liouville form that we showed in class is self\nadjoint.\nSubtracting a constant from an operator just shifts all of the eigenvalues by that constant,\nkeeping the eigenfunctions the same. Thus Aˆ is still positive-definite if ω2 is < the smallest\neigenvalue of -∂2/∂x2, and positive semidefinite if ω2 = the smallest eigenvalue. In this case,\nwe know analytically that the eigenvalues of -∂2/∂x2 with these boundary conditions are\n(nπ/L)2 for n = 1, 2, . . .. So Aˆ is positive-definite if ω2 < (π/L)2, it is positive-semidefinite if\nω2 = (π/L)2, and it is indefinite otherwise.\n(c) We know that ˆ\n' ) = 0 for x\nx '. Also, just as in class and as in the notes, the fact\nAG(x, x\n=\nthat ˆ\n' ) =\n' ) meas that G must be continuous (otherwise there would be a δ '\nAG(x, x\nδ(x - x\nfactor) and ∂G/∂x must have a jump discontinuity:\n\n∂G\n∂G\n-\n= -1\n∂x\n∂x\nx=x1+\nx=x1-\n\nZ\nZ\n\nfor -∂2G/∂x2 to give δ(x - x ' ). We could also show this more explicitly by integrating:\nx 1+0+\nx 1+0+\nˆAG dx =\nδ(x - x ' ) dx = 1\nx1-0+\nx1-0+\nx +0+\nZ x +0+\n= - ∂G\n-\nω2G dx,\n∂x\n1-0+\nx1-0+\nx\nwhich gives the same result.\n'\nNow, similar to class, we will solve it separately for x < x and for x > x ', and then im\n'\npose the continuity requirements at x = x to find the unknown coefficients.\n'\nˆ\nG\nFor x < x , AG = 0 means that ∂2\n= -ω2G, hence G(x, x ' ) is some sine or cosine of\n∂x2\nωx. But since G(0, x ' ) = 0, we must therefore have G(x, x ' ) = α sin(ωx) for some coefficient\nα.\nSimilarly, for x > x ', we also have a sine or cosine of ωx. To get G(L, x ' ) = 0, the sim\nplest way to do this is to use a sine with a phase shift: G(x, x ' ) = β sin(ω[L - x]) for some\ncoefficient β.\nContinuity now gives two equations in the two unknowns α and β:\nα sin(ωx ' ) = β sin(ω[L - x ' ])\nαω cos(ωx ' ) = -βω cos(ω[L - x ' ]) + 1,\nsin(ω[L-x ])\nsin(ωx1)\nwhich has the solution α =\n, β =\n.\nω cos(ωx1) sin(ω[L-x1])+sin(ωx1 ) cos(ω[L-x1])\nω cos(ωx1) sin(ω[L-x1])+sin(ωx1) cos(ω[L-x1])\nThis simplifies a bit from the identity sin A cos B + cos B sin A = sin(A + B), and hence\n_\nsin(ωx) sin(ω[L - x ' ]) x < x '\nG(x, x ' ) =\n,\n'\nω sin(ωL)\nsin(ωx ' ) sin(ω[L - x]) x ≥ x\nwhich obviously obeys reciprocity.\nNote that if ω is an eigenfrequency, i.e. ω = nπ/L for some n, then this G blows up.\nThe reason is that Aˆ in that case is singular (the n-th eigenvalue was shifted to zero), and\nA-1\ndefining ˆ\nis more problematical. (Physically, this corresponds to driving the oscillating\nstring at a resonance frequency, which generally leads to a diverging solution unless there is\ndissipation in the system.)\nd2\n(d) It is critical to get the signs right. Recall that we approximated\nby -DT D for a 1st\ndx2\nderivative matrix D. Therefore, you want to make a matrix A = DT D - ω2I. In Matlab, A\n= D'*D - omega^2 * eye(N) where N is the size of the matrix D = diff1(N) / dx with dx\n= 1/(N+1). We make dk by dk = zeros(N,1); dk(k) = 1/dx. I used N = 100.\nThe resulting plots are shown in figure 1, for ω = 0.4π/L (left) and ω = 5.4π/L (right)\n'\nand x = 0.5 and 0.75. As expected, for ω < π/L where it is positive-definite, the Green's\nfunction is positive and qualitatively similar to that for ω = 0, except that it is slightly curved.\nFor larger ω, though, it becomes oscillatory and much more interesting in shape. The exact\nG and the finite-difference G match very well, as they should, although the match becomes\nworse at higher frequencies--the difference approximations become less and less accurate as\nthe function becomes more and more oscillatory relative to the grid Δx, just as was the case\n\n0.2\n0.4\n0.6\n0.8\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nx\nG(x,x')\nω = 0.4π/L\n\nexact x'=0.5\nexact x'=0.75\nN=100 x'=0.5\nN=100 x'=0.75\n0.2\n0.4\n0.6\n0.8\n-0.06\n-0.04\n-0.02\n0.02\n0.04\n0.06\nx\nG(x,x')\nω = 5.4π/L\n\nexact x'=0.5\nexact x'=0.75\nN=100 x'=0.5\nN=100 x'=0.75\n- ∂2\nFigure 1: Exact (lines) and finite-difference (dots) Green's functions G(x, x ' ) for Aˆ =\n- ω2 ,\n∂x2\n'\nfor two different ω values (left and right) and two different x values (red and blue).\nfor the eigenfunctions as discussed in class.\n[If you try to plug in an eigenfrequency (e.g. 4π/L) for ω, you may notice that the exact\nG blows up but the finite-difference G is finite. The reason for this is simple: the eigenvalues\nof the finite-difference matrix DT D are not exactly (nπ/L)2, so the matrix is not exactly\nsingular, nor is it realistically possible to make it exactly singular thanks to rounding errors.\nIf you plug in something very close to an eigenfrequency, then G becomes very close to an\neigenfunction multipled by a large amplitude. It is easy to see why this is if you look at the\nexpansion of the solution in terms of the eigenfunctions.]\n(e) For small ω, sin(ωy) ≈ ωy for any y, and hence\nG(x, x ' ) → 1\n\nω2L\n_\n\nω2x(L - x ' )\n\nω2x ' (L - x)\nx < x '\nx ≥ x ' ,\nwhich matches the Green's function of - d2 from class. (Equivalently, you get 0/0 as ω → 0\ndx2\nand hence must use L'Hopital's rule or similar to take the limit.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Linear Partial Differential Equations, Problem 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/09c1716433c040001efb2c2ddb3ed043_MIT18_303F14_pset5.pdf",
      "content": "V\nV\nn\nFigure 1: A volume V with a surface ∂V , and an outward unit normal vector n at each point on ∂V .\n18.303 Problem Set 5\nDue Monday, 27 October 2014.\nProblem 1: Distributions\nThis problem concerns distributions as defined in the notes: continuous linear functionals f{φ} from test functions\nφ ∈D, where D is the set of infinitely differentiable functions with compact support (i.e. φ = 0 outside some region\nwith finite diameter [differing for different φ], i.e. outside some finite interval [a, b] in 1d).\n\nln |x| x = 0\n\n(a) In this part, you will consider the function f(x) =\nand its (weak) derivative, which is connected\nx = 0\nto something called the Cauchy Principal Value.\n(i) Show that f(x) defines a regular distribution, by showing that f(x) is locally integrable for all intervals\n[a, b].\n\n= 0\nx\n(ii) Consider the 18.01 derivative of f(x), which gives f '(x) =\nx\n. Suppose we just set\nundefined x = 0\n\n= 0\nx\n\"f '(0) = 0\" at the origin to define g(x) =\nx\n. Show that this g(x) is not locally integrable,\nx = 0\nand hence does not define a distribution.\nBut the weak derivative f '{φ} must exist, so this means that we have to do something different from\nthe 18.01 derivative, and moreover f '{φ} is not a regular distribution. What is it?\n-f\ninf\n(iii) Write f{φ} = limf→0+ ff{φ} where ff{φ} =\n-inf ln(-x)φ(x)dx + f ln(x)φ(x)dx, since this limit exists\nand equals f{φ} for all φ from your proof in the previous part.1 Compute the distributional derivative\nf '{φ} = limf→0+ ff\n'{φ}, and show that f '{φ} is precisely the Cauchy Principal Value (google the definition,\ne.g. on Wikipedia) of the integral of g(x)φ(x).\ninf\n(iv) Alternatively, show that f '{φ(x)} = g{φ(x) - φ(0)} =\ng(x)[φ(x) - φ(0)]dx (which is a well-defined\n-inf\nintegral for all φ ∈D).\n(b) In class, we only looked explicitly at 1d distributions, but a distribution in d dimensions Rd can obviously be\ndefined similarly, as maps f{φ} from smooth localized functions φ(x) to numbers. Analogous to class, define the\ndistributional gradient 'f by 'f{φ} = f{-'φ}.\nConsider some finite volume V with a surface ∂V , and assume ∂V is differentiable so that at each point\nit has an outward-pointing unit normal vector n, as shown in figure 1. Define a \"surface delta function\"\n\nδ(∂V ){φ} =\nφ(x)dd-1x to give the surface integral\nof the test function.\n∂V\n∂V\nd\nd\n1More explicitly, f{φ} - fd{φ} =\nln |x|φ(x)dx ≤ (max φ)\nln |x|dx → 0, since you should have done the something like the last\n-d\n-d\nintegral explicitly in the previous part.\n\nf1(x) x ∈ V\nSuppose we have a regular distribution f{φ} defined by the function f(x) =\n, where we may\nf2(x) x ∈/ V\nhave a discontinuity f2 - f1 = 0 at ∂V .\n(i) Show that the distributional gradient of f is\n'f1(x) x ∈ V\n'f = δ(∂V ) [f1(x) - f2(x)] n(x) +\n,\n'f2(x) x ∈/ V\nwhere the second term is a regular distribution given by the ordinary 18.02 gradient of f1 and f2 (assumed\nto be differentiable), while the first term is the singular distribution\n\nδ(∂V ) [f1(x) - f2(x)] n(x){φ} =\n[f1(x) - f2(x)] n(x)φ(x)dd-1 x.\n∂V\n\nYou can use the integral identity that\n'ψddx = ∂V ψndd-1x to help you integrate by parts.\nV\n(ii) Defining '2f{φ} = f{'2φ}, derive a similar expression to the above for '2f. Note that you should have\none term from the discontinuity f1 - f2, and another term from the discontinuity 'f1 -'f2. (Recall how\nwe integrated '2 by parts in class some time ago.)\nProblem 2: Green's functions\nConsider Green's functions of the self-adjoint indefinite operator Aˆ = -'2 - ω2 (κ > 0) over all space (Ω = R3 in\n3d), with solutions that → 0 at infinity. (This is the multidimensional version of problem 2 from pset 5.) As in class,\nthanks to the translational and rotational invariance of this problem, we can find G(x, x ' ) = g(|x - x ' |) for some g(r)\nin spherical coordinates.\n(a) Solve for g(r) in 3d, similar to the procedure in class.\n(i) Similar to the case of Aˆ = -'2 in class, first solve for g(r) for r > 0, and write g(r) = limf→0+ ff(r)\nwhere ff(r) = 0 for r ≤ E. [Hint: although Wikipedia writes the spherical '2g(r) as 1 (r g ' ) ', it may be\nr\nmore convenient to write it equivalently as '2g =\n(rg) '', as in class, and to solve for h(r) = rg(r) first.\nr\nHint: if you get sines and cosines from this differential equation, it will probably be easier to use complex\nexponentials, e.g. eiωr, instead.]\n(ii) In the previous part, you should find two solutions, both of which go to zero at infinity. To choose between\nthem, remember that this operator arose from a e-iωt time dependence. Plug in this time dependence and\nimpose an \"outgoing wave\" boundary condition (also called a Sommerfield or radiation boundary condition):\nrequire that waves be traveling outward far away, not inward.\n(iii) Then, evaluate Ag\nˆ\n=\nAg){q} = g{ ˆ\n= q(0) for an arbitrary (smooth,\nδ(x) in the distributional sense: ( ˆ\nAq}\nlocalized) test function q(x) to solve for the unknown constants in g(r). [Hint: when evaluating g{ ˆAq}, you\nmay need to integrate by parts on the radial-derivative term of '2q; don't forget the boundary term(s).]\n(b) Check that the ω → 0+ limit gives the answer from class.\n(\n=\n(\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Linear Partial Differential Equations, Solution to Problem 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/c8778f356ecf49052c9e5e41b1872e60_MIT18_303F14_pset5sol.pdf",
      "content": "18.303 Problem Set 5 Solutions\nProblem 1: ((3+2+5+5)+(5+5) points)\n(a) Solutions:\n(i) f(x) is bounded in every interval except intervals containing x = 0, so local integrability is trivial except\nb\nfor intervals containing x = 0. It is sufficient to consider integrals\n, because any interval [a, b] containing\nb\n0 can be broken up into\n+\n, and f(-x) = f(x) so we only need to show that the latter is finite. But\na\nwe can now just do the integral explicitly:\nˆ b\nˆ b\nln x dx = lim\nln x dx = lim x ln x - x|b\nl = b ln b - b,\nl→0+\nl→0+\nl\nsince\nlim x ln x = 0,\nl→0+\nas can easily be seen e.g. by L'Hopital's rule applied to lim ln x = lim 1/x = lim x = 0.\n1/x\n-1/x2\n(ii) g(x) is not locally integrable for intervals containing the origin. For example\nˆ b\nˆ b dx\nb\n|g(x)|dx = lim\n= lim ln x|l = inf.\nl→0+\nx\nl→0+\nl\nTherefore, f '{φ} = f{-φ'} is a singular distribution.\n(iii) We write f '{φ} = liml→0+ fl\n'{φ}, and integrate by parts in fl\n'{φ} = fl{-φ'}:\n⎡\n⎤\nˆ -l\nˆ inf\n⎣\n⎦\nl→0+\n' -v \" ' -v \"\n' -v \" ' -v \"\nf '{φ} = - lim\nln(-x) φ'(x)dx +\nln(x) φ'(x)dx\n-inf\nl\nu\ndv\nu\ndv\n\nˆ -l\nˆ inf\n-1\n= - lim ln(E)φ(-E) -\nφ(x)dx - ln(E)φ(E) -\nφ(x)dx\nl→0+\nx\nx\nˆ -l\nˆ\n-inf\ninf\n\nl\n\n= lim\nφ(x)dx +\nφ(x)dx + lim [ln(E) (φ(E) - φ(-E))].\n\nx\n→0+\nl→0+\nx\nl\n-inf\nl\n\nIn the last line, the first limit is precisely the Cauchy Principal Value of g(x)φ(x)dx (CPV = remove a\nball of radius E around the singularity, do the integral, and then take the E → 0+ limit). The second term\nvanishes because, since φ(x) is continuous and infinitely differentiable, φ(E) - φ(-E) vanishes at least as fast\nas E as E → 0, so its product with ln E vanishes in the limit as in part (i).1\n(iv) Use f '{φ} = f{-φ'} = f{-[φ - φ(0)]'} = f '{φ - φ(0)}. Then substitute φ - φ(0) into the previous part,\nand note that since the integrand φ(x)-φ(0) is now finite as x → 0 [since φ(x) - φ(0) goes to zero as x → 0,\nx\nat least proportionally to x or faster as in the previous part], we can now just take the limit to write\ninf\nf '{φ} =\ng(x)[φ(x) - φ(0)]dx without using the CPV.\n-inf\n(b) Solutions:\n(i) Let V c denote the complement of V (the exterior region outside V , i.e. V c = Rd\\V ), and note that\n1This last fact is a little subtle. Naively, we could just write the Taylor expansion φ(x) = φ(0) + φ'(0)E + O(E2) to obtain φ(E) - φ(-E) =\n2φ'(0)E+O(E2). However, it turns out that test functions φ(x) are not analytic (do not have a convergent Taylor series) at all x. Nevertheless,\nbecause φ is differentiable with bounded φ', it follows that φ(x) is \"Lipshitz continuous:\" |φ(x) - φ(y)| < K|x - y| for all x, y and for some\nconstant K > 0 (e.g. you can prove this from the mean-value theorem of analysis), so φ(E) - φ(-E) < 2KE and the result follows. But I\ndon't expect you to provide this level of proof; a Taylor argument is acceptable.\n\n∂V c = ∂V (but with the outward-normal vector reversed in sign). We write:\nˆ\nˆ\nVf{φ} = f{-Vφ} = -\nf1Vφ -\nf2Vφ\nV c\nV\nˆ\nˆ\n= -\n[V(f1φ) - φVf1] -\n[V(f2φ) - φVf2]\nV\nV c\n\nˆ\n\nˆ\n= -\nf1φn +\nφVf1 +\nf2φn +\nφVf2\n∂V\nV\n∂V\nV c\n\nVf1(x) x ∈ V\n=\nδ(∂V ) [f2(x) - f1(x)] n(x) +\n{φ}\nVf2(x) x ∈/ V\nas desired.\n(ii) In this case, we will need to integrate by parts twice, but we can just quote the results from the \"notes on\nelliptic operators\" from class (where we integrated by parts twice with -V2 already), albeit keeping the\nboundary terms from ∂V that were zero in the notes:\nˆ\nˆ\nV2f{φ} = f{V2φ} =\nf1V2φ +\nf2V2φ\nV c\nV\n\nˆ\nˆ\n=\n[(f1 - f2)Vφ - φ(Vf1 -Vf2)] · n +\nφV2f1 +\nφV2f2.\nV c\n∂V\nV\nBut the first term is δ(∂V ) [f1(x) - f2(x)] {n·Vφ} = (n·V)δ(∂V ) [f2(x) - f1(x)] {φ} by the definition (note\nthe sign change) of the distributional derivative n ·V (note that this is a scalar derivative in the n direction,\nnot a gradient vector). The second term is a surface delta function weighted by (n · Vf1 - n · Vf2) ,the\ndiscontinuity in the normal derivative. And the last terms are just a regular distribution. So, we have\n\nV2f1(x) x ∈ V\nV2f = (n · V)δ(∂V ) [f2 - f1] + δ(∂V ) [n · Vf1 - n · Vf2] +\n.\nV2f2(x) x ∈/ V\nAs noted in class, n · V of a delta function is a \"dipole\" oriented in the n direction, so the first term is a\n\"dipole layer\".\nProblem 2: ((5+10)+2 points)\n(a) We solve for g(r) in 3d as follows:\nh ''\n(i) For r > 0, -V2g - ω2g = 0 and hence -ω2g = V2g =\n(rg) '' =⇒\n= -ω2h where h(r) = rg(r). The\nr\niωr +de-iωr\niωr + de-iωr\nce\nsolution to this is h(r) = ce\nfor some constants c and d, or g(r) =\nr\nIt is a little more tricky to determine whether we should use the c or the d term than in class, since\nboth decay at the same rate. The ratio c/d will be determined by some kind of boundary condition at\n±iωr\ninfinity, but what might this be? It is acceptable for you to just punt on this here; since e\nare complex\nconjugates of each other, your analysis will apply equally well to either one, and you can arbitrarily pick\none, say ceiωr/r, to analyze.\nHowever, to see why there should be a sensible choice, recall that this operator arose in pset 5 by as\n-iωt\nsuming a time dependence e\nmultiplying the solution, in which case we are looking at wave solutions\niω(r-t) +de-iω(r+t)\nce\n, where the c term describes waves moving out towards r →inf, while the d term describes\nr\nwaves moving in from infinity. In wave problems, we typically impose a boundary condition of outgoing\nwaves at infinity, in which case we must set d = 0. (However, the choice would have been reversed if we\npicked the opposite sign convention, e+iωt, for the time dependence.)\n(ii) Let's focus on g(r) = ceiωr/r. As in class, the 1/r singularity is no problem in 3d (it is cancelled by the\n\nJacobian factor r2dr), so g is a regular distribution. Given an arbitrary test function q(x), we now evaluate\nˆ\n( ˆ\nˆ\nAg){q} = g{Aq\nˆ } =\ngAq\n⎤\n⎡\nˆ inf\nˆ 2π\nˆ π\nr 2dr\ndφ\nsin θ dθ\n⎢⎢⎣-ω2 gq -\n⎥⎥⎦\nintegrate to 0, from class\n∂2\ng\n\"\n(θ, φ derivatives of q)\n-v\n'\n(rq) +\n=\nr ∂r2\n⎞\n⎛\n⎡\ndr\n⎤\n⎥⎥⎥⎦\n⎢⎢⎢⎣\nc\n⎜\n⎜\n⎜\n⎝\nint. by parts\n⎟\n⎟\n⎟\n⎠\nˆ inf\n∂2\n\niωr -\niωr\n-ω2 rqe\ne\nsin θ dθ dφ\n[rq]\n=\n∂r2\n'\n\"\n-v\n⎡\n⎛\n⎜\n⎜\n⎜\n⎝\n⎤\n⎞\n⎢⎢⎢⎣\n⎟\n⎟\n⎟\n⎠ dr\n⎥⎥⎥⎦\nˆ inf\n\n∂\n∂\n[rq]\ninf\n\n+ c\niωr\niωr\n-ω2 rqe\n+ iωeiωr\n-ce\nsin θ dθ dφ\n[rq]\n=\n∂r\n∂r\n'\n\"\n-v\nint. by parts\n\nˆ inf\n\n-\n(0)\n\ncq\nicωeiωr[rq] 0\ninf\niωr\niωr[rq]\n-ω2 rqe\n+ ω2 e\nsin θ dθ dφ\ndr\n+ c\n=\n= 4πcq(0),\nand hence c = 1/4π . Thus\n/|\niω|x-x\ne\nG(x, x ' ) = 4π|x - x '|\nassuming boundary conditions such that d = 0. More generally, since exactly the same result applies to the\nde-iωr/r term, we obtain,\n/|\n/ |\niω|x-x\nce\n+ de-iω|x-x\nG(x, x ' ) =\n|x - x '|\n'\nfor c + d = 1/4π , with the ratio c/d being set by the boundary conditions at inf. The value at x = x\nbeing irrelevant in the distribution sense, e.g. we can assign it to zero, since this is a regular distribution\nwith a finite integral, similar to class.)\n(b) The ω → 0 limit gives 1/4π|x - x ' | as in class, by inspection.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Linear Partial Differential Equations, Midterm Exam",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/d53f8880eab061419f4de824ae120b4d_MIT18_303F14_midterm.pdf",
      "content": "18.303 Midterm, Fall 2014\nEach problem has equal weight. You have 55 minutes.\nProblem 1: Adjoints (25 points)\nd2\nConsider the operator Au\nˆ\n=\n(cu) on the domain Ω = [0, L] with Dirichlet boundaries u|∂Ω = 0, where c(x) > 0.\ndx2\nShow that Aˆ = Aˆ∗ for an appropriate choice of inner product (u, v).\nProblem 2: Green (25 points)\nConsider the operator Aˆ = -\\2 in 2d, where the domain is the entire x, y plane. In cylindrical (r, φ) coordinates, we\ncan write\n\n1 ∂\n∂u\n1 ∂2u\n\\2 u =\nr\n+\n.\nr ∂r\n∂r\nr2 ∂φ2\nWe want to solve for the Green's function G(x, x' ) = g(|x - x' |) of Aˆ, reduced to a function g(r) by the symmetry\nof the problem. By looking at r > 0, one can quickly show that g(r) = c ln r for some unknown constant c. This g\ndefines a regular distribution, if we just set g(0) = 0, despite the fact that ln blows up [it is an \"integrable\" singularity\nin 2d: the integral exists for any test function ψ(x, y)]. Find c.\nProblem 3: Waves (25 points)\n\nu\n∂/∂x\n''\nIn class, we re-wrote the wave equation u = u in the form Dˆw = ∂w/∂t, where w =\nand Dˆ =\n.\nv\n∂/∂x\nWe showed that Dˆ ∗ = -Dˆ (anti-Hermitian) for appropriate boundary conditions (let's say u = 0 on the boundaries)\ni\n∗\nfor the obvious inner product (w, w' ) =\nw w' , and hence Dˆ has purely imaginary eigenvalues λ = -iω (oscillating\nΩ\nsolutions), and IwI2 = (w, w) was a conserved \"energy.\"\nConsider a new operator:\n\n-σ\n∂/∂x\nˆDσ =\n∂/∂x\n-σ\nwhere σ(x) > 0.\n(a) Show that IwI2 is not conserved; it is _______ in time. Hint: consider ∂(w, w)/∂t as in class.\n(b) Show that if wn is an eigenfunction, i.e. Dˆσwn = λnwn, then the real part of λ is negative (hence the\neigensolutions are ________ in time). Hint: consider (wn, (Dˆσ + Dˆσ\n∗ )wn).\nProblem 4: Discrete Waves (25 points)\nConsider the operator\n\n-σ\n∂/∂x\nˆDσ =\n∂/∂x\n-σ\n\nu\nn\nfrom the previous problem, acting on w =\n. For σ = 0, we discretized this in class via u\n≈ u(mΔx, nΔt) and\nm\nv\nn+0.5\nv\n(a staggered grid in space and time):\nm+0.5\nn+0.5\nn+0.5\nn+1\nn\nu\n- u\nv\n- v\nm\nm\nm+0.5\nm- 0.5\n=\n,\nΔt\nΔx\nn+0.5\nn- 0.5\nn\nn\nv\n- v\nu\nm+0.5\nm+0.5\nm+1 - um\n=\n.\nΔt\nΔx\nWrite down a center-difference (second-order accurate) scheme for ∂w/∂t = Dˆσ w. For simplicity, take σ to be a\nconstant (independent of x or t), and solve for:\nn+1\nu\n=?\nm\nn+0.5\nv\n=?\nm+0.5\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Exam",
      "title": "Linear Partial Differential Equations, Midterm Exam Solution",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/65841b2e9ddd926b7d6450923326531b_MIT18_303F14_midtermsol.pdf",
      "content": "18.303 Midterm Solutions, Fall 2014\nProblem 1:\n\nDefine the inner product (u, v)c =\nuv =\ncuv = (cu, v) where (u, v) =\n\nAv)c = (cu, (cv)\"\" ) =\nc\nuv . Then (u, ˆ\nΩ\nΩ\nΩ\n((cu)\"\"\nˆ\n, cv) = (cAu, v) = ( ˆ\n, where we have used the self-adjointness of d2/dx2 under (·, ·) from class. Therefore,\nAu, v)c\nAˆ = Aˆ∗ under the (u, v)c inner product (which is a proper inner product for real c > 0).\nProblem 2:\nWe need -'2g = δ(x), and we determine this by evaluating both sides with an arbitrary test function ψ, using the\ndistributional derivative (-'2g){ψ} = g{-'2ψ} as in class. In cylindrical coordinates:\ninf\n2π\n\n1 ∂\n∂ψ\n1 ∂2ψ\ng{-'2ψ} = - lim\nr dr\ndφ c ln r\nr\n+\n1→ 0+\nr ∂r\n∂r\nr2 ∂φ2\n(\n2π\ninf\n\ninf\n2π\n∂\n∂ψ\nc ln r\n(\ndr\n;\n∂ψ\n;(;\n\n= -c\ndφ lim\ndr ln r\nr\n- lim\n(\n1→ 0+\n∂r\n∂r\n1→ 0+\nr\n∂φ\n\n(\n\n2π\ninf\ninf\n(\n\n∂ψ\n∂(ln r)\n∂ψ\n(\n= -c\ndφ lim\nr ln r\n(\n-\ndr\nTr\n1→ 0+\n∂r\n∂r\n∂r\n\n2π\nψ|r=inf\n= c\ndφ lim\n= -2πcψ(0).\nr=1\n1→ 0+\nTo get δψ = ψ(0), therefore, we need c = -1/2π .\nProblem 3:\nIt is convenient to write Dˆσ = Dˆ - σI, where I is the 2 × 2 identity matrix. Then it follows from Dˆ ∗ = -Dˆ and\n\n\" ) =\n∗\n\"\n(σI)∗ = σI (since σ is a real scalar and I is obviously self-adjoint) under the usual inner product (w, w\nw w\nΩ\nthat we have Dˆ ∗ = -Dˆ - σI and Dˆσ + Dˆ ∗ = -2σI.\nσ\nσ\n(a) For a solution w of Dˆσ w = ∂w/∂t , we have\n∂(w, w)/∂t = (∂w/∂t, w) + (w, ∂w/∂t)\n= (Dˆσw, w) + (w, Dˆσw)\n\n= (w, Dˆσ\n∗ w) + (w, Dˆσw) = (w, Dˆσ\n∗ + Dˆσ\nw)\n\n= -2(w, σw) = -2\nσ(x)lw(x)l2 < 0\nand hence lwl2 = (w, w) is decreasing in time.\nIf σ(x) ≥ σ0 > 0 for some σ0, then we can go further and say that E(t) = lwl2 is decaying at least expo\n- 2σ0t\nnentially fast in time, since in that case dE/dt ≤-2σ0E and from this one can show that E(t) ≤ E(0)e\n.\n(i) Given an eigensolution Dˆσwn = λnwn , we can consider\n(wn, (Dˆσ + Dˆσ\n∗ )wn) = -2(wn, σwn)\n= (wn, Dˆσwn) + (Dˆσwn, wn)\n= (wn, λnwn) + (λnwn, wn)\n\n= λn + λn (wn, wn) = (wn, wn)2 Re λn.\nNote that we moved Dˆ ∗ to act on the left via its adjoint. It is not in general true that Dˆσ\n∗ wn = λnwn.\nσ\nThen we have:\n(wn, σwn)\nRe λn = -\n< 0\n(wn, wn)\nsince σ > 0. Hence the eigensolutions are decaying exponentially in time (while they oscillate via the\nλnt\nimaginary part of λn), from their time dependence e\n.\n\nProblem 4:\nWe will have ∂u/∂t = ∂v/∂x - σu and ∂v/∂t = ∂u/∂x - σv, so the only new terms are the σ terms. In the discretized\nn\nn+1\nu +u\nn+0.5\nm\nm\n∂u/∂t equation, the left-hand side is evaluated at point m and time n+0.5, so we have to get u\n=\n+O(Δt2)\nm\nby averaging (similarly to how we handled the Crank-Nicolson discretization in class). Similarly for the ∂v/∂t equation.\nHence, we obtain:\nn+0.5\nn+0.5\nn+1\nn\nn+1\nn\nu\n- u\nv\n- v\nu\n+ u\nm\nm\nm+0.5\nm-0.5\nm\nm\n=\n- σ\n,\nΔt\nΔx\nn+0.5\nn-0.5\nn\nn\nn+0.5\nn-0.5\nv\n- v\nu\nv\nm+0.5\nm+0.5\nm+1 - um\nm+0.5 + vm+0.5\n=\n- σ\n.\nΔt\nΔx\nn+1\nn+0.5\nSolving for u\nand v\n, we obtain the \"leap-frog\" equations:\nm\nm+0.5\n-1\nσΔt\nσΔt\nΔt\nn+1\nn\nn+0.5\nn+0.5\nu\n=\n1 +\n1 -\nu +\nv\n- v\n,\nm\nm\nm+0.5\nm-0.5\nΔx\n-1\nσΔt\nσΔt\nΔt\nn+0.5\nn-0.5\nn\nn\nv\n=\n1 +\n1 -\nv\nu\n.\nm+0.5\nm+0.5 +\nm+1 - um\nΔx\nNote that σ > 0, so we are never dividing by zero in 1 + σΔt/2, regardless of Δt, which is comforting.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Linear Partial Differential Equations, Lec 1 Handout",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/2b1299540eb0dd84fe45d44ee9a8f72b_MIT18_303F14_Lec1.pdf",
      "content": "A few important PDEs\nconstant coefficients = 1\nvariable coefficients = c(x)\nPoisson's equation:\n∇2u = f\n∇⋅c∇u\n(\n) = f\nexample: f = charge density,\nc = permittivity ε\nu = -electric potential\nexample: f = heat source/sink rate\nc = thermal conductivity\nu = steady-state temperature\nexample: f = solute source/sink rate,\nc = diffusion coefficient\nu = steady-state concentration\nexample: f ~ force on stretched string/drum\nc ~ \"springy-ness\"\nu = steady-state displacement\nlace's equation:\n∇u = 0\n∇⋅(c∇u) = 0\nexamples: as for Poisson, but no sources\nt/diffusion equation:\n∂u\n= ∇u\n∂u = ∇⋅(c∇u)\n∂t\n∂t\nexamples: u = temperature\nc = thermal conductivity\nu = solute concentration\nc = diffusion coefficient\nar wave equation:\n∂2u\n= ∇\n2 = ∇u\n∂u\n⋅(c∇u)\n∂t\n∂t\nexamples: u = displacement of stretched string/drum\nc2 = 1 / wave speed\nu = density of gas/fluid\nLap\nHea\nScal\n+ many, many others...\nMaxwell (electromagnetism)\nSchrodinger (quantum mechanics)\nNavier-Stokes / Stokes / Euler (fluids)\nBlack-Scholes (options pricing)\nLame-Navier (linear elastic solids)\nbeam equation (bending thin solid strips)\nadvection-diffusion (diffusion in flows)\nreaction-diffusion (diffusion+chemistry)\nminimal-surface equation (soap films)\nnonlinear wave equation (e.g. solitary ocean waves)\n\n(one of several possible examples of)\n\n18.06\nfinite-dimensional linear algebra\nlinear algebra w/ functions & derivatives\nvector space of\nvector space of real-valued (or complex)\ncolumn vectors x (or ) in\nunknowns:\nn\nn\nx\n(or\n),\nfunctions u(x) [for x in some domain Ω],\nor possibly x(t) [time-dependent]\nor possibly u(x,t) [time-dependent],\n...\nvector space:\npossibly restricted by some boundary conditions\nwe can add, subtract, &\nat the boundary ∂Ω [e.g. u(x) = 0 on ∂Ω]\nmultiply by constants\n...\nwithout leaving the space\npossibly with vector-valued u(x) [vector fields]\nlinear operators:\nmatrices A\nlinear operators on functions A,\n[ Au = function ]\nusing partial derivatives. examples:\nlinearity:\nA u =\n2u [ Laplacian operator ]\nA(αx+ y) =\n∇\nβ\nαAx + βAy\nA u\nu\nA(αu+ v) = αAu\nβ\n+ β\n\n= 3 [ mult. by constant ]\nAv\nA3u |x = a(x) u(x) [ mult. by function ]\nA = 4A1 + A2 + 7A3 [ linear comb. of ops. ]\ndot product\nx y\nx*\n⋅ =\ny = Σi xiyi\ncomplex x:\n\n*\nu(x) ⋅ v(x) = 〈u,v〉 = ???????? [inner product]\nand transpose:\n\nx ⋅ Ay = x*Ay = (Ax)*y\nxT\nT = x*\n→x\n\n∂\n\n= ???\n∂\n〈u,Av\n[= some integral]\nx\n〉 =\n*\n〈A u,v〉\n(A)*\nji [conjugate & swap rows/cols]\nA* = ???????? (= A+\n⇔\nij = A\n⇒\nin physics) [adjoint]\nbasis:\nset of vectors bi with span = whole space\ninf set of functions b\n) with span = whole space\n⇔\nb\ni(x\nany x = Σi ci\ni for some coefficients ci\n[ e.g.\n⇔ any\n\nu(x) = Σi ci bi(x) for some coefficients ci\n... if orthonormal basis, then c = b *x\nFourier series! ]\ni\n\ni\n... if orthonormal basis, then ci = 〈bi, u〉\nlinear equations:\nsolve Ax = b for x\nsolve Au = f for u(x)\nexistence\nAx = b solvable if b in column space of A.\nAu = f solvable if f(x) in col. space (image) of A.\n& uniqueness:\nSolution unique if null space of A = {0},\nSolution unique if null space (kernel) of A = {0},\nor equivalently if eigenvalues of A are = 0.\nor equivalently if eigenvalues of A are = 0.\neigenvalues/vectors: solve Ax = λx for x and λ.\nsolve Au = λu for u(x) [eigenfunction] and λ.\nFor this x, A acts just like a number (λ).\nFor this u, A acts just like a number (λ).\n[e.g. Anx = λnx, eAx = eλx.]\n[e.g. Anu = λnu, eAu = eλu.]\n2 example:\n∂\nkx\n∂x 2 sin(\n) = ( k 2\n-)sin(kx)\ntime-evolution\nsolve dx/dt = Ax for x(0)=b [system of ODEs]\nsolve ∂u/∂t = Au for u(x,0)=f(x)\ninitial-value\nA constant ]\nu(x,\nAt\n⇒ x\n\n= eAt b [ if\n⇒\nt) = e f(x) [ if A constant ]\nproblem:\n... expand b in eigenvectors, mult. each by eλt\n... expand f in eigenfunctions, mult. each by eλt\nreal-symmetric\nA = A*\nA = A* [??????]\nor Hermitian:\n⇒ real λ, orthogonal eigenvectors, diagonalizable\n⇒ real\n\nλ, orthogonal eigenvectors (???)\ndiagonalizable (???)\npositive definite\nA = A*, x*Ax > 0 for any x = 0 / x*Ax ≥ 0\nA = A*, 〈u,Au >0 / ≥0 for u = 0 (???)\n〉\n/ semi-definite:\n*\n⇔ real λ>0/≥0, A=B B for some B\n⇔ real λ>0/≥0, A=ˆB *Bˆ for some ˆB (???)\nimportant fact: -\n∇ is symmetric positive definite or semi-definite!\ninverses:\nA-1 A = A A-1 = 1 [if it exists]\n∂\n-\n\n???\n∂x =\n\nA-1 = ??????\n[...delta functions\n= A-1b\n& Green's functions]\n⇒ Ax=b solved by x\n-1\n⇒\n... some kind of integral?\nAu = f solved by f = A u ???\n(real) orthogonal\nA-1 = A* ⇔ (Ax) ⋅ (Ax) = x ⋅ x for any x\nA-1 = A* ⇔ 〈Au,Au〉 = 〈u,u〉 for any u\nor unitary:\n⇒ |λ|=1, orthogonal eigenvectors, diagonalizable\n⇒ | λ|=1, orthogonal eigenvectors (???)\ndiagonalizable (???)\n18.303\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Linear Partial Differential Equations, Lec 1 Summary",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/92fe67895ebad3ef83866defaa4c6beb_MIT18_303F14_Lecture1.pdf",
      "content": "Lecture 1\nGeneral overview of what a PDE is and why they are important. Discussed examples of some\ntypical and important PDEs (see handout, page 1). With non-constant coefficients (the most\ncommon case in real-world physics and engineering), even the simplest PDEs are rarely solvable\nby hand; even with constant coefficients, only a relative handful of cases are solvable, usually\nhigh-symmetry cases (spheres, cylinders, etc.) solvable. Therefore, although we will solve a few\nsimple cases by hand in 18.303, the emphasis will instead be on two things: learning to think\nabout PDEs by recognizing how their structure relates to concepts from finite-dimensional linear\nalgebra (matrices), and learning to approximate PDEs by actual matrices in order to solve them\non computers.\nWent through 2nd page of handout, comparing a number of concepts in finite-dimensional linear\nalgebra (ala 18.06) with linear PDEs (18.303). The things in the \"18.06\" column of the handout\nshould already be familiar to you (although you may need to review a bit if it's been a while\nsince you took 18.06)--this is the kind of thing I care about from 18.06 for this course, not how\ngood you are at Gaussian elimination or solving 2×2 eigenproblems by hand. The things in the\n\"18.303\" column are perhaps unfamiliar to you, and some of the relationships may not be clear at\nall: what is the dot product of two functions, or the transpose of a derivative, or the inverse of a\nderivative operator? Unraveling and elucidating these relationships will occupy a large part of\nthis course.\nCovered the concept of nondimensionalization: rescaling the units so that dimensionful\nconstants and other odd numbers disappear, making as many things \"1\" as possible. Gave an\nexample of a heat equation κ∇2T = ∂T/∂t in an L×L box in SI units, where we have a thermal\nconductivity κ in m2/s. By rescaling the spatial coordinates to x/L and y/L, and rescaling the time\ncoordinate to κt/L2, we obtained a simplified equation of the form ∇2T = ∂T/∂t in a 1×1 box. Not\nonly does this simplify the equations, but it can also improve our understanding: by rescaling\nwith characteristic times and distances, we are left with distance and time units where 1 is the\ncharacteristic time and distance, and so in these units it is immediately obvious what we should\nconsider \"big\" and \"small\". For example, in the rescaled time units, 0.01 is a small time in which\nprobably not much happens, while 100 is a big time in which the solution has probably changed\na lot. In the original SI units we would have had to explicitly compare to the characteristic time\nL2/κ.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Linear Partial Differential Equations, Lec 2 Handout",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/c3fb398ffd7cce7234931f2b0a93f3d9_MIT18_303F14_sines.pdf",
      "content": "Fourier Sine Series Examples\nMarch 16, 2009\nThe Fourier sine series for a function f(x) defined on x ∈[0,1] writes f(x) as\ninf\nf(x) = ∑bn sin(nπx)\nn=1\nfor some coefficients bn. The key point is that these functions are orthogonal, given the\n\"dot product\" f(x)·g(x) =\nR 1\n0 f(x)g(x)dx. It is a simple calculus exercise to show that\nthe dot product of two sine functions is sin(nπx)·sin(\nR\nmπx) =\n0 sin(nπx)sin(mπx) =\n0 [cos((n-m)πx)-cos((n+m)πx)p]/2, which equals 0 if n =\nR\nm and 1/2 if n = m. [If\nwe divide the sin(nπx) functions by\n1/2, they are orthonormal.] Because of orthog-\nonality, we can compute the bn very simply: for any given m, we integrate both sides\nagainst sin(mπx). In the summation, this gives zero for n = m, and\n0 sin2(mπx) = 1/2\nfor n = m, resulting in the equation\nR\nbm = 2\nZ 1\nf(x) sin(mπx)dx.\nFourier claimed (without proof) in 1822 that any function f(x) can be expanded in\nterms of sines in this way, even discontinuous function! That is, these sine functions\nform an orthogonal basis for \"all\" functions! This turned out to be false for various\nbadly behaved f(x), and controversy over the exact conditions for convergence of the\nFourier series lasted for well over a century, until the question was finally settled by\nCarleson (1966) and Hunt (1968): any function f(x) where\n|f(x)|pdx is finite for\nsome p > 1 has a Fourier series that converges almost everywhere to f(x) [except at\nisolated points]. At points where f(x) has a jump discontinuity\nR\n, the Fourier series\nconverges to the midpoint of the jump. So, as long as one does not care about crazy di-\nvergent functions or the function value exactly at points of discontinuity (which usually\nhas no physical significance), Fourier's remarkable claim is essentially true.\nTo illustrate the convergence of the sine series, let's consider a couple of examples.\nFirst, consider the function f(x) = 1, which seems impossible to expand in sines be-\ncause it is not zero at the endpoints, but nevertheless it works...if you don't care about\nthe value exactly at x = 0 or x = 1. From the formula above, we obtain\nbm = 2\nZ 1\nsin(nπx)dx =\n-nπ cos(nπx)\n\n=\n\nnπ\nn odd\nn even ,\n\nFigure 2: Fourier sine series (blue lines) for the triangle function f(x) = 1\n2 -|x -1\n2|\n(dashed black lines), truncated to a finite number of terms (from 1 to 32), showing that\nthe series indeed converges everywhere to f(x).\nthe region:\n1/2\nbm\n= 2\nf(x) sin(mπx)d = 4\nodd\nZ\nx\nZ\nxsin(mπx)dx =\n(mπ)2 (-1)\nm-1\n2 ,\nwhere for the last step one must do some tedious integration by parts, and thus\nf(x) = π2 sin(πx)-\n(3π)2 sin(3πx)+\nsin(5\n(5π)2\nπx)+···.\nThis is plotted in figure. 2 for 1 to 8 terms--it converges faster than for f(x) = 1\nbecause there are no discontinuities in the function to match, only discontinuities in\nthe derivative.\n\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n1 term\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n2 terms\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n4 terms\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n8 terms\nFigure 2: Fourier sine series (blue lines) for the triangle function f(x) = 1\n2 -|x -1\n2|\n(dashed black lines), truncated to a finite number of terms (from 1 to 32), showing that\nthe series indeed converges everywhere to f(x).\nthe region:\n1/2\nbm\n= 2\nf(x) sin(mπx)d = 4\nodd\nZ\nx\nZ\nxsin(mπx)dx =\n(mπ)2 (-1)\nm-1\n2 ,\nwhere for the last step one must do some tedious integration by parts, and thus\nf(x) = π2 sin(πx)-\n(3π)2 sin(3πx)+\nsin(5\n(5π)2\nπx)+···.\nThis is plotted in figure. 2 for 1 to 8 terms--it converges faster than for f(x) = 1\nbecause there are no discontinuities in the function to match, only discontinuities in\nthe derivative.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Linear Partial Differential Equations, Lec 2 Summary",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/d2d752d0d3370cc8fee328fcf07afd37_MIT18_303F14_Lecture2.pdf",
      "content": "Lecture 2\nStarted with a very simple vector space V of functions: functions u(x) on [0,L] with u(0)=u(L)=0\n(Dirichlet boundary conditions), and with one of the simplest operators: the 1d Laplacian\nA=d2/dx2. Explained how this describes some simple problems like a stretched string, 1d\nelectrostatic problems, and heat flow between two reservoirs.\nInspired by 18.06, we begin by asking what the null space of A is, and we quickly see that it is\n{0}. Thus, any solution to Au=f must be unique. We then ask what the eigenfunctions are, and\nquickly see that they are sin(nπx/L) with eigenvalues -(nπ/L)2. If we can expand functions in this\nbasis, then we can treat A as a number, just like in 18.06, and solve lots of problems easily. Such\nan expansion is precisely a Fourier sine series (see handout).\nIn terms of sine series for f(x), solve Au=f (Poisson's equation) and Au=∂u/∂t with u(x,0)=f(x)\n(heat equation). In the latter case, we immediately see that the solutions are decaying, and that\nthe high-frequency terms decay faster...eventually, no matter how complicated the initial\ncondition, it will eventually be dominated by the smallest-n nonzero term in the series (usually\nn=1). Physically, diffusion processes like this smooth out oscillations, and nonuniformities\neventually decay away. Sketched what the solution looks like in a typical case.\nAs a preview of things to come later, by a simple change to the time-dependence found a\nsolution to the wave equation Au=∂2u/∂t2 from the same sine series, which gives \"wavelike\"\nbehavior. (This is an instance of what we will later call a \"separation of variables\" technique.)\nFurther reading: Section 4.1 of the Strang book (Fourier series and solutions to the heat equation).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Linear Partial Differential Equations, Lec 3 Handout",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/edea18830013f8c73172ae6bc0eeaafb_MIT18_303F14_difference.pdf",
      "content": "18.303 notes on finite differences\nS. G. Johnson\nSeptember 10, 2013\nThe most basic way to approximate a derivative on\na computer is by a difference. In fact, you probably\nlearned the definition of a derivative as being the\nlimit of a difference:\nu(x + ∆x)\n)\nu′(x = lim\n-u(x\n)\n∆x→0\n.\n∆x\nTo get an approximation, all we have to do is to\nremove the limit, instead using a small but non-\ninfinitesimal ∆x.\nIn fact, there are at least three\nobvious variations (these are not the only possibili-\nties) of such a difference formula:\nu(x + ∆x)\nu′(x)\n-u(x)\n≈\n∆x\nforward difference\n≈u(x) -u(x -∆x)\nbackward difference\n∆x\nu(x + ∆x) -u(x -∆x)\n≈\ncenter difference,\n2∆x\nwith all three of course being equivalent in the ∆x →\n0 limit (assuming a continuous derivative). Viewed\nas a numerical method, the key questions are:\n- How big is the error from a nonzero ∆x?\n- How fast does the error vanish as ∆x →0?\n- How do the answers depend on the difference ap-\nproximation, and how can we analyze and design\nthese approximations?\nLet's try these for a simple example: u(x) = sin(x),\ntaking the derivative at x = 1 for a variety of ∆x val-\nues using each of the three difference formulas above.\nThe exact derivative, of course, is u′(1) = cos(1), so\nwe will compute the error |approximation -cos(1)|\n10-8\n10-7\n10-6\n10-5\n10-4\n10-3\n10-2\n10-1\n∆x\n10-18\n10-16\n10-14\n10-12\n10-10\n10-8\n10-6\n10-4\n10-2\nforward\nbackward\ncenter\n2sin(1)∆x\n6cos(1)∆x2\n|error| in derivative\nFigure 1: Error in forward- (blue circles), backward-\n(red stars), and center-difference (green squares) ap-\nproximations for the derivative u′(1) of u(x) = sin(x).\nAlso plotted are the predicted errors (dashed and\nsolid black lines) from a Taylor-series analysis. Note\nthat, for small ∆x, the center-difference accuracy\nceases to decline because rounding errors dominate\n(15-16 significant digits for standard double preci-\nsion).\n\nversus ∆x. This can be done in Julia with the fol-\nlowing commands (which include analytical error es-\ntimates described below):\nx = 1\ndx = logspace(-8,-1,50)\nf = (sin(x+dx) - sin(x)) ./ dx\nb = (sin(x) - sin(x-dx)) ./ dx\nc = (sin(x+dx) - sin(x-dx)) ./ (2*dx)\nusing PyPlot\nloglog(dx, abs(cos(x) - f), \"o\",\nmarkerfacecolor=\"none\",\nmarkeredgecolor=\"b\")\nloglog(dx, abs(cos(x) - b), \"r*\")\nloglog(dx, abs(cos(x) - c), \"gs\")\nloglog(dx, sin(x) * dx/2, \"k--\")\nloglog(dx, cos(x) * dx.^2/6, \"k-\")\nlegend([\"forward\", \"backward\", \"center\",\nL\"$\\frac{1}{2}\\sin(1) \\Delta x$\",\nL\"$\\frac{1}{6}\\cos(1) \\Delta x^2$\"],\n\"lower right\")\nxlabel(L\"$\\Delta x$\")\nylabel(\"|error| in derivative\")\nThe resulting plot is shown in Figure 1. The obvi-\nous conclusion is that the forward- and backward-\ndifference approximations are about the same, but\nthat center differences are dramatically more accu-\nrate--not only is the absolute value of the error\nsmaller for the center differences, but the rate at\nwhich it goes to zero with ∆x is also qualitatively\nfaster. Since this is a log-log plot, a straight line cor-\nresponds to a power law, and the forward/backward-\ndifference errors shrink proportional to ∼∆x, while\nthe center-difference errors shrink proportional to\n∼∆x2! For very small ∆x, the error appears to go\ncrazy--what you are seeing here is simply the effect\nof roundofferrors, which take over at this point be-\ncause the computer rounds every operation to about\n15-16 decimal digits.\nWe can understand this completely by analyzing\nthe differences via Taylor expansions of u(x). Recall\nthat, for small ∆x, we have\n∆x2\nu(x+∆x) ≈u(x)+∆x u′(x)+ 2 u′′(x)+∆x3\n3! u′′′(x)+· · · .\nu(x-∆x) ≈u(x)-∆x u′(x)+∆x2\n2 u′′(x)-∆x3\nu′′′(x)+\n3!\n· · · .\nIf we plug this into the difference formulas, after some\nalgebra we find:\n∆x\nforward difference ≈u′(x)+ 2 u′′(x)+∆x2\nu′′′(x)+\n3!\n· · · ,\n∆x\nbackward difference ≈u′(x)-2 u′′(x)+∆x2\n3! u′′′(x)+· · · ,\ncenter difference ≈u′(x) + ∆x2\nu′′′(x) +\n3!\n· · · .\nFor the forward and backward differences, the error\nin the difference approximation is dominated by the\nu′′(x) term in the Taylor series, which leads to an\nerror that (for small ∆x) scales linearly with ∆x.\nFor the center-difference formula, however, the u′′(x)\nterm cancelled in u(x + ∆x) -u(x -∆x), leaving us\nwith an error dominated by the u′′′(x) term, which\nscales as ∆x2.\nIn fact, we can even quantitatively predict the er-\nror magnitude: it should be about sin(1)∆x/2 for\nthe forward and backward differences, and about\ncos(1)∆x2/6 for the center differences.\nPrecisely\nthese predictions are shown as dotted and solid lines,\nrespectively, in Figure 1, and match the computed er-\nrors almost exactly, until rounding errors take over.\nOf course, these are not the only possible difference\napproximations. If the center difference is devised so\nas to exactly cancel the u′′(x) term, why not also add\nin additional terms to cancel the u′′′(x) term? Pre-\ncisely this strategy can be pursued to obtain higher-\norder difference approximations, at the cost of mak-\ning the differences more expensive to compute [more\nu(x) terms]. Besides computational expense, there\nare several other considerations that can limit one in\npractice. Most notably, practical PDE problems of-\nten contain discontinuities (e.g. think of heat flow or\nwaves with two or more materials), and in the face\nof these discontinuities the Taylor-series approxima-\ntion is no longer correct, breaking the prediction of\nhigh-order accuracy in finite differences.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Linear Partial Differential Equations, Lec 3 Handout",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/86787f1c30f1c5a013c7e75531ba00a6_MIT18_303F14_Julia_cheat.pdf",
      "content": "Julia & IJulia Cheat-sheet (for 18.xxx at MIT)\n\nBasics:\nArithmetic and functions of numbers:\njulialang.org\ndocumentation\n3*4, 7+4, 2-6, 8/3 mult., add, sub., divide numbers\ngithub.com/stevengj/julia-mit\ninstallation & tutorial\ncompute 37\n3^7, 3^(8+2im)\nor 38+2i power\nipython notebook --profile-julia start IJulia browser\n\n-5\nshift-return\n\nexecute input cell in IJulia\nsqrt(-5+0im)\nas a complex number\n\nexp(12)\ne\nnatural log (ln), base-10 log (log )\nDefining/changing variables:\nlog(3), log10(100)\nabs(-5), abs(2+3im) absolute value |-5| or |2+3i|\nx = 3 define variable x to be 3\nsin(5pi/3)\ncompute sin(5π/3)\nx = [1,2,3] array/\"column\"-vector (1,2,3)\nbesselj(2,6) compute Bessel function J2(6)\ny = [1 2 3] 1×3 row-vector (1,2,3)\n\nA = [1 2 3 4; 5 6 7 8; 9 10 11 12]\n--set A to 3×4 matrix with rows 1,2,3,4 etc.\nArithmetic and functions of vectors and matrices:\nx * 3, x + 3\nx[2] = 7\nmultiply/add every element of\n\nchange from (1,2,3) to (1,7,3)\nx by 3\nx\nx + y\nA[2,1] = 0\n\nelement-wise addition of two vectors\n\nchange\nfrom 5 to 0\nx and y\nA2,1\nset u=15.03, v=1.2×10-27\nA*y, A*B\nu, v = (15.03, 1.2e-27)\n\nproduct of matrix A and vector y or matrix B\nx * y\nf(x) = 3x\n\nnot defined for two vectors!\n\ndefine a function f(x)\nx .* y\nx -> 3x\n\nelement-wise product of vectors and\n\nan \"anonymous\" function\nx\ny\n\nevery element of is cubed\n\nx .^ 3\nx\nConstructing a few simple matrices:\ncos(x), cos(A)\ncosine of every element of x or A\nexp(A), expm(A)\nexp of each element of A, matrix exp eA\nrand(12), rand(12,4)\nrandom length-12 vector or 12×4 matrix\n\nconjugate-transpose of vector or matrix\nwith uniform random numbers in [0,1)\nxʹ′, Aʹ′\nthree ways to compute x y\nrandn(12)\nGaussian random numbers (mean 0, std. dev. 1)\nx'*y, dot(x,y), sum(conj(x).*y)\n·\nA \\ b, inv(A)\nreturn solution to Ax=b, or the matrix A-1\neye(5)\n5×5 identity matrix I\n\neigenvals λ and eigenvectors (columns of V) of A\nlinspace(1.2,4.7,100)\n100 equally spaced points from 1.2 to 4.7\nλ, V = eig(A)\n\ndiagm(x)\nmatrix whose diagonal is the entries of x\n\nPlotting (type using PyPlot first)\nPortions of matrices and vectors:\nplot(y), plot(x,y) plot y vs. 0,1,2,3,... or versus x\nnd\nth\nloglog(x,y), semilogx(x,y), semilogy(x,y)\nlog-scale plots\nx[2:12]\nthe 2 to 12 elements of x\n\nset labels\nx[2:end]\nthe 2nd to the last elements of x\ntitle(\"A title\"), xlabel(\"x-axis\"), ylabel(\"foo\")\nlegend([\"curve 1\", \"curve 2\"], \"northwest\")\nlegend at upper-left\n\nrow vector of 1st 3 elements in 5th\n\nA[5,1:3]\nrow of A\n\nrow vector of 5th row of A\ngrid(), axis(\"equal\")\nadd grid lines, use equal x and y scaling\nA[5,:]\ntitle with LaTeX equation\ndiag(A)\nvector of diagonals of A\ntitle(L\"the curve $e^\\sqrt{x}$\")\n\nsavefig(\"fig.png\"), savefig(\"fig.eps\") save as PNG or EPS image\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Linear Partial Differential Equations, Lec 3 Summary",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/cb356fd55e96f5d879271582df709f55_MIT18_303F14_Lecture3.pdf",
      "content": "Lecture 3\nNow, we will go back to the happy land of finite-ness for a while, by learning to approximate a\nPDE by a matrix. This will not only give us a way to compute things we cannot solve by hand,\nbut it will also give us a different perspective on certain properties of the solutions that may\nmake certain abstract concepts of the PDE clearer. We begin with one of the simplest numerical\nmethods: we replace the continuous space by a grid, the function by the values on a grid, and\nderivatives by differences on the grid. This is called a finite-difference method.\nWent over the basic concepts and accuracy of approximating derivatives by differences; see\nhandout.\nArmed with center differences (see handout), went about approximating the 1d Laplacian\noperator d2/dx2 by a matrix, resulting in a famous tridiagonal matrix known as a discrete\nLaplacian. The properties of this matrix will mirror many properties of the underlying PDE, but\nin a more familiar context. We already see by inspection that it is real-symmetric, and hence we\nmust have real eigenvalues, diagonalizability, and orthogonal eigenvectors--much as we\nobserved for the d2/dx2 operator--and in the next lecture we will show that the eigenvalues are\nnegative, i.e. that the matrix is negative-definite.\nThe negative eigenvalues mean that the discrete Laplacian is negative definite, and also suggest\nthat it can be written in the form -DTD for some D. Reviewed the proof that this means the\nmatrix is negative definite, which also relies on D being full column rank.\nFurther reading: notes on finite-difference approximations from 18.330. See the matrix K\nsection 1.1 (\"Four special matrices\") of the Strang book, and in general chapter 1 of that book.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Linear Partial Differential Equations, Lec 4 Summary",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/8c06d9a3e2a80741dfb13c87ffaf6efd_MIT18_303F14_Lecture4.pdf",
      "content": "Lecture 4\nShowed that our A indeed has this -DTD form and hence is negative-definite: we derived the\ndiscrete Laplacian by turning two derivatives into differences, one by one, and now by writing\nthe first step as a matrix we get D, while writing the second step as a matrix shows that it is -DT.\nTo get a negative definite matrix (as opposed to just negative semidefinite), we additionally\nrequire that D be full column rank; showed that this is easy to see from DT since it is upper-\ntriangular.\nTo do a similar analysis of the actual Laplacian, we first have to have a dot product, or inner\nproduct. Defined an abstract ⟨u,v⟩ notation (a map from functions u and v to scalars ⟨u,v⟩) for\ninner products, as well as three key properties. First, ⟨u,v⟩ = complex conjugate of ⟨v,u⟩. Second,\n|u|2=⟨u,u⟩ must be nonnegative, and zero only if u=0. Third, it must be linear:\n⟨u,αv+βw⟩=α⟨u,v⟩+β⟨u,w⟩. (Note: some textbooks, especially in functional analysis, put the\nconjugation on the second argument instead of the first.) For functions, the most common inner\nproduct (though not the only choice and not always the best choice, as we will see next time) is a\nsimple integral ∫uv (conjugating u for complex functions); we will look at this more next time.\nReviewed inner products of functions. A vector space with an inner product (plus a technical\ncriterion called \"completeness\" that is almost always satisfied in practice) is called a Hilbert space. Note that\nwe include only functions with finite norm ⟨u,u⟩ in the Hilbert space (i.e. we consider only\nsquare-integrable functions), which throws out a lot of divergent functions and means that\neverything has a convergent Fourier series. (Another omitted technicality: we have to ignore finite\ndiscrepancies at isolated points, or otherwise you can have ⟨u,u⟩=0 for u(x) nonzero; there is a rigorous way to do\nthis, which we will come back to later.)\nDefined the adjoint A* of a linear operator: whatever we have to do to move it from one side of\nthe inner product to the other, i.e. whatever A* satisfies ⟨u,Av⟩=⟨A*u,v⟩ for all u,v. (Omitted\ntechicality: we must further restrict ourselves to functions that are sufficiently differentiable that ⟨u,Au⟩ is finite,\nwhich is called a Sobolev space for this A, a subset of the Hilbert space.) For matrices and ordinary vector\ndot products, this is equivalent to the \"swap rows and columns\" definition. For differential\noperators, it corresponds to integration by parts, and depends on the boundary conditions as well\nas on the operator and on the inner product.\nShowed that with u(0)=u(L)=0 boundary conditions and this inner product, (d2/dx2)T is real-\nsymmetric (also called \"Hermitian\" or \"self-adjoint\"). [There is an omitted technicality here: technically,\nwe have only showed that the operator is symmetric. To show that it is Hermitian, we must also show that the\nadjoint has the same domain in the Hilbert space. Mostly we can avoid this technical distinction in real applications;\nit doesn't arise explicitly in the proofs here.]\nNot only that, but next time we will show that d2/dx2 is negative-definite on this space, since\n⟨u,u''⟩=-∫|u'|2, and u'=0 only if u=constant=0 with these boundary conditions.\nShowed that the proof of real eigenvalues from 18.06 carries over without modification for\nHermitian operators; similarly for the proof of orthogonal eigenvectors, hence the orthogonality\nof the Fourier sine series. Similarly for the proof of negative eigenvalues.\n\nSo, many of the key properties of d2/dx2 follow \"by inspection\" once you learn how to transpose\noperators (integrate by parts). And this immediately tells us key properties of the solutions, if we\nassume the spectral theorem: Poisson's equation has a unique solution, the diffusion equation has\ndecaying solutions, and the wave equation has oscillating solutions.\nFurther reading: Notes on function spaces, Hermitian operators, and Fourier series that I once\nwrote for 18.06 (slightly different notation). Textbook, section 3.1: transpose of a derivative. The\ngeneral topic of linear algebra for functions leads to a subject called functional analysis; a\nrigorous introduction to functional analysis can be found in, for example, the book Basic Classes\nof Linear Operators by Gohberg et al. There are some technicalities that I omit: a differential\noperator is only called \"self-adjoint\" if it is equal to its adjoint and is \"densely defined\", and\nshowing that an operator equals its adjoint furthermore requires an extra step of showing that A\nand A* act on the same domains.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Linear Partial Differential Equations, Lec 5 Summary",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/2300d93d8e2ec4e32dd579ba45d59bc5_MIT18_303F14_Lecture5.pdf",
      "content": "Lecture 5\nFinished negative-definiteness proof from previous lecture.\nDiscussed diagonalizability of infinite-dimensional Hermitian operators. Unlike the proof of real\neigenvalues, etcetera, we cannot simply repeat the proof from the matrix case (where one can\nproceed by induction on the dimension). In practice, however, real-symmetric operators arising\nfrom physical systems are almost always diagonalizable; the precise conditions for this lead to\nthe \"spectral theorem\" of functional analysis.) (One hand-wavy argument: all physical PDEs can\napparently be simulated by a sufficiently powerful computer to any desired accuracy, in\nprinciple. Since the discrete approximation is diagonalizable, and converges to the continuous\nsolution, it would be surprising if the eigenfunctions of the continous problem \"missed\" some\nsolution. In fact, all the counter-examples of self-adjoint operators that lack a spectral theorem\nseem to involve unphysical solutions that oscillate infinitely fast as they approach some point,\nand hence cannot be captured by any discrete approximation no matter how fine.) In 18.303, we\nwill typically just assume that that all functions of interest lie in the span of the eigenfunctions,\nand focus on the consequences of this assumption.\nShowed how this immediately tells us key properties of the solutions, if we assume the spectral\ntheorem: Poisson's equation has a unique solution, the diffusion equation has decaying solutions\n(with larger eigenvalues = faster oscillations = decaying faster, making the solution smoother\nover time), and the wave equation has oscillating solutions.\nNot only do we now understand d2/dx2 at a much deeper level, but you can obtain the same\ninsights for many operators that cannot be solved analytically. For example, showed that the\noperator d/dx [c(x) d/dx], which is the 1d Laplacian operator for a non-uniform \"medium\", is\nalso real-symmetric positive definite if c(x)>0, given the same u(0)=u(L)=0 boundary conditions.\nAs another example, considered the operator c(x)d2/dx2 for real c(x)>0. This is not self-adjoint\nunder the usual inner product, but is self-adjoint if we use the modified inner product ⟨u,v⟩=∫uv/c\nwith a \"weight\" 1/c(x). (This modified inner product satisfies all of our required inner-product\nproperties for positive c(x).) Therefore, c(x)d2/dx2 indeed has real, negative eigenvalues, and has\neigenfunctions that are orthogonal under this new inner product. Later on, we will see more\nexamples of how sometimes you have to change the inner product in order to understand the self-\nadjointness of A.\nFortunately, it's usually pretty obvious how to change the inner product, typically some simple\nweighting factor that falls out of the definition of A. (In fact, for matrices, it turns out that every\ndiagonalizable matrix with real eigenvalues is Hermitian under some modified inner product. I\ndidn't prove this, however.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Linear Partial Differential Equations, Lec 6 Handout",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/fb4bc411e5283ba8d642338a2ce2017b_MIT18_303F14_Lec6.pdf",
      "content": "WI.\n~ v;e.vle!\nf~ e..\nhcft-,) ~e-I et\n<..s\n\"'-\"'\"V\"rd\nC./JhAv..\",M\ne 1.-\"\\\"h',,'V\n~ Y1OI/\ni/e vI,'11 J. ~\n~+W-f-\npi {+~ t4(\nJ/)(re,e\nt?('v~ iL\"1\n-I\nl,.\\fI\nU!1+\\\ntJf-'?\n----7\nc~\"\"j:! ,\"\", (UI;,+L\n1\"0-1\nL\n1\"0.1\n~\n~\n(rtoohi)\nl;\\VI\n,\nk lLA..,+-\\~lA.\",)\nk CIA\" -\nlA,,_\\~\nL.A\"\n~\n\"F.\n\"\n~\nntl\n\"\n'-\nF\" _1-\nk tnt\\-\")... lII\"\n\"\n+- 01,,_ \\\n)\n~ \\\n( I\" %\n); ~ '\" ~ '\"i rLo\"l r f~~ CJ>(1. !\n/V\ntL)(.l.\nMore Srsk \",,<,A' (c, / If\n(l}\n\"\nI\"\nse)\n~~~L\n(5\n~~\"'<1.\nk\n( ~ \"~~( \"'~\nVI.., f;,. )\n(I i) St-f\n'let\nt;:r((\n0-0\"'1\nJ'J-kre., w\n/vi\n'F\"tJ )\n..\n\nF1\n~\n.\n0')\nI\"\n-I\nFlJtl\nL\n~\n.I\nF\n~~~\",pu~vrl.J )\n...:>\nF\n-I\n,\n-I 1\n-I\n- -\n-I\n\\\n-I\n,\n,\n\"\n-4\n~lI\\\n. ,\n:: \"A ~\n~\nriU;I -S; r \"'I1Yl e.J.n c\nfi'<G c.JNe\nJe..hl.-, Ik\n::: ~ .A;<1. (-~ ,~~ \"I -', _\n. I\n-'L 1\n1 -'L\n\"'Xl...\n~\n:;\",Me.. d; s(~k\nl.\"pL;c. ;\"\"\n\n-\nA\n,,\nOr+~I\"drMJ :\n->Jl:--'>\nLI\" lA '\"\n::\n)\" 0 nj.tV\n0'\\\"-'\"\neXp <>1.11 J ~(+)\nI'\"\nt~[j 4c.s;.:,:\nN\n~ ( ~) -\n~ ~ L+')\n~V1\n~\nSO~e\nc.oeJ~~c ieFJ+S\nC'1 Lf-')::\n(;~If\n~ (+')\nby\n.r+~o'lo(W!<;li-!y\n. ,\n..:;\nAlA\nVI\n::.\n..\nvJ~e. ~\n>0 ......<:. coeMc,'(r.J\n.1\nr;;j1(\n\"'-\" ) /\"'/\\\ndQ.-l-er,vl;I/€~\n0y\n(I) ih'\" \\ Co(lC )·Ab\".J\n.--\n~(O):: 2: W~f'l ~/\\\n-\n3 (3,,=- -v..,.:\"(;..{O)\no\n\n=.?;l\n&/) e\nI'lO <k)\n01 Co\nW\"\n~ J+'-I:::\n\"\"\n------7\n-7\n(\",,\"ov,'''..'>\ntvje.r-~r')\n----::;>\nE-\nA:::. k (-<- \\ \\\n/VI\n'-l.. J\nw = IK\n\\\n'I! ~\n(\"\"\"\"'~\n0pf>J::'; k ')\n.( vJ,-:: J:?~\n~::J(~)\n-:;:>\n~\n--=:.>\nWI\n~ (). 7b5\" IT\nI\nl..\nI\n.(\n/'\n-\\\n~ ~ I (I)\n-\nE\n1\",\\, I)(.\n' . '-/ I,-{ J ~\nIA 1.. -\n-It.\n...o(\n~\nc;:\n<\n'-'\n-\"')\nC\niN\n~\n~ :: ~ C~)\nI. 'lLf 9 ~\n:>\nt-\\\n\n-\nII Ie.f\nfY\\ =- ,? .6 X\n'I\ndW;';'y\n(w IVi..5r~ )\n,\nI\n-\nbhort-c r\\ ,'(1.5\n~pri ~.r\n1(\\ Cru..sc.J\nk\n~\n(~\ni::. \\\nl<:. '--\n::.\n(-1<,' *,S\n- -\n....:>\n.-:>.\n-\"\nc..\n- -\nk L:.X' ))-0 \\A\n-\n-\n1T 1)\n\\1\\\n::::=3>\nV\\\nM\nP\n(- t;'D\n~ )7.)\n')\n~ )\\,-1\",+1\n+- c\no2..\\A,(x)+)\n--\n~J'.'\nAX\"'> 0\nof\nr>\n() )('\n,\nsc\",kr\nWe-lie\ne...r..'\"c... fl' 0'\"\ns:><J Vld.s:\nq\nLA\n,\nlA(O)t)\nI\n::lAk+)=-O\nf'.A\n;;s;:/ I\nI\n~ r\n-~/Vll-k\n..J.\nse-lf-c;J)o\\~f\n~r lASV' \" I\n<\ne.t A./)\nSv: vi\n:0\nl\n~\n~\",I, 1\\< 0\n~ o>(I'I/c, il\\;lj s\"ls\n\",~tl\ne--J,,:= ~\n\n'\" ) k\nIi\n....::,\n~ (t')\nF\n~\n--\nK b VI\nC:X\n1\\\n!:::tIl.. blL\nCtJ+-1 '»< N+I )\nditA.J 0\" t:, I fVt ... .J!\\X;\nKIJ,J.J\n'-\n~+\nk's\n-.\n-I\n-\"\nlL ~ ')\n=: - M\nh,/,L. b' kD\n..>\n-\n4~\nVI.\n\\;\\\n\n~\n-::.\nN;< rJ d j'lJ~n<-.1\nCv~ --- I J\nof\nIS\n/\"'/J\n\"''' J.r-; )(\n~\"\"\nA - - £>x\nL tJ\\\n-I b\nT\n1< b\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}