{
  "course_name": "Mathematical Methods for Engineers II",
  "course_description": "No description found.",
  "topics": [
    "Engineering",
    "Systems Engineering",
    "Computational Science and Engineering",
    "Mathematics",
    "Applied Mathematics",
    "Differential Equations",
    "Linear Algebra",
    "Engineering",
    "Systems Engineering",
    "Computational Science and Engineering",
    "Mathematics",
    "Applied Mathematics",
    "Differential Equations",
    "Linear Algebra"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nPrerequisites\n\nCalculus (18.02), Differential Equations (\n18.03\n) or Honors Differential Equations (\n18.034\n).\n\nTextbooks\n\nThis course as taught during the Spring 2006 term on the MIT campus used the following text:\n\nStrang, Gilbert.\nIntroduction to Applied Mathematics.\nWellesley, MA:\nWellesley-Cambridge Press\n, 1986. ISBN: 9780961408800. (\nTable of Contents\n)\n\nSince that time, Professor Strang has published a new textbook that is being used for this course as it is currently taught on the MIT campus, as well as for Mathematical Methods for Engineers I (18.085). Information about the new book can be found at the\nWellesley-Cambridge Press\nWeb site, along with a link to Prof. Strang's new \"Computational Science and Engineering\" Web page developed as a resource for everyone learning and doing Computational Science and Engineering.\n\nStrang, Gilbert.\nComputational Science and Engineering\n. Wellesley, MA:\nWellesley-Cambridge Press\n, 2007. ISBN: 9780961408817.\n\nDescription\n\nThis course has two major topics:\n\nInitial Value Problems\n\nLinear: Wave Equation, Heat Equation, Convection Equation\n\nNonlinear: Conservation Laws, Navier-Stokes Equation\n\nFinite Difference Methods: Accuracy and Stability\n\nLax Equivalence Theorem: CFL and Von Neumann Conditions\n\nFourier Analysis: Diffusion, Dissipation, Dispersion\n\nSeparation of Variables and Spectral Methods\n\nSolution of Large Linear Systems\n\nFinite Differences, Finite Elements, Optimization\n\nDirect Methods: Reordering by Minimum Degree\n\nIterative Methods and Preconditioning\n\nSimple Iteration (Jacobi, Gauss-Seidel, Incomplete LU)\n\nKrylov Methods: Arnoldi Orthogonalization\n\nConjugate Gradients and GMRES\n\nMultigrid Methods\n\nInverse Problems and Regularization\n\nRequirements\n\nThere are no exams in 18.086. Two computational projects take their place, one on each of the major topics in the course. The projects are chosen by each student and they include a brief report.",
  "files": [
    {
      "category": "Resource",
      "title": "josephkovacpro.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/12c176f3c3db7c3d8f72c9dcad3e9390_josephkovacpro.pdf",
      "content": "Joseph Kovac\n18.086 Final Project\nSpring 2005\nProf. Gilbert Strang\n\nThe Fundamentals and Advantages of Multi-grid Techniques\n\nIntroduction\n\nThe finite difference method represents a highly straightforward and logical\napproach to approximating continuous problems using discrete methods. At its heart is a\nsimple idea: substitute finite, discrete differences for derivatives in some way appropriate\nfor a given problem, make the time and space steps the right size, run the difference\nmethod, and get an approximation of the answer.\n\nMany of these finite difference methods can ultimately be written in a matrix\nform, with a finite difference matrix multiplying a vector of unknowns to equal a known\nquantity or source term. In this paper, we will be examining the problem Au=f, where A\nrepresents a finite difference matrix operating on u, a vector of unknowns, and f\nrepresents a time-independent vector of source terms. While this is a general problem,\nwe will specifically examine the case where A is the finite difference approximation to\nthe centered second derivative. We will examine solutions arising when f is zero\n(Laplace's equation) and when it is nonzero (Poisson's equation).\n\nThe discussion would be quite straightforward if we wanted it to be; to find u, we\nwould simply need to multiply both sides of the equation by A-1, explicitly finding\nu= A-1f. While straightforward, this method becomes highly impractical as the mesh\nbecomes fine and A becomes large, requiring inversion of an impractically large matrix.\nThis is especially true for the 2D and 3D finite difference matrices, whose dimensions\ngrow as the square and cube of the length of one edge of the square grid.\n\nIt is for this reason that relaxation methods became both popular and necessary.\nMany times in engineering applications, getting the exact answer is not necessary; getting\nthe answer right to within a certain percentage of the actual answer is often good enough.\nTo this end, relaxation methods allow us to take steps toward the right answer. The\nadvantage here is that we can take a few iterations toward the answer, see if the answer is\ngood enough, and if it is not, iterate until it is. Oftentimes, using such an approach,\ngetting an answer \"good enough\" could be done with orders of magnitude less time and\ncomputational energy than with an exact method.\n\nHowever, relaxation methods are not without their tradeoffs. As will be shown,\nthe error between the actual answer and the last iteration's answer ultimately will decay\nto zero. However, not all frequency components of the error will get to zero at the same\nrate. Some error modes will get there faster than others. What we seek is to make all the\nerror components get to zero as fast as possible by compensating for this difference in\ndecay rates. This is the essence of multi-grid; multi-grid seeks to allow the error modes\nof the solution to decay as quickly as possible by changing the resolution of the grid to\nlet the error decay properties of the grid be an advantage rather than a liability.\n\nBasic Theory of the Jacobi Relaxation Method\n\nBefore going into the theory of the method, I first want to state that much of the\nfollowing closely comes from an explanation in A Multi-grid Tutorial by William Briggs\net al. This text explained the material as clearly and concisely as one could hope for. To\na large extent, much of the \"theory section\" following will be reiteration of their\nexplanation, but with emphasis on concepts which will be validated in the numerical\nexperiments later. In no way do I claim these derivations as my own. The following is a\nderivation of the Jacobi method in matrix form, which is the relaxation method which\nwill be used for the rest of the paper.\n\nWe can first express the matrix A as a sum of its diagonal component D and lower\nand upper triangular components L and U:\n\nU\nL\nD\n+\n+\n=\nA\n(1)\n\nso\n\nf\nu\nU\nL\nD\n=\n+\n+\n)\n(\n(2)\n\nWe can move the upper and lower triangular parts to the right side:\n\nf\nu\nU\nL\nDu\n+\n+\n-\n=\n)\n(\n(3)\n\nWe can then multiply both sides by D-1:\n\n)\n)\n(\n(\nf\nu\nU\nL\nD\nu\n+\n+\n-\n=\n-\n(4)\n\nWe can define\n\n)\n(\nU\nL\nD\nRJ\n+\n-\n=\n-\n(5)\n\nTherefore, we have defined the iteration in matrix form, and can write, in the notation of\nBriggs's chapter in Multi-grid Methods:\n\nf\nD\nu\nR\nu\nJ\n)\n(\n)\n(\n-\n+\n=\n(6)\n\nWeighed Jacobi takes a fraction of the previous iteration and adds it to a fraction of the\nprevious iteration with the Jacobi iteration applied:\n\nf\nD\nu\nR\nI\nu\nJ\n)\n(\n)\n1(\n]\n)\n[(\n-\n+\n+\n-\n=\nω\nω\nω\n(7)\n\nWe can rewrite the above as\n\nf\nD\nu\nR\nu\n)\n(\n)\n1(\n-\n+\n=\nω\nω\n(8)\n\nwhere\n]\n)\n[(\nJ\nR\nI\nR\nω\nω\nω\n+\n-\n=\n(9)\n\nThis iteration and its characteristics on the grid is the focus of this paper. Before\nattempting to implement this method, it is first good to predict the behavior we expect to\nsee theoretically. One way to do this is to look at the eigenvalues of the matrix Rω. The\nfollowing again stems from Briggs, but some of the following was not explicitly\nexplained and left as an \"exercise\" in the text.\n\nWe first note that, by the properties of eigenvalues and by eq. 9,\n\nRJ\nR\nωλ\nω\nλ ω\n+\n-\n=\n)\n1(\n(10)\n\nTherefore, we first need to find λRJ. We observe that:\n\nI\nA\nU\nL\n-\n=\n+\n(11)\n\nTherefore,\n\n-\n=\n+\nA\nU\nL\nλ\nλ\n(12)\n\nNoting that, for the 1D case,\n\nI\nD\n1 =\n-\n(13)\n\nSo, using eq. 5 and properties of eigenvalues,\n\n)\n(\n+\n-\n=\n-\n-\n=\nA\nA\nRJ\nλ\nλ\nλ\n(14)\n\nTherefore, remembering eq. 10,\n\nA\nR\nωλ\nλ ω\n-\n=\n(15)\n\nThe kth eigenvalue of the matrix A is:\n\n),\n(\nsin\n)\n(\n-\n≤\n≤\n=\nn\nk\nn\nk\nA\nk\nπ\nλ\n(16)\n\nSo, by eq. 15, the eigenvalues λω are:\n\n1,\nsin\n)\n(\n-\n≤\n≤\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n-\n=\nn\nk\nn\nk\nR\nk\nπ\nω\nλ\nω\n(17)\n\nAnd the jth component of the kth eigenvector is:\n\nn\nj\nn\nk\nn\njk\nj\nk\n≤\n≤\n-\n≤\n≤\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n=\n0,1\n1,\nsin\n,\nπ\nω\n(18)\n\nWe can make two quick observations here. First, for ω between 0 and 1, the\neigenvalues will always lie between -1 and 1, implying stability to the iteration. Second,\nwe remember that all vectors in the space of the matrix A can be represented as a\nweighed sum of the eigenvectors:\n\n∑\n-\n=\n=\n)\n(\nn\nk\nk\nkc\nu\nω (19)\n\nIn this case, since the eigenvectors are Fourier modes, there is an additional useful\ninterpretation of the weighting coefficients ck of the linear combination of eigenvectors;\nthese are analogous to the Fourier series coefficients in a periodic replication of the\nvector u. The other key point to see here is that varying the value of ω allows us to adjust\nhow the eigenvalues of A vary with the frequency of the Fourier modes. Plotted below is\nthe eigenvalue magnitude versus k, for n=32. We can easily see that varying ω\nsignificantly changes the relative eigenvalue magnitude at various frequencies.\n\nFigure 1: Distribution of eigenvalue magnitude as ω is varied\n\nThe implications of the graph above manifest themselves when we think of the\nhomogenous case of Au=0. If we were to use the Jacobi iteration in this case, and started\nfrom a vector as our \"guess\" at the final answer:\n\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n=\nn\njk\nu\nπ\nsin\n)\n(\n(20)\n\nwhere k is between 1 and n-1, we would have an error made of only one mode, i.e. the\nerror would lie perfectly along a single eigenvector of the matrix A and ck would be zero\nfor all k except the k which the vector u lay along. A priori, in this simple case, we know\nthat the solution should converge to 0 with enough steps, as there are no source or sink\nterms.\n\nIn practical situations, we won't know the correct u, and the error will not be\nsimply along one eigenvector. Now, the importance of Figure 1 becomes clear: adjusting\nω allows us to decide which error frequencies on the grid we want to decay quickly\nrelative to others. Picking the correct ω is somewhat application specific. In Briggs'\nexample, he picks to have the eigenvalue magnitudes of the middle frequency and highest\nfrequency match, so that the grid we work on will be decidedly favored towards either\ndecay of high frequency modes or low frequency modes. The motive for this choice will\nbecome apparent later. For this condition, ω=2/3. This value of ω will be used\nthroughout the numerical experiments.\n\nBasic Theory of Multi-grid\n\nThere is an obvious disadvantage to the relaxation method so far: while high\nfrequency error components can quickly decay, the eigenvalues of lower frequency\ncomponents approach 1, meaning that these lower frequency error components take many\nmore iterations to be damped out than higher frequencies. The essence of multi-grid is to\nuse this feature to our advantage rather than to our detriment. What if we were to\nsomehow take a few iterations to first smooth out the high-frequency components on the\nfine grid, then downsample the problem onto a coarser grid where the lower frequency\ncomponents would decay faster, then somehow go back up to the fine grid?\n\nFirst, consider what happens to the kth mode when downsampled onto a grid half\nas fine as the original vector (i.e. downsampling by a factor of 2). The kth mode on the\nfine grid becomes the kth mode on the coarse grid. This also implies that the \"more\noscillatory\" modes on the fine grid become aliased on the coarse grid. A rigorous\nargument complete with Fourier series plots could be made here, but that is not the point.\nThe implication is that now the error that refused to decay quickly on the fine grid has\nbeen frequency-shifted so that it has become high-frequency error on the coarse grid and\nwill decay quickly.\n\nAll that is left to do is to define what it means to move from a fine grid to a coarse\ngrid and eventually come back again, and how to correctly state the problem so that the\nanswer achieved is accurate. First, a few basic relationships need to be established.\nAgain, this is not original thought, and closely follows the Briggs text. First, the\nalgebraic error of the current iteration is defined as\n\n)\n(\n)\n(\nn\nn\nu\nu\ne\n-\n=\n(21)\n\nwhere u is the correct value of u, and u(n) is the resulting approximation after n steps.\n\nThe residual is defined as the amount by which the current guess at u(n) fails to satisfy\nAu=f:\n\n)\n(\n)\n(\nn\nn\nu\nf\nr\nA\n-\n=\n(22)\n\nGiven these relationships, we can also state that\n\n)\n(\n)\n(\nn\nn\nr\ne\n=\nA\n(23)\n\nThis fact lets us make the following statement about relaxation, as quoted from Briggs:\n\n\"Relaxation on the original equation Au=f with an arbitrary initial guess v is equivalent to\nrelaxing on the residual equation Ae=r with the specific initial guess e=0.\"\n\nThis makes intuitive sense by eqs. 21-23: We don't know the error, but we know\nthat the error will be zero when the residual is zero. Therefore, we can either iterate to\nsolve Au=f or we can ask, what would the error vector have to be to yield the current\nresidual? If we know the error, we know the answer by simple rearrangement of eq. 21.\nIn more mathematical terms, what the above statements are saying is the\nfollowing: if we take a few iterations to get the current value of r, we could then\nreformulate the problem by taking that value of r, then solving the new problem Ae=r\nusing Jacobi iteration, and read off the value of e after a few iterations. This will give us\na guess at what the error was before the problem was restated. Rearrangement of eq. 21\nwould then imply that if we just added the calculated value of e to the u(n) we had before\nrestating the problem, we would get a refined guess at the true vector u.\n\nPutting this fact together with the idea of moving from grid to grid, we can\ncombine the overall idea into the following:\n\n1) Relax the problem for a few steps on the fine grid with Au=f\n2) Calculate the residual r=f-Au(n)\n3) Downsample the residual onto a coarser grid\n4) Relax on Ae=r for a number of steps, starting with a guess of e=0\n5) Upsample and interpolate the resulting e onto the fine grid\n6) Refine our guess at u by adding e on the fine grid to the original value of u(n)\n\nThe above method is the central theory of multi-grid and variations of it will show\nthat there are significant gains to be made by changing the grid.\n\nImplementing a Multi-grid Solver - 1D\n\nUp to this point, the paper has mostly been a reiteration and thorough explanation\nof the Briggs text, specifically highlighting points which will be of importance later. At\n\nthis point, however, the subtleties and difficulties of actually implementing a multi-grid\nsolver arise, and while a few of the upcoming points were explained in the Briggs text,\nmuch of the actual implementation required original struggling on my part. It was quite\ndifficult despite the clarity of the theoretical basis of multi-grid. I also consulted with\ntwo students in Prof. Jacob White's group to help me think about aspects of boundary\nconditions more clearly.\n\nIn the 1-D case, I sought to implement as simple of a solver as possible; I was far\nmore interested in developing a more feature-rich 2D solver. Therefore, in the 1-D case,\nI developed a solver which would solve Laplace's equation only, and with zero boundary\nconditions. In other words, I wanted to solve only the homogenous case to demonstrate\nthat the error decays faster on the fine grid for high frequencies versus low frequencies,\nand that an inter-grid transfer would make error decay faster.\n\nSince I only needed to deal with zero boundary conditions in this case, I was able\nto use the standard, second finite difference matrix with zero boundary conditions from\nclass. To demonstrate the multi-grid method, I designed one solver and its associated\nfinite difference matrix for a 16 point grid problem, and another which would operate on\nan 8 point grid. The finite difference method was the standard one from class.\n\nThe inter-grid transfers between the fine and coarse grids were the trickier parts.\nBriggs implements downsampling from the fine grid to the coarse grid by the following\n\"full weighting\" definition:\n\n(\n)\n-\n≤\n≤\n+\n+\n=\n+\n-\nn\nj\nv\nv\nv\nv\nh\nj\nh\nj\nh\nj\nh\nj\n(24)\n\nFor a vector 7 components long, this operation can be implemented by a\nmultiplication by the following matrix:\n\n1 2 1 0 0 0 0\n\n1⁄4 *\n0 0 1 2 1 0 0 (25)\n\n0 0 0 0 1 2 1\n\nSuch a matrix would move the vector from a grid of seven points to a grid of three\npoints. This takes care of the coarsening operation; a scaled transpose of this matrix\nperforms linear interpolation, and allows us to transfer data from the coarse grid to the\nfine grid. That fact is the primary motivation for using the full weighting method rather\nthan simply downsampling by taking every other point from the fine grid.\nUnfortunately, practical, non-ideal interpolators will also introduce error through\nthe interpolation; this error will need to be smoothed out by relaxing again on the fine\ngrid as it will likely have some higher-frequency components in the interpolation error\nbest smoothed by the finer grid. If one transposes the above matrix and scales it by 2, the\nlinear interpolation scheme would be realized.\nAs stated before, I only sought to confirm the idea that the higher frequency error\nwill decay faster on the fine grid than the low frequency error. In the graph below, I\ndefined the initial \"guess\" as the sum of a low frequency (discrete frequency π/8) and a\nhigher frequency (discrete frequency 15π/16). It is obvious that the high frequency\ncomponent decays much faster than the low frequency component.\n\nFigure 2: High-frequency component of error decays faster than low frequency component\n\nThe only other thing left to confirm in the 1D case was that a multi-grid approach\nshowed some promise of benefit. To demonstrate this, I used the same initial function\nand compared a relaxation of thirty steps on the fine grid with a relaxation of ten steps on\nthe fine grid, ten on the coarser grid, and ten more to smooth out interpolation error at the\nend on the fine grid, giving both approaches the same total number of steps. The results\nfor the single grid approach versus the multi-grid approach are shown below.\n\nFigure 3: The advantage of the grid transfer quickly becomes apparent\nI must qualify the above plot with the following information. There was a bit of a\ndiscrepancy with the definition of h in the finite difference method (i.e. the 1/h2 term in\nfront of the matrix K). Intuitively, as the grid coarsens, h should change. This change\nwas necessary and gave the best results in the 2D case. However, in the 1D case I had to\ntweak this factor a bit; I had to multiply the proper K on the coarse grid by 4 to get the\n\nexpected advantage working with the grid transfer. I couldn't find the source of the\ndiscrepancy, and it might be a subtlety that I missed somewhere. Nonetheless, even with\nthis mysterious \"gain factor,\" the above experiment proves that faster convergence to the\nzero error state can happen with a grid transfer rather than simply staying on the fine grid\nfor all steps.\n\nImplementing a Multi-grid Solver - 2D\n\nThe 2D case shares a number of similarities with the 1D case, but it carries a\nnumber of subtleties with it that make implementation of the method significantly more\ndifficult than the 1D case. The most difficult aspect to attack was getting the boundary\nconditions right. I decided that I would stick to Dirichlet boundary conditions for this\nproject, as their implementation was significant work, let alone think about Neumann\nconditions.\n\nThe 1D case was implemented minimally, only thoroughly enough to demonstrate\nthe relative rates at which the different modes of the error in the homogenous case\ndecayed and that grid transfers showed a hint of promise. In the 2D case, I wanted to\nimplement a more useful and practical solver. Specifically, I wanted to be able to specify\nDirichlet boundaries, source terms in the grid, and boundaries within the grid. In the\nelectrostatics case, this would be like saying that I wanted to be able to specify the\nboundary voltages of my simulation grid, any charge source in the medium, and the\nvoltages of any electrodes existing internal to the grid.\n\nSpecifying charge sources is very easy: just specify them in f. However,\nspecifying boundary conditions is more difficult. I decided to incorporate the boundary\nvalues by altering both the matrix A and the right-hand side f. As we learned, the 2D\nfinite difference matrix generally has the following form:\n\nFigure 4:The K matrices from Prof. Strang's new text\n\nIn order to properly implement the boundary condition, we must remember the\nequations underlying the K2D matrix: we are simply solving an N2 by N2 system of linear\nequations. Therefore, if we fix u(p)=b for some value p and constant b, this means that in\nour system of linear equations, whenever the value u(p) shows up in one of the\nsimultaneous equations, its value must be b. The way to accomplish this is simple; we\nmust alter Au=f to reflect this fact. If we simply set the pth row of A to zero, and then set\nthe pth column of that row to be 1 (i.e. set the pth diagonal entry to 1), the value at u(p)\nwill be forced to f(p). Therefore, assuming f was originally the zero vector, we must now\nsatisfy that the pth entry of f now be equal to u(p), so now f(p)=b. This has forced u(p)=b.\nOne might wonder if we should also set the pth column to zero. We should not, as\nthe columns allow the forced value of u(p) to propagate its information into other parts of\n\nthe grid. Physically, at least in electrostatics, there is no intuition of having a source at a\npoint where there is a boundary condition, because the boundary manifests itself as a\nsource in this implementation. Therefore, if there is a boundary at u(p), f(p) will be zero\nat that point before we put the constraint on the point.\n\nThe above method works excellently for interior boundary points. The actual grid\nboundaries, where Dirichlet conditions were sought, are not as straightforward. Some\nfinite difference methods deal with these points implicitly by never explicitly defining\ngrid points at the edges. Instead, I decided to explicitly define these points and alter the\nA matrix, creating a matrix format which deviated from that in the figure above.\n\nThe difficulties in the above implementation arise when the difference matrix of\n\"K2D\" \"looks\" outside the grid implicitly when it touches the edges of the grid. This is\neasier to see in the 1D K matrix. The first and last rows of K are missing -1's in that\nmatrix. Implicitly, this means that the finite difference operator looked \"off the grid\" and\nfound zero, unless a nonzero value shows up to make a non-zero entry in f. I decided to\nexplicitly define the boundary values instead of trying to keep up with these issues.\n\nFirst, the ordering definition of u and A must be defined. For my implementation,\nu(0) was the upper-left corner of the grid and u(N) was the lower-left corner. u(N+1) was\nthe point right of u(0), and u(2N) was the point to the right of u(N). u(N2-N+1) was the\nupper-right corner, and u(N2) was the lower-right corner.\n\nTherefore, to define explicit boundaries, I needed to set the first N values of u to\nthe Dirichlet conditions. Therefore, when constructing A, by the reasoning from the\ninterior boundary points described above, the upper-left corner of A was a block identity\nmatrix of size N, and f(1...N) was set to the boundary value. This construction dealt with\nthe left edge easily. I constructed the rest of A by using the traditional 2D stencil from\nclass. In order to account for the top and bottom edges, I made sure to set those\ncorresponding rows in A to zero, except with a 1 on the diagonal and the corresponding\nvalue of f to the boundary condition. When I reached the lower-right corner, I augmented\nA with another block identity matrix of size N, and set f to the boundary condition at\nthose points. A matrix density plot is shown below to illustrate this construction.\nApproaching the boundaries explicitly made them easier to track, but an algorithm to\nconstruct a general A for a given mesh size was quite difficult; that is the tradeoff.\n\nFigure 5: Sparsity pattern of my altered finite difference matrix which allows for explicit boundary\ndefinition. Notice the periodic gaps along the diagonal representing the top and bottom edges of the\ngrid.\n\nWith boundary conditions properly incorporated, the last topic to address was that\nof inter-grid transfers: what matrix downsamples the original data to a coarser grid?\nWhich matrix transfers from the coarse grid to the fine grid? The proper way to phrase\nthe question is this: what is the proper way to transfer data from one grid to another?\n\nIn going from a coarse grid to a fine grid, the central problem is interpolation.\nThe central ideas of interpolation and downsampling were discussed in the 1-D section.\nThe 2D implementation is highly similar, but with a little more complexity than the 1D\ncase due to slightly trickier boundaries on the edges. I decided that I would again seek to\ndo downsampling as a weighted averaging of neighboring points rather than by injection.\nAgain, the reason for this approach was so that simply transposing the downsampling\nmatrix would yield the linear interpolation matrix for upsampling and linear interpolation.\n\nSuch a downsampling matrix was rather straightforward to implement for the\ninterior points of the grid. Incorporating the edges would have been somewhat trickier,\nand the averaging scheme used, if simply allowed to include the edges, would have\nchanged the boundary values themselves, which is to be avoided at all costs. Therefore, I\ntook the following approach.\n\nDownsampling\n\n1) Calculate the residual\n2) Remove the edges from the fine grid residual data\n3) Design a downsampling matrix to transform the inner grid residual data from the\nfine grid to the twice-as-coarse grid\n4) Apply the downsampling matrix to the interior residual data\n\n5) Append zeros around downsampled residual data grid to represent the fact that the\nresiduals are by definition zero at the edges where we have defined the exterior\nboundaries.\n\nUpsampling\n\n1) Remove the zeros from the coarse grid's edges (this is after we have relaxed the\nresidual problem on the coarser grid)\n2) Apply the scaled, transposed downsampling matrix to the interior points to get the\ninterpolated guess at the error on the fine grid\n3) Pad the resulting upsampled interior points with zeros since there is no refinement\nin the error at the known boundaries\n4) Add the upsampled guess at the error to the original guess at u\n\nThe downsampling operator was defined explicitly in Briggs, though in an index form\nrather than matrix form. I implemented the operation as a matrix in order to speed up\ncomputation in MatLab. Briggs defines the downsampling operation as follows in 2D\n(v2h is the vector represented on the coarse grid, vh is the grid on the fine grid):\n\n(\n)\n,\n1,\n,\n,1\n,1\n,\n,\n,1\n,1\n,1\n,1\n-\n≤\n≤\n⎥\n⎥\n⎥\n⎥\n⎦\n⎤\n⎢\n⎢\n⎢\n⎢\n⎣\n⎡\n+\n+\n+\n+\n+\n+\n+\n+\n=\n+\n-\n+\n-\n+\n+\n-\n+\n+\n-\n-\n-\nn\nj\ni\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nij\n(26)\n\nI implemented it instead as a matrix operator. In order to implement the above\noperation, the following stencil was used:\n\n))\n)(\n)\n((\n)\n)(\n)3\n)((\n((\nzeros\nM\nzeros\nM\n-\n-\n\n(27)\n\nM was the number of interior points in one edge of the fine grid from which the\nmapping was supposed to take place. As a final point, the stencil was not simply\nreplicated along the diagonal, rather to implement the correct operation it was\nimplemented in a staggered pattern (similar to 1D) as shown in the sparsity pattern\nbelow.\n\nFigure 6: Sparsity pattern for the downsampling matrix; stencil is replicated in the matrix in a\nstaggered fashion\n\nBriggs describes the indexed form of the 2-D linear interpolation operator, and it is\nsimply implemented by transposing the above matrix and scaling by 4.\n\nNumerical Experiments in 2-D\n\nFinally, with all the tools in place for 2-D, numerical experiments could be\nundertaken. A convincing example that the system was working properly would be\nsolution of a problem with a known solution. To this end, I decided to compare the\nmulti-grid solver's solution to the actual solution to Laplace's equation with the\nfollowing boundary conditions:\n\nFigure 7: Boundary conditions for the known solution to Laplace's equation\n\nThe solution to Laplace's equation in this case can be expressed as a sum of sines\nand hyperbolic sines. I will not go through the derivation here for that answer, but I\nwrote a loop in MatLab to compute a partial sum of the relevant terms to produce the\ncorrect answer so that it could be compared to the multi-grid solver's output. The two\nsolutions produced are very similar. The Gibbs phenomenon is apparent in the partial\nsum of sines. They are plotted below.\n\nFigure 8: Fourier expansion of actual solution. Right edge converges to 1; perspective of plot is\nmisleading.\n\nFigure 9: My solver's output after 1000 steps on the fine grid\n\nThe next obvious experiment is to see how quickly the relaxation error decays to\nzero. The decay of pure modes was examined for the 1-D case. Now however, the solver\nwas considerably more powerful, so examining more sophisticated problems would be\ninteresting. In general, we don't know the final solution; we only know the residual after\na step. So, from now on, instead of discussing error, we will examine how the norm of\nthe residual decays.\n\nAn interesting case to examine would be a unit spatial impulse. The Fourier\nexpansion of an impulse has equal weights on all frequencies, so examining how an\ninitial guess of an \"impulse\" decays to zero everywhere in homogenous conditions would\nbe insightful. The following plots show a unit impulse located at 67,67 on a 133 by 133\ngrid after various numbers of steps.\nFigure 10: Decay of a unit impulse. Notice that after 30 steps, the solution is much \"smoother\" than\nafter 10. This is because the higher-frequency modes have been filtered out by the fine grid.\n\nFigure 11: Stalling of residual decay on the fine grid\n\nWe can see that the residual decays very quickly initially, but the decay rate then\nstalls. This is because the error that is left is lower-frequency error which does not decay\nquickly on the fine grid. This is seen in the figure, as after twenty and thirty iterations,\nthe solution looks very smooth.\n\nThe question to ask now is, how much better could the answer be after a number\nof steps if we employ a multi-grid approach? In the following experiment, three grid\ncoarseness levels were available. Grid 1 was the fine grid. Grid 2 was the \"medium\"\ngrid, and was twice as coarse as Grid 1. Grid 3 was the \"coarse\" grid, and was twice as\ncoarse as grid 2.\n\nAn experiment similar to the one in Figure 10 was attempted with the unit\nimpulse. Three relaxations were performed, starting with homogenous conditions and a\nunit impulse initial condition.\n\nTrial 1: Relax with 2500 steps on Grid 1\n\nTrial 2:\na) Relax with 534 steps on Grid 1\nb) Move to Grid 2 and relax for 534 steps\nc) Move back to Grid 1, incorporate the refinement from (b), and relax\nfor 1432 steps for a total of 2500 steps\n\nTrial 3:\n\na) Relax with 300 steps on Grid 1\n\nb) Relax with 300 steps on Grid 2\n\nc) Relax with 300 steps on Grid 3\n\nd) Move back to Grid 2, incorporate refinement from (c) and relax for 800\n\nsteps\n\ne) Move back to Grid 1, incorporate refinement from (d) and relax for 800\n\nsteps for a total of 2500 steps\n\nThis scheme was chosen because it gave all methods the total number of steps.\nAdditionally, for trial 2 and trial 3, the ratio of forward relaxations (i.e. relaxation after\nmoving from fine to coarse) to backwards relaxation was constant at 3/8. The detail after\n2500 steps is shown below for all three cases.\n\nFigure 12: The three-grid scheme outperforms both the single and dual grid schemes.\n\nIt is clearly visible that given the same number of steps, the three-grid scheme\noutperforms the single grid scheme and the dual grid scheme. However, it is not a given\nthat this result will always be the case. If the error, for example, was known to be almost\npurely high-frequency, the advantage of the grid transfers might be outweighed by the\ncomputation power necessary to keep making the transfers and interpolations.\nThe case shown above for the unit impulse is a case where the frequencies are\nequally weighted in the initial conditions. As a second trial, I examined how the\nresiduals decayed for an initial condition with more low-frequency content. This case\nwas again homogenous with boundary conditions of zero, but the initial \"guess\" was 1\neverywhere except at the boundaries. I repeated the experiment with these initial\nconditions, and the results are shown below.\n\nFigure 13: Decay of residuals for the three schemes. The only fair comparison across methods is\nafter all three trials have made it back to the fine grid (i.e. after the last spike in residual on the green\nline which comes from the interpolation error). The three-grid method is the most accurate after\n2500 steps.\n\nFigure 14: Detail of the final residual values for the three methods. The three-grid method clearly\nwins out over the others. This is a more drastic out-performance than before since the initial\ncondition contained more low frequency error, which was better managed on the coarser grids.\nOnce again, the three-grid scheme wins. It is important to note that in the first\nfigure, the \"norm\" during the steps while the problem's residual resides in the coarser\ngrid is not comparable to the norm of vectors in other grids, as the norm is vector-size\ndependent. Therefore, the only truly fair points on the graph to compare the methods are\nwhen all methods are on the same grids, namely the very beginning and very end (shown\nin detail in Figure 14).\n\nThere are two ways to interpret the results. We can get a better answer with the\nsame number of steps by using the three-grid cycle. Alternatively, we could stop earlier\nwith the three-grid cycle and settle for the value that the other methods would have\nproduced with more steps. The tradeoff is completely problem dependent.\nI was suspicious as to how much difference the above residuals made in the\nappearance of the answer, especially given the much higher initial values of the residuals.\nThe difference after trials 1, 2 and 3 is stark and is shown below. Remember, with an\ninfinite number of steps, all three methods would converge to a value of zero everywhere.\n\nThe results are obviously different. Trial 3 yielded an answer visually very close\nto the correct answer of 0. It is clear that going beyond simply one coarsening operation\nyielded great benefits. The natural next step would be to try a fourth, coarser grid, and\ncontinue coarsening. One could coarsen the grid all the way to a single point. Also,\n\ntrying a multitude of different step distribution schemes in order to maximize efficiency\nof steps at each grid could be tried too.\nOne could easily write a book about these concerns, but going far down either of\nthese paths would step outside the scope of this introduction to multi-grid and its benefits.\nInstead, it would be more appropriate to confirm this limited-multi-grid system on other\nproblems.\nAs stated earlier, a key goal of my 2D implementation was the ability to impose\nboundary conditions within the grid. I designed my Jacobi relaxer, as described earlier,\nto support internal boundaries as well. I implemented the system so that I could simply\nuse Windows Paint (r) to draw whatever regions I wanted to specify as at a particular\n\"voltage.\" As an appropriate example, I decided to determine the potential distribution\nresulting from having an electrode in the shape of the letters \"MIT\" in the grid, with 0\nvolt boundary conditions on the edge of the grid. The bitmap used to create the boundary\nconditions is shown below. The black letters are defined to be 1 volt, the white area zero\nvolts.\n\nShown below is a plot of the relaxation solution (still staying all the time on the\nfine grid) of the solution to the problem.\n\nFigure 15: \"Electrodes\" in shape \"MIT\" relaxed on fine grid\n\nFinally, just to prove that the ability to add charge into the simulation was added,\nI added a line of charge the under the \"electrodes\" used to write \"MIT\" to underline the\nword.\n\nFigure 16: MIT electrodes with a line of charge underlining them\n\nPlacement of arbitrary charge within the medium with arbitrary boundary\nconditions was supported as well. The figure below shows the gains made with a 930\nstep double-grid method vs. a single grid method; the point is to show that the charge\nplacement was supported across multiple grids.\n\nFigure 17: Multi-grid support included for arbitrary charge distributions as well\n\nAs for multi-grid support of internal boundary conditions (i.e. if we wanted to\nrelax the MIT electrode problem with multi-grid), I did not quite get around to that. I\nthought I had it done, but I discovered too late that I had forgotten a subtle aspect. When\nrelaxing on Ae=r, I forgot to pin internal values of e at the boundaries to zero, as by\ndefinition there would never be error at one of the internal boundaries. Without doing\nthis, the compensated error approximation from the coarse grid will attempt to force the\nvalue at the boundary to deviate from the correct internal boundary condition.\nThis could be fixed by changing the matrix A by making the rows corresponding\nto these points zero, except for a 1 on the diagonal. Additionally, r at that point would\nneed to be 0, but I had already thought of and taken care of that and had implemented that\naspect. As simple as the fix sounds, I had an elaborate setup in the algorithm for the\ncurrent system, and making the change would have meant tearing the whole system down\nand building it back up, which was unrealistic as late as I found the problem. However, I\ndid determine the source of the problem and its likely fix.\n\nConclusion\n\nThe most convincing figure of the paper is replicated below.\n\nThis figure truly sums up the power of multi-grid. In the same number of steps,\nthe approach with the largest utilization of coarsening got closest to the right answer.\nOne can be more quantitative about the true efficiency of the method: what is the\ncomputational tradeoff between doing a few more iterations on the fine grid and moving\nto a coarse grid? Do the benefits of moving outweigh the computational costs of\ndownsampling and interpolation? What is the best way to design a multi-grid cycle\nscheme? How long should one spend on a coarse grid versus a fine one? These are all\nexcellent questions of multi-grid, and there is no definitive right answer.\n\nAs for the tradeoff between interpolation and downsampling versus spending time\non a fine grid, making an absolutely definitive answer is difficult. However, multiplying\nby the Jacobi matrix for an iteration and multiplying by an upsampling or downsampling\nmatrix consist of matrix multiplications of relatively the same size and density, making\nthe intergrid transfers relatively cheap and insignificant compared to large numbers of\n\nsteps of relaxation computation. It is more likely that the tradeoffs will come from\ndetermining the proper amount of time to spend at each grid. A possible way to do this\nwould be to look at the FFT of the residual, try to predict the spectral content of the error,\nand adaptively decide which grid to move to based on that result. Other ways would be\nto look for patterns in the particular class of data being examined. Such design issues\nwould make excellent projects in and of themselves.\n\nWhat is definite, however, is that multi-grid can yield astonishing results in the\nright circumstances and can give excellent answers in a fraction of the time that a single-\ngrid relaxation would need. If an exact solution is not necessary, and the grid is huge,\nmulti-grid is an excellent way to go.\n\nReferences\n\nBriggs, William, et al. A Multigrid Tutorial, Second Edition. (c) 2000 by Society for\nIndustrial and Applied Mathematics.\n\nJaydeep Bardhan and David Willis, two great advisors from Prof. Jacob White's group.\n\nProf. Gilbert Strang, for his draft of his new textbook."
    },
    {
      "category": "Resource",
      "title": "overview.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/4cd6d25812214df9a2883e6a96ab3229_overview.pdf",
      "content": "Overview of Multi-grid Project\n\nThis project example on this OCW site is a slight refinement of my final\nproject in Prof. Strang's 18.086 class. The project implements a 2D multi-grid solver\nfor Laplace's and Poisson's equation. Multi-grid support is included for Dirichlet\nboundary conditions, as well as for sources within the grid. Internally pinned\nDirichlet boundaries, while possible to specify, do not function properly if multi-grid\nschemes are used in my implementation. I wanted to add this feature, but ran out of\ntime at the end of the project.\nI have attempted to give an explanation of how to use the solver by including\nthe file \"ProjectOCW.m,\" a simple numerical experiment conducted with the solver.\nOnce all project files are placed in the same directory, one should simply be able to\nopen the mentioned file and run it in MatLab. The file will plot the error and the\nguess at the solution at various steps in a V-cycle approach to multi-grid.\nAdditionally, I have extensively commented this file to give the user enough\nknowledge to modify the file to conduct experiments of one's own. The solver tracks\nthe norm of the residual as the solver progresses, which is a key statistic to keep an\neye on. Upon altering the MatLab script, the user can decide parameters like how\nmany steps to stay on a grid, what Jacobi damping factor to use, or a number of other\nparameters.\nThe user can also import images to use as a two-dimensional source term in\nthe problem, or import other images to define the boundary conditions or initial guess.\nImages can be imported into MatLab. In order to be incorporated into the solver,\nhowever, the images must become appropriately sized matrices with values\ncorresponding to some interpretation in Laplace's equation. The simplest example is\nimporting a monochrome bitmap for source terms, where black pixels could map to\nzero entries in the source terms, and white to some determined value.\nI attempted to thoroughly explain \"ProjectOCW.m,\" but left explanation of the\nhelper functions more vague. This project was not intended originally to teach multi-\ngrid, but rather act as an example of implementation of a multi-grid solver.\nFurthermore, there are definitely more efficient and elegant ways to implement a\nnumber of the operations of the solver in MatLab. For these reasons, I found a\nthorough explanation of the inner workings of the solver to be outside the scope of\nthis posting, which is meant to show multi-grid in action, rather than thoroughly\nexplain every detail of design decision in implementing the system.\nI hope you enjoy running the pre-designed experiment and will alter the\nexample file to run experiments of your own. The system here is capable of a good\nnumber of experiments. I learned a great deal about the method by simply changing\nparameters easily alterable in this example.\nFinally, I'd like to thank Prof. Strang for all of his help and advice, Dave\nWillis and Jaydeep Bardhan for their help, and would like to acknowledge the\nimmense help of A Multigrid Tutorial by Briggs et al., a must-read for anyone\ninterested in learning multi-grid.\n\nJoseph Kovac, S.B. MIT '05\n\n9/12/05\n\nGraduate Student, MIT EECS"
    },
    {
      "category": "Resource",
      "title": "project1domnguez.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/4bfb14a306e4cc0a7f89e0d719754fdb_project1domnguez.pdf",
      "content": "MASSACHUSETTS INSTITUTE OF\nTECHNOLOGY\nDEPARTMENT OF MECHANICAL ENGINEERING\nCOURSE: 18.086\nMathematical Methods for Engineers II\nProject 1\nExperimental Analysis of the Two Dimensional Laplacian\nMatrix (K2D): Applications and Reordering Algorithms\n\nProject 1\nCourse: 18.086\nExperimental Analysis of the Two Dimensional Laplacian\nMatrix (K2D): Applications and Reordering Algorithms\nSection 1: Introduction\nThis project focuses on the analysis and experimentation of the two dimensional (2D)\nLaplacian matrix (K2D). Special emphasis is put on solving large systems, particularly\nfor problems related to digital image processing and optical imaging. We discuss a\ntypical application of the K2D matrix in finding the edges of an image. In the forward\n2 ⋅\nproblem, F = K D U , the input is the image we want to process by a Laplacian based\nedge detection algorithm. As will be discussed in the next section, the input image is\npreprocessed by a raster scanning algorithm to form the vector U. The output vector F is\nalso processed by the inverse raster scanning algorithm to form the 2D Laplacian of the\nimage. This new matrix is used in the edge detection algorithm to find all the edges.\n-1 ⋅\n=\nIn the inverse problem, U\nK D\nF\n\n, the Laplacian of the image is the input and we\ntry to recover the original object. This is the case for certain optical systems as discussed\nin Section 2. In order to solve this problem efficiently, we discuss the properties of K2D\nand experiment several ways to speed up the elimination and also reduce the storage\nduring computations.\nIn Section 3, we analyze three popular reordering algorithms: minimum degree, red-black\nand graph separator orderings. We developed several experiments to explore their\nbehavior under conditions such as variable matrix sizes. In addition, experiments are\ndeveloped to estimate their required computational time and efficiency under LU and\nCholesky decompositions.\n\nSection 2: Application of the Laplacian Matrix in Digital\nImage Processing\nIn this section, the implementation of the two dimensional (2D) Laplacian Matrix (K2D)\non a digital image processing problem is discussed. In this problem, the second derivative\nof the image (i.e. the Laplacian) is used to find discontinuities by implementing an edge\ndetection algorithm. Edge detection algorithms are widely used in applications such as\nobject detection and recognition, shape measurement and profilometry, image analysis,\ndigital holographic imaging and robotic vision.\nEdges in images appear as regions with strong intensity variations. In the case of images\nobtained with a conventional camera, edges typically represent the contour and/or\nmorphology of the object(s) contained in the field of view (FOV) of the imaging system.\nFrom an optics perspective, edges represent the high spatial frequency information of the\nscattered wave coming from an illuminated object. If an object contains a sharp\ndiscontinuity, the information for this region will be mapped in a region of high\nfrequency in the Fourier plane. Edge detecting an image is very important as it\nsignificantly reduces the amount of data and filters out useless information, while\npreserving structural properties of the image [1].\nThere are two main categories of edge detection algorithms: gradient and Laplacian based\nalgorithms. In a typical gradient based algorithm, the first derivative of the image is\ncomputed in both dimensions (row and column wise). In this new image, the edges\nbecome apparent and the maximum and minimum gradients of the image are compared to\na threshold. If there is a maximum with a value larger than the threshold, the spatial\ncoordinate of that maximum is considered an edge. An example of gradient based\nalgorithms is the Sobel edge detection algorithm. Figure 2.1 shows a 1D discontinuity\ncentered at x=0. Figure 2.2 is a plot of the first derivative of the intensity function of\nFigure 2.1. It is important to note that the maximum is also centered at x=0. If we set the\nthreshold of the edge detection algorithm equal to 0.2, an edge would be detected at x=0.\nFigure 2.1: Example of 1D edge in an image\n\nFigure 2.2: First derivative of the 1D edge of Figure 2.1.\nFor a Laplacian based algorithm, the second derivative of the original image is computed.\nThe zero crossings of the resulted matrix are used to find the edges. Figure 2.3 shows the\nsecond derivative computed for the 1D example discussed above. The Matlab code used\nto generate Figures 2.1-2.3 is included in Appendix A.1.\nFigure 2.3: Second derivative of the 1D edge of Figure 2.1.\nIn the 2D case, the Laplacian is computed in both dimensions (row and column wise).\nThis can be achieved by using the matrix K2D in the forward problem:\n⋅\nK D U = F ,\nwhere U is a vector formed after raster scanning the original image. The measurement\nvector F is the raster scanned version of the 2D Laplacian of the image. The matrix K2D\nis an N × N matrix formed by the addition of the Kronecker tensor products\n2 =\n(\n,\n( ,\nK D kron K I ) + kron I K ).\nHere, I is the N\nN identity matrix and K is also N\nN and tridiagonal of the form:\n×\n×\n\n-1\n\"0\nK =\n-1\n-1 \"0\n\n.\n#\n%\n%\n%\n\n\" -1\nTo exemplify the 2D forward problem, consider the 500 ×500 pixels image of Figure 2.4.\nTo form the vector U, we need to implement a raster scanning algorithm. In the raster\nscanning algorithm, the N\nN matrix (i.e. the image) is decomposed into a vector of size\n×\nN 2 . This is achieved by extracting each column of the original matrix and placing it at\nthe bottom of the vector. For example, a 3 3\npixels image would produce\n×\nu1\n\nu4\nu7\n\nu3\nu1\nu2\nraster\nu2\nl\n\nU = u4\nu5\nu6\n→ scanning → U = u .\n\nu7\n\nu8\nu9\nu8\n\nu3\nu6\nu9\n\nFigure 2.4: Photograph of Prof. Gilbert Strang (500 × 500 pixels ).\nThe forward problem is solved by multiplying the K2D matrix, which in this case is\n250000 × 250000 , with the vector U. Since both the vectors and the matrix are large, it is\nnecessary to implement the 'sparse' function available in Matlab. The 'sparse' function\nreduces the required storage by only considering the coordinates of the nonzero elements\nin the sparse matrix. The resultant matrix is obtained by the implementation of the\ninverse-raster scanning algorithm. Figure 2.5 shows the 2D Laplacian obtained after\n\nsolving the forward problem for the image of Figure 2.4. The code used to generate\nFigure 2.5 is included in Appendix A.2.\nFigure 2.5: 2D Laplacian of the image of Figure 2.4.\nFor a Laplacian based algorithm, the information contained in Figure 2.5 is used to find\nthe edges of the image. The result of this algorithm is a binary image such as the one\nshown in Figure 2.6. For more information about edge detection algorithms refer to [2].\nFigure 2.6: Result of applying the Laplacian edge detection algorithm on Figure 2.4.\nNow we discuss the inverse problem. In the inverse problem, we are given the Laplacian\nof the image and our task is to find the original image\nU\nK2D 1\n.\n=\n- F\n\nThis is a common problem in communications where we want to transmit as little\ninformation as possible. In an extreme case, we would like to transmit just the edges of\nthe image (a sparse matrix) and from this recover the original image. Another example is\nan optical imaging system that has a high-pass Fourier filter in the Fourier plane, such as\nthe one shown in Figure 2.7. In the optical configuration of Figure 2.7, an object is\nilluminated by a mutually-coherent monochromatic wave and the forward scatter light\nenters a 4f system. The 4f system is a combination of two positive lenses that are\n= 1+\nseparated by a total distance of d\nf\nf\n\n2 , where f1and f 2 are the focal lengths of\nboth lenses. The first lens functions like an optical Fourier Transform operator. A high-\npass Fourier filter is positioned in between both lenses (at the focal plane). This high-pass\nfilter is designed to block the low spatial frequency content of the optical field and only\nlet the high frequency information pass. The filtered spectrum is inverse Fourier\ntransformed by the second lens and forms an image on a CCD detector. If the cut-off\nfrequency is set relatively high (i.e. cutting a great part of the low frequency\ninformation), the edges of the object start being accentuated at the image plane.\nFigure 2.7: Optical realization of edge extraction\nIn the inverse problem, the input is an image similar to the one shown in Figure 2.5 and\nthe output is another image like the one shown in Figure 2.4. The algorithm to compute\nthis includes an elimination step (for the code included in Appendix A.3, the elimination\nis done using Matlab's '\\' function) and a reordering. Different types of reordering such\nas minimum degree, red-black and nested dissection orderings will be analyzed and\ncompared in Section 3. Choosing a suitable reordering algorithm is very important\nespecially for large systems (for images with a high space-bandwidth product), as it\nreduces the number of fill-ins during elimination.\n2.1 The K2D Matrix\nThe K2D matrix can be though of as the discrete Laplacian operator applied to vector U.\nAs mentioned before, K2D can be formed from the addition of the Kronecker products of\nK and I. K2D has 4's in the main diagonal and -1's for the off-diagonal terms. For\nexample, the 9 9 K2D matrix is given by\n×\n\n-1\n-1\n-1\n-1\n-1\n\n-1\n-1\n\n-1\n-1\n-1\n\nK D = 0\n-1\n-1\n-1\n-1\n\n-1\n-1\n-1\n-1\n-1\n\n-1\n-1\n-1\n-1\n-1\n\nThe main characteristics of K2D is that it is symmetric and positive definite. From\n2 =\nsymmetry we know that K D K DT and that all its eigenvalues are real and its\neigenvectors are orthogonal. From positive defines we know that all the eigenvalues of\nK2D are positive and larger than zero; therefore, its determinant is larger than zero. The\ndeterminant for the 9 9 matrix shown above is det\n×\nK D = 100352 . The Gershgorin\ncircle theorem applied to the matrix K2D indicates that all its eigenvalues should be\ncontained inside a circle centered at 4 and with a radius of 4. Figure 2.8 shows the\neigenvalues of the 9 9 K2D matrix in the complex plane. As predicted by the\n×\nGershgorin circle theorem, all the eigenvalues are between 0 and 8. The code to generate\nthe plot of Figure 2.8 is included in Appendix A.4. Figure 2.9 shows the eigenvalues for a\nmuch larger K2D matrix ( 4900 × 4900 ). These 4900 eigenvalues also remain between 0\nand 8.\nFigure 2.8: Eigenvalues of the 9 9K2D matrix\n×\n\nFigure 2.9: Eigenvalues of a 4900 ×4900 K2D matrix\nAnother interesting measure that describes the behavior of K2D is the condition number\nor in the context of information theory, the Rayleigh metric. The condition number is\ndefined as\nλmax\nc =\n,\nλmin\nand gives information about how close the matrix is to ill-posedness. If c = 1, the matrix\nis said to be well-posed, but if the condition number is large ( c →inf), the matrix gets\nclose to becoming singular. In other words, we cannot completely trust the results\nobtained from an ill-posed matrix. Figure 2.10 shows the condition number for K2D of\ndifferent sizes. The code used to generate this figure is included in Appendix A.5.\nFigure 2.10: Condition number of K2D as a function of size\n\nSection 3: Reordering Algorithms\nIn the previous section we described some of the properties of the K2D matrix along with\nrelated problems (forward and inverse problems) that are important for digital image\nprocessing. The main difficulty in solving such systems arises when the system is large.\nA large system would be required if we intend to process an image with a large number\nof pixels. If we do the elimination directly on K2D, several fill-ins would appear and the\ncomputational costs (memory and computational time) would increase. For this reason, in\nthis section we discuss different reordering algorithms that help reduce the number of fill-\nins during elimination.\n3.1: Minimum Degree Algorithm\nn\n(\n)\nThe minimum degree algorithms reorder K2D to form a new K D = P K\n2D PT by\neliminating the pivots that have the minimum degree. If we draw the graph corresponding\nto K2D, the minimum degree algorithm eliminates the edges (off-diagonal terms) from\nthe nodes that have the minimum degree (i.e. the least number of edges connected to it).\nTo describe the steps involved in this algorithm, we carry the reordering algorithm on the\n9 9K2D matrix described above. Figure 3.1 shows its corresponding graph. The steps of\n×\nthe algorithm are as follows:\nFigure 3.1: Graph corresponding to a 9 × 9 K2D matrix\n1. We start by choosing the first element in the matrix as the pivot (in reality, you\ncan choose any of the corner nodes of the graph, as all of them are degree 2).\nUsing this pivot, carry out elimination as usual. This is equivalent to eliminating\nthe edges connected to the pivot node. Two new fill-ins are generated in this step,\nand this is shown in the graph as a new edge as shown in Figure 3.2.\n2. Update the permutation vector: P = [1]. The permutation vector keeps track of the\norder of the nodes we choose for elimination.\n3. Go back to step one and choose one of the three remaining nodes that are degree\n2. For example, if the next node we choose is node 3, after elimination, the\npermutation vector becomes: P = [1 3].\nFigure 3.3 shows the graph sequence for the remaining steps of the minimum degree\nalgorithm. The final permutation vector is P = [1\n4]. Figure 3.4\n\nshows a comparison between the structures of the original and the reordered K2D\nmatrices. Figure 3.5 shows the lower triangular matrices produced after LU decomposing\nthe original and the reordered K2D matrices. As can be seen from this figure, reordering\nthe matrix produces fewer numbers of nonzeros during the LU decomposition. The\nMatlab code used to generate Figures 3.1-3.5 was obtained at [3].\nFigure 3.2: Graph of the 9 ×9 K2D matrix after step one of the minimum degree\nalgorithm\nFigure 3.3: Sequence of graphs for the remaining steps in the minimum degree algorithm\n\nFigure 3.4: Structural comparison between the original and the reordered K2D matrices\nFigure 3.5: Lower triangular matrices produced after LU decomposition of the original\n(#nonzeros = 29) and the reordered (#nonzeros = 26) K2D matrices\nSeveral minimum degree reordering algorithms are available. Each of these algorithms\nproduces a different permutation vector. We now compare five of such algorithms:\n1. Matlab's \"symamd\" algorithm: Symmetric approximate minimum degree\npermutation.\n2. Matlab's \"symmmd\" algorithm: Symmetric minimum degree permutation.\n3. Matlab's \"colamd\" algorithm: Column approximate minimum degree\npermutation.\n4. Matlab's \"colmmd\" algorithm: Column minimum degree permutation.\n5. Function \"realmmd\" algorithm [3]: Real minimum degree algorithm for\nsymmetric matrices.\nFigure 3.6 shows the structure of a 81× 81K2D matrix. Figure 3.7 shows a structural\ncomparison of the reordered matrices using the five minimum degree algorithms\nmentioned above. Figure 3.8 shows their respective lower triangular matrix produced\nafter LU decomposing the reordered K2D matrix. From this figure, it is evident that the\nreal minimum degree algorithm produces the least number of nonzero elements (469) in\n\nthe decomposed lower triangular matrix. The code used to generate Figures 3.6-3.8 is\nincluded in Appendix B.1.\nFigure 3.6: Structure of a 81 × 81K2D matrix\nFigure 3.7: Structural comparison of reordered matrices using the five algorithms\ndescribed above\n\nFigure 3.8: Structural comparison of the lower triangular matrices decomposed from the\nreordered matrices using the five algorithms.\nNow we compare how the five minimum degree algorithms behave as a function of\nmatrix size. In particular, we are interested to know the number of nonzeros in the lower\ntriangular matrix, L, after LU decomposing the reordered K2D matrices for different\nmatrix sizes. Figure 3.9 shows this comparison for matrices with sizes ranging from 4 4\n×\nto 3844 ×3844 entries. Figure 3.10 zooms into Figure 3.9 to show an interesting behavior\nof the 'realmmd' and the 'symamd' algorithms with relative small sized matrices. For\nsmall matrices, the 'realmmd' algorithm produces the least number of nonzeros in L;\nhowever, if the matrix size increases (especially for large matrices), the 'symamd'\nalgorithm is the winner. The code used to generate Figure 3.9-3.10 is included in\nAppendix B.2.\nIn our next experiment, we compare four of these minimum degree methods for different\nmatrix sizes after performing a Cholesky decomposition on the reordered matrix. The\n×\nsize of the matrices ranges from 4 4 to 120409 ×120409 in steps of 25. The results of\nthis experiment are plotted in Figure 3.11. As in the previous experiment, we can see that\nthe methods 'symamd' and 'symmmd' produce the least number of nonzeros entries in\nthe factorized matrix. However, the question now is: are these methods (symamd and\nsymmmd) fast to compute?\n\nFigure 3.9: Comparison of the nonzeros generated by the five algorithms as a function of\nmatrix size\nFigure 3.10: Zoomed in from Figure 3.9.\nFigure 3.11: Nonzeros generated by the Cholesky factorized matrix, reordered with four\ndifferent minimum degree algorithms\n\nTo answer the question stated above, we now turn to compare the required computational\ntime for reordering K2D by these four minimum degree algorithms. The results are\nshown in Figure 3.12. From this figure we can see that although it is slightly faster to\nreorder a large matrix with 'colamd' rather than using 'symamd', the number of nonzeros\ngenerated in the decomposed matrix is significantly less for a matrix reordered by the\n'sysmamd' algorithm. In conclusion, 'sysmamd' is our big winner.\nFigure 3.12: Computational time for reordering K2D of different sizes\nThe reason we didn't include the 'realmmd' algorithm in the comparison of Figure 3.13,\nis because this algorithm requires a long computational time (even for relatively small\nmatrices) as shown in Figure 3.13.\nFigure 3.13: Computational time required by 'realmmd' as a function of matrix size\nIf we use Matlab's functions 'chol' and 'lu' to produce the Cholesky and LU\ndecompositions respectively, it is fair to ask: what is the computational time required by\nboth functions? Is the computational time dependent on the input matrix (i.e. if the matrix\nwas generated by 'symamd', 'symmd', 'colamd' or 'colmmd')? How does the\ncomputational time behave as a function of the matrix size? To answer all of these\nquestions, we generated the plots shown in Figures 3.14 and 3.15. The Matlab code used\nto generate Figures 3.11-3.15 is included in Appendix B.3.\n\nFigure 3.14: Computational time required by Matlab's Cholesky decomposition function\n'chol'\nFigure 3.15: Computational time required by Matlab's LU decomposition function 'lu'\n3.2: Red-Black Ordering Algorithm\nThe red-black ordering algorithm is an alternative reordering technique in which it is\ndivided in a way similar to a checkerboard (red and black or odd and even squares). The\npermutation vector is simply generated by first selecting for elimination the odd nodes\nand then all the even nodes on the grid. The red-black permutation is given by\nP K )\nBT\n(\n2D PT =\n4Ired\n4I\nB\nred\n,\nwhere B is a matrix composed by -1s from the off-diagonal or black elements of K2D.\n\nFor example, Figure 3.16 shows a structural comparison between the original 9 9K2D\n×\nmatrix and the its equivalent reordered using the red-black ordering algorithm. The\npermutation vector in this example is: P = [1\n8] . The reordered\nK2D matrix is given by\n-1 -1\n-1\n-1\n\n-1 -1 -1 -1\n\n-1\n-1\n(\n)\nP K D PT = 0\n-1 -1 .\n\n-1 -1 -1\n-1\n-1 -1\n\n-1 -1\n-1\n-1 -1 -1\n\nOriginal 9x9 K2D\nK2D reordered by red-black algorithm\nnz = 33\nnz = 33\nFigure 3.16: Structural comparison between the original K2D (to the left) and the\nreordered matrix using the red-black algorithm (to the right)\nFigure 3.17 shows the sequence of eliminations that occur on the graph representation of\nthe matrix described above. From this figure, we can see that the algorithm starts by\neliminating all the edges of the odd/red nodes (remember that the graph is numbered row\nby row starting from the bottom left corner). After finishing with the odd nodes, the\nalgorithm continues to eliminate the even nodes until all the edges have been eliminated.\nA comparison between the lower triangular matrices produced after elimination of the\noriginal and reorder matrices is shown in Figure 3.18. The reordered matrix produced less\nnumber of nonzero entries in L (27 nonzeros instead of 29). The Matlab code used to\ngenerate Figures 3.16-3.18 is included in Appendices B.4 and B.5. This code generates a\nmovie of the sequence followed during elimination in the graph.\n\nFigure 3.17: Sequence of eliminations for the red-black ordering algorithm\nFigure 3.18: Comparison of lower triangular matrices produced after LU decomposing\nthe original and the reordered K2D matrices\n\nWe now try to study the behavior of the red-black ordering algorithm under matrices of\ndifferent sizes. Figure 3.19 shows a comparison between the original and the reordered\nmatrices for the number of nonzeros generated in the lower triangular matrix during LU\ndecomposition. Especially for large matrices, the red-black algorithm shows an important\nimprovement due to the significant reduction of the number of nonzeros in L. From this\nfigure we can see that the red-black algorithm performs worse than the minimum degree\nalgorithms discussed earlier. However, the main advantage of the red-black algorithm\nresides in its simplicity and easy implementation.\nFigure 3.19: Number of nonzeros generated in L after LU decomposing the original and\nthe reordered K2D matrices\nFigure 3.20 shows the computational time required by the red-black algorithm as a\nfunction of matrix size.\nFigure 3.20: Computational time required by the red-black ordering algorithm as a\nfunction of matrix size\n\nFinally, we want to compare the computational time required for the LU decomposition\nof the original and the reordered matrices as a function of matrix size. This is shown in\nFigure 3.21. As expected, the reordered matrix requires less time to produce the factors L\nand U. The Matlab code used to generate Figures 3.19-3.21 is included in Appendix B.6.\nFigure 3.21: Computational time required by the LU decomposition of the original and\nthe reordered matrices\n3.3: Graph Separators Algorithm and Nested Dissection\nIn this subsection we discuss an alternative reordering algorithm called graph separators.\nAgain, it is easier to understand the logic behind this algorithm by looking directly at the\ngraph. The key idea is that a separator, S, is introduced to divide the graph in two parts P\nand Q (subsequent divisions of the graph would lead to nested dissection algorithms).\nThe separator S is formed by a collection of nodes and it has typically a smaller or equal\nsize than P and Q. To illustrate this algorithm, we go back to our 9 9matrix. The graph\n×\nof this matrix is shown in Figure 3.22. As shown in this figure, the graph is divided in\ntwo parts by S. In this case, P, Q and S have the same size. The graph was initially\nnumbered row wise as before. In the graph separators algorithm, the graph is reordered\nstarting from all nodes in P, then all nodes in Q and finally all nodes in S as shown in the\nsame\nfigure.\nFor\nthis\nexample,\nthe\npermutation\nvector\nbecomes:\nP = [1\n8] . The elimination sequence is performed following\nthis new order.\nFigure 3.23 shows the structure of the reordered matrix and its corresponding lower\ntriangular factor. The graph separator permutation is given by\nKP\nKPS\nP K )\n\n(\n2D PT = 0\nKQ\nKQS\n\n.\n\nK\n\nSP\nKSQ\nKS\n\nFigure 3.22: Reordering occurred in the graph separators algorithm\nFigure 3.23: Structure of the reordered and the lower triangular factor of the 9 × 9K2D\nmatrices\nAs we mentioned before, the introduction of additional separators will produce a nested\ndissection algorithm. For the example described above, we can introduce a maximum of\ntwo additional separators as shown in Figure 3.24. With this ordering, the permutation\nvector becomes: P = [1\n8] . Figure 3.25 shows the new structure\nof the reordered matrix as well as the structure for L. The total number of nonzeros got\nreduced from 28 to 26 (remember that with out any reordering L has 29 nonzeros).\n\nFigure 3.24: Nested dissection algorithm\nFigure 3.25: Structure of the reordered matrix and its lower triangular factor\nWe now turn to experiment with more sophisticated nested dissection algorithms\navailable in Matlab Mesh Partitioning and Graph Separator Toolbox [4]. In\nparticular, we will compare two algorithms:\n1. \"specnd\": Spectral nested dissection ordering. This algorithm makes use of\ncertain properties of the Laplacian matrix to compute the proper separators. For\nmore information regarding spectral nested dissection algorithms refer to [7].\n2. \"gsnd\": Geometric spectral nested dissection ordering.\n\nFigure 3.26 compares the number of nonzero entries in L after LU decomposing the\nmatrices reordered by both algorithms and the original K2D. From this figure we can see\nthat the results produced by the spectral-based nested dissection algorithms get closer to\nthose obtained by the minimum degree algorithms; however, some of the minimum\ndegree algorithms such as the \"symamd\" algorithm perform much better.\nFigure 3.26: Number of nonzeros generated in L after LU decomposing the original and\nthe reordered K2D matrices (by \"gsnd\" and \"specnd\")\nA comparison of the computational time required by both algorithms is plotted in Figure\n3.27. Again, we confirm that Matlab's minimum degree algorithms perform much faster\non large matrices than the nested dissection algorithms.\nFigure 3.27: Computational time required by \"gsnd\" and \"specnd\" as a function of matrix\nsize\n\nFinally, we compare the computational time required by the LU factorization algorithm\nwhen the input matrix was reordered by both algorithms. The results are plotted in Figure\n3.28. The Matlab codes used to generate Figures 3.26-3.28 is included in Appendix B.6.\nFigure 3.28: Computational time required by the LU decomposition of the original and\nthe reordered matrices\nSection 4: References\n[1]: Internet access: http://www.pages.drexel.edu/~weg22/edge.html. Date accessed:\n04/02/2006.\n[2]: J. S. Lim, Two-Dimensional Signal and Image Processing, Prentice Hall, 1990.\n[3]: Internet access: http://www.cerfacs.fr/algor/Softs/MESHPART/, Matlab mesh\npartitioning and graph separator toolbox. Date accessed: 04/04/06.\n[4]: G. Strang, Introduction to applied mathematics, Wellesley, Cambridge Press.\n[5]: A. Pothen, H.D. Simon, L. Wang, Spectral Nested Dissection, 1992.\nSection 5: Appendices\nA.1: This Appendix contains the Matlab code used to generate Figures 2.1-2.3.\n%Computation of the first and second derivatives of an intensity function\n%I(x) to be used to detect discontinuities using and edge detection\n%algorithm\n%General Parameters\nm = 0:0.01:1;\nK = 5;\nx = -100:100;\n%1D discontinuity\nF1 = exp(-K*m);\nF2 = -F1+2;\nF = fliplr(F1);\n\nF(length(F2)+1:length(F2)*2-1)=F2(2:end);\n%Plot 1\nfigure;\nplot(x,F,zeros(length(x),1)',[0:0.01:2],'--r',x,zeros(length(x),1)','--r')\ntitle('Intensity variation at the discontinuity')\nxlabel('x')\nylabel('Intensity')\n%Gradient\ndelF = gradient(F);\n%Plot 2\nfigure;\nplot(x,delF,zeros(length(x),1)',[0: 2.4279e-004:0.0488],'--r',x,zeros(length(x),1)','--r')\ntitle('First derivative')\nxlabel('x')\nylabel('Intensity')\n%Laplacian\ndel2F = gradient(gradient(F));\n%Plot 2\nfigure;\nplot(x,del2F,zeros(length(x),1)',[-0.0023:2.3e-005:0.0023],'--r',x,zeros(length(x),1)','--\nr')\ntitle('Second derivative')\nxlabel('x')\nylabel('Intensity')\nA.2: Matlab code used to generate Figure 2.4.\n%Forward problem: given an image, find the Laplacian for edge detection\n%General parameters\nU = imread('g1','bmp');\nU = double(U);\nN = size(U,1);\nh = 0.5;\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\n%Raster Scanning the image\ncount = 1;\nfor m = 1:N\nUr(count:count+N-1,1) = U(:,m);\ncount = count+N;\nend\n%Forward problem\nF = K2D*Ur;\n%Inverse-raster scanning for F:\ncount = 1;\nfor m = 1:N\nFnew(:,m) = F(count:count+N-1,1);\ncount = count+N;\nend\nFnew = Fnew(2:499,2:499);\nFnew = -Fnew*h^2;\nfigure; imagesc(Fnew);\ncolorbar\ncolormap gray\naxis equal\n%Note: alternative solution is obtained using Matlab's del2 function:\n% Fnew = del2(U);\nA.3: Inverse Problem code\n%Inverse Problem: Given the Laplacian of an image, find original image\n%General parameters\n\nF = imread('g1','bmp');\nF = double(F);\nN = size(F,1);\nh = 0.5;\n%Computing the 2D Laplacian\nF = del2(F);\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\n%Raster Scanning Measurement:\ncount = 1;\nfor m = 1:N\nFr(count:count+N-1,1) = F(:,m);\ncount = count+N;\nend\n%Elimination\nU = K2D\\Fr;\n%Undo raster scanning for U:\ncount = 1;\nfor m = 1:N\nUnew(:,m) = U(count:count+N-1,1);\ncount = count+N;\nend\nUnew = -Unew/h^2;\nfigure; imagesc(Unew);\ncolorbar\ncolormap gray\naxis equal\nA.4: Plots the eigenvalues of a 9 9 K2D matrix\n×\n%Code to plot the eigenvalues of a 9X9 K2D matrix inside Gershgorin circle\n%General Parameters\nb=-1:0.0125:9;\nc = -4:0.01:4;\nN=3;\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\n%Eigenvalues of K2D\nE = eig(K2D);\n%Gershgorin Circle\nx = 0:0.01:8;\ny = sqrt(4^2-(x-4).^2);\n%Plot\nfigure;\nhold on\nplot(x,y,'--r',x,-y,'--r',b,zeros(801,1),'--r',zeros(801,1),c,'--r')\nplot(E,zeros(length(E),1),'X')\nhold off\naxis equal\nxlabel('Real')\nylabel('Imaginary')\ntitle('Eigenvalues of K2D')\nA.5: Plots the condition number as a function of size for K2D\n%Code to plot condition number as a function of size for matrix K2D\nT=2:70;\nfor m = 1:length(T)\nN = T(m);\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\n\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear K; clear I; clear N;\n%Compute Eigenvalues and condition number\nE = eig(K2D);\ncondi(m) = max(E)/min(E);\nclear E;\nend\n%Plot\nplot(T.^2,condi)\nxlabel('size: N^2')\nylabel('Condition Number')\ntitle('Condition Number vs size of K2D')\nB.1: Generates plots that compare different minimum degree algorithms\n%Comparison of different minimum degree algorithms\nN = 9;\n%1D difference matrix\nK = sparse(diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1));\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear I K;\nfor m = 1:5\nif m ==1\np=symamd(K2D);\ns = 'symamd';\nend\nif m ==2\np=symmmd(K2D);\ns = 'symmmd';\nend\nif m ==3\np=colamd(K2D);\ns = 'colamd';\nend\nif m ==4\np=colmmd(K2D);\ns = 'colmmd';\nend\nif m ==5\np=realmmd(K2D);\ns = 'realmmd'\nend\nK2Dmod=K2D(p,p);\nfigure;\nspy(K2Dmod)\ntitle(['Matrix reordered using: ',s]);\n[L,U]=lu(K2Dmod);\nfigure;\nspy(L)\ntitle(['Lower triangular matrix from K2D reordered with: ',s]);\nend\nB.2: Comparison of different minimum degree algorithms as a function of matrix size\n%Comparison of different minimum degree algorithms\nT = 2:5:62;\nfor k = 1:length(T)\nN = T(k);\n%1D difference matrix\nK = sparse(diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1));\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear I K;\nfor m = 1:5\nif m ==1\np=symamd(K2D);\nend\nif m ==2\np=symmmd(K2D);\nend\nif m ==3\np=colamd(K2D);\n\nend\nif m ==4\np=colmmd(K2D);\nend\nif m ==5\np=realmmd(K2D);\nend\nK2Dmod=K2D(p,p);\n[L,U]=lu(K2Dmod);\nNoN(m,k) = nnz(L);\nclear L K2Dmod;\nend\nend\n%Plot\nfigure;\nplot(T.^2,NoN(1,:),T.^2,NoN(2,:),T.^2,NoN(3,:),T.^2,NoN(4,:),T.^2,NoN(5,:))\nlegend('symamd','symmmd','colamd','colmmd','realmmd')\ntitle('Comparison of minimum degree algorithms vs matrix size')\nylabel('Number of nonzeros in L');\nxlabel('Size: N^2')\nfigure;\nplot(T(4:6).^2,NoN(1,4:6),T(4:6).^2,NoN(2,4:6),T(4:6).^2,NoN(5,4:6))\nlegend('symamd','symmmd','realmmd')\ntitle('Comparison of minimum degree algorithms vs matrix size')\nylabel('Number of nonzeros in L');\nxlabel('Size: N^2')\nB.3: Comparison: nonzeros in Cholesky factor, computational time for LU and Cholesky\ndecompositions, computational time in reordering\n%Comparison of different minimum degree algorithms: Computational time,\n%Cholesky and LU decomposition and number of nonzero elements\nT = 2:5:347;\nfor k = 1:length(T)\nN = T(k);\n%1D difference matrix\nK = sparse(diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1));\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear I K;\nfor m = 1:4\nif m ==1\ntic\np=symamd(K2D);\ntime = toc;\nK2Dmod=K2D(p,p);\nend\nif m ==2\ntic\np=symmmd(K2D);\ntime = toc;\nK2Dmod=K2D(p,p);\nend\nif m ==3\ntic\np=colamd(K2D);\ntime =toc;\nK2Dmod=K2D(p,p);\nend\nif m ==4\ntic\np=colmmd(K2D);\ntime=toc;\nK2Dmod=K2D(p,p);\nend\nif m ==5\ntic\np=realmmd(K2D);\ntime=toc;\nK2Dmod=K2D(p,p);\nend\ntic\n[L]=chol(K2Dmod);\ntime2 = toc;\ntic\n[L2,U2]=lu(K2Dmod);\ntime4 = toc;\nNoN1(m,k) = nnz(L);\n\nNoN2(m,k) = nnz(L2);\nTiMe(m,k)=time;\nTiMeChol(m,k)=time2;\nTiMeLU(m,k)=time4;\nclear L K2Dmod time conn time2 time3 time4 time5;\nend\nend\n%Plot\nfigure;\nplot(T.^2,NoN1(1,:),T.^2,NoN1(2,:),T.^2,NoN1(3,:),T.^2,NoN1(4,:))\nlegend('symamd','symmmd','colamd','colmmd')\ntitle('Comparison of minimum degree algorithms vs matrix size')\nylabel('Number of nonzeros in U');\nxlabel('Size: N^2')\nfigure;\nplot(T.^2,TiMeChol(1,:),T.^2,TiMeChol(2,:),T.^2,TiMeChol(3,:),T.^2,TiMeChol(4,:))\nlegend('Cholesky <= symamd','Cholesky <= symmmd','Cholesky <= colamd','Cholesky <=\ncolmmd')\ntitle('Computational time for Cholesky decomposing reordered matrices vs size')\nylabel('Time (sec)');\nxlabel('Size: N^2')\nfigure;\nplot(T.^2,TiMeLU(1,:),T.^2,TiMeLU(2,:),T.^2,TiMeLU(3,:),T.^2,TiMeLU(4,:))\nlegend('LU <= symamd','LU <= symmmd','LU <= colamd','LU <= colmmd')\ntitle('Computational time for LU decomposing reordered matrices vs size')\nylabel('Time (sec)');\nxlabel('Size: N^2')\nfigure;\nplot(T.^2,TiMe(1,:),T.^2,TiMe(2,:),T.^2,TiMe(3,:),T.^2,TiMe(4,:))\nlegend('symamd','symmmd','colamd','colmmd')\ntitle('Reordering computational time')\nylabel('Time (sec)');\nxlabel('Size: N^2')\nB.4: Movie for red-black ordering algorithm (based on code obtain at [3])\n%Movie for red-black ordering algorithm\nN=3;\nA=sparse(toeplitz([2,-1,zeros(1,N-2)]));\nI=speye(N,N);\nB=kron(A,I)+kron(I,A);\n[x,y]=ndgrid(1:N,1:N);\np=redblack(B);\nB=B(p,p);\nR=chol(B(p,p));\nxy=[x(p);y(p)]';\nn=size(B,1);\nL=zeros(n);\nlpars={'marker','.','linestyle','none','markersize',64};\ntpars={'fontname','helvetica','fontsize',16,'horiz','center','col','g'};\nfor i=1:n\nclf\ngplot(B,xy);\nline(xy(i:N^2,1),xy(i:N^2,2),'color','k',lpars{:});\nfor j=i:n\ndegree=length(find(B(:,j)))-1;\ntext(xy(j,1),xy(j,2),int2str(degree),tpars{:});\nend\naxis equal,axis off,axis([1,N,1,N])\npause(0.3);\nline(xy(i,1),xy(i,2),'color','r',lpars{:});\npause(0.3);\nL(i,i)=sqrt(B(i,i));\nL(i+1:n,i)=B(i+1:n,i)/L(i,i);\nB(i+1:n,i+1:n)=B(i+1:n,i+1:n)-L(i+1:n,i)*L(i+1:n,i)';\nB(i, i:n)=0;\nB(i:n, i)=0;\nend\nspy(L,32)\nB.5: Red-black ordering algorithm\nfunction p=redblack(A)\n%REDBLACK: Computes the permutation vector implementing the red-black\n%ordering algorithm\n\nn=size(A,1);\ntemp = 1:n;\ncount = 0;\nodd = 2;\nflag =1;\nfor m = 1:n\nif flag\np(m)=temp(m+count);\ncount = count+1;\nif p(m)==n|p(m)+2>n\nflag = 0;\nend\nelse\np(m)=temp(odd);\nodd = odd+2;\nend\nend\nB.5: Red-black algorithm: comparison code\n%Comparison code: red-black algorithm\nT=3:2:233;\nfor m = 1:length(T)\nN = T(m);\nA=sparse(toeplitz([2,-1,zeros(1,N-2)]));\nI=speye(N,N);\nB=kron(A,I)+kron(I,A);\nclear N I A;\ntic\np=redblack(B);\ntime = toc;\nC=B(p,p);\ntic\n[L,U]= lu(B);\ntime2 = toc;\ntic\n[L2,U2]= lu(C);\ntime3 = toc;\nNoN1(m)=nnz(L);\nNoN2(m)=nnz(L2);\nTiMe(m)=time;\nTiMeLUor(m)=time2;\nTiMeLUrb(m)=time3;\nclear L L2 U U2 time time2 time3 p B C m;\nend\nfigure;\nplot(T.^2,NoN1,T.^2,NoN2)\ntitle('Number of nonzeros in L as a function of matrix size')\nlegend('L<=Original K2D','L<=Reordered K2D');\nxlabel('Size: N^2')\nylabel('Number of nonzeros in L')\nfigure;\nplot(T.^2,TiMe)\ntitle('Computational time for red-black ordering algorithm')\nxlabel('Size: N^2')\nylabel('Time (sec)')\nfigure;\nplot(T.^2,TiMeLUor,T.^2,TiMeLUrb)\ntitle('Computational time of the LU decomposition algorithm')\nlegend('L<=Original K2D','L<=Reordered K2D');\nxlabel('Size: N^2')\nylabel('Time (sec)')\nB.6: Nested dissection ordering algorithm: comparison code\n%Comparison code: red-black algorithm\nT=3:4:100;\nfor m = 1:length(T)\nN = T(m);\nA=sparse(toeplitz([2,-1,zeros(1,N-2)]));\nI=speye(N,N);\nB=kron(A,I)+kron(I,A);\nclear N I A;\ntic\np = gsnd(B);\ntime = toc;\ntic\nd = specnd(B);\n\ntime4 = toc;\nC=B(p,p);\nD = B(d,d);\ntic\n[L,U]= lu(B);\ntime2 = toc;\ntic\n[L2,U2]= lu(C);\ntime3 = toc;\ntic\n[L3,U3]= lu(D);\ntime6 = toc;\nNoN1(m)=nnz(L);\nNoN2(m)=nnz(L2);\nNoN3(m)=nnz(L3);\nTiMe(m)=time;\nTiMe2(m)=time4;\nTiMeLUor(m)=time2;\nTiMeLUnd1(m)=time3;\nTiMeLUnd2(m)=time6;\nclear L L2 U U2 time time2 time3 p B C m time4 time6 D d;\nend\nfigure;\nplot(T.^2,NoN1,T.^2,NoN2,T.^2,NoN3)\ntitle('Number of nonzeros in L as a function of matrix size')\nlegend('L<=Original K2D','L<=Reordered by \"gsnd\"','L<=Reordered by \"specnd\"');\nxlabel('Size: N^2')\nylabel('Number of nonzeros in L')\nfigure;\nplot(T.^2,TiMe,T.^2,TiMe2)\ntitle('Computational times for nested dissection algorithms: \"gsnd\" and \"specnd\"')\nlegend('\"gsnd\"','\"specnd\"')\nxlabel('Size: N^2')\nylabel('Time (sec)')\nfigure;\nplot(T.^2,TiMeLUor,T.^2,TiMeLUnd1,T.^2,TiMeLUnd2)\ntitle('Computational time of the LU decomposition algorithm')\nlegend('L<=Original K2D','L<=Reordered by \"gsnd\"','L<=Reordered by \"specnd\"');\nxlabel('Size: N^2')\nylabel('Time (sec)')"
    },
    {
      "category": "Resource",
      "title": "am35.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/5f469576670a2c7a874999dbafc8c3cb_am35.pdf",
      "content": "c2006 Gilbert Strang\n3.5\nFinite Differences and Fast Poisson Solvers\nK\nIt is extremely unusual to use eigenvectors to solve a linear system KU = F. You\nneed to know all the eigenvectors of K, and (much more than that) the eigenvector\nmatrix S must be especially fast to work with. Both S and S-1 are required, because\n-1 = S-1S-1 . The eigenvalue matrices and -1 are diagonal and quick. When\nthe derivatives in Poisson's equation -uxx\nyy = f(x, y) are replaced by second\n- u\ndifferences, we do know the eigenvectors (discrete sines in the columns of S). Then\nthe Fourier transform quickly inverts and multiplies by S.\nOn a square mesh those differences have -1, 2, -1 in the x-direction and -1, 2, -1\nin the y-direction (divided by h2, where h = meshwidth). Figure 3.20 shows how\nthe second differences combine into a \"5-point molecule\" for the discrete Laplacian.\nBoundary values u = u0(x, y) are assumed to be given along the sides of a unit square.\nThe square mesh has N interior points in each direction (N = 5 in the figure).\nIn this case there are n = N 2 = 25 unknown mesh values Uij . When the molecule is\ncentered at position (i, j), the discrete Poisson equation gives a row of K2D U = F:\nK2D U = F\n4uij - ui, j-1 - ui-1, j - ui+1, j - ui, j+1 = h2f(ih, jh) = Fij .(1)\nThe inside rows of K2D have five nonzero entries 4, -1, -1, -1, -1. When\n(i, j) is next to a boundary point of the square, the known value of u0 at that neigh\nboring boundary point moves to the right side of equation (1). It becomes part of\nthe vector F, and a -1 drops out of the corresponding row of K. So K2D has five\nnonzeros on inside rows and fewer nonzeros on next-to-boundary rows.\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nN\nN\nFigure 3.20: 5-point molecules at inside points (fewer -1's next to boundary).\nThis matrix K2D is sparse. Using blocks of size N, we can create the 2D matrix\nfrom the familiar N by N second difference matrix K. Number the nodes of the\nsquare a row at a time (this \"natural numbering\" is not necessarily best). Then the\n-1's for the neighbor above and the neighbor below are N positions away from the\n\n3.5. FINITE DIFFERENCES AND FAST POISSON SOLVERS c2006 Gilbert Strang\nmain diagonal of K2D. The 2D matrix is block tridiagonal with tridiagonal blocks:\n⎡\n⎡\n2 -1\nK + 2I\n-I\n⎢\n2 -1\n⎢\nK + 2I -I\nK = 6 -1\n⎢\nK2D = 6\n-I\n⎢\n(2)\n⎣\n⎣\n·\n·\n·\n·\n·\n·\n-I\nK + 2I\n-1\nN 2\nSize N\nSize n =\nBandwidth w = N\nTime N\nSpace nw = N 3\nTime nw2 = N 4\nThe matrix K2D has 4's down the main diagonal in equation (1). Its bandwidth\nw = N is the distance from the main diagonal to the nonzeros in -I. Many of the\nspaces in between are filled during elimination ! This is discussed in Section 6.1.\nKronecker product\nOne good way to create K2D from K and I is by the kron\ncommand. When A and B have size N by N, the matrix kron(A, B) is N 2 by N 2 .\nEach number aij is replaced by the block aij B.\nTo take second differences in all rows at the same time, kron(I, K) produces a\nblock diagonal matrix of K's. In the y-direction, kron(K, I) multiplies -1 and 2 and\n-1 by I (dealing with a column of meshpoints at a time). Add x and y differences:\n⎡\n⎡\nK\n2I\n-I ·\nK2D = kron(I, K) + kron(K, I) = 4\nK\n⎣ + 4 -I\n2I\n· ⎣\n(3)\n·\n·\n·\n·\nThis sum agrees with the 5-point matrix in (2). The computational question is how\nto work with K2D. We will propose three methods:\n1.\nElimination in a good order (not using the special structure of K2D)\n2.\nFast Poisson Solver (applying the FFT = Fast Fourier Transform)\n3.\nOdd-Even Reduction (since K2D is block tridiagonal).\nThe novelty is in the Fast Poisson Solver, which uses the known eigenvalues and\neigenvectors of K and K2D. It is strange to solve linear equations KU = F by\nexpanding F and U in eigenvectors, but here it is extremely successful.\nElimination and Fill-in\nFor most two-dimensional problems, elimination is the way to go. The matrix from\na partial differential equation is sparse (like K2D). It is banded but the bandwidth\nis not so small. (Meshpoints cannot be numbered so that all five neighbors in the\nmolecule receive nearby numbers.) Figure 3.21 has points 1, . . . , N along the first\nrow, then a row at a time going up the square. The neighbors above and below point\nj have numbers j - N and j + N.\n\nc\n2006 Gilbert Strang\nOrdering by rows produces the -1's in K2D that are N places away from the\ndiagonal. The matrix K2D has bandwidth N. The key point is that elimination\nfills in the zeros inside the band. We add row 1 (times 4 ) to row 2, to eliminate\nthe -1 in position (2, 1). But the last -1 in row 1 produces a new -1 in row 2. A\nzero inside the band has disappeared. As elimination continues from A to U, virtually\nthe whole band is filled in.\nIn the end, U has about 5 times 25 nonzeros (this is N 3, the space needed to store\nU). There will be about N nonzeros next to the pivot when we reach a typical row,\nand N nonzeros below the pivot. Row operations to remove those nonzeros will require\nup to N 2 multiplications, and there are N 2 pivots. So the count of multiplications is\nabout 25 times 25 (this is N 4, for elimination in 2D).\nFigure 3.21: Typical rows of K2D have 5 nonzeros. Elimination fills in the band.\nSection 6.1 will propose a different numbering of the meshpoints, to reduce the\nfill-in that we see in U. This reorders the rows of K2D by a permutation matrix\nP, and the columns by P T . The new matrix P(K2D)P T is still symmetric, but\nelimination (with fill-in) proceeds in a completely different order. The MATLAB\ncommand symamd(K) produces a nearly optimal choice of P.\nElimination is fast in two dimensions (but a Fast Poisson Solver is faster !). In\nthree dimensions the matrix size is N 3 and the bandwidth is N 2 . By numbering the\nnodes a plane at a time, vertical neighbors are N 2 nodes apart. The operation count\nfor elimination becomes N 7, which can be seriously large. Chapter 6 on Solving Large\nSystems will introduce badly needed alternatives to elimination in 3D.\nSolvers Using Eigenvalues\nOur matrices K and K2D are extremely special. We know the eigenvalues and eigen\nvectors of the second-difference matrix K. The eigenvalues have the special form\n= 2 -2 cos ∂, and the eigenvectors are discrete sines. There will be a similar pattern\nfor K2D, which is formed in a neat way from K (by Kronecker product). The Poisson\nSolver uses those eigenvalues and eigenvectors to solve (K2D)(U2D) = (F2D). On a\nsquare mesh it is much faster than elimination.\nHere is the idea, first in one dimension. The matrix K has eigenvalues 1, . . . , N\nand eigenvectors y1, . . . , yN . There are three steps to the solution of KU = F:\n1.\nExpand F\nF = a1y1 +\n+ aN yN\n2.\nak\nk\n3.\nU\na1/1) y1 +\naN /N ) yN .\nas a combination\n· · ·\nof the eigenvectors\nDivide each\nby\nRecombine eigenvectors into\n= (\n· · · + (\n\n3.5. FINITE DIFFERENCES AND FAST POISSON SOLVERS c2006 Gilbert Strang\nThe success of the method depends on the speed of steps 1 and 3. Step 2 is fast.\nTo see that U in step 3 is correct, multiply it by the matrix K. Every eigenvector\ngives Ky = y. That cancels the in each denominator. The result is that KU\nagrees with the vector F in step 1.\nNow look at the calculation required in each step, using matrices. Suppose S is\nthe eigenvector matrix, with the eigenvectors y1, . . . , yN of K in its columns. Then\nthe coefficients a1, . . . , aN come by solving Sa = F:\n⎡\n⎡\na1\n6 . ⎢\nStep 1\nSolve Sa = F\n4 y1 · · · yN ⎣4 .. ⎣ = a1y1 +\n+ aN yN = F . (4)\n· · ·\naN\nThus a = S-1F. Then step 2 divides the a's by the 's to find -1a = -1S-1F.\n(The eigenvalue matrix is just the diagonal matrix of 's.) Step 3 uses those\ncoefficients ak/k in recombining the eigenvectors into the solution vector U:\n⎡\n⎡\na1/1\n.\n⎢\nStep 3\nU = 4 y1\nyN ⎣4\n.\n⎣ = S-1 a = S-1S-1F .\n(5)\n.\n· · ·\naN /N\nYou see the matrix K-1 = S-1S-1 appearing in that last formula. We have K-1F\nbecause K itself is SS-1--the usual diagonalization of a matrix. The eigenvalue\nmethod is using this SS-1 factorization instead of K = LU from elimination.\nThe speed of steps 1 and 3 depends on multiplying quickly by S-1 and S. Those\nare full matrices, not sparse like K. Normally they both need N 2 operations in one\ndimension (where the matrix size is N). But the \"sine eigenvectors\" in S give the\nDiscrete Sine Transform, and the Fast Fourier Transform executes S and S-1 in\nN log2 N steps. In one dimension this is not as fast as cN from elimination, but in\ntwo dimensions N 2 log(N 2) easily wins.\nColumn k of the matrix S contains the eigenvector yk. The number Sjk = sin jk\nN+1\nis the jth component of that eigenvector. For our example with N = 5 and N +1 = 6,\nyou could list the sines of every multiple of /6. Here are those numbers:\np\np\n3 1\nSines ,\n, 1,\n,\n, 0, (repeat with minus signs) (repeat 12 numbers forever) .\nThe kth column of S (kth eigenvector yk) takes every kth number from that list:\n⎡\n⎡\n⎡\n⎡\n⎡\n1/2\np\n3/2\np\n3/2\n1/2\n6 p\n3/2⎢\n6 p\n3/2⎢\n⎢\n6 p\n3/2⎢\n6 p\n3/2⎢\n6 0\n⎢\n⎢\n⎢\n6 -\n⎢\n6 -\n⎢\n0 ⎢\n⎢\n0 ⎢\ny5 = 6\n1 ⎢\ny1 = 6\n1 ⎢\ny2 = 6\n-\np\n3/2\n⎢\ny3 = 6-1 ⎢\ny4 =\n⎢\n⎢\n⎢\n4 p\n3/2⎣\n⎣\n4 0⎣\n4 p\n3/2⎣\n⎣\n1/2\n-\np\n3/2\n-\np\n3/2\n-\np\n3/2\n1/2\nThose eigenvectors are orthogonal ! This is guaranteed by the symmetry of K. All\nthese eigenvectors have length 3 = (N + 1)/2. Dividing each column by\np\n3, we have\northonormal eigenvectors. S/\np\n3 is an orthogonal matrix Q, with QTQ = I.\n\nc2006 Gilbert Strang\nIn this special case, S and Q are also symmetric. So Q-1 = QT = Q.\nNotice that yk has k - 1 changes of sign. It comes from k loops of the sine curve.\nThe eigenvalues are increasing: = 2 -\np\n3, 2 - 1, 2 - 0, 2 + 1, 2 +\np\n3. Those\neigenvalues add to 10, which is the sum down the diagonal (the trace) of K5. The\nproduct of the 5 eigenvalues (easiest by pairs) confirms that det(K5) = 6.\nFast Poisson Solvers\nTo extend this eigenvalue method to two dimensions, we need the eigenvalues and\neigenvectors of K2D. The key point is that the N 2 eigenvectors of K2D are separable.\nEach eigenvector ykl separates into a product of sines:\ny\ny\nis sin N\nsin N+1 .\n(6)\nEigenvectors\nkl\nThe (i, j) component of\nkl\nik\n+1\njl\nWhen you multiply that eigenvector by K2D, you have second differences in the x-\ndirection and y-direction. The second differences of the first sine (x-direction) produce\na factor k = 2 - 2 cos k . This is the eigenvalue of K in 1D. The second differences\nN+1\nof the other sine (y-direction) produce a factor l = 2 - 2 cos l . So the eigenvalue\nN+1\nin two dimensions is the sum k + l of one-dimensional eigenvalues:\n(K\ny\n= y\n\n-\nN+1\n-\nN+1\n(7)\n2D)\nkl\nkl\nkl\nkl = (2\n2 cos k ) + (2\n2 cos l ) .\nNow the solution of K2D U = F comes by a two-dimensional sine transform:\n⎤⎤ akl\nik\njl\nFi,j = ⎤⎤ akl sin ik sin jl\nUi,j =\nsin N +1 sin\n(8)\nN +1\nN+1\nkl\nN+1\nAgain we find the a's, divide by the 's, and build U from the eigenvectors in S:\na = S-1F\n-1a\n-1S-1F\nU = S-1S-1F\nStep 1\nStep 2\n=\nStep 3\nSwartztrauber [SIAM Review 19 (1977) 490] gives the operation count 2N 2 log2 N.\nThis uses the Fast Sine Transform (based on the FFT) to multiply by S-1 and S.\nThe Fast Fourier Transform is explained in Section 4.3.\nS\nNote\nWe take this chance to notice the good properties of a Kronecker product\nC = kron(A, B). Suppose A and B have their eigenvectors in the columns of SA and\nB . The eigenvalues are in A and B . Then we know SC and C :\nThe eigenvectors of kron(A, B) are in kron(SA, SB ).\n(9)\nThe eigenvalues are in kron(A, B ).\nThe diagonal blocks in kron(A, B) are entries k(A) times the diagonal matrix B.\nSo the eigenvalues kl of C are just the products k(A)l(B).\n\n3.5. FINITE DIFFERENCES AND FAST POISSON SOLVERS c2006 Gilbert Strang\nIn our case A and B were I and K. The matrix K2D added the two products\nkron(I, K) and kron(K, I). Normally we cannot know the eigenvectors and eigen\nvalues of a matrix sum--except when the matrices commute. Since all our matrices\nare formed from K, these Kronecker products do commute. This gives the separable\neigenvectors and eigenvalues in (6) and (7).\nCyclic Odd-Even Reduction\nThere is an entirely different (and very simple) approach to KU = F. I will start in\none dimension, by writing down three rows of the usual second difference equation:\nRow i - 1\n-Ui-2 + 2Ui-1 - Ui\n= Fi-1\nRow i\n-Ui-1 + 2Ui - Ui+1\n= Fi\n(10)\nRow i + 1\n-Ui + 2Ui+1 - Ui+2 = Fi+1\nMultiply the middle equation by 2, and add. This eliminates Ui-1 and Ui+1:\n-Ui-2\nUi - Ui\n= Fi-1\nFi + Fi\n.\n(11)\nOdd-even reduction in 1D\n+ 2\n+2\n+ 2\n+1\nNow we have a half-size system, involving only half of the U's (with even indices).\nThe new system (11) has the same tridiagonal form as before. When we repeat, cyclic\nreduction produces a quarter-size system. Eventually we can reduce KU = F to a\nvery small problem, and then cyclic back-substitution produces the whole solution.\nHow does this look in two dimensions ? The big matrix K2D is block triangular:\n⎡\nA\n-I\n⎢\nA\n-I\nK2D = 6 -I\n⎢\nwith A = K + 2I\nfrom equation (4) .\n(12)\n⎣\n·\n·\n·\nA\n-I\nThe three equations in (10) become block equations for whole rows of N mesh values.\nWe are taking the unknowns Ui = (Ui1, . . . , UiN ) a row at a time. If we write three\nrows of (10), the block A replaces the number 2 in the scalar equation. The block\n-I replaces the number -1. To reduce (K2D)(U2D) = (F2D) to a half-size system,\nmultiply the middle equation (with i even) by A and add the three block equations:\nReduction in 2D\n-IUi-2 + (A2 - 2I)Ui - IUi+2 = Fi-1 + AFi + Fi+1 . (13)\nThe new half-size matrix is still block tridiagonal. The diagonal blocks that were\npreviously A in (12) are now A2 - 2I, with the same eigenvectors. The unknowns are\nthe 1 N 2 values Ui,j at meshpoints with even indices i.\nThe bad point is that each new diagonal block A2 - 2I has five diagonals, where\nthe original block A = K+2I had three diagonals. This bad point gets worse as cyclic\nreduction continues. At step r, the diagonal blocks become Ar = A2\nr-1 - 2I and their\nbandwidth doubles. We could find tridiagonal factors (A-\np\n2I)(A+\np\n2I) = A2 - 2I,\n\nc2006 Gilbert Strang\nbut the number of factors grows quickly. Storage and computation and roundoff error\nare increasing too rapidly with more reduction steps.\nStable variants of cyclic reduction were developed by Buneman and Hockney. The\nclear explanation by Buzbee, Golub, and Nielson [SIAM Journal of Numerical Anal\nysis 7 (1970) 627] allows other boundary conditions and other separable equations,\ncoming from polar coordinates or convection terms like C@u/@x + D@u/@y. After m\nsteps of cyclic reduction, Hockney went back to a Fast Poisson Solver.\nThis combination FACR(m) is widely used, and the optimal number of cyclic\nreduction steps (before the FFT takes over) is small. For N = 128 a frequent choice\nis m = 2. Asymptotically mopt grows like log log N and the operation count for\nFACR(mopt) is 3N 2 log log N. In practical scientific computing with N 2 unknowns\n(or with N 3 unknowns in three dimensions), a Fast Poisson Solver is a winner.\n****** Add code for Fast Poisson *******\nProblem Set 3.5\nProblems 1-\nare for readers who get enthusiastic about kron.\nWhy is the transpose of C = kron(A, B) equal to kron(AT, BT) ? Why is the\ninverse equal to C-1 = kron(A-1, B-1) ? You have to transpose each block aij B\nof the Kronecker product C, and then patiently multiply CC-1 by blocks.\nC is symmetric (or orthogonal) when A and B are symmetric (or orthogonal).\nWhy is the matrix C = kron(A, B) times the matrix D = kron(S, T ) equal to\nCD = kron(AS, BT ) ? This needs even more patience with block multiplica\ntion. The inverse CC-1 = kron(I, I) = I is a special case.\nNote\nSuppose S and T are eigenvector matrices for A and B. From AS =\nSA and BT = T B we have CD = kron(AS, BT ) = kron(SA, T B ). Then\nCD = D kron(A, B ) = DC . So D = kron(S, T ) is the eigenvector matrix\nfor C."
    },
    {
      "category": "Resource",
      "title": "am36.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/cb76484c6f1637c4ae205b66064c28c8_am36.pdf",
      "content": "CHAPTER 3. BOUNDARY VALUE PROBLEMS\n3.6\nSolving Large Linear Systems\nFinite elements and finite differences produce large linear systems KU = F. The\nmatrices K are extremely sparse. They have only a small number of nonzero entries in\na typical row. In \"physical space\" those nonzeros are clustered tightly together--they\ncome from neighboring nodes and meshpoints. But we cannot number N2 nodes in a\nplane in any way that keeps neighbors close together! So in 2-dimensional problems,\nand even more in 3-dimensional problems, we meet three questions right away:\n1. How best to number the nodes\n2. How to use the sparseness of K (when nonzeros can be widely separated)\n3. Whether to choose direct elimination or an iterative method.\nThat last point will split this section into two parts--elimination methods in 2D\n(where node order is important) and iterative methods in 3D (where preconditioning\nis crucial).\nTo fix ideas, we will create the n equations KU = F from Laplace's difference\nequation in an interval, a square, and a cube. With N unknowns in each direction,\nK has order n = N or N2 or N3 . There are 3 or 5 or 7 nonzeros in a typical row of\nthe matrix. Second differences in 1D, 2D, and 3D are shown in Figure 3.17.\n-1\n-1\nBlock\n-1\nTridiagonal K\nTridiagonal\nK\n-1\n-1\n-1\n-1\n-1\n-1\nN2 by N2\nN by N\n-1\nN3 by N3\n-1 -1\nFigure 3.17: 3, 5, 7 point difference molecules for -uxx, -uxx - uyy , -uxx - uyy - uzz .\nAlong a typical row of the matrix, the entries add to zero. In two dimensions this\nis 4 - 1 - 1 - 1 - 1 = 0. This \"zero sum\" remains true for finite elements (the element\nshapes decide the exact numerical entries). It reflects the fact that u = 1 solves\nLaplace's equation and Ui = 1 has differences equal to zero. The constant vector\nsolves KU = 0 except near the boundaries. When a neighbor is a boundary point\nwhere Ui is known, its value moves onto the right side of KU = F. Then that row of\nK is not zero sum. Otherwise K would be singular, if K ∗ ones(n, 1) = zeros(n, 1).\nUsing block matrix notation, we can create the 2D matrix K = K2D from the\nfamiliar N by N second difference matrix K. We number the nodes of the square a\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nrow at a time (this \"natural numbering\" is not necessarily best). Then the -1's for\nthe neighbor above and the neighbor below are N positions away from the main\ndiagonal of K2D. The 2D matrix is block tridiagonal with tridiagonal blocks:\n⎡\n⎤\n⎤\n⎡\n2 -1\nK + 2I\n-I\n⎢⎢⎣\n-1\n2 -1\n·\n·\n·\n⎥⎥⎦\nK2D =\n⎢⎢⎣\n-I\nK + 2I -I\n·\n·\n·\n⎥⎥⎦\nK =\n(1)\n-1\n\n-I\nK\n\n+ 2I\nSize N\nElimination in this order: K2D has size n = N2\nTime N\nBandwidth w = N, Space nw = N3 , Time nw2 = N4\nThe matrix K2D has 4's down the main diagonal. Its bandwidth w = N is the\ndistance from the diagonal to the nonzeros in -I. Many of the spaces in between are\nfilled during elimination! Then the storage space required for the factors in K = LU\nis of order nw = N3 . The time is proportional to nw2 = N4, when n rows each\ncontain w nonzeros, and w nonzeros below the pivot require elimination.\nThose counts are not impossibly large in many practical 2D problems (and we\nshow how they can be reduced). The horrifying large counts come for K3D in three\ndimensions. Suppose the 3D grid is numbered by square cross-sections in the natural\norder 1, . . . , N. Then K3D has blocks of order N2 from those squares. Each square\nis numbered as above to produce blocks coming from K2D and I = I2D:\n⎤\n⎡ K2D + 2I\n-I\nSize n = N3\nK3D =\n⎢⎢⎣\n-I\nK2D + 2I -I\n·\n·\n·\n⎥⎥⎦\nBandwidth w = N2\nElimination space nw = N5\n-I\nK2D + 2I\nElimination time ≈ nw2 = N7\nNow the main diagonal contains 6's, and \"inside rows\" have six -1's. Next to a point\nor edge or corner of the boundary cube, we lose one or two or three of those -1's.\nThe good way to create K2D from K and I (N by N) is to use the kron(A, B)\ncommand. This Kronecker product replaces each entry aij by the block aij B. To take\nsecond differences in all rows at the same time, and then all columns, use kron:\nK2D = kron(K, I) + kron(I, K) .\n(2)\nThe identity matrix in two dimensions is I2D = kron(I, I). This adjusts to allow\nrectangles, with I's of different sizes, and in three dimensions to allow boxes. For a\ncube we take second differences inside all planes and also in the z-direction:\nK3D = kron(K2D, I) + kron(I2D, K) .\nHaving set up these special matrices K2D and K3D, we have to say that there are\nspecial ways to work with them. The x, y, z directions are separable. The geometry\n(a box) is also separable. See Section 7.2 on Fast Poisson Solvers. Here the matrices\nK and K2D and K3D are serving as models of the type of matrices that we meet.\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nMinimum Degree Algorithm\nWe now describe (a little roughly) a useful reordering of the nodes and the equations\nin K2DU = F. The ordering achieves minimum degree at each step--the number\nof nonzeros below the pivot row is minimized. This is essentially the algorithm\nused in MATLAB's command U = K\\F , when K has been defined as a sparse matrix.\nWe list some of the functions from the sparfun directory:\nspeye (sparse identity I)\nnnz (number of nonzero entries)\nfind\n(find indices of nonzeros)\nspy\n(visualize sparsity pattern)\ncolamd and symamd\n(approximate minimum degree permutation of K)\nYou can test and use the minimum degree algorithms without a careful analysis. The\napproximations are faster than the exact minimum degree permutations colmmd and\nsymmmd. The speed (in two dimensions) and the roundoff errors are quite reasonable.\nIn the Laplace examples, the minimum degree ordering of nodes is irregular com\npared to \"a row at a time.\" The final bandwidth is probably not decreased. But the\nnonzero entries are postponed as long as possible! That is the key.\nThe difference is shown in the arrow matrix of Figure 3.18. On the left, minimum\ndegree (one nonzero off the diagonal) leads to large bandwidth. But there is no fill-in.\nElimination will only change its last row and column. The triangular factors L and\nU have all the same zeros as A. The space for storage stays at 3n, and elimination\nneeds only n divisions and multiplications and subtractions.\n⎤\n⎡\n⎤\n⎡ ∗\n∗\n∗\n∗\n⎢⎢⎢⎢⎢⎢⎢⎢⎣\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n⎥⎥⎥⎥⎥⎥⎥⎥⎦\nBandwidth\n6 and 3\nFill-in\n0 and 6\n⎢⎢⎢⎢⎢⎢⎢⎢⎣\n∗\n∗\n∗\n∗\n\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n∗\n∗\nF\nF\n\n∗\nF ∗ F\n⎥⎥⎥⎥⎥⎥⎥⎥⎦\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n∗\nF\nF\n∗\nFigure 3.18: Arrow matrix: Minimum degree (no F) against minimum bandwidth.\nThe second ordering reduces the bandwidth from 6 to 3. But when row 4 is\nreached as the pivot row, the entries indicated by F are filled in. That full lower\nquarter of A gives 1\n8n 2 nonzeros to both factors L and U . You see that the whole\n\"profile\" of the matrix decides the fill-in, not just the bandwidth.\nThe minimum degree algorithm chooses the (k + 1)st pivot column, after k\ncolumns have been eliminated as usual below the diagonal, by the following rule:\nIn the remaining matrix of size n -k, select the column with the fewest nonzeros.\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nThe component of U corresponding to that column is renumbered k + 1. So is the\nnode in the finite difference grid. Of course elimination in that column will normally\nproduce new nonzeros in the remaining columns! Some fill-in is unavoidable. So\nthe algorithm must keep track of the new positions of nonzeros, and also the actual\nentries. It is the positions that decide the ordering of unknowns. Then the entries\ndecide the numbers in L and U .\nExample\nFigure 3.19 shows a small example of the minimal degree ordering, for\nLaplace's 5-point scheme. The node connections produce nonzero entries (indicated by\n∗) in K. The problem has six unknowns. K has two 3 by 3 tridiagonal blocks from\nhorizontal links, and two 3 by 3 blocks with -I from vertical links.\nThe degree of a node is the number of connections to other nodes. This is the\nnumber of nonzeros in that column of K. The corner nodes 1, 3, 4, 6 all have degree\n2. Nodes 2 and 5 have degree 3. A larger region has inside nodes of degree 4, which\nwill not be eliminated first. The degrees change as elimination proceeds, because of\nfill-in.\nThe first elimination step chooses row 1 as pivot row, because node 1 has minimum\ndegree 2. (We had to break a tie! Any degree 2 node could come first, leading to different\nelimination orders.) The pivot is P, the other nonzeros in that row are boxed. When\nrow 1 operates on rows 2 and 4, it changes six entries below it. In particular, the two\nfill-in entries marked by F change to nonzeros. This fill-in of the (2, 4) and (4, 2) entries\ncorresponds to the dashed line connecting nodes 2 and 4 in the graph.\nF\nF\nF\n∗\n∗\nP\n∗\nF\n∗\nF\n∗\nP\nF\n∗\nF\n∗\n∗\n\n∗\n∗\n∗\n\n∗\n∗\nF\n∗\n∗\n∗\n∗\n\nP\nFill-in F\nfrom\nPivots\nZeros\nelimination\n∗\nF\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n∗\n∗\n\n∗\n∗\n\nFigure 3.19: Minimum degree nodes 1 and 3. The pivots P are in rows 1 and 3; new\nedges 2-4 and 2-6 in the graph match the matrix entries F filled in by elimination.\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nNodes that were connected to the eliminated node are now connected to each other.\nElimination continues on the 5 by 5 matrix (and the graph with 5 nodes). Node 2 still has\ndegree 3, so it is not eliminated next. If we break the tie by choosing node 3, elimination\nusing the new pivot P will fill in the (2, 6) and (6, 2) positions. Node 2 becomes linked\nto node 6 because they were both linked to the eliminated node 3.\nThe problem is reduced to 4 by 4, for the unknown U 's at the remaining nodes\n2, 4, 5, 6. Problem\nasks you to take the next step--choose a minimum degree node\nand reduce the system to 3 by 3.\nStoring the Nonzero Structure = Sparsity Pattern\nA large system KU = F needs a fast and economical storage of the node connections\n(which match the positions of nonzeros in K). The connections and nonzeros change\nas elimination proceeds. The list of edges and nonzero positions corresponds to the\n\"adjacency matrix \" of the graph of nodes. The adjacency matrix has 1 or 0 to indicate\nnonzero or zero in K.\nFor each node i, we have a list adj(i) of the nodes connected to i. How to combine\nthese into one master list NZ for the whole graph and the whole matrix K? A simple\nway is to store the lists adj(i) sequentially in NZ (the nonzeros for i = 1 up to i = n).\nAn index array IND of pointers tells the starting position of the sublist adj(i) within\nthe master list NZ. It is useful to give IND an (n + 1)st entry to point to the final\nentry in NZ (or to the blank that follows, in Figure 3.20). MATLAB will store one\nmore array (the same length nnz(K) as NZ) to give the actual nonzero entries.\nNZ\n| 2\n\n↑\n| 1\n\n↑\n5 | 2\n\n↑\n| 1\n\n↑\n|\n↑\n4 6 |\n↑\n5 |\n↑\nIND\n\nNode\nFigure 3.20: Master list NZ of nonzeros (neighbors in Figure 3.19). Positions in\nIND.\nThe indices i are the \"original numbering\" of the nodes. If there is renumbering,\nthe new ordering can be stored as a permutation PERM. Then PERM(i) = k when\nthe new number i is assigned to the node with original number k. The text [GL] by\nGeorge and Liu is the classic reference for this entire section on ordering of the nodes.\nGraph Separators\nHere is another good ordering, different from minimum degree. Graphs or meshes are\noften separated into disjoint pieces by a cut. The cut goes through a small number\nof nodes or meshpoints (a separator). It is a good idea to number the nodes in the\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nseparator last. Elimination is relatively fast for the disjoint pieces P and Q. It only\nslows down at the end, for the (smaller) separator S.\nThe three groups P, Q, S of meshpoints have no direct connections between P and\nQ (they are both connected to the separator S). Numbered in that order, the \"block\narrow\" stiffness matrix and its K = LU factorization look like this:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nK\nKP\nKP S\nLP\nUP\nA\nK = ⎣\nKQ\nKQS ⎦\nL = ⎣ 0\nLQ\n⎦\nU = ⎣\nUQ B ⎦\nSP KSQ\nKS\nX\nY\nZ\nC\n(3)\nThe zero blocks in K give zero blocks in L and U. The submatrix KP comes first\nin elimination, to produce LP and UP . Then come the factors LQUQ of KQ, followed\nby the connections through the separator. The major cost is often that last step, the\nsolution of a fairly dense system of the size of the separator.\nS\nQ\nP\nP\nS\nQ\nArrow matrix\nSeparator comes last\nBlocks P, Q\n(Figure 3.18)\n(Figure 3.19)\nSeparator S\nFigure 3.21: A graph separator numbered last produces a block arrow matrix K.\nFigure 3.21 shows three examples, each with separators. The graph for a perfect\narrow matrix has a one-point separator (very unusual). The 6-node rectangle has a\ntwo-node separator in the middle. Every N by N grid can be cut by an N-point\nseparator (and N is much smaller than N2). If the meshpoints form a rectangle, the\nbest cut is down the middle in the shorter direction.\nYou could say that the numbering of P then Q then S is block minimum degree.\nBut one cut with one separator will not come close to an optimal numbering. It\nis natural to extend the idea to a nested sequence of cuts. P and Q have their\nown separators at the next level. This nested dissection continues until it is not\nproductive to cut further. It is a strategy of \"divide and conquer.\"\nFigure 3.22 illustrates three levels of nested dissection on a 7 by 7 grid. The first\ncut is down the middle. Then two cuts go across and four cuts go down. Numbering\nthe separators last within each stage, the matrix K of size 49 has arrows inside arrows\ninside arrows. The spy command will display the pattern of nonzeros.\nSeparators and nested dissection show how numbering strategies are based on the\ngraph of nodes and edges in the mesh. Those edges correspond to nonzeros in the\nmatrix K. The nonzeros created by elimination (filled entries in L and U) correspond\nto paths in the graph. In practice, there has to be a balance between simplicity and\noptimality in the numbering--in scientific computing simplicity is a very good thing!\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nzero\nzero\n3 × 18\n3 × 18\nzero\nzero\nzero\nzero\n∗0 ∗\n\n0 ∗∗ 5\n∗∗∗\n9×9\n1 to 9\n22 to 30\n19 to 21\n40 to 42\nK =\n10 to 18\n31 to 39\n7 × 42\n7×7\nFigure 3.22: Three levels of separators. Still\nnonzeros in K, only\nin L.\nA very reasonable compromise is the backslash command U = K\\F that uses a\nnearly minimum degree ordering in Sparse MATLAB.\nOperation Counts (page K)\nHere are the complexity estimates for the 5-point Laplacian with N 2 or N 3 nodes:\nMinimum Degree\nSpace (nonzeros from fill-in)\nTime (flops for elimination)\nn = N 2 in 2D\nn = N 3 in 3D\nX\nX\nX\nX\nNested Dissection\nSpace (nonzeros from fill-in)\nX\nX\nTime (flops for elimination)\nX\nX\nIn the last century, nested dissection lost out--it was slower on almost all applica\ntions. Now larger problems are appearing and the asymptotics eventually give nested\ndissection an edge. Algorithms for cutting graphs can produce short cuts into nearly\nequal pieces. Of course a new idea for ordering could still win.\nIterative versus Direct Methods\nThis section is a guide to solution methods for problems Ax = b that are too large\nand expensive for ordinary elimination. We are thinking of sparse matrices A, when\na multiplication Ax is relatively cheap. If A has at most p nonzeros in every row,\nthen Ax needs at most pn multiplications. Typical applications are to large finite\ndifference equations or finite element problems on unstructured grids. In the special\ncase of a square grid for Laplace's equation, a Fast Poisson Solver (Section 7.2) is\navailable.\nWe turn away from elimination to iterative methods and Krylov subspaces.\nPure iterative methods are easier to analyze, but the Krylov subspace methods are\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nmore powerful. So the older iterations of Jacobi and Gauss-Seidel and overrelaxation\nare less favored in scientific computing, compared to conjugate gradients and GM\nRES. When the growing Krylov subspaces reach the whole space Rn, these methods\n(in exact arithmetic) give the exact solution A-1b. But in reality we stop much ear\nlier, long before n steps are complete. The conjugate gradient method (for positive\ndefinite A, and with a good preconditioner ) has become truly important.\nThe next ten pages will introduce you to numerical linear algebra. This has\nbecome a central part of scientific computing, with a clear goal: Find a fast stable\nalgorithm that uses the special properties of the matrices. We meet matrices that\nare sparse or symmetric or triangular or orthogonal or tridiagonal or Hessenberg or\nGivens or Householder. Those matrices are at the core of so many computational\nproblems. The algorithm doesn't need details of the entries (which come from the\nspecific application). By using only their structure, numerical linear algebra offers\nmajor help.\nOverall, elimination with good numbering is the first choice until storage and CPU\ntime become excessive. This high cost often arises first in three dimensions. At that\npoint we turn to iterative methods, which require more expertise. You must choose\nthe method and the preconditioner. The next pages aim to help the reader at this\nfrontier of scientific computing.\nPure Iterations\nWe begin with old-style pure iteration (not obsolete). The letter K will be reserved\nfor \"Krylov\" so we leave behind the notation KU = F. The linear system becomes\nAx = b with a large sparse matrix A, not necessarily symmetric or positive definite:\nLinear system Ax = b\nResidual rk = b - Axk\nPreconditioner P ≈ A\nThe preconditioner P attempts to be \"close to A\" and at the same time much easier\nto work with. A diagonal P is one extreme (not very close). P = A is the other\nextreme (too close). Splitting the matrix A gives an equivalent form of Ax = b:\nSplitting\nPx = (P - A)x + b .\n(4)\nThis suggests an iteration, in which every vector xk leads to the next xk+1:\nIteration\nPxk+1 = (P - A)xk + b .\n(5)\nStarting from any x0, the first step finds x1 from Px1 = (P - A)x0 + b. The iteration\ncontinues to x2 with the same matrix P, so it often helps to know its triangular factors\nL and U. Sometimes P itself is triangular, or its factors L and U are approximations\nto the triangular factors of A. Two conditions on P make the iteration successful:\n1. The new xk+1 must be quickly computable. Equation (5) must be fast to solve.\n2. The errors ek = x - xk must converge quickly to zero.\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nSubtract equation (5) from (4) to find the error equation. It connects ek to ek+1:\nError\nPek+1 = (P - A)ek which means ek+1 = (I - P -1A)ek = Mek .\n(6)\nThe right side b disappears in this error equation. Each step multiplies the error\nvector by M = I - P -1A. The speed of convergence of xk to x (and of ek to zero)\ndepends entirely on M . The test for convergence is given by the eigenvalues of M :\nConvergence test\nEvery eigenvalue of M must have |λ(M )| < 1.\nThe largest eigenvalue (in absolute value) is the spectral radius ρ(M ) = max |λ(M )|.\nConvergence requires ρ(M ) < 1. The convergence rate is set by the largest eigen\nvalue. For a large problem, we are happy with ρ(M ) = .9 and even ρ(M ) = .99.\nSuppose that the initial error e0 happens to be an eigenvector of M . Then the\nnext error is e1 = Me0 = λe0. At every step the error is multiplied by λ, so we must\nhave |λ| < 1. Normally e0 is a combination of all the eigenvectors. When the iteration\nmultiplies by M , each eigenvector is multiplied by its own eigenvalue. After k steps\nthose multipliers are λk . We have convergence if all |λ| < 1.\nFor preconditioner we first propose two simple choices:\nJacobi iteration\nP = diagonal part of A\nGauss-Seidel iteration\nP = lower triangular part of A\nTypical examples have spectral radius ρ(M ) = 1 - cN -1 . This comes closer and\ncloser to 1 as the mesh is refined and the matrix grows. An improved preconditioner\nP can give ρ(M ) = 1 - cN -1/2. Then ρ is smaller and convergence is faster, as in\n\"overrelaxation.\" But a different approach has given more flexibility in constructing\na good P , from a quick incomplete LU factorization of the true matrix A:\nI ncomplete LU\nP = (approximation to L)(approximation to U ) .\nThe exact A = LU has fill-in, so zero entries in A become nonzero in L and U . The ap\nproximate L and U could ignore this fill-in (fairly dangerous). Or P = LapproxUapprox\ncan keep only the fill-in entries F above a fixed threshold. The variety of options,\nand the fact that the computer can decide automatically which entries to keep, has\nmade the ILU idea (incomplete LU ) a very popular starting point.\nExample\nThe -1, 2, -1 matrix A = K provides an excellent example. We choose\nthe preconditioner P = T , the same matrix with T11 = 1 instead of K11 = 2. The LU\nfactors of T are perfect first differences, with diagonals of +1 and -1. (Remember that\nall pivots of T equal 1, while the pivots of K are 2/1, 3/2, 4/3, . . .) We can compute the\nright side of T -1Kx = T -1b with only 2N additions and no multiplications (just back\nsubstitution using L and U ). Idea: This L and U are approximately correct for K.\nThe matrix P -1A = T -1K on the left side is triangular. More than that, T is a rank 1\nchange from K (the 1, 1 entry changes from 2 to 1). It follows that T -1K and K-1T\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nare rank 1 changes from the identity matrix I. A calculation in Problem\nshows that\nonly the first column of I is changed, by the \"linear vector\" l = (N, N - 1, . . . , 1):\nP -1A = T -1K = I + leT\nand\nK-1T = I - (leT\n1 )/(N + 1) .\n(7)\nT\nHere e1 =\n. . . 0\nso leT has first column l. This example finds x = K-1b by\na quick exact formula (K-1T)T -1b, needing only 2N additions for T -1 and N additions\nand multiplications for K-1T. In practice we wouldn't precondition this K (just solve).\nThe usual purpose of preconditioning is to speed up convergence for iterative methods,\nand that depends on the eigenvalues of P -1A. Here the eigenvalues of T -1K are its\ndiagonal entries N+1, 1, . . ., 1. This example will illustrate a special property of conjugate\ngradients, that with only two different eigenvalues it reaches the true solution x in two\nsteps.\nThe iteration Pxk+1 = (P - A)xk + b is too simple! It is choosing one particular\nvector in a \"Krylov subspace.\" With relatively little work we can make a much better\nchoice of xk . Krylov projections are the state of the art in today's iterative methods.\nKrylov Subspaces\nOur original equation is Ax = b. The preconditioned equation is P -1Ax = P -1b.\nWhen we write P -1, we never intend that an inverse would be explicitly computed\n(except in our example). The ordinary iteration is a correction to xk by the vector\nP -1rk:\nPxk+1 = (P - A)xk + b\nor\nPxk+1 = Pxk + rk\nor\nxk+1 = xk + P -1 rk . (8)\nHere rk = b - Axk is the residual. It is the error in Ax = b, not the error ek\nin x. The symbol P -1rk represents the change from xk to xk+1, but that step is\nnot computed by multiplying P -1 times rk . We might use incomplete LU, or a few\nsteps of a \"multigrid\" iteration, or \"domain decomposition.\" Or an entirely new\npreconditioner.\nIn describing Krylov subspaces, I should work with P -1A. For simplicity I will\nonly write A. I am assuming that P has been chosen and used, and the precondi\ntioned equation P -1Ax = P -1b is given the notation Ax = b. The preconditioner is\nnow P = I. Our new matrix A is probably better than the original matrix with that\nname.\nThe Krylov subspace Kk(A, b) contains all combinations of b, Ab, . . . , Ak-1b.\nThese are the vectors that we can compute quickly, multiplying by a sparse A. We\nlook in this space Kk for the approximation xk to the true solution of Ax = b. Notice\nthat the pure iteration xk = (I - A)xk-1 + b does produce a vector in Kk when xk-1\nis in Kk-1 . The Krylov subspace methods make other choices of xk . Here are four\ndifferent approaches to choosing a good xk in Kk --this is the important decision:\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\n1. The residual rk = b -Axk is orthogonal to Kk (Conjugate Gradients, . . . )\n2. The residual rk has minimum norm for xk in Kk (GMRES, MINRES, . . . )\n3. rk is orthogonal to a different space like Kk(AT) (BiConjugate Gradients, . . . )\n4. ek has minimum norm (SYMMLQ; for BiCGStab xk is in ATKk(AT); . . . )\nIn every case we hope to compute the new xk quickly and stably from the earlier x's.\nIf that recursion only involves xk-1 and xk-2 (short recurrence) it is especially fast.\nWe will see this happen for conjugate gradients and symmetric positive definite A.\nThe BiCG method in 3 is a natural extension of short recurrences to unsymmetric\nA--but stability and other questions open the door to the whole range of methods.\nTo compute xk we need a basis for Kk . The best basis q1, . . . , qk is orthonormal.\nEach new qk comes from orthogonalizing t = Aqk-1 to the basis vectors q1, . . . , qk-1\nthat are already chosen. This is the Gram-Schmidt idea (called modified Gram-\nSchmidt when we subtract projections of t onto the q's one at a time, for numerical\nstability). The iteration to compute the orthonormal q's is known as Arnoldi's\nmethod:\n1 q1 = b/∥b∥2;\n% Normalize to ∥q1∥ = 1\nfor j = 1, . . ., k -1\nt = Aqj ;\n% t is in the Krylov space Kj+1(A, b)\nfor i = 1, . . . , j\nhij = qi\nTt;\n% hij qi = projection of t onto qi\nt = t -hij qi; % Subtract component of t along qi\nq\nh\nend;\nj+1,j = ∥t∥2;\n% t is now orthogonal to q1, . . ., qj\nj+1 = t/hj+1,j ;\n% Normalize t to ∥qj+1∥ = 1\nend\n% q1, . . . , qk are orthonormal in Kk\nPut the column vectors q1, . . . , qk into an n by k matrix Qk . Multiplying rows of\nT\nQT by columns of Qk produces all the inner products qi qj , which are the 0's and 1's\nk\nin the identity matrix. The orthonormal property means that QT\nk Qk = Ik .\nArnoldi constructs each qj+1 from Aqj by subtracting projections hij qi.\nIf\nwe\n\nexpress the steps up to j = k-1 in matrix notation, they become AQk-1 = Qk Hk,k-1:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nArnoldi\nh11\nh12\n· h1,k-1\nAQk-1 =\n⎢⎢⎣ Aq1\n· · · Aqk-1\n⎥⎥⎦ =\n⎢⎢⎣ q1\n· · · qk\n⎢⎢⎣\n⎥⎥⎦\nh21\nh22\n· h2,k-1\nh23\n·\n·\n· hk,k-1\n⎥⎥⎦ .\n(9)\nn by k -1\nn by k\nk by k -1\nThat matrix Hk,k-1 is \"upper Hessenberg\" because it has only one nonzero diagonal\nbelow the main diagonal. We check that the first column of this matrix equation\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\n(multiplying by columns!) produces q2:\nAq1 - h11q1\nh\nAq1 = h11q1 + h21q2\nor\nq2 =\n.\n(10)\nThat subtraction is Step 4 in Arnoldi's algorithm. Division by h21 is Step 6.\nUnless more of the hij are zero, the cost is increasing at every iteration. We have\nk dot products to compute at step 3 and 5, and k vector updates in steps 4 and 6. A\nshort recurrence means that most of these hij are zero. That happens when A = AT .\nThe matrix H is tridiagonal when A is symmetric. This fact is the founda\ntion of conjugate gradients. For a matrix proof, multiply equation (9) by QT\n. The\nk-1\nright side becomes H without its last row, because (QT\nk-1Qk )Hk,k-1 = [ I 0 ] Hk,k-1.\nThe left side QT\nk-1AQk-1 is always symmetric when A is symmetric. So that H matrix\nhas to be symmetric, which makes it tridiagonal. There are only three nonzeros in\nthe rows and columns of H, and Gram-Schmidt to find qk+1 only involves qk and qk-1:\nArnoldi when A = AT\nAqk = hk+1,k qk+1 + hk,k qk + hk-1,k qk-1 .\n(11)\nThis is the Lanczos iteration. Each new qk+1 = (Aqk - hk,kqk - hk-1,kqk-1)/hk+1,k\ninvolves one multiplication Aqk , two dot products for new h's, and two vector updates.\nThe QR Method for Eigenvalues\nAllow me an important comment on the eigenvalue problem Ax = λx. We have seen\nthat Hk-1 = QT\nk-1AQk-1 is tridiagonal if A = AT. When k - 1 reaches n and Qn is\nsquare, the matrix H = QTAQn = Q-1AQn has the same eigenvalues as A:\nn\nn\nSame λ\nHy = Q-1AQny = λy\ngives\nAx = λx with x = Qny .\n(12)\nn\nIt is much easier to find the eigenvalues λ for a tridiagonal H than the for original A.\nThe famous \"QR method\" for the eigenvalue problem starts with T1 = H, factors\nit into T1 = Q1R1 (this is Gram-Schmidt on the short columns of T1), and reverses\norder to produce T2 = R1Q1. The matrix T2 is again tridiagonal, and its off-diagonal\nentries are normally smaller than for T1. The next step is Gram-Schmidt on T2,\northogonalizing its columns in Q2 by the combinations in the upper triangular R2:\nQR Method Factor T2 into Q2R2 . Reverse order to T3 = R2Q2 = Q-1T2Q2 (13)\n.\nBy the reasoning in (12), any Q-1TQ has the same eigenvalues as T . So the matrices\nT2, T3, . . . all have the same eigenvalues as T1 = H and A. (These square Qk from\nGram-Schmidt are entirely different from the rectangular Qk in Arnoldi.) We can\neven shift T before Gram-Schmidt, and we should, provided we remember to shift\nback:\nShifted QR\nFactor Tk - skI = Qk Rk . Reverse to Tk+1 = Rk Qk + bk I .\n(14)\nT\nWhen the shift sk is chosen to be the n, n entry of Tk , the last off-diagonal entry of\nk+1 becomes very small. The n, n entry of Tk+1 moves close to an eigenvalue. Shifted\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nQR is one of the great algorithms of numerical linear algebra. It solves moderate-size\neigenvalue problems with great efficiency. This is the core of MATLAB's eig(A).\nFor a large symmetric matrix, we often stop the Arnoldi-Lanczos iteration at a\ntridiagonal Hk with k < n. The full n-step process to reach Hn is too expensive, and\noften we don't need all n eigenvalues. So we compute (by the same QR method) the k\neigenvalues of Hk instead of the n eigenvalues of Hn. These computed λ1k, λ2k, . . . , λkk\ncan provide good approximations to the first k eigenvalues of A. And we have an\nexcellent start on the eigenvalue problem for Hk+1, if we decide to take a further step.\nThis Lanczos method will find, approximately and iteratively and quickly, the\nleading eigenvalues of a large symmetric matrix.\nThe Conjugate Gradient Method\nWe return to iterative methods for Ax = b. The Arnoldi algorithm produced or\nthonormal basis vectors q1, q2, . . . for the growing Krylov subspaces K1 , K2 , . . .. Now\nwe select vectors x1, x2, . . . in those subspaces that approach the exact solution to\nAx = b. We concentrate on the conjugate gradient method for symmetric positive\ndefinite A.\nThe rule for xk in conjugate gradients is that the residual rk = b - Axk should\nbe orthogonal to all vectors in Kk. Since rk will be in Kk+1, it must be a multiple\nof Arnoldi's next vector qk+1! Each residual is therefore orthogonal to all previous\nresiduals (which are multiples of the previous q's):\nOrthogonal residuals\nri\nT rk = 0\nfor i < k .\n(15)\nThe difference between rk and qk+1 is that the q's are normalized, as in q1 = b/∥b∥.\nK\nSince rk-1 is a multiple of qk, the difference rk -rk-1 is orthogonal to each subspace\ni with i < k. Certainly xi - xi-1 lies in that Ki. So ∆r is orthogonal to earlier\n∆x's:\n(xi - xi-1)T(rk - rk-1) = 0\nfor i < k .\n(16)\nThese differences ∆x and ∆r are directly connected, because the b's cancel in ∆r:\nrk - rk-1 = (b - Axk) - (b - Axk-1) = -A(xk - xk-1) .\n(17)\nSubstituting (17) into (16), the updates in the x's are \"A-orthogonal\" or conjugate:\nConjugate updates ∆x (xi - xi-1)TA(xk - xk-1) = 0\nfor i < k .\n(18)\nNow we have all the requirements. Each conjugate gradient step will find a new\n\"search direction\" dk for the update xk - xk-1. From xk-1 it will move the right\ndistance αkdk to xk. Using (17) it will compute the new rk. The constants βk in\nthe search direction and αk in the update will be determined by (15) and (16) for\ni = k - 1. For symmetric A the orthogonality in (15) and (16) will be automatic for\ni < k - 1, as in Arnoldi. We have a \"short recurrence\" for the new xk and rk.\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nHere is one cycle of the algorithm, starting from x0 = 0 and r0 = b and β1 = 0. It\ninvolves only two new dot products and one matrix-vector multiplication Ad:\nConjugate 1 βk = rk-1rk-1/rT\nT\nk-2rk-2\n% Improvement this step\nGradient\n2 dk = rk-1 + βk dk-1\n% Next search direction\nT\nMethod\n3 αk = rk-1rk-1/dTAdk\n% Step length to next xk\nk\n4 xk = xk-1 + αk dk\n% Approximate solution\n5 rk = rk-1 - αk Adk\n% New residual from (17)\nThe formulas 1 and 3 for βk and αk are explained briefly below--and fully by Trefethen-\nBau ( ) and Shewchuk ( ) and many other good references.\nDifferent Viewpoints on Conjugate Gradients\nI want to describe the (same!) conjugate gradient method in two different ways:\n1. It solves a tridiagonal system Hy = f recursively\n2. It minimizes the energy 1 xTAx - xTb recursively.\nHow does Ax = b change to the tridiagonal Hy = f ? That uses Arnoldi's or\nthonormal columns q1, . . . , qn in Q, with QTQ = I and QTAQ = H:\nAx = b is (QTAQ)(QT x) = QTb which is Hy = f = (∥b∥, 0, . . . , 0) .\n(19)\nT\nSince q1 is b/∥b∥, the first component of f = QTb is q1 b = ∥b∥ and the other com\nponents are qTb = 0. The conjugate gradient method is implicitly computing this\ni\nsymmetric tridiagonal H and updating the solution y at each step. Here is the third\nstep:\n⎡\n⎤ ⎡\n⎤\n⎡\n⎤\nh11\nh12\n∥b∥\nH3y3 = ⎣ h21\nh22\nh23 ⎦ ⎣ y3 ⎦ = ⎣ 0 ⎦ .\n(20)\nh32\nh33\nThis is the equation Ax = b projected by Q3 onto the third Krylov subspace K3 .\nThese h's never appear in conjugate gradients. We don't want to do Arnoldi too!\nIt is the LDLT factors of H that CG is somehow computing--two new numbers at\neach step. Those give a fast update from yj-1 to yj . The corresponding xj = Qj yj\nfrom conjugate gradients approaches the exact solution xn = Qnyn which is x = A-1b.\nIf we can see conjugate gradients also as an energy minimizing algorithm, we can\nextend it to nonlinear problems and use it in optimization. For our linear equation\nAx = b, the energy is E(x) = 1 xTAx - xTb. Minimizing E(x) is the same as solving\nAx = b, when A is positive definite (the main point of Section 1.\n). The CG\niteration minimizes E(x) on the growing Krylov subspaces. On the first\nsubspace K1, the line where x is αb = αd1, this minimization produces the right\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nvalue for α1:\nbTb\nE(αb) = α2bTAb - αbTb\nis minimized at\nα1 =\n.\n(21)\nbTAb\nThat α1 is the constant chosen in step 3 of the first conjugate gradient cycle.\nThe gradient of E(x) = 2\n1 xTAx - xTb is exactly Ax - b. The steepest descent\ndirection at x1 is along the negative gradient, which is r1! This sounds like the perfect\ndirection d2 for the next move. But the great difficulty with steepest descent is that\nthis r1 can be too close to the first direction. Little progress that way. So we add\nthe right multiple β2d1, in order to make d2 = r1 + β2d1 A-orthogonal to the first\ndirection d1.\nThen we move in this conjugate direction d2 to x2 = x1 + α2d2. This explains the\nname conjugate gradients, rather than the pure gradients of steepest descent. Every\ncycle of CG chooses αj to minimize E(x) in the new search direction x = xj-1 + αdj .\nThe last cycle (if we go that far) gives the overall minimizer xn = x = A-1b.\nExample\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n⎣ 1\n⎦⎣ -1 ⎦ = ⎣\nAx = b\nis\n0 ⎦ .\n\n-1\nFrom x0 = 0 and β1 = 0 and r0 = d1 = b the first cycle gives α1 = 2 and x1 = 1b =\n(2, 0, 0). The new residual is r1 = b - Ax1 = (0, -2, -2). Then the second cycle yields\n⎡\n⎤\n⎡\n⎤\nβ2 =\n,\nd2 = ⎣ -2 ⎦ ,\nα2 =\n,\nx2 = ⎣ -1 ⎦ = A-1b !\n-2\n-1\nA\nThe correct solution is reached in two steps, where normally it will take n = 3 steps. The\nreason is that this particular A has only two distinct eigenvalues 4 and 1. In that case\n-1b is a combination of b and Ab, and this best combination x2 is found at cycle 2. The\nresidual r2 is zero and the cycles stop early--very unusual.\nEnergy minimization leads in [ ] to an estimate of the convergence rate for the\n√\nerror e = x - xj in conjugate gradients, using the A-norm ∥e∥A =\neTAe:\nλ\n√\n√\n\nj\nmax -\nλmin\nλ\nError estimate\n∥x - xj ∥A ≤ 2 √\n√\n∥x - x0∥A .\n(22)\nmax +\nλmin\nλ\nThis is the best-known error estimate, although it doesn't account for any clustering of\nthe eigenvalues of A. It involves only the condition number λmax/λmin. Problem\ngives the \"optimal\" error estimate but it is not so easy to compute. That optimal\nestimate needs all the eigenvalues of A, while (22) uses only the extreme eigenvalues\nmax(A) and λmin(A)--which in practice we can bound above and below.\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nMinimum Residual Methods\nWhen A is not symmetric positive definite, conjugate gradient is not guaranteed to\nsolve Ax = b. Most likely it won't. We will follow van der Vorst [ ] in briefly\ndescribing the minimum norm residual approach, leading to MINRES and GMRES.\nThese methods choose xj in the Krylov subspace Kj so that ∥b- Axj ∥ is minimal.\nFirst we compute the orthonormal Arnoldi vectors q1, . . . , qj . They go in the columns\nof Qj , so QT\nj Qj = I. As in (19) we set xj = Qj y, to express the solution as a\ncombination of those q's. Then the norm of the residual rj using (9) is\n∥b - Axj ∥ = ∥b - AQj y∥ = ∥b - Qj+1Hj+1,j y∥ .\n(23)\nT\nT\nThese vectors are all in the Krylov space Kj+1, where rj (Qj+1Qj\nT\n+1rj ) = rj rj . This\nsays that the norm is not changed when we multiply by Qj\nT\n+1. Our problem becomes:\nChoose y to minimize ∥rj ∥ = ∥Qj\nT\n+1b - Hj+1,j y∥ = ∥f - Hy∥ .\n(24)\nThis is an ordinary least squares problem for the equation Hy = f with only j + 1\nequations and j unknowns. The right side f = QT\nj+1b is (∥r0∥, 0, . . ., 0) as in (19).\nThe matrix H = Hj+1,j is Hessenberg as in (9), with one nonzero diagonal below the\nmain diagonal. We face a completely typical problem of numerical linear algebra:\nUse the special properties of H and f to find a fast algorithm that computes y. The\ntwo favorite algorithms for this least squares problem are closely related:\nMINRES A is symmetric (probably indefinite, or we use CG) and H is tridiagonal\nGMRES\nA is not symmetric and the upper triangular part of H can be full\nIn both cases we want to clear out that nonzero diagonal below the main diagonal of\nH. The natural way to do that, one nonzero entry at a time, is by \"Givens rotations.\"\nThese plane rotations are so useful and simple (the essential part is only 2 by 2) that\nwe complete this section by explaining them.\nGivens Rotations\nThe direct approach to the least squares solution of Hy = f constructs the normal\nequations HTHy = HTf. That was the central idea in Chapter 1, but you see what\nwe lose. If H is Hessenberg, with many good zeros, HTH is full. Those zeros in H\nshould simplify and shorten the computations, so we don't want the normal equations.\nThe other approach to least squares is by Gram-Schmidt. We factor H into\northogonal times upper triangular. Since the letter Q is already used, the or\nthogonal matrix will be called G (after Givens). The upper triangular matrix is G-1H.\nThe 3 by 2 case shows how a plane rotation G-1 can clear out the subdiagonal entry\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nh21:\n⎡\n⎤ ⎡\n⎤\n⎡\n⎤\ncos θ sin θ 0\nh11\nh12\n∗\n∗\n\nG-1\n21 H = ⎣-sin θ cos θ 0 ⎦ ⎣ h21\nh22 ⎦ = ⎣ 0 ∗⎦ .\n(25)\nh32\n0 ∗\nG\nThat bold zero entry requires h11 sin θ = h21 cos θ, which determines θ. A second\n-1\nrotation G-1\n32 , in the 2-3 plane, will zero out the 3, 2 entry. Then G-1\nH is a\nsquare upper triangular matrix U above a row of zeros!\nThe Givens orthogonal matrix is G = G21G32 but there is no reason to do this mul\ntiplication. We use each Gij as it is constructed, to simplify the least squares problem.\nRotations (and all orthogonal matrices) leave the lengths of vectors unchanged:\n21 Hy -G-1G-1\nU\nF\n∥Hy -f ∥= ∥G-1G-1\nf ∥= ∥\ny -\n∥.\n(26)\ne\nThis length is what MINRES and GMRES minimize. The row of zeros below U\nmeans that the last entry e is the error--we can't reduce it. But we get all the other\nentries exactly right by solving the j by j system Uy = F (here j = 2). This gives\nthe best least squares solution y. Going back to the original problem of minimizing\n∥r∥= ∥b -Axj ∥, the best xj in the Krylov space Kj is Qj y.\nFor non-symmetric A (GMRES rather than MINRES) we don't have a short\nrecurrence. The upper triangle in H can be full, and step j becomes expensive and\npossibly inaccurate as j increases. So we may change \"full GMRES\" to GMRES(m),\nwhich restarts the algorithm every m steps. It is not so easy to choose a good m.\nProblem Set 3.6\nCreate K2D for a 4 by 4 square grid with N 2 = 32 interior mesh points (so\nn = 9). Print out its factors K = LU (or its Cholesky factor C = chol(K) for\nthe symmetrized form K = CTC). How many zeros in these triangular factors?\nAlso print out inv(K) to see that it is full.\nAs N increases, what parts of the LU factors of K2D are filled in?\nCan you answer the same question for K3D? In each case we really want an\nestimate cN p of the number of nonzeros (the most important number is p).\nUse the tic; ...; toc clocking command to compare the solution time for K2Dx =\nrandom f in ordinary MATLAB and sparse MATLAB (where K2D is defined as\na sparse matrix). Above what value of N does the sparse routine K\\f win?\nCompare ordinary vs. sparse solution times in the three-dimensional K3Dx =\nrandom f . At which N does the sparse K\\f begin to win?\nIncomplete LU\nConjugate gradients\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nDraw the next step after Figure 3.19 when the matrix has become 4 by 4 and\nthe graph has nodes 2-4-5-6. Which have minimum degree? Is there more\nfill-in?\nRedraw the right side of Figure 3.19 if row number 2 is chosen as the second\npivot row. Node 2 does not have minimum degree. Indicate new edges in the\n5-node graph and new nonzeros F in the matrix.\nT\nTo show that T -1K = I + leT\n1 in (7), with e1 = [ 1 0 . . . 0 ], we can start from\nT\nT\nK = T + e1e1 . Then T -1K = I + (T -1e1)e1 and we verify that e1 = Tl:\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n1 -1\nN\nTl =\n⎢⎢⎣\n-1\n2 -1\n·\n·\n·\n⎢⎢⎣\n⎥⎥⎦\nN -1\n·\n⎥⎥⎦ =\n⎢⎢⎣\n·\n⎥⎥⎦ = e1 .\n-1\nSecond differences of a linear vector l are zero. Now multiply T -1K = I + leT\ntimes I -(leT\n1 )/(N + 1) to establish the inverse matrix K-1T in (7).\nArnoldi expresses each Aqk as hk+1,kqk+1 + hk,kqk + · · · + h1,k q1. Multiply by qT\ni\nT\nto find hi,k = qi Aqk . If A is symmetric you can write this as (Aqi)Tqk. Explain\nwhy (Aqi)Tqk = 0 for i < k -1 by expanding Aqi into hi+1,iqi+1 + · · · + h1,iq1.\nWe have a short recurrence if A = AT (only hk+1,k and hk,k and hk-1,k are\nnonzero)."
    },
    {
      "category": "Resource",
      "title": "am37.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/d9ad8a20efdcd46c4cccd981bc4306e5_am37.pdf",
      "content": "Partial Differential Equations\n3.7\nFour Model Examples\nThe differential equations in Chapter 1 were very ordinary. There were time deriva\ntives d/dt or space derivatives d/dx but not both:\ndu\nd2u\nd\ndu\n= -Ku or M\n+ Ku = F (t)\nor\n-\nc(x)\n= f (x) .\n(1)\ndt\ndt2\ndx\ndx\nA partial differential equation contains two or more derivatives (they have to be partial\nderivatives like ∂/∂x and ∂/∂y and ∂/∂t so we can tell them apart). The solution\nu(x, y) or u(x, t) or even u(x, y, t) is a function of those \"independent variables\" x\nand y and t.\nIt is important to distinguish different types of equations, above all the distinction\nbetween \"boundary value problems\" and initial value problems\". The time variable t\nindicates an initial value problem. The first equation in (1) starts from an initial value\nu(0). The solution u(ε) evolves?? for t > 0 by obeying the equation du/dt = Au.\nThe second equation needs also an initial value du/dt(0) for the velocity, because\nthe leading term involves d2u/dt2 . Boundary values were given at endpoints x = 0\nand x = 1. Inside the boundary (in the interior) u(x) solved the equation, with just\nenough freedom (two arbitrary constants) to satisfy the two boundary conditions. All\ngood. The third equation in (1) described a steady state u(x).\nFor partial differential equations, start with initial value problems. We will focus\non three examples. They involve first or second order derivatives in t and in x and\nu(x, t) is a scalar. The names of the equations are important too:\n∂u\n∂u\nOne way wave equation\n∂t = ∂x\n(2)\n∂u\n∂2u\nHeat equation, diffusion equation\n=\n∂t\n∂x2\n(3)\n∂2u\nWave equation (with velocity c)\n∂t2 = c 2 ∂2u\n(4)\n∂x2\nThe first two equations involve ∂/∂t (first order) so initial values u(x, 0) will\nbe given (at t = 0). We know where the solution starts, and in Figure 3.1 those\ninitial values are delta functions. Notice the difference at t = 1! In the one way\nwave equation, the delta function moved to the left. In the heat equation, the delta\nfunction diffused into a Gaussian. And it spreads out even further by the time t = 2.\n\n′\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nSince the initial value is symmetric around the centerpoint x = 0, so is the solution\nu(x, t). The heat equation doesn't notice if you change x to -x, but it sure notices if\nyou switch t to -t. The \"backward heat equation\" -∂u/∂t = ∂2u/∂x2 is impossible\nto solve. Physically, hot air can spread into a room, but time doesn't reverse and the\ndiffused heat doesn't return back to the starting point.\nThe full wave equation involves ∂2u/∂t2, so we need an initial velocity ∂u/∂t(x, 0)\nin addition to u(x, 0). In Figure 3.2a that initial velocity is zero. We see waves going in\nboth directions (symmetrically). In Figure 3.2b the initial velocity is ∂u/∂t(x, 0) = 1.\nThe wave to the left is different from the wave to the right. You might note that the\nsame word \"velocity\" was used for the number c (velocity in x - t space) and for\n∂u/∂t (velocity in u - t space).\nHow could those examples be extended? The one way wave equation could become\n∂u\n∂u\n∂u\n∂u\n∂u\n∂u\n???\n= c\nor\n= c(x)\nor even\n= c(u)\n.\n(5)\n∂t\n∂x\n∂t\n∂x\n∂t\n∂x\nThe last of those is nonlinear! It is highly important, one good application is to traffic\nflow. At a point x on the highway, the car density is u(x, t) at time t. If the density\nup ahead (one way drivers!) is greater, then cars slow down and get denser. The\nrelation depends on u itself, it is not linear. This produces the waves of stop and go\ndriving that a helicopter sees in a traffic jam.\nThe heat equation should have a \"diffusivity constant\" c, with the dimension of\n(distance)2/time. In fact this fits our framework exactly, there is a perfect analogy\nwith K = ATCA and u = -Ku:\n∂u\n∂\n∂u\n=\nc(x)\n.\n(6)\n∂t\n∂x\n∂x\nWhen c(x) is a positive constant, we can rescale time to make c = 1. That is the case\nwe can solve. (When c is a negative constant, nobody can solve the backward heat\nequation. We never allowed c < 0 in Chapters 1 and 2 either.) When c depends on\nu or ∂u/∂x, the equation becomes nonlinear and we don't expect an exact formula\n(but we can compute!).\n??? The wave equation would also look better in its symmetric form using ATCA.\nNotice also that it can be rewritten as\n\n∂u\n∂u\n∂\n∂t\n0 c\n∂\n=\n∂t\n∂t\nc ∂u\nc 0\n∂x\nc ∂u\n.\n(7)\n∂x\n∂x\nIn a sense (Problem A) this is a pair of one way wave equations!\nThose time-dependent wave and heat equations will come after we study the\nall-important equation of equilibrium: Laplace's equation. This describes a steady\nstate. The variables are x, y, z (in three space dimensions) or x and y (in two dimensions--\nwe will concentrate on this very remarkable model).\n\n3.7. FOUR MODEL EXAMPLES\nLaplace's equation has pure second derivatives ∂2u/∂x2 = uxx and ∂2u/∂y2 = uyy :\n∂2u\nd2u\nLaplace:\n+ ∂y2 = uxx + uyy = 0 in a plane region R\n(8)\n∂x2\nThis describes the steady state temperature u(x, y) over the region R, when there is\nno heat source inside (the right side of the equation is zero). The problem doesn't\nhave initial conditions, it has boundary conditions! The boundary of R is a closed\ncurve C. At every point of C we may prescribe either a fixed temperature u0 or a\nheat flux F0:\n∂u\nBoundary conditions:\nu = u0 or\n= F0\nat each point of C .\n(9)\n∂n\n∂u\nThat \"normal derivative\" ∂n is the rate of change of u in the direction perpendicular\nto the boundary. At a point where the boundary is insulated (meaning that no heat\ncan flow through) the flux is ∂u/∂n = 0.\nThis is the problem of Section 3.2: Laplace's equation (8) with boundary condi\ntions (9). It is the two-dimensional analogue, a partial differential equation, of the\nmost basic two-point value problem:\nd2u = 0 with [u(0) or u ′(0)] and [u(1) or u ′(1)] given at the endpoints . (10)\ndx2\nThis describes the displacement (or it could be the temperature) in a rod. The\nsolution to equation (10) is just u(x) = A + Bx. For Laplace's equation we will list\nan infinite family of solutions (which we need because there are infinitely many more\nboundary points!).\nEquation (10) was our simple model, with no applied force f and with a constant\ncoefficient c = 1. The more general form in Chapter 2 was\nd\ndu\ndu\n-\nc(x)\n= f(x), with boundary conditions on u or w = c\n.\n(11)\ndx\ndx\ndx\nThose possibilities for f and c are also seen in two dimensions. When there is a source\nterm f(x, y) we have Poisson's equation (pronounced Pwa-son):\n∂2u\n∂2u\nPoisson:\n+ ∂y2 = uxx + uyy = f(x, y) in the region R .\n(12)\n∂x2\nWhen the material in R is not homogeneous, the constant coefficient c = 1 becomes\na variable coefficient c(x, y):\n∂\n∂u\n∂\n∂u\nNonhomogeneous:\nc\n+\nc\n= f(x, y) in the region R\n(13)\n∂x\n∂x\n∂y\n∂y\nMaybe you can see that we are closing in on our favorite framework ATCAu = f!\nSection 3.2 sets this framework, by identifying A and AT . Those are the key\noperators of vector calculus, the gradient and the divergence. Laplace's equation,\nwith c = 1 and f = 0, is seen as div grad u = 0. Then we concentrate on solving this\nexceptional equation, by analysis or by scientific computing:\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nExact solution (formula and series): Section 3.2\nNumerical solution (finite differences and finite elements): Section 3.3."
    },
    {
      "category": "Resource",
      "title": "am51.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/e548f1cb5f6c6bc9a5c90ee5180a3b91_am51.pdf",
      "content": "Chapter 5\nInitial Value Problems\n5.1\nFinite Difference Methods\nWe don't plan to study highly complicated nonlinear differential equations. Our first\ngoal is to see why a difference method is successful (or not). The crucial questions\nof stability and accuracy can be clearly understood for linear equations. Then we\ncan construct difference approximations to a great variety of practical problems.\nAnother property we need is computational speed. That depends partly on\nthe complexity of the equation u0 = f (u, t). Often we measure speed by the number\nof times that f (u, t) has to be computed in each time step (that number could be\none). When we turn to implicit methods and predictor-corrector methods, to improve\nstability, the cost per step goes up but we gain speed with a larger step t.\nThis chapter begins with basic methods (forward Euler, backward Euler) and then\nimproves. Each time, we test stability on u0 = a u. When a is negative, t is often\nlimited by -a t ↑ C. This has an immediate effect: the equation with a = -99\nrequires a much smaller t than a = -1. Let me organize the equations as scalar\nand vector, nonlinear in general or linear with constant coefficients a and A:\nN equations\nu 0 = f (\n)\nu 0\ni = fi(\n)\nu 0 = au\nu 0 = Au\na\na 0\nAij\ni\nj\nRe (A) 0\n1 equation\nu, t\nu, t\n@f/@u\n@f /@u\nFor an ordinary differential equation u0 = f (u, t), good codes will increase the\naccuracy (and keep stability) far beyond the O(t) error in Euler's methods. You\ncan rely on freely available software like ode45 to make the two crucial decisions:\n1.\nto choose an accurate difference method (and change the formula adaptively)\n2.\nto choose a stable time step (and change t adaptively).\nWe will introduce accurate methods, and find the stability limits on t.\nc2006 Gilbert Strang\n\n\"\n\"\n\nc2006 Gilbert Strang\nCHAPTER 5. INITIAL VALUE PROBLEMS\nStiff Differential Equations\nFirst we call attention to one extra question: Is the equation stiff ? Let me begin\nwith a made-up example to introduce stiffness and its effects:\nv(t) = e-t + e-99t\ncontrols decay\ncontrols t\nThe step t is 99 times\nsmaller because of e -99t\nthat disappears anyway\nThose decay rates -1 and -99 could be the eigenvalues of A, as in Example 1.\nExample 1\nd\nv\n-50\nv\nv(0)\n=\nwith\n=\n.\n(1)\ndt w\n49 -50\nw\nw(0)\n-99t\nThe solution has v(t) = e-t + e-99t and w(t) = e-t - e\n. The time scales are different\nby a factor of 99 (the condition number of A). The solution will decay at the slow\ntime scale of e-t, but computing e-99t may require a very small t for stability.\nIt is frustrating to have t controlled by the component that is decaying so fast.\nAny explicit method will have a requirement 99t ↑ C. We will see how this happens\nand how an implicit method (like ode15s and od23t in MATLAB) can avoid it.\nTrefethen [ ] points to these applications where stiffness comes with the problem:\n1.\nChemical kinetics (reactions go at very different speeds)\n2.\nControl theory (probably the largest application area for MATLAB)\n3.\nCircuit simulation (components respond at widely different time scales)\n4.\nMethod of Lines (large systems come from partial differential equations).\nExample 2 The N by N second difference matrix K produces a large stiff system:\ndu\n-Ku\ndui\nui+1 - 2ui + ui-1\nMethod of Lines\n=\nhas\n=\n(x)2\n.\n(2)\ndt\n(x)2\ndt\nThis comes from the heat equation @u/@t = @2 u/@x2, by discretizing only the space\nderivative. Example 1 had eigenvalues -1 and -99, while Example 2 has N eigenvalues--\nbut the difficulty is essentially the same ! The most negative eigenvalue here is about\na = -4/(x)2 . So a small x (for accuracy) will require a very small t (for stability).\nThis \"semidiscrete\" method of lines is an important idea. Discretizing the\nspace variables first produces a large system that can be given to an ODE solver. (We\nhave ordinary differential equations in time.) If it wants to, that solver can vary the time\nstep t and even the discretization formula as u(t) speeds up or slows down.\n\nreplacements\nc\n5.1. FINITE DIFFERENCE METHODS\n2006 Gilbert Strang\nThis method splits the approximation of a PDE into two parts. Finite differences/finite\nelements in earlier chapters produce the first part (discrete in space). The upcoming\nstability-accuracy analysis applies to the second part (discrete in time). This idea is very\nsimple and useful, even if it misses the good methods later in this chapter that take full\nadvantage of space-time. For the heat equation ut = uxx, the useful fact utt = uxxxx\nallows us to cancel space errors with time errors--which we won't notice when they are\nseparated in the semidiscrete method of lines.\nForward Euler and Backward Euler\nThe equation u 0 = f(u, t) starts from an initial value u(0). The key point is that the\nrate of change u0 is determined by the current state u at any moment t. This model\nof reality, where all the history is contained in the current state u(t), is a tremendous\nsuccess throughout science and engineering. (It makes calculus almost as important\nas linear algebra.) But for a computer, continuous time has to change to discrete\ntime. One differential equation allows many difference equations!\nThe simplest method (Euler is pronounced \"Oiler\") uses a forward difference:\nUn+1 - Un\nForward Euler\n= f(Un, tn)\nis\nUn+1 = Un + t fn .\n(3)\nt\nOver each t interval, the slope of U doesn't change. Figure 5.1 shows how the correct\nsolution to u0 = au follows a smooth curve, while U(t) is only piecewise linear. A\nbetter method (higher accuracy) will stay much closer to the curve by using more\ninformation than the one slope fn = f(Un, tn) at the start of the step.\nu\nt\nu = et\nU\nt\n(0) = 1\n= 1 +\n(forward)\nt\nt\nt\nu = et\nU =\n1 - t\n(backward)\nFigure 5.1: Forward Euler and Backward Euler for u0 = u. One-step errors 1\n2 (t)2 .\nBackward Euler comes from using fn+1 at the end of the step, when t = tn+1:\nUn+1 - Un\nBackward Euler\n= f(Un+1, tn+1) is Un+1 - t fn+1 = Un . (4)\nt\nThis is an implicit method. To find Un+1, we have to solve equation (4). When\nf is linear in u, we are solving a linear system at each step (and the cost is low if\n\nc2006 Gilbert Strang\nCHAPTER 5. INITIAL VALUE PROBLEMS\nthe matrix is tridiagonal, like I - t K in Example 2). We will comment later on\niterations like Newton's method or predictor-corrector in the nonlinear case.\nThe first example to study is the linear scalar equation u 0 = au. Compare forward\nand backward Euler, for one step and for n steps:\nForward\nUn+1 = (1 + a t)Un leads to Un = (1 + a t)nU0 .\n(5)\nBackward\n(1 - a t)Un+1 = Un leads to Un = (1 - a t)-nU0 .\n(6)\nForward Euler is like compound interest, at the rate a. Each step starts with Un and\nadds the interest a t Un. As the stepsize t goes to zero and we need T/t steps to\nreach time T, this discrete compounding approaches continuous compounding. The\ndiscrete solution Un approaches the continuous solution of u0 = au:\n(1 + a t)T /t approaches e aT\nas t ! 0 .\nThis is the convergence of U to u that we will prove below, more generally. It holds\nfor backward Euler too, because (1 - a t)-1 = 1 + at + higher order terms.\nThe stability question arises for very negative a. The true solution e-atu(0)\nis extremely stable, approaching zero. (If this is your own money, you should change\nbanks and get a > 0.) Backward Euler will be stable because it divides by 1 - a t\n(which is greater than 1 when a is negative). Forward Euler will explode if 1 + a t\nis smaller than -1, because its powers grow exponentially :\nInstability\n1 + a t < -1\nwhen a t < -2\n(7)\n1+at\n-1\nThat is a pretty sharp borderline between stability and instability, at -a t = 2. For\nu 0 = -20u which has a = -20, the borderline is t = 20 =\n. Compare the results\nat time T = 2 from t = 1 (22 steps) and t = 9 (18 steps):\n\nStable t =\n(1 + a t)22 = -\n.012\n\nUnstable t =\n(1 + a t)18 = -\n37.043\nI would describe backward Euler as absolutely stable (A-stable) because it is stable\nwhenever Re a < 0. Only an implicit method can be A-stable. Forward Euler is a\nstable method(!) because it succeeds as t ! 0. For small enough t, it is on the\nstable side of the borderline.\nIn this example a good quality approximation requires more than stability (even\nt = 1 is too big). Those powers of - 9 alternate in sign, while e-20t stays positive.\nAccuracy and Convergence\nSince forward and backward differences are first order accurate, it is no surprise that\nthe errors from forward and backward Euler are O(t). This error e = u - U is\n\nreplacements\nc\n5.1. FINITE DIFFERENCE METHODS\n2006 Gilbert Strang\nt\nt\nt is small\nU = 1 - t < -1\nu = e-t\nU\n- t\nu = e-t\nt\n= 1\ntoo big\nFigure 5.2: Forward Euler (stable and unstable, with -at going above 2).\nmeasured at a fixed time T. As t decreases, so does the new error added at each\nstep. But the number of steps to reach that time increases, keeping n t = T.\nTo see why the error u(T) - U(T) is O(t), the key is stability. We need to know\nthat earlier errors don't increase as they are carried forward to time T. Forward Euler\nis the simplest difference equation, so it is the perfect example to follow through first.\n(The next sections will apply the same idea to partial differential equations.) Euler's\ndifference equation for u0 = f(u, t) = au is\nUn+1 = Un + t f(Un, tn) = Un + a t Un .\n(8)\nThe true solution at time n t satisfies (8) except for a discretization error DE:\nun+1 = un + t u n\n0 + DE = un + a t un + DEn+1 .\n(9)\nThat error DE is of order (t)2 because the second-order term is missing (it\n2 (t)2\ne\ndifference equation for the error\nshould be\nun, but Euler didn't keep it). Subtracting (8) from (9) gives a\n= u - U, propagating forward in time:\nError equation\nen+1 = en + a t en + DEn+1 .\n(10)\nYou could think of this one-step error DEn+1 as a deposit like (t)2 into your account.\nOnce the deposit is made, it will grow or decay according to the error equation. To\nreach time T = N t, each error DEk at step k has N - k more steps to go:\neN = (1 + a t)N -1 DE1 + · · · +\na t)N -k DEk\n(1 +\n+ · · · + DEN .\n(11)\nNow stability plays its part. If a is negative, those powers are all less than 1 (in\nabsolute value)--provided 1 + a t does not go below -1. If a is positive, those\na t)N\naT\npowers of 1 + a t are all less than (e\n= e\n. Then the error eN has N terms\nin (11), and every term is less than c(t)2 for a fixed constant c:\nError bound\n|eN | = |uN -UN | N c (t)2 = c T t .\n(12)\nThe errors along the way, of size (t)2 , combined after N = T/t steps into an\noverall t error. This depended on stability--the local errors didn't explode.\n\nc2006 Gilbert Strang\nCHAPTER 5. INITIAL VALUE PROBLEMS\nNotice that the error growth follows the difference equation in (10), not the dif\nferential equation. The stiff example with a t = (-20)( 1\n9 ) gave a large 1 + a t,\neven when ea t was small. We still call forward Euler a stable method, because\nas soon as t is small enough the danger has passed. Backward Euler also gives\n|eN | = O(t). The problem with these first-order methods is their low accuracy.\nThe local discretization error DE tells us the accuracy. For Euler that error\nDE 1\n2 (t)2u 00 is the first term that Euler misses in the Taylor series for u(t + t).\nBetter methods will capture that term exactly, and miss on a higher-order term. The\ndiscretization error (we find it by comparing u(t+t) with Un+1, assuming u(t) agrees\nwith Un) begins with a power (t)p+1:\nDE c(t)p+1 dp+1u\ndtp+1 .\n(13)\nLocal discretization error\nThe method decides c and p + 1. With stability, T/t steps give a global error of\norder (t)p. The derivative of u shows whether the problem is hard or easy.\nError estimates like (13) appear everywhere in numerical analysis. The 1, -2, 1\nsecond difference has error 12 (x)4u 0000 . Partial differential equations (Laplace, wave,\nheat) produce similar terms. In one line we find c = -1 for backward Euler :\n(t)2\n(t)2\n(un+1 -un) -t u n0\n+1 t u 0 +\nu 00 -t(u 0 + t u n ) -\nun . (14)\nn\nn\nn\nThe global estimate |u -U| ↑ C t max |u 00| shows no error when u is linear and\nu 00 is zero (Euler's approximation of constant slope becomes true). For nonlinear\nequations, the key is in the subtraction of (8) from (9). A \"Lipschitz\" bound L on\n@f/@u replaces the number a in the error equation:\n|f(u, t) -f(U, t)| ↑L|u -U|\ngives\nen+1 ↑(1 + Lt) en + DEn+1 .\n(15)\nSecond-Order Methods\nHere is a first way to increase the accuracy. We could center the equation at the\nmidpoint (n + 2 )t of the step, by averaging f(Un, tn) and f(Un+1, tn+1). The result\nis an implicit second-order method use in MATLAB's ode23t:\nUn+1 - Un\nTrapezoidal rule/Crank-Nicolson\n=\n(fn+1 + fn) .\n(16)\nt\nSo Un+1 -1 t fn+1 = Un + 2 t fn. For our model u0 = f(u) = au, this is\n1 + 1 a t\n(1 - a t) Un+1 = (1 + a t) Un which gives Un+1 =\nUn . (17)\n1 -1 a t\nThe true solution has un+1 = eat un. Problem 1 will find DE (t)3 . The equation\nis stable. So N = T/t steps produce |eN | = |uN -UN | ↑cT(t)2 .\n\nc\n5.1. FINITE DIFFERENCE METHODS\n2006 Gilbert Strang\nHow to improve forward Euler and still keep it explicit ? We don't want Un+1 on\nthe right side of the equation, but we are happy with Un-1 (from the previous step !).\nHere is a combination that gives second-order accuracy in a two-step method:\nUn+1 - Un\n\"Adams-Bashforth\"\n=\nf(Un, tn) -\nf(Un-1, tn-1) .\n(18)\nt\nRemember that f(Un-1, tn-1) is already computed in the previous step, going from\nn-1 to n. So this explicit multistep method requires no extra work, and improves\nthe accuracy. To see how (t)2 comes out correctly, write u for f:\nu 0 -\nun-1 2 un\n0 - (u 0 - t u n) = un + 2 t u n .\nn\nn\nMultiplied by t in (18), that new term 1\n2 (t)2u00 is exactly what Euler missed. Each\nn\nextra term in the difference equation can increase the accuracy by 1.\nA third possibility uses the already computed value Un-1 (instead of the slope\n2 , - 4\nfn-1). With 3\n2 , chosen for second-order accuracy, we get an implicit backward\ndifference method:\n3Un+1 - 4Un + Un-1\nBackward differences\n= f(Un+1, tn+1) .\n(19)\n2t\nWhat about stability ? The implicit method (17) is stable even in the stiff case,\nwhen a is very negative. 1 - 1 a t (left side) will be larger than 1 + 1 a t (right side).\n(19) is also stable. The explicit method (18) will be stable if t is small enough, but\nthere is always a limit on t for stiff systems.\nHere is a quick way to find the stability limit in (18) when a is real. The limit\noccurs when the growth factor is exactly G = -1. Set Un+1 = -1 and Un = 1\nand Un-1 = -1 in (18). Solve for a when f(u, t) = au:\n-2\nStability limit in (18)\n=\na +\na\ngives a t = -1 .\nSo C = 1 . (20)\nt\nThose second-order methods (17)-(18)-(19) are definitely useful ! The reader might\nsuggest including both Un-1 and fn-1 to increase the accuracy to p = 3. Sadly, this\nmethod is violently unstable (Problem 4). We may use older values of U in backward\ndifferences or f(U) in Adams formulas, but including both U and f(U) for even higher\naccuracy produces instability for all t.\nMultistep Methods: Explicit and Implicit\nBy using p older values of either U or f(U) (already computed at previous steps !),\nthe accuracy can be increased to order p. Each ∗U is U(t) - U(t - t) and p = 2\n\nc2006 Gilbert Strang\nCHAPTER 5. INITIAL VALUE PROBLEMS\nis (19):\n\n∗ + 1\n2 ∗2 + · · · + 1\np ∗p\nUn+1\n(Un+1\nn+1) .\nBackward differences\n= t f\n, t\n(21)\nU\nMATLAB's stiff code ode15s varies from p = 1 to p = 5 depending on the local error.\nUsing older values of the right side f(U) gives an Adams-Bashforth method:\nn+1 - Un = t(b1 fn + · · · + bp fn-p+1) with fn = f(Un, tn) .\n(22)\nThe table shows the numbers b up to p = 4, starting with Euler for p = 1.\norder of\nb1\nb2\nb3\nb4\naccuracy\np = 1\np = 2\n3/2\n-1/2\np = 3\n23/12 -16/12\n5/12\np = 4\n55/24 -59/24 37/24 -9/24\nlimit on at\nfor stability\n-2\n-1\n-6/11\n-3/10\nconstant c\nin error DE\n1/2\n5/12\n3/8\n251/720\nThe fourth-order method is often a good choice, although astronomers go above p = 8.\nAt the stability limit G = -1 as in (20). The local error DE c(t)p+1u(p+1) is a\nproblem of step control. Whether it is amplified by later steps is a problem of stability\ncontrol.\nImplicit methods have an extra term c0fn+1 at the new time level. Properly\nchosen, that adds one extra order of accuracy--as it did for the trapezoidal rule,\nwhich is the second method in the new table. Backward Euler is the first:\norder of\naccuracy\np = 1\np = 2\np = 3\np = 4\nc0\nc1\nc2\nc3\n1/2\n1/2\n5/12\n8/12\n-1/12\n9/24 19/24 -5/24 1/24\nlimit on at\nfor stability\n-→\n-→\n-6\n-3\nconstant c\nin error DE\n-1/2\n-1/12\n-1/24\n-19/720\nEvery row of both tables adds to 1, so u 0 = 1 is solved exactly.\nYou see that the error constants and stability are all in favor of implicit methods.\nSo is the user, except when solving for Un+1 becomes expensive. Then there is a\nsimple and successful predictor-corrector method, or else Newton's method.\nP:\nUse the explicit formula to predict a new U≈\nPredictor-\n≈\nn+1\nE:\nUse un+1 to evaluate the right side f≈\nCorrector\nn+1\nC: Use fn≈\n+1 in the implicit formula to correct to a new Un+1.\n\nc\n5.1. FINITE DIFFERENCE METHODS\n2006 Gilbert Strang\nu\nThe stability is much improved if another E step evaluates fn+1 with the corrected\nn+1. In principle we should continue the corrector to convergence. Frequently 1-2\ncorrections are enough, and the extra calculation can be put to use. By comparing the\npredicted U≈\nand corrected Un+1 the code can estimate the local error and change\nn+1\nt:\nc\nlocal error DE c≈-c (Un+1 -Un≈\n+1),\n(23)\nwhere c≈ and c are the error constants in the tables for the predictor and corrector.\nU\nImplicit methods often have a Newton loop inside each time step, to compute\nn+1. The kth iteration in the Newton loop is a linear system, with the Jacobian\nmatrix A(k) = @fi/@uj evaluated at the latest approximation U (k) with t = tn+1.\nij\nn+1\nHere is the kth i to solve Un+1 -c0f(Un+1, tn+1) = old values:\nNewton iteration (I -c0A(k))(U (k+1)\nn+1) = c0f(U (k)\n-U (k)\nn+1, tn+1) + old values .(24)\nn+1\nWhen f(u) = Au is linear, you only need one iteration (k = 0 starts with U (0) = Un).\nn+1\nFor nonlinear f(u), Newton's rapid convergence squares the error at each step (when\nit gets close). The price is a new evaluation of f(u) and its matrix A(k) of derivatives.\nRunge-Kutta Methods\nA-stable methods have no stability limit. Multistep methods achieve high accuracy\nwith one or two evaluations of f. If more evaluations are not too expensive, Runge-\nKutta methods are definitely competitive (and these are self-starting). They are\ncompound one-step methods, using Euler's Un + t fn inside f :\n\nUn+1 -Un\nSimplified Runge-Kutta\n=\nfn + f(Un + t fn, tn+1) .\n(25)\nt\nYou see the compounding of f. For the model equation u 0 = au the result is\n\nun+1 = un +\nt aun + a(un + t aun) = 1 + at + 1 a 2t2\nun = G un .\n(26)\nThis confirms the second-order accuracy; the growth factor G agrees with eat through\n(t)2 . There will be a stability threshold at = C, where |G| = 1:\n\nStability limit\n|G| = 1 + C +\nC2 = 1 (for complex C = a + ib)\nThe complete stability limit is not a point but a closed curve in the complex plane.\nFigure 5.\nshows all the complex numbers C at which |G| = 1.\n\nc2006 Gilbert Strang\nCHAPTER 5. INITIAL VALUE PROBLEMS\nThe famous version of Runge-Kutta is compounded four times and achieves p = 4:\nUn+1 - Un\nRunge-Kutta\n= (k1 + 2k2 + 2k3 + k4)\n(27)\nt\nk1 =\nf(Un, tn)\nk3 =\nf(Un + t k2, tn+1/2)\nk2 =\nf(Un + t k1, tn+1/2)\nk4 =\nf(Un + 2t k3, tn+1)\nFor this one-step method, no special starting instructions are necessary. It is simple\nto change t as you go. The growth factor reproduces eat through 24 a4t4 . The\nerror constant is the next coefficient\n1 . Among highly accurate methods, Runge\nKutta is especially easy to code and run--probably the easiest there is. MATLAB's\nworkhorse is ode45.\nTo prove that the stability threshold at = -2.78 is genuine, we reproduce the\nsolution of u 0 = -100u + 100 sin t. With u(0) = 0, Runge-Kutta gives\nU120\n.\nu(3)\nt = 3\n120 and at = -2.5\nU100\n,000,000,000\nt = 3\n100 and at = -3\n(28)\n= 0 151 =\nwith\n= 670\nwith\nDifferential-Algebraic Equations\nOur system of equations might not come in the form u0 = f(u, t). The more general\nform F(u0, u, t) = 0 appears in many applications. It may not be easy (or possible)\nto solve this system explicitly for u0 . For large electrical networks (VLSI on chips),\ntransistors produce a highly nonlinear dependence on u0 and u.\nHere is a simple model in which solving for the vector u0 is impossible:\nMu 0 = f(u, t)\nwith a singular matrix M (rank r < N) .\n(29)\nSuppose we change variables from u to v, so as to diagonalize M. In the new variables\nwe have r true differential equations and N - r algebraic equations (no derivatives).\nThe system becomes a differential-algebraic equation (DAE):\ndvi\n= fi(v1, . . . , vN , t) i = 1, . . . , r\nDAE\ndt\n(30)\n= fi(v1, . . . , vN , t) i = r + 1, . . . , N .\nThe N - r algebraic equations restrict v to a surface in N-dimensional space. The r\ndifferential equations move the vector v(t) along that surface.\nPetzold [46] developed the DASSL package for solving DAE's in the general form\nF(u0, u, t) = 0, replacing u by a backward difference of u. The best recommendation\nwe can make is to experiment with this software !\n\nc\n5.1. FINITE DIFFERENCE METHODS\n2006 Gilbert Strang\nProblem Set 5.1\nThe error in the trapezoidal (Crank-Nicolson) method (\n) comes from the\ndifference\n\ne at - 1 + (at/2)\n\nat\nat\nat\n= e at -\n1 +\n1 +\n+\n+ · · ·\n1 - (at/2)\nThis involves the two series that everyone should learn: the exponential series\nfor eat and the geometric series 1 + x + x2 + · · · for 1-x .\nMultiply inside the brackets to produce the correct 2 (at)2 . Show that the\n(at)3 term is wrong by c = 12 . Then the error is DE 12 (t)3 u .\nTry Runge-Kutta on u 0 = -100u + 100 sin t with t = -.0275 and -.028.\nThose are close to the stability limit -.0278.\nFor the backward difference error in (19), expand 1\n2 (3eat - 4 + e-at ) - at eat\ninto a series in at. Show that the leading error is - 1\n3 (at)3 so that c = - 1 .\nStability in (19)."
    },
    {
      "category": "Resource",
      "title": "am52.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/307c9b34baca5b281add586a56de5756_am52.pdf",
      "content": "c2006 Gilbert Strang\n5.2\nAccuracy and Stability for ut = c ux\nThis section begins a major topic in scientific computing: Initial-value problems\nfor partial differential equations. Naturally we start with linear equations that\ninvolve only one space dimension x (and time t). The exact solution is u(x, t) and its\ndiscrete approximation on a space-time grid has the form Uj,n = U(jx, nt). We\nwant to know if U is near u--how close they are and how stable U is.\nBegin with the simplest wave equation (first-order, linear, constant coefficient):\n= c\n.\n(1)\nOne-way wave equation\n@u\n@t\n@u\n@x\nWe are given u(x, 0) at time t = 0. We want to find u(x, t) for all t > 0. For simplicity,\nthese functions are defined on the whole line -∗ < x < ∗. There are no difficulties\nwith boundaries (where waves could change direction and bounce back).\nThe solution u(x, t) will have the typical feature of hyperbolic equations: signals\ntravel at finite speed. Unlike the second-order wave equation utt = c2uxx, this first-\norder equation ut = c ux sends signals in one direction only.\nSolution for u(x, 0) = eikx\nThroughout this chapter I will solve for a pure exponential u(x, 0) = eikx . At every\ntime t, the solution remains a multiple Geikx . The growth factor G will depend\non the frequency k and the time t, but different frequencies do not mix. Substituting\nu = G(k, t) eikx into ut = c ux yields a simple ordinary differential equation for G,\nbecause we can cancel eikx . The derivative of eikx produces the factor ik:\ndG\nut = c ux\nis\ndGe ikx = ikc Geikx\nor\n= ikcG .\n(2)\ndt\ndt\nThe growth factor is G(k, t) = eikc t . The initial value is G = 1.\nAn exponential solution to @u = c @u is u(x, t) = e ikc t e ikx = e ik(x+ct) .\n(3)\n@t\n@x\nImmediately we see two important features of this solution:\n1. The growth factor G = eikc t has absolute value |G = 1.\n|\nik(x+ct)\n2. The initial function eikx moves to the left with fixed velocity c, to e\n.\nThe initial value at the origin is u(0, 0) = eik0 = 1. This value u = 1 appears\nat all points on the line x + ct = 0. The initial data propagates along the\ncharacteristic lines x + ct = constant, in Figure 5.3. Right now we know this\nfact for the special solutions eik(x+ct). Soon we will know it for all solutions.\n\nc\n5.2. ACCURACY AND STABILITY FOR UT = C UX\n2006 Gilbert Strang\n\nx\nt\nu(P\nu(0, 0)\nu(Q\nu(\n0)\nP\nx + ct\nQ\nx + ct = X\n) =\n) =\nX,\ncharacteristic line to\n= 0\ncharacteristic line to\nu(0, 0)\nu(X, 0)\nFigure 5.3: The solution u(x, t) moves left with speed c, along characteristic lines.\nFigure 5.3 shows the travel path of the solution in the x-t plane (we are introducing\nthe characteristic lines). Figure 5.4 will graph the solution itself at times 0 and t.\nThat step function combines exponentials eik(x+ct) for different frequencies k. By\nlinearity we can add those solutions.\nSolution for Every u(x, 0)\nIn almost all partial differential equations, the solution changes shape as it travels.\nHere the shape stays the same. All pure exponentials travel at the same velocity c,\nso every initial function moves with that velocity. We can write down the solution:\n@u\n@u\nGeneral solution\n= c\nis solved by u(x, t) = u(x + ct, 0) .\n(4)\n@t\n@x\nThe solution is a function only of x + ct. That makes it constant along characteristic\nlines, where x + ct is constant. This dependence on x + ct also makes it satisfy the\nequation ut = c ux, by the chain rule. If we take u = (x + ct)n as an example, the\nextra factor c appears in @u/@t:\n@u\n@u\n@u\n= n (x + ct)n-1\nand\n= cn (x + ct)n-1\nwhich is c\n.\n@x\n@t\n@x\nA Taylor series person would combine those powers (different n) to produce a large\nfamily of solutions. A Fourier series person combines exponentials (different k) to\nproduce an even larger family. In fact all solutions are functions of x + ct alone.\nHere are two important initial functions--a light flashes or a dam breaks.\nExample 1 u(x, 0) = delta function ∂(x) = flash of light at x = 0, t = 0\nBy our formula (4), the solution is u(x, t) = ∂(x + ct). The light flash reaches the point\nx = -c at the time t = 1. It reaches x = -2c at the time t = 2. The impulse is traveling\nto the left at speed dx/dt = c. In this example all frequencies k are present in equal\n|\n|\namounts, because the Fourier transform of a delta function is a constant.\n\nc2006 Gilbert Strang\nNotice that a point goes dark again as soon as the flash passes through. This is the\nHuygens principle in 1 and 3 dimensions. If we lived in two or four dimensions, the wave\nwould not pass all at once and we wouldn't see clearly.\nExample 2 u(x, 0) = step function S(x) = wall of water at x = 0, t = 0\nThe solution S(x + ct) is the moving step function in Figure 5.4. The wall of water travels\nto the left (one-way wave). At time t, the \"tsunami\" reaches the point x = -ct. The\nflash of light will get there first, because its speed c is greater than the tsunami speed.\nThat is why a warning is possible for an approaching tsunami.\nx\nx\n-ct\nu(x, 0)\nu(\n)\nS(x\nt\nS(x + ct\nt\nx, t\ninitial profile\n) at = 0\nlater profile\n) at time\nFigure 5.4: The wall travels left with velocity c (all waves eikx do too).\nAn actual tsunami is described by the nonlinear \"shallow water equations\" that come\nlater. The feature of finite speed still holds.\nFinite Difference Methods for ut = c ux\nThe one-way wave equation is a perfect example for creating and testing finite dif\nference approximations. We can replace @u/@t by a forward difference with step t.\nHere are four choices for the discrete form of @u/@x at meshpoint ix:\nUi+1 - Ui\n1. Forward =\n= upwind: Low accuracy, conditionally stable for c > 0.\nx\nUi+1 - Ui-1\n2. Centered =\n: Unstable after a few steps as we will prove !\n2x\n3. Lax-Friedrichs: (20) has low accuracy, conditionally stable also for c < 0.\n4. Lax-Wendroff : (14) has extra accuracy, conditionally stable also for c < 0.\nThe list doesn't end there. We have reached a central problem of scientific computing,\nto construct approximations that are stable and accurate and fast. That topic can't\nbe developed on one page, especially when we move to nonlinear equations.\nConditionally stable means that the time step t is restricted. The need for this\nrestriction was noticed by Courant, Friedrichs, and Lewy. When the space difference\n\nc\n5.2. ACCURACY AND STABILITY FOR UT = C UX\n2006 Gilbert Strang\nreaches no further than x + x, there is an automatic stability restriction:\nt\nCFL requirement for stability\nr = c\n1 .\n(5)\nx\nThat number c t/x is often called the Courant number. (It was really Lewy who\nrecognized that r √ 1 is necessary for stability and convergence.) The reasoning is\nstraightforward, based on using the initial value that controls u(x, t):\nThe true solution at (x, t) equals the initial value u(x + ct, 0). Taking n\ndiscrete steps to reach t = n t uses information on the initial values as far\nout as x + n x. If x + ct is further than x + n x, the method can't work:\nt\nCFL condition x + c t √ x + n x\nor c n t √ n x\nor r = c x √ 1 . (6)\nIf the difference equation uses U(x + 2x, t), then CFL relaxes to r √ 2.\nA particular finite difference equation might require a tighter restriction on t for\nstability. It might even be unstable for all ratios r (we hope not). The only route to\nunconditional stability for all t is an implicit method, which computes x-differences\nat the new time t + t. This will be useful later for diffusion terms like uxx. For\nadvection terms (first derivatives), explicit methods with a CFL limitation are usually\naccepted because a much larger t would lose accuracy as well as stability.\nTo repeat, if r > 1 then the finite difference solution at x, t does not use initial\nvalue information near the correct point x← = x + ct. Hopeless.\nAccuracy of the Upwind Difference Equation\nLinear problems with constant coefficients are the ones to understand first. Exactly\nas for differential equations, we can follow each pure exponential eikx . After a single\ntime step, there will be a growth factor in U(x, t) = Geikx . That growth factor\nG(k, t, x) may have magnitude G < 1 or G > 1. This will control stability or\n| |\n| |\ninstability. The order of accuracy (if we compute in the k-! domain) comes from\ncomparing G with the true factor eikct from the differential equation.\nWe now determine that the order of accuracy is p = 1 for the upwind method.\nU(x, t + t) -U(x, t)\nU(x + x, t) -U(x, t)\nForward differences\n= c\n.\n(7)\nt\nx\nWe will test the accuracy in the x-t domain and then the k-! domain. Either way we\nuse Taylor series to check the leading terms. Substituting the true solution u(x, t) in\nplace of U(x, t), its forward differences are\nTime\n[u(x, t + t) -u(x, t)] = ut + t utt +\n(8)\nt\n· · ·\nc\nSpace\n[u(x + x, t) -u(x, t)] = c ux + 2 c x uxx +\n(9)\nx\n· · ·\nOn the right side, ut = c ux is good. One more derivative gives utt = c uxt = c uxx.\nNotice c2 . Then t utt matches c x uxx only in the special case ct = x:\n\nc2006 Gilbert Strang\nct\nt c2 uxx\nequals\nc x uxx\nonly if\nr =\n= 1.\nx\nFor any ratio r = 1, the difference between (8) and (9) has a first-order error. Let\n≥\nme show this also in the k-! Fourier picture and then improve to second-order.\nFix the ratio r = ct/x as x ! 0 and t ! 0. In the difference equation (7),\nwrite each new value at time t + t as a combination of two old values of U:\nU(\nt\n1 - r) U(\nr U(x\n) .\n(10)\nDifference equation\nx, t + ) = (\nx, t) +\n+ x, t\nStarting from U(x, 0) = eikx we quickly find the growth factor G at time t:\nikx\nAfter 1 step (1 -r)e ikx + r e ik(x+x) =\n\n1 - r + r eikx e\n= G e ikx .\n(11)\nTo test the accuracy, compare this G = Gapprox to the exact growth factor eickt .\nUse the power series 1 + x + x2/2! + · · · for any ex:\nAccuracyGapprox = 1 -r + r e ikx = (1 -r) + r + r(ikx) + r (ikx)2 +\n· · ·\nirkx\nGexact = e ickt = e\n= 1 + irkx +\n(irkx)2 +\n(12)\n· · ·\nThe first terms agree as expected. Forward differences replaced derivatives, and the\nmethod is consistent. We saw ut = c ux in comparing (8) with (9). The next terms\ndo not agree unless r = r2:\nCompare r(ikx)2 with 1 r 2(ikx)2 . Single-step error of order (kt)2 . (13)\nAfter 1/t steps, those errors of order k2(t)2 give a final error O(k2t). Forward\ndifferences are only first order accurate, and so is the whole method.\nThe special case r = 1 means ct = x. The difference equation is exactly\ncorrect. The true and approximate solutions at (x, t) are both u(x + x, 0). We\nare on the characteristic line in Figure 5.3. This is an interesting special case (the\ngolden t, but hard to repeat in scientific computing when c varies).\nConclusion\nExcept when r = r2 , the upwind method is first-order accurate.\nHigher Accuracy for Lax-Wendroff\nTo upgrade the accuracy, we need to match the 1 t utt error term in the forward time\ndifference by an additional space difference that gives 2 t c2uxx. This is achieved by\nthe Lax-Wendroff method:\nU(x, t + t) -U(x, t)\nU(x + x, t) -U(x -x, t)\n= c\nt\n2x\nt 2\nU(x + x, t) -2U(x, t) + U(x -x, t)\n(14)\n+\nc\n.\n(x)2\nSubstituting the true solution, that second difference produces 1 c2t uxx plus higher\norder terms. This cancels the 1 t utt error term in the time difference, computed\n\nc\n5.2. ACCURACY AND STABILITY FOR UT = C UX\n2006 Gilbert Strang\nin equation (8). (Remember utt = cuxt = c2uxx. The centered difference has no x\nterm.) Thus Lax-Wendroff has second-order accuracy.\nTo see this in the k-! frequency domain, rewrite the LW difference equation (14):\nU (x, t + t) = (1 - r 2)U (x, t) + (r 2 + r)U (x + x, t) + (r 2 - r)U (x - x, t) . (15)\nSubstitute U (x, t) = eikx to find the one-step growth factor G at time t + t:\nikx\n-ikx\nGrowth factor for LW\nG = (1 - r 2) + (r 2 + r)e\n+\n(r 2 - r)e\n.\nExpanding eikx and e-ikx in powers of ik x, this becomes\n1 2\nG = 1 + r(ikx) + r (ikx)2 + O(kx)3 .\n(16)\nComparing with Gexact = eirkx in equation (12), three terms agree. So the one-step\nerror is of order (kx)3 . After 1/t steps the second-order accuracy of Lax-Wendroff\nis confirmed.\nFigure 5.5 shows by actual computation the improvement in accuracy. For a first-\norder method, the \"wall of water\" is smeared out. High frequencies have growth\nfactors G(k) much smaller than 1. There is too much dissipation. For the first-order\n|\n|\nLax-Friedrichs method, the dissipation is even worse (Problem 2). The second-order\nLax-Wendroff method stays much closer to the discontinuity. But it's not perfect--\nthose oscillations are not good.\nFor an ideal difference equation, we want to add enough dissipation very close to\nthe shock, to avoid that oscillation (the Gibbs phenomenon). A lot of thought has\ngone into high resolution methods, to capture shock waves cleanly.\nGreater accuracy is achievable by including more terms in the difference equation.\nIf we go from the three terms in Lax-Wendroff to five terms, we can reach fourth-\norder accuracy. If we use all values U (jx, nt) at every time step, which requires\nmore work, we can achieve spectral accuracy. Then the error decreases faster than\nany power of x, provided u(x, t) is smooth enough to allow derivatives of all orders.\nSection\ngives a separate discussion of this spectral method.\nStability of the Four Finite Difference Methods\nNow we turn from accuracy to stability. Accuracy requires G to stay close to the\ntrue eickt . Stability requires G to stay inside the unit circle. We need G √ 1 for all\n| |\nfrequencies k or the finite difference approximation Gneikx will blow up.\nWe now check whether or not G √ 1, in the four methods.\n| |\n1. Forward differences in space and time: U/t = c U/x (upwind).\n\nc2006 Gilbert Strang\nRecall from equation (11) that G = 1 - r + reikx . If the Courant number r is\nbetween 0 and 1, the triangle inequality gives G √ 1:\n| |\nStability for 0 r 1\n1 - r + re\n= 1 - r + r = 1 .\n(17)\n|G| √|\n|\n|\nikx |\nThis sufficient condition 0\nc t/x √ 1 is exactly the same as the Courant-\n√\nFriedrichs-Lewy necessary condition ! They reasoned that U(x, nt) depends on the\ninitial values between x and x + nx. That domain of dependence must include\nthe point x + c nt. (Otherwise, changing the initial value at the point x + c nt\nwould change the true solution u but not the approximation U.) Then c nt must\nlie between 0 and nx, which means that 0 √ r √ 1.\nFigure 5.5 shows G in the stable case r = 3 and the unstable case r = 4 (when t\nis too large). As k varies, and eikx goes around a unit circle, the complex number\nG = 1 - r + reikx goes in a circle of radius r. The center is 1 - r. Always G = 1 at\nzero frequency (constant solution, no growth).\n1-r\nStable\n|G|<1\nr\nunit circle\nG = 1 - r + reikx\nG = 1-2r\nG = 1\n1-r\nr\nk = 0\nkx =\nUnstable\n|G| > 1\nFigure 5.5: Stable (upwind) and unstable (centered): CFL numbers r = 3 and r = 4 .\n2. Forward difference in time, centered difference in space.\nThis combination is never stable ! The shorthand Uj,n will stand for U(jx, nt):\nUj,n+1 - Uj,n\nUj+1,n - Uj-1,n\nr\n= c\nor\nUj,n+1 = Uj,n +\n(Uj+1,n - Uj-1,n) . (18)\nt\n2x\nThose coefficients 1 and r/2 and -r/2 go into the growth factor G, when the solution\nis a pure exponential and eikx is factored out:\nr\nr -ikx\ne\nUnstable: |G| > 1\nG = 1 +\nikx\ne\n= 1 + ir sin kx .\n(19)\n- 2\nThe real part is 1. The magnitude is G → 1. Its graph is on the left side of Figure 5.6.\n| |\n\nc\n5.2. ACCURACY AND STABILITY FOR UT = C UX\n2006 Gilbert Strang\n3. Lax-Friedrichs Method (upwind-downwind).\nWe can recover\n+ Uj-1,n) of its neighbors:\nReplace Uj,n\n2 (Uj+1,n\nstability for centered differences by changing the time difference.\nby the average\nUj,n+1 - 1\n2 (Uj+1,n + Uj-1,n)\nUj+1,n -Uj-1,n\nLax-Friedrichs\n= c\n.\n(20)\nt\n2x\nTwo old values Uj+1,n and Uj-1,n produce each new value Uj,n+1. Moving terms to\nthe right-hand side, the coefficients are 2\nr\n2 (1 -r\n(1 + ) and\n). The growth factor is\n1 + r\n-ikx\nG =\ne ikx + 1 -r e\n= cos kx + ir sin kx .\n(21)\nThe absolute value is G 2 = (cos kx)2 + r2(sin kx)2 . In Figure 5.6, G √ 1 when\n| |\n| |\nr √ 1. This stability condition agrees again with the CFL condition.\nG = 1 + ir sin kx\nLax-Friedrichs\nStable for r2 1\n|G| > 1\nUnstable\n|G| < 1\nG\nkx + ir sin kx !\nr\nForward in time-centered in space\n= cos\nFigure 5.6: Equation (18) is unstable for all r. Equation (20) is stable for r2 √ 1.\nNotice that c and r can be negative. The wave can go either way! This will\nbe useful for the two-way wave equation, but the accuracy is still first-order. The\nLax-Friedrichs G matches the next term in the exact growth factor only if r2 = 1:\nG = cos kx + ir sin kx = 1 + irkx - (kx)2 +\n(22)\nG\n· · ·\nexact = e ikrx = 1 + irk x + 1 i2 r 2(k x)2 +\n· · ·\nIn the exceptional cases r = 1 and r = -1, G agrees with Gexact . Staying exactly\non the characteristic line, Uj,n+1 matches the true u(jx, t + t). For r2 < 1, Lax-\nFriedrichs has an important advantage and disadvantage:\nGood Each new Uj,n+1 is a positive combination of old values.\nNot good The accuracy is only first-order.\nProblem 6 will show that second-order is impossible with positive coefficients.\n\nc2006 Gilbert Strang\n4. Lax-Wendroff Method (second-order accurate).\nThe LW difference equation (14) combines Uj,n and Uj-1,n and Uj+1,n to compute the\nnew value Uj,n+1. The coefficients of these three old values go into G:\nLax-Wendroff\nG = (1 - r 2) + 1(r 2 + r)e ikx + 1(r 2 - r)e -ikx .\n(23)\nThis is G = 1 - r2 + r2 cos kx + ir sin kx. At the dangerous frequency kx = ,\nthe growth factor is 1 - 2r2 . That stays above -1 if r2 √ 1.\nProblem 5 shows that G √ 1 for every kx. Lax-Wendroff is stable when\n| |\never the CFL condition r2 1 is satisfied. Again the wave can go either way\n(or both ways) since c and r can be negative. This is the most accurate of the five\nmethods in Figure 5.7.\nupwind\nwrong way\ncentered\nLax-Friedrichs\nLax-Wendroff\nstable\nunstable\nunstable\nstable\nstable\nif r √ 1\nall t\nall t\nif |r| √ 1\nif |r| √ 1\nFigure 5.7: Difference methods for the one-way wave equation ut = cux.\nEquivalence of Stability and Convergence\nDoes the discrete solution U approach the true solution u as t ! 0 ? The ex\npected answer is yes. But there are two requirements for convergence, and one of\nthem--stability --is by no means automatic. The other requirement is consistency --\nthe discrete problem must approximate the correct continuous problem. The fact that\nthese two properties are sufficient for convergence, and also necessary for convergence,\nis the fundamental theorem of numerical analysis:\nLax equivalence theorem\nStability is equivalent to convergence, for a consistent\napproximation to a well-posed linear problem.\nLax proved the equivalence theorem for initial-value problems. The rate of conver\ngence is given in (26). The theorem is equally true for boundary-value problems, and\nfor the approximation of functions, and for the approximation of integrals. It applies\nto every discretization, when the given problem Lu = f is replaced by LhUh = fh.\nAssuming the inputs f and fh are close, we will prove that u and Uh are close--\nprovided Lh is stable. The key points of the proof take only a few lines when the\nequation is linear, and you will see the essence of this fundamental theorem.\nSuppose f is changed to fh and L is replaced by Lh. The requirements are\n\nc\n5.2. ACCURACY AND STABILITY FOR UT = C UX\n2006 Gilbert Strang\nConsistency: fh ! f and Lhu ! Lu for smooth solutions u.\nWell-posed:\nThe inverse of L is bounded: ≤u≤=\n.\n≤L-1f ≤√C≤f ≤\nStability:\nThe inverses L-1 remain uniformly bounded: ≤L-1\n.\nh\nh fh≤√C≤fh≤\nUnder those conditions, the approximation Uh = L-1fh will approach u as h goes to\nh\nzero. We subtract and add L-1Lu = L-1f when u is smooth:\nh\nh\nConvergence\nu -Uh = L-1(Lhu -Lu) + L-1(f -fh) ! 0 .\n(24)\nh\nh\nConsistency controls the quantities in parentheses (they go to zero). Stability controls\nthe operators L-1 that act on them. Well-posedness controls the approximation of\nh\nall solutions by smooth solutions. Then always Uh converges to u.\nIf stability fails, there will be an input for which the approximations Uh = L-1f\nh\nare not bounded. The uniform boundedness theorem produces this bad f , from the\ninputs fh on which instability gives ≤L-1\n. Convergence fails for this f .\nh fh≤! ∗\nA perfect equivalence theorem goes a little further, after careful definitions:\nConsistency + Stability () Well-posedness + Convergence .\nOur effort will now concentrate on initial-value problems, to estimate the error (the\nconvergence rate) in u -Uh. The parameter h becomes t. We take n steps.\nThe Rate of Convergence\nConsistency means that the error at each time step goes to zero as the mesh is refined.\nOur Taylor series estimates have done more: The order of accuracy gives the rate\nthat this one-step error goes to zero. The problem is to extend this local rate to a\nglobal rate of convergence, accumulating the errors over n time steps.\nLet me write S for a single finite difference step, so U(t + t) = S U(t). The\ncorresponding step for the differential equation will be u(t + t) = R u(t). Then\nconsistency means that Su is close to Ru, and the order of accuracy p tells how close:\nAccuracy of discretization ≤Su -Ru≤√C1(t)p+1 for smooth solutions u.\nWell-posed problem\n≤Rn\nu≤for n t √T .\nu≤√C2≤\nStable approximations\n≤SnU ≤√C3≤U ≤for n t √T .\nThe difference between U = Snu(0) and the true u = Rnu( ) is (Sn -Rn)u( ).\nThe key idea is a \"telescoping identity\" that involves n single-step differences S -R:\nSn -Rn = Sn-1(S -R) + Sn-2(S -R)R + · · · + (S -R)Rn-1 .\n(25)\nEach of those n terms has a clear meaning. First, a power Rk carries u(0) to the true\nsolution u(k t). Then (S -R)u(k t) gives the error at step k of order (t)p+1 .\nThen powers of S carry that one-step error forward to time nt. By stability, this\n\nP\nP\nP\nP\nP\nP\nP\nP\nc2006 Gilbert Strang\namplifies the error by no more than C3. There are n √ T/t steps. The final rate\nof convergence for smooth solutions is (t)p:\nT\n≤U(n t) - u(n t)≤ = ≤(Sn - Rn)u(0)≤ √ C1C2C3\n(t)p+1 = C1C2C3 T (t)p .\nt\nu\n(26)\nNotice how smoothness was needed in the Taylor series (8) and (9), when t and\nx multiplied utt and uxx. That first-order accuracy would not apply if u or ut or\nx had a jump. Still the order of accuracy p gives a practical estimate of the overall\napproximation error u - U. The problem of scientific computing is to get beyond\np = 1 while maintaining stability and speed.\nProblem Set 5.2\nIntegrate ut = c ux from -∗ to ∗ to prove that mass is conserved: dM/dt = 0.\nMultiply by u and integrate uut = c uux to prove that energy is also conserved:\nZ 1\nZ 1\nM(t) =\nu(x, t) dx and E(t) = 1\n(u(x, t))2 dx stay constant in time.\n-1\n-1\nSubstitute the true u(x, t) into the Lax-Friedrichs method (21) and use ut = cux\nand utt = c2uxx to find the coefficient of the numerical dissipation uxx.\nThe difference equation Uj,n+1 =\namUj+m,n has growth factor G =\nameimkx .\nShow consistency with eickt (first-order accuracy at least) when\nam = 1 and\nmam = ct/x = r.\nThe condition for second-order accuracy is\nm am = r2, from the Taylor se\nries. Check this for Lax-Wendroff with a0 = 1-r2, a1 = 2 (r2+r), a-1 = 2 (r -r).\nWith nonnegative coefficients am → 0, the Schwarz inequality (\nmpampam)2 √\n(\nm2am)(\nam) becomes an equality r2 = r . This equality only happens if\nmpam = (constant)pam. Second-order is impossible with am → 0, unless the\ndifference equation has only one term Uj,n+1 = Uj+m,n.\nThe Lax-Wendroff method has G = 1 - r2 + r2 cos kx + ir sin kx. Square the\nreal and imaginary parts to get (eventually!) G 2 = 1 - (r2\n4)(1 - cos kx)2 .\n- r\nProve stability, that G 2 √ 1 if r √ 1.\n| |\n| |\nSuppose the coefficients in a linear differential equation change as t changes.\nThe one-step solution operators become Sk and Rk, for the step from k t to\n(k + 1)t. After n steps, products replace the powers Sn and Rn in U and u:\nU(n t) = Sn-1Sn-2 . . . S1S0 u(0) and u(n t) = Rn-1Rn-2 . . . R1R0 u(0) .\nChange the telescoping formula (25) to produce this U - u. Which parts are\ncontrolled by stability ? Which parts by well-posedness (= stability of the dif\nferential equation) ? Consistency still controls Sk - Rk .\n\n5.2. ACCURACY AND STABILITY FOR UT = C UX\nc2006 Gilbert Strang\nEven an unstable method will converge to the true solution u = eickteikx for\neach separate frequency k. Consistency assures that the single-step growth\nfactor G is 1 + ick t + O(t)2 . Then for t = n t,\nGn =\n\n1 + ickt\nn + O( 1\nn2 )\nn\n-! e ickt which is convergence.\nHow can we have convergence for each u(0) = eikx and still prove divergence for\na combination of frequencies u(0) = P1\n-1 ck eikx ?\nThe upwind method with r > 1 is unstable because the CFL condition fails.\nBy Problem 3, it does converge to eik(x+ct) based on values of u(x, 0) = eikx\nthat do not reach as far as x + ct. The method must be finding a \"correct\"\nextrapolation of eikx . So propose an initial u(x, 0) for which convergence fails."
    },
    {
      "category": "Resource",
      "title": "am53.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/8eaa23367474c809cc0816f24fdacc7f_am53.pdf",
      "content": "c\n2006 Gilbert Strang\n5.3\nThe Wave Equation and Staggered Leapfrog\nThis section focuses on the second-order wave equation utt = c uxx. We find\nthe exact solution u(x, t). Accuracy and stability are confirmed for the leapfrog\nmethod (centered second differences in t and x). This two-step method requires that\nwe rethink the growth factor G, which was clear for a single step. The result will be\np = 2 for the order of accuracy, and ct/x 1 for stability.\nv\nIt is useful to rewrite the wave equation as a first-order system. The components\n1 and v2 of the vector unknown can be @u/@t and c @u/@x. Then we are back to a\nsingle-step growth factor, but G is now a 2 by 2 matrix.\nSecond-order accuracy extends to this system vt = Avx if we use a staggered\nmesh. The mesh for v2 lies in between the mesh for v1. This has become the\nstandard method in computational electromagnetics (solving Maxwell's equations).\nThe physical laws relating the electric field E and the magnetic field H are beautifully\ncopied by the difference equations on a staggered mesh. The mesh becomes especially\nimportant in more space dimensions (x-y and x-y-z), and in finite volume methods.\nThis section goes beyond the one-way wave equation in at least five ways:\n1.\nTwo characteristic lines x+ ct = Cleft and x- ct = Cright go through each (x, t).\n2.\nThe leapfrog method involves three time levels t + t, t, and t - t.\n3.\nFirst-order systems have vector unknowns v(x, t) and growth matrices G.\n4.\nStaggered grids give the much-used FDTD method for Maxwell's equations.\n5.\nMore space dimensions lead to new CFL and vN stability conditions on t.\nWith -≈ < x < ≈, we don't yet have boundary conditions in space. And we are\nnot facing real problems like utt = c2(x) uxx + F eikx , with a high frequency forcing\nterms (k >> 1) and a varying speed c(x).\nSolution of the Wave Equation\nExactly as for the one-way equation ut = cux, we solve the two-way wave equation\nutt = c2uxx for each pure exponential. That allows us to separate the variables.\nikx:\nThe space variable is in eikx , and we look for solutions u(x, t) = G(t)e\n@2u\n@2u\nEach k\n= c\nbecomes d2G e ikx = i2 c 2k2G e ikx .\n(1)\n@t2\n@x2\ndt2\nThus Gtt = i2c2k2G. This second-order equation has two solutions, Gleft = eickt and\nGright = e-ickt . So there are two waves with speed c:\nuright(x, t) = eik(x-ct)\nPure waves\nuleft(x, t) = eik(x+ct)\nand\n.\n(2)\n\n5.3. THE WAVE EQUATION AND STAGGERED LEAPFROG c\n2006 Gilbert Strang\nCombinations of left-going waves eik(x+ct) will give a general function F1(x + ct).\nCombinations of eik(x-ct) give F2(x - ct). The complete solution includes both:\nu(x, t) = uleft(x, t) + uright(x, t) = F1(x + ct) + F2(x - ct) .\n(3)\nWe need those two functions to match an initial shape u(x, 0) and velocity ut(x, 0):\nAt t = 0\nu(x, 0) = F1(x) + F2(x) and ut(x, 0) = c F 1\n0(x) - c F 2\n0(x) .\n(4)\nSolving for F1 and F2 gives the unique solution that matches u(x, 0) and ut(x, 0):\nu(x, t) = u(x + ct, 0) + u(x - ct, 0)\n1 Z x+ct\nSolution\n+\nut(x, 0) dx .\n(5)\n2c\nx-ct\nThe \"domain of dependence\" of u(x, t) includes the initial values from x - ct to x + ct.\nThat domain is on the left side of Figure 5.9, bounded by the characteristic lines.\nExample 1 Starting with zero velocity, ut(x, 0) = 0, the integrated term in formula (5)\nis zero. A step function S(x) (wall of water ) will travel left and right along characteristic\nlines, as in Figure 5.8. It reaches the points x = 1 and x = -1 at time t = 1/c:\nTwo walls\nu(x, t) =\nS(x + ct) +\nS(x - ct) = 0 or\nor 1 .\nBy time t, the initial jump at x = 0 affects the solution between x = -ct and x = ct.\nThat is the \"domain of influence\" of the point x = 0, t = 0.\nTime 0\nu(x, 0) = 1\nTime t u(x, t) = 1\nu(x, t) = 1\ncharacteristic\nx\nx\nu(x, 0) = 0\n-ct\nct\nFigure 5.8: Two-wall solution to the wave equation starting from a step function.\nThe Semidiscrete Wave Equation\nU\nU\nLet me start by discretizing only the space derivative uxx. The second difference\nj+1 - 2Uj + Uj-1 is the natural choice, divided by (x)2 . For the approximations\nj (t) at the meshpoints x = jx, we have a family of ODEs in the time direction\n(method of lines):\nSemidiscrete utt = c2uxx\nU 00\nj =\nc2\n(x)2 (Uj+1 - 2Uj + Uj-1 .\n(6)\n\nc\n2006 Gilbert Strang\nAgain we follow every exponential, looking for Uj (t) = G(t)eikjx . Substitute into (6)\nand cancel the common factor eikjx . Instead of Gtt = -c2k2G we have\nc\nc\n-ikx\nGrowth equation Gtt = (x)2 (e ikx - 2 + e\n)G =\n(x)2 (2 - 2 cos kx)G .\n(7)\n-\nThe correct right side -c2k2G is multiplied by a factor F 2 . This F 2 turns up so often\nthat we need to recognize it! Use 2 - 2 cos α = 4 sin2(α/2):\nSinc squared F 2 = 2 - 2 cos kx = 4 sin2(kx/2)\nsin(kx/2) 2\n=\n.\n(8)\nk2(x)2\nk2(x)2\nkx/2\nThe \"sinc function\" is defined as sin α divided by α. When α = kx is small, this is\n1 + 0(α2). Then F 2 near 1 and equation (7) is close to the correct Gtt = -c2k2G.\nFor every kx, the growth equation (7) has two exponential solutions:\nSemidiscrete growth\nGtt = -c 2F 2k2G gives G(t) = e ±icF kt .\n(9)\nThe wave speed c is multiplied by F to give the numerical \"phase velocity\" cF. Notice\nthat F depends on k. Different frequencies eikx are traveling at different speeds cF(k).\nThis is dispersion and we will see it again in Section 5.\n.\nI will mention that the \"group velocity\"--the derivative of cFk with respect to\nk--is a more important quantity than the phase velocity cF.\nThe semidiscrete form suggests a good algorithm for the wave equation, if we have\nboundary conditions (say u = 0 along the lines t = 0 and t = ω). If h = x =\n\nn+1 ,\nthis interval has interior meshpoints. The n by n second difference matrix is the\nspecial K from earlier chapters (but now we have -K):\n2c\nSemidiscrete with boundaries\nU 00(t) =\nKU .\n(10)\n(x)2\nThis is just the equation MU 00 + KU = 0 of oscillating springs in Section 2.2.\nThe n eigenvalues of K are positive numbers 2 - 2 cos jx. The only change\nfrom the equation on an infinite line is that j takes only the values 1, 2, . . . , n. The\noscillations go on forever as in (8), the energy is conserved, and now the waves bounce\nback from the boundaries instead of continuing out to x = ±≈.\nLeapfrog from Centered Differences\nA fully discrete method also approximates utt by a centered differences. This time\ndifference \"leaps over \" the space difference at t = nt:\nLeapfrog method\nUj,n+1 - 2Uj,n + Uj,n-1\n2 Uj+1,n - 2Uj,n + Uj-1,n\n= c\n.\n(11)\n(t)2\n(x)2\nThis has two key differences from the 5-point molecule for uxx + uyy = 0 (Laplace).\nFirst, utt - c2uxx has a minus sign. Second, we have two conditions at t = 0 and no\n\n5.3. THE WAVE EQUATION AND STAGGERED LEAPFROG c\n2006 Gilbert Strang\nconditions at a later time. We are marching forward in time (marching with Laplace's\nequation would be totally unstable). A separate calculation for the first time step\ncomputes Uj,1 from the initial shape u(x, 0) and the velocity ut(x, 0).\nThe accuracy of leapfrog is normally second-order. Substitute the true u(x, t)\ninto (11), and use the Taylor series for second differences (Section 1.2). The first\nterms in the local error give consistency.\nSecond-order\nutt + 1 (t)2 utttt +\n= c 2(uxx +\n(x)2 uxxxx +\n(12)\n· · ·\n· · ·).\nIn this case utttt = c2uxxtt = c uttxx = c uxxxx. The two sides of (12) differ by\nLocal discretization error\n1 [(t)2 c 4 -(x)2 c ]uxxxx +\n(13)\n· · ·\nAgain ct = x is the golden time step that follows the characteristic exactly. The\ntwo triangles in Figure 5.9 become exactly the same in this borderline case r = 1.\nThe CFL reasoning shows instability for r > 1. We now show that r 1 is stable.\nlines\n\nt\nx\nu(\n)\nc and - 1\nc\n\nU (\nt)\nt\nx\n- t\nx\nx, t\ncharacteristic\nslopes\nx, n\nslope\nslope\nx -ct\nx + ct\nx -nx\nx + nx\nFigure 5.9: Domains of dependence: u from wave equation and U from leapfrog.\nStability of the Leapfrog Method\nA difference equation must use the initial conditions in this whole interval, to have\na chance of converging to u(x, t). The domain of dependence for U must include the\ndomain of dependence for u. The slopes must have t/x 1/c. Since convergence\nrequires stability, we have a Courant-Friedrichs-Lewy condition on t:\nr = c t/x 1.\nCFL condition The leapfrog method will require\nFor a double-step difference equation, we still look for pure solutions U (x, nt) =\nGneikx , separating time from space. In the leapfrog equation (11) this gives\nGn+1 -2Gn + Gn-1\nikx -2 + e-ikx\nikx\nikx\ne\n= c 2Gn e\ne\n.\n(t)2\n(x)2\nG\nSet r = ct/x and cancel Gn-1eikx . This leaves a quadratic equation for G:\n2 -2G + 1 = r 2 G (2 cos kx -2) .\n(14)\nx\n\nc\n2006 Gilbert Strang\nThe two-step leapfrog equation allows two G's (of course !). For stability, both must\nsatisfy G 1 for all frequencies k. Rewrite equation (14) for G:\n| |\nG2 -2\n\n1 -r 2\nGrowth factor equation\n+ r cos kx G + 1 = 0 .\n(15)\nThe roots of G2 -2aG + 1 = 0 are G = a ±\np\na -1. Everything depends on that\nsquare root giving an imaginary number, when a2 = [1 -r2 + r2 cos kx]2 1:\nIf a 2 1 then G = a ± i\np\n1 -a2 has\n= a + (1 -a 2) = 1 .\n|G| 2\nThe CFL condition r 1 does produce a2 1 and the leapfrog method is stable:\n1 -r 2\nStability\nIf r 1 then a =\n+ r cos kx (1 -r ) + r 2 = 1 .\n(16)\n| |\n|\n|\nAn unstable r > 1 would produce a = 1 -2r2 > 1 at the dangerous kx = ω.\n| |\n|\n|\nThen both G's are real, and their product is 1, and one of them has G > 1.\n| |\nNote 1\nSuppose r is exactly 1, so that ct = x. At this \"golden ratio\" we\nexpect perfect accuracy. The middle terms -2Uj,n and -2r2Uj,n cancel in the leapfrog\nequation (11), leaving a complete leap over the center points at (j, n) when r = 1:\nExact leapfrog\nUj,n+1 + Uj,n-1 = Uj+1,n + Uj-1,n .\n(17)\nThe difference equation is satisfied by u(x, t), because it is satisfied by every wave\nU (x + ct) and U (x -ct). Take Uj,n = U (jx + cnt) and use ct = x:\nU\nUj,n+1 and Uj+1,n\nare both equal to\nU (jx + cnt + x)\nj,n-1 and Uj-1,n\nare both equal to\nU (jx + cnt -x)\nSo (17) is satisfied by all traveling waves U (x + ct), and similarly by U (x -ct).\nNote 2\nYou could also apply leapfrog to the one-way equation ut = c ux:\nUj,n+1 -Uj,n-1 = ct\nx (Uj+1,n -Uj-1,n).\n(18)\nOne-way leapfrog\nNow the growth factor equation is G2 -2(ir sin kx)G -1 = 0. Problem\nconfirms that the stability condition is again r 1. In that stable case, one growth\nfactor is sensible and the other is strange:\nG1 = e ir sin kx\nickt\nand G2 =\ne -ir sin kx\n(19)\ne\nG\n-\n-1 .\n1 and G2 are exactly on the unit circle. With G = 1 there is no room to move.\n| |\nNumerical diffusion ∂(Uj+1,n -2Uj,n + Uj-1,n) usually adds extra stability, but not\nhere. So leapfrog for first-order equations can be dangerous.\nSection 5.4 will study the convection-diffusion equation ut = c ux + d uxx.\n\nZ Z Z\n5.3. THE WAVE EQUATION AND STAGGERED LEAPFROG c\n2006 Gilbert Strang\nWave Equation in Higher Dimensions\nThe wave equation extends to three-dimensional space (with speed set at c = 1):\n3D Wave equation\nutt = uxx + uyy + uzz .\n(20)\nWaves go in all directions, and the solution is a superposition of pure harmonics.\nThese plane waves now have three wave numbers k, π, m, and frequency w:\nExponential solutions\nu(x, y, z, t) = e i(!t + kx + `y + mz) .\nSubstituting into the wave equation gives !2 = k2 + π2 + m . So there are two\nfrequencies ± !. These exponential solutions combine to match the initial wave height\nu(x, y, z, 0) and its velocity ut(x, y, z, 0).\nSuppose the initial velocity is a three-dimensional delta function (x, y, z):\nλ(x, y, z) = λ(x)λ(y)λ(z) gives\nf(x, y, z) λ(x, y, z) dV = f(0, 0, 0) .\n(21)\nThe resulting u(x, y, z, t) will be the fundamental solution of the wave equation. It is\nthe response to the delta function, which gives equal weight to all harmonics. Rather\nthan computing that superposition we find it from the wave equation itself. Spherical\nsymmetry greatly simplifies uxx + uyy + uzz , when u depends only on r and t:\n@2 u\n@2 u\n2 @u\nSymmetry produces u(r, t)\n=\n+\n.\n(22)\n@t2\n@r2\nr @r\nMultiplying by r, this is a one-dimensional equation (ru)tt = (ru)rr ! Its\nsolutions ru will be functions of r - t and r + t. Starting from a delta function is like\nsound going out from a bell, or light from a point source. The solution is nonzero\nonly on the sphere r = t. So every point hears the bell only once, as the sound wave\npasses by. An impulse in 3D produces a sharp response (this is Huygen's principle).\nIn 2D, the solution does not return to zero for t > r. We couldn't hear or see\nclearly in Flatland. You might imagine a point source in two dimensions as a line\nsource in the z-direction in three dimensions. The solution is independent of z, so it\nsatisfies utt = uxx + uyy . But in three dimensions, spheres starting from sources along\nthe line continue to hit the listener. They come from further and further away, so the\nsolution decays--but it is not zero. The wave front passes, but waves keep coming.\nEXERCISE ON EQ.(26)\nLeapfrog Method in Higher Dimensions\nIn one dimension, two characteristics go out from each point (x, 0). In 2D and 3D,\na characteristic cone goes out from (x, y, 0) and (x, y, z, 0). It is essential to see how\nthe stability condition changes from r = ct/x 1.\n\n\"\n#\n\nc\n2006 Gilbert Strang\nThe leapfrog method replaces uxx and uyy by centered differences at time nt:\nUn+1 - 2Un + Un-1\n2 Un\ny Un\nLeapfrog for utt = uxx + uyy\n=\nx\n+\n.\n(t)2\n(x)2\n(y)2\nU0 and U1 come from the given initial conditions u(x, y, 0) and ut(x, y, 0). We look\nfor a solution Un = Gneikxei`y with separation of variables. Substituting into the\nleapfrog equation and canceling Gn-1eikxei`y produces the 2D equation for two G's:\nG2 - 2G + 1\n(2 cos k x - 2)\n(2 cos π y - 2)\nGrowth factor\n= G\n+ G\n. (23)\n(t)2\n(x)2\n(y)2\nAgain this has the form G2 - 2aG + 1 = 0. You can see a in brackets:\n\nt\nt\nG2 - 2 1 -\n(1 - cos k x) -\ny\n(1 - cos π y) G + 1 = 0 .\n(24)\nx\nBoth roots must have G = 1 for stability. This still requires -1 a 1. When the\n| |\ncosines are -1 (the dangerous value) we find the stability condition for leapfrog:\n\nt\nt\nt 2\nt 2\nStability\n-1 1 - 2\nneeds\n+\n1.\n(25)\nx\n- 2\ny\nx\ny\nFor x = y on a square grid, this is t x/\np\n2. In three dimensions it would be\nt x/\np\n3. Those also come from the CFL condition, that the characteristic cone\nmust lie inside the pyramid that gives the leapfrog domain of dependence. Figure 5.10\nshows the cone and pyramid just touching, when t = x/\np\n2.\n(0, x, 0)\nCone has circular base for utt = uxx + uyy\nPyramid has diamond base for leapfrog\nt\n(x, 0, 0)\nCone and pyramid go up to (0, 0, t)\nFigure 5.10: The pyramid contains and touches the cone when (t)2 = (x)2/2.\nAn Equivalent First-order System\nI can display a system of two equations vt = Avx that is equivalent to utt = c2uxx:\n@\nut\n0 c\n@\nut\nFirst-order system\n=\n.\n(26)\n@t cux\nc 0 @x cux\nThe first equation recovers utt = c2uxx. The second is the identity cuxt = cutx. Notice\nthat the 2 by 2 matrix is symmetric and its eigenvalues are the wave velocities ±c.\n\nR\n\n@\n@\n@\n@\n5.3. THE WAVE EQUATION AND STAGGERED LEAPFROG c\n2006 Gilbert Strang\nThis \"symmetric hyperbolic\" form vt = Avx is useful in theory and practice. The\nenergy E(t) =\n2 √v(x, t)√2 dx is automatically constant in time ! Here is the proof\nfor any equation vt = Avx with a symmetric matrix A:\nX @vi\n@\n@\nT\n@t\n2 √v√\n=\nvi @t = v vt = v TAvx =\nv TAv .\n(27)\n@x\nWhen you integrate over all x, the left side is @E/@t. The right side is 2 vTAv at\nthe limits x =\n. Those limits give zero (no signal has reached that far). So the\n±≈\nderivative of E(t) is zero, and E(t) stays constant.\nThe Euler equations of compressible flow are also a first-order system, but not\nlinear. In physics and engineering, a linear equation deals with a small disturbance.\nSomething from outside acts to change the equilibrium, but not by much:\nin acoustics it is a slowly moving body\nin aerodynamics it is a slender wing\nin elasticity it is a small load\nin electromagnetism it is a small source.\nBelow some level, the cause-effect relation is very close to linear. In acoustics, the\nsound speed is steady when pressure is nearly constant. In elasticity, Hooke's law\nholds until the geometry changes or the material begins to break down. In electro\nmagnetism, nonlinearity comes with relativistic and quantum effects.\nThe case to understand has A = constant matrix, with n real eigenvalues and\neigenvectors w. The vector equation vt = Avx will split into n scalar one-way wave\nequations Ut = Ux. When Aw = w we look for v(x, t) = U(x, t)w:\n@U\n@U\n@U\n@U\n@U\nvt = Avx becomes\nw = A\nw =\nw so\n=\n.\n(28)\n@t\n@x\n@x\n@t\n@x\nThe complete solution vector is v(x, t) = U1(x + 1t)w1 +\n+ Un(x + nt)wn. The\n· · ·\nproblem vt = Avx has n signal speeds i and it sends out n waves.\nThere are n characteristic lines x + it = constant. The wave equation has\n0 c\nc 0\n2 , -2 ) and ( 1\nand two eigenvalues = c and = -c. The eigenvectors w are ( 1\n2 , 2 ).\nThen the two scalar equations Ut = Ux produce left and right waves:\n1 =\nc\n(ut + c ux) =\nc\n(ut + c ux)\n@t\n@x\n(29)\n2 = -c\n(ut -c ux) = -c\n(ut -c ux) .\nu\n@t\n@x\nEach equation agrees with utt = c2uxx. The one-way waves are U(x, t) = U(x+ t, 0).\nThe vector solution v(x, t) is recovered from U1w1 + U2w2:\n\nt\nv =\n= (ut + c ux) -1 + (ut -c ux)\n1 .\n(30)\nc ux\nA stable difference method for vt = Avx comes from a stable method for ut = ±cux.\nJust replace c by A in Lax-Friedrichs and Lax-Wendroff, or go to leapfrog.\n\nc\n2006 Gilbert Strang\nLeapfrog on a Staggered Grid\nThe discrete case should copy the continuous case. The two-step leapfrog difference\nequation should reduce to a pair of one-step equations. But if we don't keep the\nindividual equations centered, they will lose second-order accuracy. The way to\ncenter both first-order equations is to use a staggered grid (Figure 5.11).\nPlease allow me to name the two components v1 = E and v2 = H. Then the\nstaggered grid for the wave equation matches Yee's method for Maxwell's equations.\nYee's idea transformed the whole subject of computational electromagnetics (it is\nnow called the FDTD method: finite differences in the time domain). Previously\nthe moment method, which is Galerkin's method, had been dominant--but staggered\ngrids are so natural for E and H. We stay with the wave equation here, copying (26):\nMaxwell in 1D\n@E/@t = c @H/@x\ntE/t=cxH/x\nbecomes\n(31)\n(normalized)\n@H/@t = c @E/@x\ntH/t=cxE/x .\nThose first derivatives of E and H are replaced by first differences. I will put E\non the standard grid and H on the staggered (half-integer) grid. Notice how all the\ndifferences are centered in Figure 5.11a. This gives second-order accuracy.\nThe identities Etx = Ext and Htx = Hxt lead to wave equations for E and H:\nEt = cHx\nEtt\n= cHxt = cHtx = c2Exx\nbecomes\nHt = cEx\nHtt = cExt\n= cEtx\n= c2Hxx\n(32)\nIn the discrete case, the identity is x(t) = t(x). Differences copy derivatives.\nWhen we eliminate H, we get the two-step leapfrog equation for E.\nAnd eliminating E gives the leapfrog equation for H. This all comes from the finite\ndifference analogue of the cross-derivative identity utx = uxt:\n@\n@u\n@\n@u\nx(tU)\nt(xU)\n=\ncorresponds to\n=\n.\n(33)\n@x\n@t\n@t\n@x\n(x)(t)\n(t)(x)\nWith equal denominators, we only need to check the numerators. On any grid, the\nsame 1's and -1's appear both ways in xt and tx !\n-1\nx(tU )\n=\n(Un+1,j+1 - Un,j+1)\n(Un+1,j - Un,j )\n-\n-1\nt(xU )\n=\n(Un+1,j+1 - Un+1,j )\n(Un,j+1 - Un,j )\n-\nYou could compare ( ) with the Cauchy-Riemann equations ux = sy and uy = -sx\nfor the potential u(x, y) and stream function s(x, y). (Those solve Laplace's equation\nand not the wave equation.) It would be natural to discretize the Cauchy-Riemann\nequations on a staggered grid.\nMay I emphasize that these grids are useful for many other equations too. We will\nsee the \"half-point\" grid values in Section 5.\nfor the flux F in the conservation\nlaw ut + F (u)x = 0, which is a nonlinear extension of the one-way wave equation.\nHalf-point values are centrally important throughout the finite volume method.\nMaxwell's equations in integral form lead to the finite integration technique [ ].\n\n5.3. THE WAVE EQUATION AND STAGGERED LEAPFROG c\n2006 Gilbert Strang\nMaxwell's Equations\nFor electrodynamics, the number c in the wave equation is the speed of light. It is the\nsame large number that appears in Einstein's e = mc2 . The CFL stability condition\nc2(t)2 (x)2 + (y)2 + (z)2 for the leapfrog method might require very small\ntime steps (on the scale of ordinary life). But we all know that the wavelength for light\nis nothing like a meter or a centimeter. The leapfrog method is entirely appropriate,\nand we write Maxwell's equations without source terms:\n@E\n@H\nMaxwell's equations in free space\n=\ncurl H and\n= - μ curl E .\n(34)\n@t\nδ\n@t\nAn important application is the reflection of a radar signal by an airplane. The region\nof interest is exterior to the plane. In principle that region extends infinitely far\nin all directions. In practice we compute inside a large box, and choose boundary\nconditions that don't reflect waves back into the box from its artificial boundary\n(which is a computational region and not physical).\nThose absorbing boundary conditions [ ] are crucial to a good discretization.\nChapter 7 of [ ] describes how a \"perfectly matched layer \" can select coefficients\nso that waves go through the boundary with very little reflection. Applications of\nMaxwell's equations range all the way from the Earth's electromagnetic environment\nto cell phones (safety of the user) to micron-scale lasers and photonics.\nThe first of Maxwell's six equations in ( ) involves the electric field component\nEx:\n@\n1 @\n@\nEx =\nHy .\n(35)\n@t\nδ @y Hz - @z\nYee's difference equation computes Ex at the new time (n + 1)t from Ex at time\nnt and the space differences of Hz and Hy at time (n + 2 )t. Figure 5.11\nshows how those components of the magnetic field H are on a grid that is staggered\nwith respect to the grid for E. We have six differential equations like (35) and six\ndifference equations, to produce Ex, Ey , Ez at time (n + 1)t and then Hx, Hy , Hz at\ntime (n + 1.5)t.\nThe stability condition c2(t)2 (x)2 +(y)2 +(z)2 is acceptable. Perhaps the\ngreatest drawback is the rectangular grid (finite elements are always more adaptable).\nBut the FDTD method has been used with 109 meshpoints, which we cannot afford\non an unstructured mesh. Finite differences also have numerical dispersion--the\ndiscrete wave speeds depend on the wave number k = (kx, ky , kz ). Those speeds\ndon't exactly match c. We will have phase factors like F in equation ( ), extended\nto include y and z. When the dispersion creates significant errors, we can upgrade\nthe spatial differences to fourth-order accuracy (using more mesh values). But those\nwider difference methods can go across material interfaces and external boundaries.\nThis produces the ever-present give and take of numerical analysis: higher accuracy\nbrings greater complexity. We can take larger steps t but every step is slower (and\nharder to code).\n\nc\n2006 Gilbert Strang\nFIGURE TO COME...\nE\nH\nH\nE\nFigure 5.11:\n= c\nand\n= c\nare staggered but centered.\nt\nx\nt\nx\nProblem Set 5.3\nWrite the equation utt = uxx + uyy as a first-order system vt = Avx + Bvy\n(u\nwith the vector unknown v = (ut, ux, uy ). The matrices A and B should be\nsymmetric. Then the energy E(t) = 1 R\n+ u2 + uy ) dx is constant.\nt\nx\nHow was the symmetry of A used in the final step vTAvx = ( 1 vTAv)x in equa-\nP P\ntion (27) ? You could write out vTAv =\naij vi(x)vj (x) and take the deriva\ntive of each term by the product rule.\nAdd Gauss law div D = 0 and div B = 0 with D = δE and B = μH."
    },
    {
      "category": "Resource",
      "title": "am54.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/5db29e69494eb09a26f7224d43adc6f6_am54.pdf",
      "content": "5.4. THE HEAT EQUATION AND CONVECTION-DIFFUSION c2006 Gilbert Strang\n5.4\nThe Heat Equation and Convection-Diffusion\nThe wave equation conserves energy. The heat equation ut = uxx dissipates energy.\nThe starting conditions for the wave equation can be recovered by going backward in\ntime. The starting conditions for the heat equation can never be recovered. Compare\nikx:\nut = cux with ut = uxx, and look for pure exponential solutions u(x, t) = G(t) e\nWave equation: G 0 = ickG\nG(t) = eickt has |G = 1 (conserving energy)\n|\nt\nHeat equation: G 0 = -k2G\nG(t) = e-k2\nhas G < 1 (dissipating energy)\nDiscontinuities are immediately smoothed out by the heat equation, since G is ex\nponentially small when k is large. This section solves ut = uxx first analytically and\nthen by finite differences. The key to the analysis is the beautiful fundamental\nsolution starting from a point source (delta function). We will show in equation (7)\nthat this special solution is a bell-shaped curve:\nu(\np\ne-x2/4t\nu(x,\n∂(x) .\n(1)\nx, t) =\nt\ncomes from the initial condition\n0) =\nNotice that ut = cux + duxx has convection and diffusion at the same time. The\nwave is smoothed out as it travels. This is a much simplified linear model of the\nnonlinear Navier-Stokes equations for fluid flow. The relative strength of convection\nby cux and diffusion by duxx will be given below by the Peclet number.\nThe Black-Scholes equation for option pricing in mathematical finance also has\nthis form. So do the key equations of environmental and chemical engineering.\nFor difference equations, explicit methods have stability conditions like t\n2 (x)2\n≈\n. This very short time step is more expensive than ct ≈ x. Implicit\nmethods can avoid that stability condition by computing the space difference 2U\nat the new time level n + 1. This requires solving a linear system at each time step.\nWe can already see two major differences between the heat equation and the wave\nequation (and also one conservation law that applies to both):\n1. Infinite signal speed. The initial condition at a single point immediately\naffects the solution at all points. The effect far away is not large, because of the\nvery small exponential e-x2/4t in the fundamental solution. But it is not zero.\n(A wave produces no effect at all until the signal arrives, with speed c.)\n2. Dissipation of energy. The energy 2\n1 R\n(u(x, t))2 dx is a decreasing function\nof t. For proof, multiply the heat equation ut = uxx by u. Integrate uuxx by\nparts with u(√) = u(-√) = 0 to produce the integral of -(ux)2:\nd Z 1 1\nZ 1\nZ 1\nEnergy decay\nu 2 dx =\nuuxx dx =\n(ux)2 dx ≈ 0 .\n(2)\ndt\n-\n-1\n-1\n-1\n\nZ 1\nZ 1\nh\ni 1\nZ\nZZ 1\nc2006 Gilbert Strang\n3. Conservation of heat (analogous to conservation of mass):\nd\nHeat is conserved\nu(x, t) dx =\nuxx dx = ux(x, t)\n= 0 .\n(3)\ndt\nx=\n-1\n-1\n-1\nAnalytic Solution of the Heat Equation\nStart with separation of variables to find solutions to the heat equation:\nE 00\nAssume u(x, t) = G(t)E(x).\nThen ut = uxx gives G 0E = GE 00 and G 0\n=\n.\nG\nE\n(4)\nThe ratio G0/G depends only on t. The ratio E00/E depends only on x. Since\nequation (4) says they are equal, they must be constant. This produces a useful\nfamily of solutions to ut = uxx:\nt\nE 00\n= G 0\nis solved by E(x) = eikx and G(t) = e-k2 .\nE\nG\nTwo x-derivatives produce the same -k2 as one t-derivative. We are led to exponential\nsolutions of eikxe-k2t and to their linear combinations (integrals over different k):\nikx e-k2t dx.\n(5)\nGeneral solution\nu(x, t) =\n(k)\nu\ne\n\n-1\nAt t = 0, formula (5) recovers the initial condition u(x, 0) because it inverts the\nFourier transform\n(Section 4.4.) So we have the analytical solution to the heat\nu0\n\nequation--not necessarily in an easily computable form ! This form usually requires\ntwo integrals, one to find the transform\n(k) of\n(\n0), and the other to find the\nu\nu x,\n\ninverse transform of\nk\n(k) -\nu\ne\n\nt in (5).\nExample 1 Suppose the initial function is a bell-shaped Gaussian u(x, 0) = e-x2/2 .\nThen the solution remains a Gaussian. The number λ that measures the width of the\nbell increases to λ + 2t at time t, as heat spreads out. This is one of the few integrals\ninvolving e-x that we can do exactly. Actually, we don't have to do the integral.\nThat function e-x2 /2 is the impulse response (fundamental solution) at time t = 0\nto a delta function ∂(x) that occurred earlier at t =\n2 λ. So the answer we want (at\n-\ntime t) is the result of starting from that ∂(x) and going forward a total time 2 λ + t:\np\n(2λ)\np\n(2λ + 4t)\ne-x2/(2 + 4t)\nWidening Gaussian\nu(x, t) =\n.\n(6)\nThis has the right start at t = 0 and it satisfies the heat equation.\n\n5.4. THE HEAT EQUATION AND CONVECTION-DIFFUSION c2006 Gilbert Strang\nThe Fundamental Solution\nFor a delta function u(x, 0) = ∂(x) at t = 0, the Fourier transform is u0(k) = 1. Then\n\nthe inverse transform in (5) produces u(x, t) = 2\n\nR\neikxe-k2t dk One computation of\nthis u uses a neat integration by parts for @u/@x. It has three -1's, from the integral\nt\nof ke-k2 and the derivative of ieikx and integration by parts itself:\nxu\n@u = 1 Z 1\n(e-k2tk)(ieikx) dk =\n1 Z 1\n(e-k2t)(xe ikx) dk =\n.\n(7)\n@x\n- 4t\n- 2t\n-1\n-1\nThis linear equation @u/@x = -xu/2t is solved by u = ce-x2/4t . The constant\nc = 1/\np\n4t is determined by the requirement\nR\nu(x, t) dx = 1. (This conserves\nthe heat\nR\nu(x, 0) dx =\nR\n∂(x) dx = 1 that we started with. It is the area under a\nbell-shaped curve.) The solution (1) for diffusion from a point source is confirmed:\nFundamental solution from\nu(\np\ne-x2/4t .\nx, t) =\nt\n(8)\nu(x, 0) = (x)\nIn two dimensions, we can separate x from y and solve ut = uxx + uyy :\nFundamental solution from\ne-x2/4t e-y2/4t\nu(x, y, t) =\n.\n(9)\nu(x, y, 0) = (x)(y)\np\n4t\nequations (Problem\nu(\nu(\nt !\ne-c/t\n/\np\nt\nR\nWith patience you can verify that\nx, t) and\nx, y, t) do solve the 1D and 2D heat\ninitial conditions away from the origin\ncorrect as\n0, because\ngoes to zero much faster than 1\nblows up.\nsince the total heat remains at\nu dx = 1 or\nRR\nu dx dy = 1, we have a valid solution.\n). The zero\nare\nAnd\nu\nIf the source is at another point x = s, then the response just shifts by s. The\nexponent becomes -(x-s)2 /4t instead of -x2/4t. If the initial u(x, 0) is a combination\nof delta functions, then by linearity the solution is the same combination of responses.\nBut every u(x, 0) is an integral\nR\n∂(x-s) u(s, 0) ds of point sources ! So the solution to\nt = uxx is an integral of the responses to ∂(x - s). Those responses are fundamental\nsolutions starting from all points x = s:\nu(x, 0)\nu(\np\nZ 1\n-1\nu(s, 0) e-(x - s)2/4t\n(10)\nSolution from any\nx, t) =\nt\nds .\nu\nNow the formula is reduced to one infinite integral--but still not simple. And for a\nproblem with boundary conditions at x = 0 and x = 1 (the temperature on a finite\ninterval, much more realistic), we have to think again. Similarly for an equation\nt = (c(x)ux)x with variable conductivity or diffusivity. That thinking probably\nleads us to finite differences.\nI see the solution u(x, t) in (10) as the convolution of the initial function u(x, 0)\nwith the fundamental solution. Three important properties are immediate:\n\nc2006 Gilbert Strang\n1.\nIf u(x, 0) 0 for all x then u(x, t) 0 for all x and t. Nothing in\nformula (10) will be negative.\n2.\nThe solution is infinitely smooth. The Fourier transform u0(k) in (5) is\n\nmultiplied by e-k2t . In (10), we can take all the x and t derivatives we want.\nu\n3.\nThe scaling matches x2 with t. A diffusion constant d in the equation\nt = duxx will lead to the same solution with t replaced by dt, when we write\nthe equation as @u/@(dt) = @2 u/@x2 . The fundamental solution has e-x2/4dt\nand its Fourier transform has e-dk2t .\nExample 2 Suppose the initial temperature is a step function u(x, 0) = 0. Then for\nnegative x and u(x, 0) = 1 for positive x. The discontinuity is smoothed out immediately,\nas heat flows to the left. The integral in formula (10) is zero up to the jump:\nu(x, t) = p\n4t\nZ\ne-(x - s)2/4t ds .\n(11)\nNo luck with this integral ! We can find the area under a complete bell-shaped curve\n(or half the curve) but there is no elementary formula for the area under a piece of the\ncurve. No elementary function has the derivative e-x . That is unfortunate, since those\nintegrals give cumulative probabilities and statisticians need them all the time. So they\nhave been normalized into the error function and tabulated to high accuracy:\nx\n2 Z\nError function\nerf(x) = p\ne-s ds .\n(12)\nThe integral from -x to 0 is also erf(x). The normalization by 2/p gives erf(√) = 1.\nWe can produce this error function from the heat equation integral (11) by setting\nS = (s - x)/\np\n4t. Then s = 0 changes to S = -x/\np\n4t as the lower limit on the integral,\nand dS = ds/\np\n4t. Split into an integral from 0 to √, and from -x/\np\n4t to 0:\np\n4t Z 1\ne-S2\nx\nu(x, t) =\ndS =\np\n4t\n1 + erf\n.\n(13)\n-x/\np\n4t\np\n4t\nGood idea to check that this gives u = 2 at x = 0 (where the error function is zero).\nThis is the only temperature we know exactly, by symmetry between left and right.\nExplicit Finite Differences\nThe simplest finite differences are forward for @u/@t and centered for @2u/@x2:\ntU\nt = 2\nx U\n(x)2\nUj,n+1 - Uj,n\nt\n= Uj+1,n - 2Uj,n + Uj-1,n\n(x)2\n. (14)\nExplicit method\n\n5.4. THE HEAT EQUATION AND CONVECTION-DIFFUSION c2006 Gilbert Strang\nEach new value Uj,n+1 is given explicitly by Uj,n + R(Uj+1,n - 2Uj,n + Uj,n-1). The\ncrucial ratio for the heat equation ut = uxx is now R = t/(x)2 .\nWe substitute Uj,n = Gn eikjx to find the growth factor G = G(k, t, x):\nOne-step\nG = 1 + R(e ikx - 2 + e-ikx) = 1 + 2R(cos kx - 1) . (15)\ngrowth factor\nG is real, just as the exact one-step factor e-k2t is real. Stability requires G ≈ 1.\n| |\nAgain the most dangerous case is when the cosine equals -1 at kx = :\nt\nStability condition\nG = 1 - 4R ≈ 1 which requires R =\n. (16)\n| |\n|\n|\n(x)2 2\nIn many cases we accept that small time step t and use this simple method. The\naccuracy from forward t and centered 2 is U - u = O(t + (x)2). Those two\nx\n|\n|\nerror terms are comparable when R is fixed.\nWe could improve this one-step method to a multistep method. The \"method\nof lines\" calls an ODE solver for the system of differential equations (continuous in\ntime, discrete in space). There is one equation for every meshpoint x = jh:\ndU\n2 U\ndUj = Uj+1 - 2Uj + Uj-1\nMethod of Lines\n=\nx\n.\n(17)\ndt\n(x)2\ndt\n(x)2\nThis is a stiff system, because its matrix -K (second difference matrix) has a large\ncondition number: max(K)/min(K) N 2 . We could choose a stiff solver like ode15s\nin MATLAB.\nImplicit Finite Differences\nA fully implicit method for ut = uxx computes 2 U at the new time (n + 1)t:\nx\ntUn\nUj+1,n+1 - 2Uj,n+1 + Uj-1,n+1\nImplicit\n=\nx Un+1\nUj,n+1 - Uj,n =\n. (18)\nt\n(x)2\nt\n(x)2\nThe accuracy is still first-order in time and second-order in space. But stability no\nlonger depends on the ratio R = t/(x)2 . We have unconditional stability, with a\ngrowth factor 0 < G ≈ 1 for all k. Substituting Uj,n = Gneijkx into (18) and then\ncanceling those terms from both sides leaves an extra G on the right side:\nG = 1 + RG(e ikx - 2 + e-ikx) leads to G = 1 + 2R(1 - cos kx) .\n(19)\nThe denominator is at least 1, which ensures that 0 < G ≈ 1. The time step is\ncontrolled by accuracy, because stability is no longer a problem.\n\nc2006 Gilbert Strang\nThere is a simple way to improve to second-order accuracy. Center everything at\nstep\nfamous Crank-Nicolson method (like the trapezoidal rule):\nn + 1 . Average an explicit 2 Un with an implicit 2 Un+1. This produces the\nx\nx\nCrank-Nicolson\nUj,n+1 - Uj,n\nxUj,n+1) .\n(20)\nxUj,n + 2\nt\n= 2(x)2 (2\nNow the growth factor G, by substituting Uj,n = Gneijkx into (20), solves\nG + 1\nG - 1 = 2(x)2 (2 cos kx - 2) .\n(21)\nt\nSeparate out the part involving G, write R for t/(x)2, and cancel the 2's:\nUnconditional stability\nG = 1 + R(cos kx - 1) has |G| 1 .\n(22)\n1 - R(cos kx - 1)\nThe numerator is smaller than the denominator, since cos kx ≈ 1. We do notice\nthat cos kx = 1 whenever kx is a multiple of 2. Then G = 1 at those frequencies,\nso Crank-Nicolson does not give the strict decay of the fully implicit method. We\ncould weight the implicit 2 Un+1\nby a > 2 and the explicit 2 Un\nx\nx\ngive a whole range of unconditionally stable methods (Problem\n).\n-\nby 1\na < , to\nNumerical example\nFinite Intervals with Boundary Conditions\nWe introduced the heat equation on the whole line -√ < x < √. But a physical\nproblem will be on a finite interval like 0 ≈ x ≈ 1. We are back to Fourier series\n(not Fourier integrals) for the solution u(x, t). And second differences bring back the\ngreat matrices K, T, B, C that depend on the boundary conditions:\nAbsorbing boundary at x = 0: The temperature is held at u(0, t) = 0.\nInsulated boundary: No heat flows through the left boundary if ux(0, t) = 0.\nIf both boundaries are held at zero temperature, the solution will approach u(x, t) = 0\neverywhere as t increases. If both boundaries are insulated as in a freezer, the solution\nwill approach u(x, t) = constant. No heat can escape, and it is evenly distributed as\nt ! √. This case still has the conservation law\nR 1 u(x, t) dx = constant.\nExample 3 (Fourier series solution) We know that eikx is multiplied by e-k2 t to give\na solution of the heat equation. Then u = e-k2t sin kx is another solution (combining\n+k with -k). With zero boundary conditions u(0, t) = u(1, t) = 0, the only allowed\n\n5.4. THE HEAT EQUATION AND CONVECTION-DIFFUSION c2006 Gilbert Strang\nfrequencies are k = n (then sin nx = 0 at both ends x = 0 and x = 1). The complete\nsolution is a combination of these exponential solutions with k = n:\nComplete solution\nu(x, t) =\nX\nbn e-n\nt sin nx .\n(23)\nn=1\nThe Fourier sine coefficients bn are chosen to match u(x, 0) = P bn sin nx at t = 0.\nYou can expect cosines to appear for insulated boundaries, where the slope (not\nthe temperature) is zero. This gives exact solutions to compare with finite difference\nsolutions. For finite differences, absorbing boundary conditions produce the matrix\nK (not B or C). The choice between explicit and implicit decides whether we have\nsecond differences -KU at time level n or level n + 1:\nExplicit method\nUn+1 - Un = -RKUn\n(24)\nFully implicit\nUn+1 - Un = -RKUn+1\n(25)\nCrank-Nicolson\nUn+1 - Un =\n1 RK(Un + Un+1) .\n(26)\n-\nThe explicit stability condition is again R ≈ 1 (Problem\n). Both implicit meth\nods are unconditionally stable (in theory). The reality test is to try them in practice.\nAn insulated boundary at x = 0 changes K to T . Two insulated boundaries\nproduce B. Periodic conditions will produce C. The fact that B and C are singular\nno longer stops the computations. In the fully implicit method (I + RB)Un+1 = Un,\nthe extra identity matrix makes I + RB invertible.\nThe two-dimensional heat equation describes the temperature distribution in\na plate. For a square plate with absorbing boundary conditions, the difference matrix\nK changes to K2D. The bandwidth jumps from 1 (triangular matrix) to N (when\nmeshpoints are ordered a row at a time). Each time step of the implicit method\nnow requires a serious computation. So implicit methods pay an increased price for\nstability, to avoid the explicit restriction t ≈ 1\n4 (x)2 + 1\n4 (y)2 .\nConvection-Diffusion\nPut a chemical into flowing water. It diffuses while it is carried along by the flow. A\ndiffusion term d uxx appears together with a convection term c ux. This is the simplest\nmodel for one of the most important differential equations in engineering:\n= c\n+ d @2u\n2 .\n(27)\nConvection-diffusion equation\n@u\n@t\n@u\n@x\n@x\nOn the whole line -√ < x < √, the flow and the diffusion don't interact. If the\nvelocity is c, convection just carries along the diffusing solution to ht = d hxx:\nDiffusing traveling wave\nu(x, t) = h(x + ct, t) .\n(28)\n\nc2006 Gilbert Strang\nSubstituting into equation (27) confirms that this is the solution (correct at t = 0):\n@u\n@h\n@h\n@h\n@2h\n@u\n@2u\nChain rule\n= c\n+\n= c\n+ d\n= c\n+ d\n.\n(29)\n@t\n@x\n@t\n@x\n@x2\n@x\n@x2\nExponentials also show this separation of convection eikct from diffusion e-dk2 t:\nt ik(x + ct)\nStarting from eikx\nu(x, t) = e-dk2 e\n.\n(30)\nConvection-diffusion is a terrific model problem, and the constants c and d clearly\nhave different units. We take this small step into dimensional analysis:\ndistance\n(distance)2\nConvection coefficient c:\nDiffusion coefficient d:\n(31)\ntime\ntime\nSuppose L is a typical length scale in the problem. The Peclet number P e = cL/d\nis dimensionless. It measures the relative importance of convection and diffusion. This\nPeclet number for the linear equation (27) corresponds to the Reynolds number for\nthe nonlinear Navier-Stokes equations (Section\n).\nIn the difference equation, the ratios r = ct/x and 2R = 2dt/(x)2 are also\ndimensionless. That is why the stability conditions r ≈ 1 and 2R ≈ 1 were natural for\nthe wave and heat equations. The new problem combines convection and diffusion,\nand the cell Peclet number P uses x/2 as the length scale in place of L:\nr\nc x\nCell Peclet Number\nP =\n=\n.\n(32)\n2R\n2d\nWe still don't have agreement on the best finite difference approximation! Here\nare three natural candidates (you may have an opinion after you try them):\n1.\nForward in time, centered convection, centered diffusion\n2.\nForward in time, upwind convection, centered diffusion\n3.\nExplicit convection (centered or upwind ), with implicit diffusion.\nEach method will show the effects of r and R and P (we can replace r/2 by RP ):\n1. Centered explicit\nUj,n+1 - Uj,n\nUj+1,n - Uj-1,n\nxUj,n\n= c\n+ d\n.\n(33)\nt\n2x\n(x)2\nEvery new value Uj,n+1 is a combination of three known values at time n:\nUj,n+1 = (1 - 2R)Uj,n + (R + RP )Uj+1,n + (R - RP )Uj-1,n .\n(34)\nThose three coefficients add to 1, and U = constant certainly solves equation (33). If\nall three coefficients are positive, the method is surely stable. More than\nthat, oscillations cannot appear. Positivity of the middle coefficient requires R ≈ 2 ,\n\nZ\n5.4. THE HEAT EQUATION AND CONVECTION-DIFFUSION c2006 Gilbert Strang\nas usual for diffusion. Positivity of the other coefficients requires P 1. Of course\n|\n|\nP will be small when x is small (so we have convergence as x ! 0). In avoiding\noscillations, the actual cell size x is crucial to the quality of U .\nFigure 5.12 was created by Strikwerda [59] and Persson to show the oscillations for\nP > 1 and the smooth approximations for P < 1. Notice how the initial hat function\nis smoothed and spread and shrunk by diffusion. Problem\nfinds the exact\nsolution, which is moved along by convection. Strictly speaking, even the oscillations\nmight pass the stability test G ≈ 1 (Problem\n). But they are unacceptable.\n| |\nFigure 5.12: Convection-diffusion with and without numerical oscillations: R =\n, r =\nand\n.\nUj,n+1 - Uj,n\nUj+1,n - Uj,n\n2. Upwind convection\n= c\n+ d\nxUj,n .\n(35)\nt\nx\n(x)2\nThe accuracy in space has dropped to first order. But the oscillations are eliminated\nwhenever r + 2R ≈ 1. That condition ensures three positive coefficients when (35) is\nsolved for the new value Uj,n+1:\nUj,n+1 = (RP + R)Uj+1,n + (1 - RP - 2R)Uj,n + RUj-1,n .\n(36)\nArguments are still going, comparing the centered method 1 and the upwind method 2.\nThe difference between the two convection terms, upwind minus centered, is ac\ntually a diffusion term hidden in (35) !\nUj+1 - Uj\nUj+1 - Uj-1\nx Uj+1 - 2Uj + Uj-1\nExtra diffusion\n=\n.\n(37)\nx\n-\n2x\n(x)2\nSo the upwind method has this extra numerical diffusion or \"artificial viscosity \"\nto kill oscillations. It is a non-physical damping. If the upwind approximation were\nincluded in Figure 5.12, it would be distinctly below the exact solution. Nobody is\nperfect.\n3. Implicit diffusion\nUj,n+1 - Uj,n = c Uj+1,n - Uj,n + d\nxUj,n+1 .\n(38)\nt\nx\n(x)2\nMORE TO DO\nProblem Set 5.4\nSolve the heat equation starting from a combination u(x, 0) = ∂(x + 1) - 2∂(x) +\n∂(x - 1) of three delta functions. What is the total heat\nR\nu(x, t) dx at time t ?\nDraw a graph of u(x, 1) by hand or by MATLAB.\nIntegrating the answer to Problem 1 gives another solution to the heat equation:\nx\nShow that w(x, t) =\nu(X, t) dX solves wt = wxx .\nGraph the initial function w(x, 0) and sketch the solution w(x, 1).\n\nc2006 Gilbert Strang\nIntegrating once more solves the heat equation ht = hxx starting from h(x, 0) =\nR\nw(X, 0) dX = hat function. Draw the graph of h(x, 0). Figure 5.12 shows the\ngraph of h(x, t), shifted along by convection to h(x + ct, t).\nIn convection-diffusion, compare the condition R ≈ 1 , P ≈ 1 (for positive coef\nficients in the centered method) with r + 2R ≈ 1 (for the upwind method). For\nwhich c and d is the upwind condition less restrictive, in avoiding oscillations ?\nThe eigenvalues of the n by n second difference matrix K are k = 2 - 2 cos k .\nn+1\nThe eigenvectors yk in Section 1.5 are discrete samples of sin kx. Write the\ngeneral solutions to the fully explicit and fully implicit equations (14) and (18)\nafter N steps, as combinations of those discrete sines yk times powers of k .\nAnother exact integral involving e-x2/4t is\nZ 1\nx e-x2/4t dx =\nh\n-2t e-x2/4ti1\n= 2t .\nFrom (17), show that the temperature is u =\np\nt\nx\nat the center point\n= 0\nstarting from a ramp u(x, 0) = max(0, x).\nA ramp is the integral of a step function. So the solution of ut = uxx starting\nfrom a ramp (Problem 6) is the integral of the solution starting from a step\nThen\np\nt must be the total amount of heat\nfunction (Example 2 in the text).\nthat has crossed from x > 0 to x < 0 in Example 2 by time t. Explain each of\nthose three sentences."
    },
    {
      "category": "Resource",
      "title": "am55.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/1d170d72359b7d27052c960197b425e5_am55.pdf",
      "content": "c\n5.5. DIFFERENCE MATRICES AND EIGENVALUES\n2006 Gilbert Strang\n5.5\nDifference Matrices and Eigenvalues\nThis brief section collects together useful notes on finite difference matrices, and\nalso finite element matrices. Certainly those special matrices K, T, B, C from the\nstart of the book are the building blocks for approximations to uxx and uyy . Second\nderivatives and fourth derivatives lead to symmetric matrices. First derivatives are\nantisymmetric. They present more difficulties.\nA symmetric matrix has orthogonal eigenvectors. For those special matrices, the\neigenvectors are discrete samples of sines and cosines and eikx . The eigenvalues are\nikx -2+e-ikx\nreal, and they often involve e\n. That is the discrete factor 2 cos kx-2.\nDivided by (x)2, it is close for small k to the factor -k2 from the second derivative\nof e ikx . The von Neumann approach using e ikx matches the eigenvectors of these\nmatrices, and the growth factors G match the eigenvalues.\nFor a one-sided (upwind) difference, the matrix eigenvalues are not always reliable.\nFor a centered difference they do follow von Neumann. Compare\n⎡\n⎡\n-1\n-1\n⎢\n1 6 -1\n0 -1\n⎢\n+ = 6\n⎢\n0 =\n⎢\n-1\n1⎣\n2 4\n-1\n0 -1 ⎣\n-1\n-1\nThe eigenvalues of the triangular upwind matrix + are all -1 (useless). The eigen\nvalues of the antisymmetric 0 are guaranteed to be imaginary like the factor ik from\nthe derivative of eikx . The eigenvalues = -1 for + do not make upwind differences\nuseless. They only mean that the von Neumann test, which produces eikx - 1, is\nbetter than relying on eigenvalues.\nAs it stands, + is exactly in \"Jordan form.\" The matrix has only one line of\neigenvectors, not n. It is an extreme example of a nondiagonalizable (and somehow\ndegenerate) matrix. If the diagonals of -1's and 1's are extended to infinity, then\nFourier and von Neumann produce vectors with components eikjx and with eigen\nvalues e ikx - 1. In summary: For normal matrices, eigenvalues are a reliable guide.\nFor other constant-diagonal matrices, better to rely on von Neumann.\nBriefly, the discrete growth factors G are exactly the eigenvalues when the matrices\nare called \"normal \" and the test is AAT = ATA (for complex matrices take the\nconjugate transpose A). This test is passed by all symmetric and antisymmetric\nand orthogonal matrices.\nOptions for First Differences\nUpwind elements\nStreamline diffusion\nDG\nBoundary conditions\nConvection-diffusion"
    },
    {
      "category": "Resource",
      "title": "am56.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/3762c803fd3f27bcc47cc76341162824_am56.pdf",
      "content": "@\n@\n@\n@\nc2006 Gilbert Strang\n5.6\nNonlinear Flow and Conservation Laws\nNature is nonlinear. The coefficients in the equation depend on the solution u. In\nplace of ut = c ux we will study ut + uux = 0 and more generally ut + f(u)x = 0.\nThese are \"conservation laws\" and the conserved quantity is the integral of u.\nThe first part of this book emphasized the balance equation: forces balance and\ncurrents balance. For steady flow this was Kirchhoff's Current Law: flow in equals\nflow out. The net flow was zero. Now the flow is unsteady --the \"mass inside\" is\nchanging. So a new @/@t term will enter the conservation law.\nThere is \"flux\" through the boundaries. In words, the rate of change of mass\ninside a region equals that incoming flux. For an interval [a, b], the incoming\nflux is the difference in fluxes at the endpoints a and b:\nd\ndt\nZ b\na\nu(\n) dx = f (u(\n)) - f (u(\n)) .\n(1)\nIntegral form\nx, t\na, t\nb, t\nIn applications, u can be a density (of cars along a highway). The integral of u gives\nthe mass (number of cars) between a and b. This number changes with time, as cars\nflow in at point a and out at point b. The flux is density u times velocity v.\nThe integral form is fundamental. We can get a differential form by allowing b to\napproach a. Suppose b - a = x. If u(x, t) is a smooth function, its integral over a\ndistance x will have leading term x u(a, t). So if we divide equation (1) by x,\nthe limit as x approaches zero is @u/@t = -@f(u)/@x:\n+ @ f(u\n+ f (u)\n(2)\nDifferential form\n@u\n@t\n@x\n) = @u\n@t\n@u\n@x = 0\nWhen f(u) = density u times velocity v(u), we can solve this single conservation\nlaw. For traffic flow, the velocity v(u) can be measured (it will decrease as density\nincreases). In gas dynamics there are also conservation laws for momentum and\nenergy. The velocity v becomes another unknown, along with the pressure p. The\nEuler equations for gas dynamics in one space dimension include two additional\nequations:\nConservation of momentum\n(uv) +\n(uv 2 + p) = 0\n(3)\n@t\n@x\nConservation of energy\n(E) +\n(Ev + Ep) = 0 .\n(4)\n@t\n@x\nSystems of conservation laws are more complicated, but our scalar equation (2) al\nready has the possibility of shocks. A shock is a discontinuity in the solution u(x, t),\nwhere the differential form breaks down and we need the integral form (1).\n\nc\n5.6. NONLINEAR FLOW AND CONSERVATION LAWS\n2006 Gilbert Strang\nThe other outstanding example, together with traffic flow, is Burger's equation,\nfor u = velocity. The flux f (u) is 2 u2 . The \"inviscid\" form has no uxx:\n@u\n@\nu2\n@u\n@u\nBurger's equation\n+\n=\n+ u\n= 0 .\n@t\n@x\n@t\n@x\nWhen both the density and velocity are unknowns, these examples combine into\nconservation of mass and conservation of momentum. Typically we change density to\nν. For small disturbances of a uniform density ν0, we could linearize the conservation\nlaws and reach the wave equation (Problem\n). But the Euler and Navier-Stokes\nequations are truly nonlinear, and we begin the task of solving them.\nWe will approach conservation laws (and these examples) in three ways:\n1.\nBy following characteristics until trouble arrives: they separate or collide\n2.\nBy a special formula ( )\n3.\nBy finite difference and finite volume methods, which are the practical choice.\nCharacteristics\nThe one-way wave equation ut = c ux is solved by u(x, t) = u(x + ct, 0). Every initial\nvalue u0 is carried along a characteristic line x + ct = x0. Those lines are parallel\nwhen the velocity c is a constant.\nThe conservation law ut = +u ux = 0 will be solved by u(x, t) = u(x - ut, 0).\nEvery initial value u0 = u(x0, 0) is carried along a characteristic line x - u0t = x0.\nThose lines are not parallel because their slopes depend on the initial value u0.\nNotice that the formula u(x, t) = u(x - ut, 0) involves u on both sides. It gives\nthe solution \"implicitly.\" If the initial function is u(x, 0) = 1 - x, for example, the\nformula must be solved for u:\nu = 1 - (x - ut) gives (1 - t)u = 1 - x and u = 1 - x .\n(5)\n1 - t\nThis does solve Burger's equation, since the time derivative ut = (1 - x)/(1 - t)2 is\nequal to -uux. The characteristic lines (with different slopes) can meet. This is an\nextreme example, where all characteristics meet at the same point:\nx - u0t = x0\nor x - (1 - x0)t = x0\nwhich goes through x = 1, t = 1\n(6)\nYou see how the solution u = (1 - x)/(1 - t) becomes 0/0 at that point x = 1, t = 1.\nBeyond their meeting point, the characteristics cannot completely decide u(x, t).\nA more fundamental example is the Riemann problem, which starts from two\nconstant values u = A and u = B. Everything depends on whether A > B or A < B.\nOn the left side of Figure 5.13, with A > B, the characteristics meet. On the right\n\nZ\nc2006 Gilbert Strang\nside, with A < B, the characteristics separate. Both cases present a new (nonlinear)\nproblem, when we don't have a single characteristic that is safely carrying the correct\ninitial value to the point. This Riemann problem has two characteristics through the\npoint, or none:\nShock Characteristics collide\n(light goes red: speed drops from 60 to 0)\nFan\nCharacteristics separate (light goes green: speed up from 0 to 60)\nThe problem is how to connect u = 60 to u = 0, when the characteristics don't give\nthe answer. A shock will be sharp breaking (drivers only see the car ahead in this\nmodel). A fan will be gradual acceleration.\nTO DO...\nFigure 5.13: A shock when characteristics collide, a fan when they separate.\nFor the conservation law ut + f(u)x = 0, the characteristics are x - f (u0)t = x0.\nThat line has the right slope to carry the constant value u = u0:\nd\n@u\n@u\nu(x0 + St, t) = S\n+\n= 0 when S = f (u) .\n(7)\ndt\n@x\n@t\nThe solution until trouble arrives is u(x, t) = u(x - f (u)t, 0).\nShocks\nAfter trouble arrives, it will be the integral form that guides the choice of the correct\nsolution u. If there is a jump in u (a shock ), that integral from tells where the jump\nmust occur. Suppose u has different values uL and uR at points xL and xR on the\nleft and right sides of the shock:\nd\nxR\nIntegral form\nu dx + f(uR) - f(uL) = 0 .\n(8)\ndt\nxL\nIf the position of the shock is x = X(t), we take xL and xR very close to X. The\nvalues of u(x, t) inside the integral are close to the constants uL and uR:\nd\n\n(x - xL) uL + (xR - X) uR + f(uR) - f(uL) 0 .\ndt\nThis gives the speed s = dX/dt of the shock curve:\nL -\nR + f(uR) - f(uL\n= f (uR) - f (uL)\nuR - uL\n= [ f ]\n[ u ] .\n(9)\nJump condition\ns u\ns u\n) = 0\nshock speed\n\nc\n5.6. NONLINEAR FLOW AND CONSERVATION LAWS\n2006 Gilbert Strang\nFor the Riemann problem, the left and right values uL and uR will be constants A\nand B. The shock speed s is the ratio between the jump [ f ] = f(B) - f(A) and\nthe jump [ u ] = B - A. Since this ration gives a constant slope, the shock line is\nstraight. For other problems, the characteristics are carrying different values of u into\nthe shock. So the shock speed s is not constant and the shock lines is curved.\n= 1\n2 u\nThe shock gives the solution when characteristics collide. With f(u)\n2 in\nBurger's equation, the shock speed is halfway between uL and uR:\n1 u\nL\n2 - u\nR\nBurger's equation\nShock speed s =\n=\n(uR + uL) .\n(10)\n2 uR - uL\nThe Riemann problem has uL = A and uR = B, and s is their average. Figure 5.14\nshows how the integral form of Burger's equation is solved by the right placement of\nthe shock.\nFans\nYou might expect a similar picture (just flipped) when A < B. Wrong. The integral\nform is still satisfied, but it is also satisfied by a fan. The choice between shock and\nfan is made by the \"entropy condition\" that as t increases, characteristics must go\ninto the shock. The wave speed is faster than the shock speed on the left, and slower\non the right:\nf (u)\n(uR)\n(11)\nEntropy condition\n> s > f\nu\nSince Burger's equation has f (u) = u, it only has shocks when uL is larger than uR.\nIn the Riemann problem that means A > B. In the opposite case, the smaller value\nL = A has to be connected to uR = B by the fan in Figure 5.14:\nx\nFan (or rarefaction)\nu =\nfor At < x < Bt .\n(12)\nt\nuse fig 6.28 p. 592 of IAM (reverse left and right figs)\nFigure 5.14: Characteristics collide in a shock and separate in a fan.\nNotice especially that in the traffic flow problem, the velocity v(u) decreases as\nthe density u increases. A good model is linear between v = vmax at zero density\nand v = 0 at maximum density. Then the flux f(u) = u v(u) is a downward parabola\n(concave instead of Burger's convex u2/2):\nTraffic speed\nu\nu\nand flux\nv(u) = vmax 1 - umax\nand f(u) = vmax u -\n. (13)\numax\nTypical values for a single lane of traffic show a maximum flux of f = 1600 vehicles\nper hour, when the density is u = 80 vehicles per mile. This maximum flow rate\n\nc2006 Gilbert Strang\nis attained when the velocity f/u is v = 20 miles per hour! Small comfort at that\nspeed, to know that other cars are getting somewhere too.\nProblems\nand\ncompute the solution when a light goes red (shock trav\nels backward) and when a light goes green (fan moves forward). Please look at the\nfigures, to see how the vehicle trajectories are entirely different form the characteris\ntics.\nA driver keeps adjusting the density to stay safely behind the car in front. (Hitting\nthe car would give u < 0.) We all recognize the frustration of braking and accelerating\nfrom a series of shocks and fans. This traffic crawl happens when the green light is\ntoo short for the shock to make it through.\nA Solution Formula for Burger's Equation\nLet me comment on three nonlinear equations. They are useful models, quite special\nbecause each one has an exact solution formula:\nConservation law\nut + u ux = 0\nBurger's with viscosity\nut + u ux = uxx\nKorteweg-de Vries\nut + u ux = -a uxxx\nThe conservation law can develop shocks. This won't happen in the second equation\nbecause the uxx viscosity term prevents it. That term can stay small when the solution\nis smooth, but it dominates when a wave is about to break. The profile is steep but\nit stays smooth.\nAs starting function for the conservation law, I will pick a point source: u(x, 0) =\n∂(x). We can guess a solution with a shock, and check the jump condition and entropy\ncondition. Then we find an exact formula when uxx is included, by a neat change of\nvariables that produces ht = hxx. When we let ! 0, the limiting formula solves\nthe conservation law--and we can check that the following solution is correct.\nSolution with u(x, 0) = (x)\nWhen u(x, 0) jumps upward, we expect a fan.\nWhen it drops we expect a shock. The delta function is an extreme case (very big\njumps up and down, very close together!). So we look for a shock curve x = X(t)\nimmediately in front of a fan!\nu(\nx\nt for 0 ≈ x ≈ X(t); otherwise u\n(14)\nExpected solution\nx, t) =\n= 0.\n⎪\nThe total mass at the start is\n∂(x) dx = 1. This never changes, and already that\nlocates the shock position X(t):\nZ X x\nX2\nMass at time t =\ndt =\n= 1\nso X(t) =\np\n2t .\n(15)\nt\n2t\n\nZ 1\n<\n:\nc\n5.6. NONLINEAR FLOW AND CONSERVATION LAWS\n2006 Gilbert Strang\nDoes the drop in u, from X/t =\np\n2t/t to zero, satisfy the jump condition?\ndX\np\nJump [ u2/2 ]\nX2/2t2\np\n2t\nShock speed s =\n=\nequals\n=\n=\n.\ndt\np\nt\nJump [ u ]\nX/t\n2t\nThe entropy condition uL > s > uR = 0 is also satisfied, and the solution ( ) looks\ngood. It is good, but because of the delta function we check it another way.\nBegin with ut + u ux = uxx, and solve that equation exactly. If u(x) is @U/@x,\nthen integrating our equation gives Ut + 2 U 2 = Uxx. The initial value U0(x) is now\nx\na step function. Then the great change of variables U = -2 log h produces the heat\nequation ht = hxx (Problem\n). The initial value becomes h(x, 0) = e-U0 (x)/2 .\nSection 5.4 found the solution to the heat equation ut = uxx from any starting function\nh(x, 0) and we just change t to t:\n⎨\n⎩\nU (x, t) = -2 log h(x, t) = -2 log p\nt\n-1\ne -U0(y)/2 e -(x-y)2 /4t dy .\n(16)\nform e\nIt doesn't look easy to let ! 0, but it can be done. That exponential has the\n-B(x,y)/2 . This is largest when B is smallest. An asymptotic method called\n\"steepest descent\" shows that as ! 0, the bracketed quantity in (16) approaches\nc e -B-min/2 . Taking its logarithm and multiplying by -2, (16) becomes U = Bmin\nin the limit:\n\nlim U (x, t) = Bmin = min U0(y) +\n(x -y)2 .\n(17)\n!0\ny\n2t\nThis is the solution formula for Ut + 2 U 2 = 0. Its derivative u = Ux solves the\nx\nconservation law ut + u ux = 0. By including the viscosity uxx with ! 0, we are\nfinding the u(x, t) that satisfies the jump condition and the entropy condition.\nExample\nStarting from u(x, 0) = ∂(x), its integral U0 is a step function. The minimum\nof B is either at y = x or at y = 0. Check each case:\n⎨\n⎩\n⎧0\nfor x ≈0\nU (t, x) = Bmin = miny\n(y ≈0) + (x -y)2\n=\nx2/2t for 0 ≈x ≈\np\n2t\n(y > 0)\n2t\n⎧\nfor x √\np\n2t\nThe result u = dU/dx is 0 or x/t or 0. This agrees with our guess in equation ( )--a\nfan rising from 0 and a shock back to 0 at x =\np\n2t."
    },
    {
      "category": "Resource",
      "title": "am57.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/49baea85bf92b8bd0c73c9a313fd3f33_am57.pdf",
      "content": "p\np\nc2006 Gilbert Strang\n5.7\nLevel Sets and the Fast Marching Method\nThe level sets of f(x, y) are the sets on which the function is constant. For example\nf(x, y) = x2 + y2 is constant on circles around the origin. Geometrically, a level plane\nz = constant will cut through the surface z = f(x, y) on a level set. One attractive\nfeature of working with level sets is that their topology can change (pieces of the level\nset can separate or come together) just by changing the constant.\nStarting from one level set, the signed distance function d(x, y) is especially\nimportant. It gives the distance to the level set, and also the sign: typically d > 0\noutside and d < 0 inside. For the unit circle, d = r - 1 =\nx2 + y2 - 1 will be the\nsigned distance function. In the mesh generation algorithm of Section 2.\n, it was\nconvenient to describe the region by its distance function d(x, y).\nA fundamental fact of calculus: The gradient of f(x, y) is perpendicular to its\nlevel sets. Reason: In the tangent direction t to the level set, f(x, y) is not changing\nand (grad f) · t is zero. So grad f is in the normal direction. For the function x2 + y ,\nthe gradient (2x, 2y) points outward from the circular level sets. The gradient of\nd(x, y) =\nx2 + y2 - 1 points the same way, and it has a special property: The\ngradient of a distance function is a unit vector. It is the unit normal n(x, y)\nto the level sets. For the circles,\ngrad( x2 + y2 - 1) = ( x, y )\nand\n| grad |2 =\n+ y\n= 1 .\n(1)\np\nx\nr r\nr2\nr2\nYou could think of the level set d(x, y) = 0 as a wall of fire. This firefront will move\nnormal to itself. If it has constant velocity 1 then at time T the fire will reach all\npoints on the level set d(x, y) = T .\nThat \"wall of fire\" example brings out an important point when the zero level set\nhas a corner (it might be shaped like a V). The points at distance d outside that set\n(the firefront at time d) will lie on lines parallel to the sides of the V, and also on a\ncircular arc of radius d around the corner. For d < 0 the V moves inward. It remains\na V (with no smoothing of the corner).\nThe central problem of the level set method is to propagate a curve like the\nfirefront. A velocity field v = (v1, v2) gives the direction and speed of each point for\nthe movement. At time t = 0, the curve is the level set where d(x, y) = 0. At later\ntimes the curve is the zero level set of a function (x, y, t). The fundamental level\nset equation in its first form is\nd + v · grad = 0, with = d(x, y) at t = 0 .\n(2)\ndt\nIn our wall of fire example, v would be the unit vector in the normal direction to\nthe firefront: v = n = grad /| grad |. In all cases it is only the normal component\nF = v · n that moves the curve! Tangential movement (like rotating a circle around\nits center) gives no change in the curve as a whole. By rewriting v · grad , the level\n\nc\n5.7. LEVEL SETS AND THE FAST MARCHING METHOD\n2006 Gilbert Strang\nset equation takes a second form that is more useful in computation:\ngrad\nd\nv · grad = v ·\n| grad | = F | grad |\nleads to\n+ F | grad | = 0 . (3)\n| grad |\ndt\nWe only need to know the velocity field v (and only its normal component F )\nnear the current location of the level curve-not everywhere else. We are propagating\na curve. The velocity field may be fixed (easiest case) or it may depend on the local\nshape of the curve (nonlinear case). An important example is motion by mean\ncurvature: F = -. The neat property | grad | = 1 of distance functions simplifies\nthe formulas for the normal n and curvature :\nWhen is a\nn = grad\nbecomes n = grad\ndistance\n| grad |\n(4)\nfunction\n= div n\nbecomes = div(grad ) = Laplacian of\nBut here is an unfortunate point for t > 0. Constant speed (F = 1) in the normal\ndirection does maintain the property | grad | = 1 of a distance function. Motion\nby mean curvature, and other motions, will destroy this property. To recover the\nsimple formulas (4) for distance functions, the level set method often reinitializes\nthe problem--restarting from the current time t0 and computing the distance function\nd(x, y) to the current level set (x, y, t0) = 0. This reinitialization was the Fast\nMarching Method, which finds distances from nearby meshpoints to the current\nlevel set.\nWe describe this quick method to compute distances to meshpoints, and then\ndiscuss the numerical solution of the level set equation (3) on the mesh.\nFast Marching Method\nThe problem is to march outward, computing distances from meshpoints to the in\nterface (the current level set where = 0). Imagine that we know these distances for\nthe grid points adjacent to the interface. (We describe fast marching but not the full\nalgorithm of reinitialization.) The key step is to compute the distance to the next\nnearest meshpoint. Then the front moves further outward with velocity F = 1. When\nthe front crosses a new meshpoint, it will become the next nearest and its distance\nwill be settled next.\nSo we accept one meshpoint at a time. Distances to further meshpoints are tenta\ntive (not accepted). They have to be recomputed using the newly accepted meshpoint\nand its distance. The Fast Marching Method must quickly take these steps recur\nsively:\n1. Find the tentative meshpoint p with smallest distance (to be accepted).\n2. Update the tentative distances to all meshpoints adjacent to p.\n\nc2006 Gilbert Strang\nTo speed up step 1, we maintain a binary tree of unaccepted meshpoints and their\ntentative distances. The smallest distance is at the top of the tree, which identifies\np. When that value is removed from the tree, others move up to form the new tree.\nRecursively, each vacancy is filled by the smaller of the two distance values below\nit. Then step 2 updates those values at points adjacent to p. These updated values\nmay have to move (a little) up or down to reset the tree. In general, the updated\nvalues should be smaller (they mostly move up, since they have the latest meshpoint\np as a new candidate in finding the shortest route to the original interface).\nThe Fast Marching Method finds distances to N meshpoints in time O(N log N ).\nThe method applies when the front moves in one direction only. The underlying\nequation is F |rT | = 1 (Eikonal equation with F > 0). The front never crosses a\npoint twice (and the crossing time is T ). If the front is allowed to move in both\ndirections, and F can change sign, we need the initial value formulation (3).\nLagrangian versus Eulerian\nA fundamental choice in analyzing and computing fluid flow is between Lagrange\nand Euler. For the minimizing function in optimization, they arrived at the same\n\"Euler-Lagrange equation\". In studying fluids, they chose very different approaches:\nLagrange follows the path of each particle of fluid. He moves.\nEuler sees which particles pass through each point. He sits.\nLagrange is more direct. He \"tracks\" the front. At time zero, points on the front have\npositions x(0). They move according to vector differential equations dx/dt = V (x). If\nwe mark and follow a finite set of points, equally spaced at the start, serious difficulties\ncan appear. Their spacing can get very tight or very wide (forcing us to remove or add\nmarker points). The initial curve can split apart or cross itself (changes of topology).\nThe level set method escapes from these difficulties by going Eulerian.\nFor Euler, the x-y coordinate system is fixed. He \"captures\" the front implicitly,\nas a level set of (x, y, t). When the computational grid is also fixed, we are con\nstantly interpolating to locate level sets and compute distance functions. Squeezing\nor stretching or tangling of the front appear as changes in , not as disasters for the\nmesh.\nThe velocity v on the interface determines its movement. When the level set\nmethod needs v at a meshpoint off the interface, a good candidate is the value of v\nat the nearest point on the interface.\nUpwind Differencing\nThe level set finite difference method is properly developed in the books by its origina\ntors: Sethian [ ] and Osher and Fedkiw [ ]. Here we concentrate on an essential point:\n\nc\n5.7. LEVEL SETS AND THE FAST MARCHING METHOD\n2006 Gilbert Strang\nupwind differencing. Recall from Section 1.\nthe three simplest approximations\nF, B, Cto the first derivative d/dx-Forward, Backward, Centered :\n(x + h) - (x)\n(x) - (x - h)\n(x + h) - (x - h)\nF\nB\nC\nh\nh\n2h\nWhich do we use in the simple convection equation d/dt + a d/dx = 0? Its true\nsolution is (x - at, 0). The choice of finite differences depends on the sign of a.\nThe flow moves left to right for a < 0. Then the backward difference is natural--the\n\"upwind\" value (x-h, t) on the left should contribute to (x, t+t). The downwind\nvalue (x + h, t) on the right moves further downwind during the time step, and has\nno influence at x.\nWhen the movement of the solution (and the wind) is right to left (with a > 0),\nthen the forward difference will use the appropriate upwind value (x + h, t) along\nwith (x, t), in computing the new (x, t + t).\nNotice the time-step limitation |a|t h. In time t, the \"wind\" will bring\nthe true value of from x + at to the point x. If a > 0 and finite differences reach\nupwind to x + h, that must be far enough to include information at x + at. So the\nCourant-Friedrichs-Lewy condition is at h. The numerical waves must propagate\nat least as fast as the physical waves (and in the right direction!). Downwind differ\nencing is looking for that information on the wrong side of the point x, and is doomed\nto failure. Centered differencing in space is unstable for ordinary forward Euler.\nBy careful choice of the right finite differences, Osher has constructed higher-order\nessentially non-oscillatory (ENO) schemes. A central idea in nonlinear problems,\nwhere the differential equation has multiple solutions (see Section\n), is to choose\nthe \"viscosity solution.\" This physically correct solution appears in the limit as an\nextra u xx diffusion term goes to zero. With good differencing the viscosity solution\nis the one that appears as x ! 0.\nAt this point, the level set method does not appear in large production codes.\nIn research papers it has successfully solved a great variety of difficult nonlinear\nproblems."
    }
  ]
}