{
  "course_name": "Advanced Algorithms",
  "course_description": "No description found.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Theory of Computation",
    "Engineering",
    "Computer Science",
    "Algorithms and Data Structures",
    "Theory of Computation"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nDescription\n\nThis is a graduate course on the design and analysis of algorithms, covering several advanced topics not studied in typical introductory courses on algorithms.\n\nPrerequisites\n\nPrerequisites include \"Introduction to algorithms\" (at the level of 18.410J / 6.046J), linear algebra (at the level of 18.06 or 18.700), and mathematical maturity (since we'll be doing a lot of correctness proofs). The course is especially designed for doctoral students interested in theoretical computer science.\n\nRequirements\n\nThere will be biweekly problem sets and students will also be expected to take turns to scribe lecture notes. It is thanks to the scribes that we can have a good set of lecture notes with many details.\n\nTextbook\n\nThere is no textbook required for the course.\nLecture notes\nare available for the current term as well as selected lecture notes from a\nprevious term\n. Reference textbooks for each topic are listed in the\nreadings\nsection.\n\nCourse Outline\n\nTopics we will be covering this year include:\n\nNetwork flows (max flow and min-cost flow/circulation)\n\nData structures (fibonacci heaps, splay trees, dynamic trees)\n\nLinear programming (structural results, algorithms)\n\nDealing with intractability: approximation algorithms (techniques for design and analysis)\n\nDealing with large data sets (compression, streaming algorithms, compressed sensing)\n\nComputational geometry",
  "files": [
    {
      "category": "Assignment",
      "title": "Problem Set 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/60796425602758eff3685ba9b179928e_ps1.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set 1\nCollaboration policy: collaboration is encouraged. Here are more precise rules:\n1. You must first think about the problems on your own,\n2. You must write up your own solutions, independently,\n3. You must record the name of every collaborator,\n4. You must actually participate in solving all the problems. This is difficult in\nvery large groups, so you should keep your collaboration group limited to 3 or\n4 people in a given week.\nProblems:\n1. Show a sequence of operations that can lead to a Fibonacci heap on n elements\nwhich is a chain.\n2. Suppose that Fibonacci heaps were modified so that a node is marked after\nlosing k - 1 children (as opposed to just one child in the original setting), and\nthus a node is cut after losing k children.\n(a) Assuming that k is a constant, show that this will improve the amortized\ncost of decrease key (to a better constant) while it will increase the amor\ntized cost of delete-min (by a constant factor). You may need to change\nthe potential function for the analysis.\n(b) Could one take k growing with n and still have logarithmic ranks for every\nnode?\n3. Consider the Ford-Fulkerson augmenting path algorithm for the maximum flow\nproblem seen in lecture, and assume that at every iteration the shortest (in\nterms of number of edges) augmenting path in the residual graph is selected\n(recall that the algorithm first finds the shortest path in the residual graph\ncorresponding to the current flow, then pushes as much flow on it so as not to\nviolate the capacity constraints, and then repeats until there is no augmenting\npath in the residual graph).\nPS1-1\n\n(a) Show that the length of the shortest path in the residual graph does not\ndecrease from one iteration to the next.\n(b) Show that the length of the shortest path in the residual graph must in\ncrease at least every m iterations, where m is the number of edges in the\ngraph.\n(c) Deduce that the total number of augmentations must be O(nm) where n\nis the number of vertices.\n(d) Deduce that the total running time of the algorithm is O(nm 2).\n4. Consider a maximum s-t flow problem with integer capacities u. We saw in lec\nture that the Ford and Fulkerson generic algorithm could take O(mU) iterations,\nwhere U is the maximum capacity of any edge, and this is not polynomial. In\nthis problem, you will show that the algorithm can be made polynomial through\na simple technique known as scaling.\n(a) Suppose we have a maximum flow f for our capacitated network, and we\nincrease the capacity of some of the edges by 1 unit. f may not remain\noptimal. Prove that the maximum flow value increases by at most m,\nwhere m is the number of edges. What is the running time of Ford and\nFulkerson if we start from our given flow f?\n(b) Let k = ∪log2 U⊆ and consider k + 1 maximum flow instances. In the jth\none where j = 0, 1, · · · , k, the capacities u(v, w) are replaced by\nu(v, w)\nuj(v, w) =\n.\n2j\nWhen j = 0, the problem is our original maximum flow instance. When\nj = k, all capacities are either 0 or 1. Describe how we can efficiently\nsolve the maximum flow problem for the original capacities by solving the\nproblem with the capacities uj for j from k down to 1. What is the running\ntime of the resulting algorithm?\n5. Consider a capacitated network G = (V, E). Let uv denote the maximum flow\nvalue from u to v. Prove that\nst min(su, ut)\nfor any s, t, u ≥ V .\n6. We have seen in lecture that the maximum cardinality matching problem in a\nbipartite graph G = (V, E) with V = A [ B can be formulated as a maximum\nflow problem in a network H with vertex set V [{s, t} and appropriately defined\narcs and capacities. For any matching M in G, there is a corresponding 0 - 1\nPS1-2\n\nflow f (i.e. with all (raw) flow values 0 or 1) in H with |f| = |M|, and vice\nversa.\nUse the max-flow min-cut theorem to show that there exists a matching M in\nG of cardinality |A| (i.e. every vertex in A is incident to an edge of M) if and\nonly if for every S ∃ A, we have |N(S)| |S| where N(S) = {v ≥ B : 9u ≥ S\nwith (u, v) ≥ E} is the set of neighbors of S (or neighborhood).\n7. Consider the following 2-player game between Alice and Bob. We are given a\nbipartite graph G = (V, E) where V is partitioned into A [ B (thus A and B\nare disjoint). Alice first chooses a vertex a1 ≥ A. Then Bob selects a vertex\nb1 ≥ B which is adjacent to a1. Alice now needs to select a vertex a2 ≥ A which\nis adjacent to b1, and different from a1. And we keep alternating between the\n2 players. At any stage, a player needs to select a vertex which is (i) adjacent\nto the last vertex selected by its opponent and (ii) distinct from all previously\nselected vertices. A player loses as soon as he/she can't select such a vertex.\nFor example, if the graph is a path u - v - w - x on 4 vertices with A = {u, w}\nand B = {v, x} then Bob has a winning strategy. If Alice first plays u then Bob\nwill play v (and then Alice w and Bob x), and if she plays w, he will play x. It\nis also easy to construct instances where Alice has a winning strategy (e.g. a\ncomplete bipartite graph with |A| > |B|).\nDesign a polynomial-time algorithm which given a bipartite graph decides whether\nAlice or Bob has a winning strategy. And explain what the corresponding win\nning strategy (for that player) is.\n8. Which question did you like the most (excluded this one...)? Which question\ndid you like the least?\nPS1-3"
    },
    {
      "category": "Assignment",
      "title": "Problem Set 1 (2001)",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/f03a89acb3d52b0876da75cad4e1b1fb_homework1.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.864 Advanced Algorithms\nProblem Set 1\n1. C d d e r P = ( z : A z < b+>0),whmeAismxn. Showtha~ifaisavertex\nofP thed we crtn 5 d m t ~1 and J wit31 tbe fo11mbg pmp&ies.\n(a) 1s {l,...lrn), J & (1,...,n} and II)=IJI.\n(b) A$ is invertible where A$ isthesubmatrix of A c o r m s p ~ ~\nto the POW\nin1 a n d & e m 1 ~ i nJ.\n(c) zj = O f a j # J md XJ = (A:)-'@\nnhm??denotes the m&ri&ion of b\nto t h ~ildics in I.\n(Aint: Consider Q ={(z,s):&+l e =b,s> 0, s 2 O).)\n2. In his paper in FOCS 92, Tbrnasz RBdaL needs a d\nt\n\nof the faII+\niorm\n(Page 662 of the hcmdiqp):\nLamma 1 Let c E EP and vk E (0,l)\" !OF k = 1,...,q such thd 2 1 ~ ~ 1 4\nl ~ e l f o . r k = I,...,q-1. Ahmrmethdyqc=l. Thsnqs f(n).\nh other nrords, given any set ofn tposgibbnepkim) numbers, one camat find\nm m\nthan f(n)~ubsumsof thaw nnmbera which chmamin e66Jolpted u e bv\n& factar of at Xm2.\nRadsiL pr-\nthe d\nt fnr f(n)= O(nPlogn) and conjectures. that f(n)=\nW(n]where €Ydeaotes the omisaicm of lo@-c\nt-.\nUelng b w pro-\ngnmtmhg3p are asked taimprove hisWt to f(n)=O(nlogn).\n(a) O m a wetor c and a set oS p\nthi hypothesis af the\nLemma,write &set ofineqmlitieainths~abJ,wx~\n2 L , i = l ...n,mch\nthat zi = 1 ~ is1 a feasible vector, and for asrg Mbl~vmti3~z' thm $\na ctmesponding vector d sslti&kg the hpothwis ofthe Lemmafor ths\na\nm\n\nset of mbsm,\n(b) P m thst &are mast exlst s vector d satisfying the hypotheah of the\nL\nm with c' ofthe forin (dl/d,&/d,. . . ,&/d) fagame intqpm Id[,idll,\n*..*\n=2O(nlw.) *\n(Hint: ase Problem 1.)\n(c) Deduce that 1(n)=0(nlogn).\n\n(d) (Not part of the problem set; only for those who like challenges ... A\nguaranteed A+ for anyone getting this part without outside help.) Show\nthat f(n) = n(n1ogn).\n3. The maximum flow problem on the directed graph G = (V,E) with capacity\nfunction u (and lower bounds 0) can be formulated by the following linear\nprogram:\nsubject to\n(xij represents the flow on edge (2, j); the flow has to be less or equal to the\ncapacity on any edge and flow conservation must be satisfied at every vertex\nexcept the source s, where we try to maximize the flow, and the sink t.)\n(a) Show that its dual is equivalent to:\nsubject to\n(b) A cut is a set of edges of the form {(i,j) E E : i E S,j $! S) for some\nS c V and its value is\nIt separates s from t if s E S and t $! S.\nShow that a cut of value W separating s from t corresponds to a feasible\nsolution y, x of the dual program such that\n\n(c) Given any (not necessarily integral) optimal solution y*, z* of the dual\nlinear program and an optimal solution x* of the primal linear program,\nshow how to construct from z* a cut separating s from t of value equal to\nthe maximum flow.\n(Hint: Consider the cut defined by S = {i : 5 0) and use complementary\nslackness conditions.)\n(d) Deduce the max-flow-min-cut theorem: the value of the maximum flow\nfrom s to t is equal to the value of the minimum cut separating s from t.\n4. Consider the following property of vector sums.\nTheorem 2 Let ul, . . . , un be d- dimensional vectors such that llvi1 1 5 1for\ni = 1,. . . , n (where 11 .I1 denotes any norm) and\nThen there exists a permutation ?r of (1,. . . , n) such that\nfor k = 1,.. . ,n.\nIn this problem, you are supposed to prove this theorem by using linear pro-\ngramming techniques.\n(a) Suppose we have a nested sequence of sets\nwhere lVk 1 = k for k = d, d + 1,. . . , n. Suppose further that we have\nnumbers Xki satisfying:\nfor k = d, . . . , n. Define a permutation n- as follows: set n-(1),. . . , a(d) to\nbe elements of Vd in any order, and set a(k) to be the unique element in\nVk \\VkP1 for k = d + 1,. ..,n.\nShow that this permutation satisfies the conditions of Theorem 2.\n\n(b) Show that there exist Xni, i = 1 . . .n, satisfying (I),(2) and (3) for k = n.\n(c) Suppose we have constructed Vn,. .. ,\nand Xji for j = k +1,.. . ,n and\ni E 1/3 satisfying (I), (2) and (3) for k + 1,.. .,n (where k 2 d). Prove\nthat the following system of d + 1 equalities ((4) contains d equalities),\nk + 1inequalities and k + 1nonnegativity constraints has a solution with\nat least one pi = 0:\nDeduce the existence of the nested sequence and the X's as described in\n(4.\n5. Consider the following optimization problem with \"robust conditions\":\nmin{cTx : x E Rn; Ax 2 b for any A E F),\nwhere b E Rm and F is a set of m x n matrices:\nF = {A: Vi, j ; a F n <- aij 5 azax}.\n(a) Considering F as a polytope in WXn,what are the vertices of F?\n(b) Show that instead of the conditions for all A E F , it is enough to consider\nthe vertices of F. Write the resulting linear program. What is its size? Is\nthis polynomial in the size of the input, namely m, n and the sizes of b, c,\narand a y 7\n(c) Derive a more efficient description of the linear program: Write the condi-\ntion on x given by one row of A, for all choices of A. Formulate this condi-\ntion as a linear program. Use duality and formulate the original problem\nas a linear program. What is the size of this one? Is this polynomial in\nthe size of the input?"
    },
    {
      "category": "Assignment",
      "title": "Problem Set Solution 1",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/ab2d12f4b9a57fa59e9a0909a36a31e6_solution1.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set Solution 1\n1. Consider P = {x : Ax 5 b, x 2 0), where A is m x n. Show that if\nx is a vertex of P then we can find sets I and J with the following\nproperties.\n(a) I c {I,.. . ,m}, J c {I7... ,n} and 111 = IJI.\n(b) A:\nis invertible where A:\nis the submatrix of A corresponding to\nthe rows in I and the columns in J.\n( c ) xj = 0 for j $- J and XJ = (A:)-lb'\nwhere b' denotes the restriction\nof b to the indices in I.\n(Hint: Consider Q = {(x, s) : Ax + Is = b, x 2 0,s 2 O).)\nUsing the hint we turn our attention to Q = {(x,s) : Ax+ Is = b, x > 0, s 2 0).\nIf we let (x, s) be a pair such that x E P and s is the unique vector of slack\nvariables associated with x (s = b - Ax), it is not hard to show that if x is a\nvertex of P then (x, s) is a vertex of Q. Assume that x is a vertex of P but\nthere is a (y, t) such that (x, s) & (y, t) E Q and (y, t) # 0. Then we have\nA(x * y) + (sh t) = b,x * y 2 0 , s * t 2 0. This implies that A(x + y) 5 b\nand A(x - y) 5 b. Since x is a vertex, this implies y = 0. Then solving for t in\nAx+ (s+t) = b we find that it must be zero as well which contradicts (y, t)# 0.\nWe can now take advantage of the fact that Q is in the special form (Ax =\nb, x 2 0). If (x, s) is a vertex of Q then there is a subset B c (1,.. . , n + m}\nsuch that 1B1 = m and\n(a) (x,s ) ~\n= 0 for N = {I,.. . ,n+ m)\\ B\n(b) (AI I) is non singular\nLet J c B be the set of columns involving A of (A I I). Notice that if I JI = k, k\nof the x variables are basic and m-k of the s variables are basic. So xj = 0 for\nj $ J and and k of the s variables are zero. We take the rows corresponding to\nthe zero components of s as the set I . Then\nand A$ is invertible, so\n\n2. In his paper in FOCS 92, Tomasz Radzik needs a result of the follow-\ning form (Page 662 of the Proceedings):\nLemma 1 Let c E Rn and yk E (0, lInfor k = 1,. ..,q such that 21yk+lcl 5\nIykcl for k = 1,..., q - 1. Assume that ly,cl = 1. Then q 5 f(n).\nIn other words, given any set of n (possibly negative) numbers, one\ncannot find more than f (n) subsums of these numbers which decrease\nin absolute value by a factor of at least 2.\nRadzik proves the result for f (n) = O(n2log n) and conjectures that\nf (n) = O*(n) where 0*denotes the omission of logarithmic terms.\nUsing linear programming, you are asked to improve his result to\nf (n) = O(n1ogn).\n(a) Given a vector c and a set of q subsums satisfying the hypothesis\nof the Lemma, write a set of inequalities in the variables xi 2\n0, i = 1...n, such that xi = lcil is a feasible vector, and for any\nfeasible vector x' there is a corresponding vector d satisfying the\nhypothesis of the Lemma for the same set of subsums.\nWe have a set of inequalities of the form 2 1 yi+lcl 5 1 yicl for 1<- i <- q- 1and\nlylcl = 1,and we have a vector c which satisfies them. To obtain a linear\nsystem, we need to remove the absolute value signs. Let yi = yisgn(yic).\nThen lyicl = yic. The system becomes 2yi+,c 5 y,!c for 1 5 i 5 q - 1\nand yic = 1and the original c is still feasible. To limit the solution space\nto vectors of the form x 2 0 a similar trick is used. We replace elements\ncj that are negative by -xj and non-negative elements by xj. The linear\nsystem that remains has a solution x 2 0, namely xj = I cj 1. A solution c'\nto the original inequalities can be obtained from any feasible x in the newly\nconstructed set of inequalities by negating the value of the ith element of\nx if the ith element of c was negative. This results in the same number of\ninequalities as we had originally, namely q.\n(b) Prove that there must exist a vector d satisfying the hypothesis\nof the Lemma, with d of the form (dl/d, dz/d, ...,dn/d) for some\nintegers ldl, Id1 1 , ...,ldnl = 2O(\"logn).\n(Hint: see Problem 1.)\nThe polytope defined by the inequalities above is nonempty. This means\nthat the polytope has a vertex. Our system looks like Ax 2 b and x 2 0,\nwhere A is a q by n matrix containing entries between -3 and 3 (since\nevery entry is the difference of two integers, one being &2 or 0, the other\nbeing f1or 0) and b has one nonzero element which is f1. From the first\nproblem, we know that the nonzero components, say y, of a vertex satisfy\n\nA'y = b' where A' is an invertible submatrix of A and b' is a subvector\nof b. Notice that 1det(Af)\nI 5 n!3\" = 2°(n logn). As in class (by Cramer's\nrule), we know that we can set d to be Idet(Af)I and the (nonzero) di's\nto be determinants of submatrices of A'. By the same argument, these\ndeterminants are also upper bounded by 2'(\"l0gn), proving the result.\n(c) Deduce from the above that f (n) = O(n log n).\nMultiplying the vector d by d yields an integer solution to 2yl+,x 5 yix\nfor 1 5 i 5 q - 1with elements of value 20(\"1°gn). Thus the largest sum\nthat can be obtained by a subset is 2°(n10gn). As the first subset sums to\nat least one (since the di's are integers), the number of times the sum can\ndouble is at most O(n log n).\n3. The maximum flow problem on the directed graph G = (V,E) with\ncapacity function u (and lower bounds 0) can be formulated by the\nfollowing linear program:\nmax w\nsubject to\n(xij represents the flow on edge (i, j); the flow has to be less or equal\nto the capacity on any edge and flow conservation must be satisfied\nat every vertex except the source s, where we try to maximize the\nflow, and the sink t.)\n(a) Show that its dual is equivalent to:\nsubject to\n\nThis is an immediate consequence of the definition of the dual. If one takes\nthe dual of the system of equations and inequalities above, then one gets\nSince adding a constant to all zi7s doesn't change anything, we can require\nthat z, = 0 and zt = 1.\n(b) A cut is a set of edges of the form {(i,j ) E E : i E S,j $ S) for\nsome S c V and its value is\nIt separates s from t if s E S and t $ S.\nShow that a cut of value W separating s from t corresponds to a\nfeasible solution (y, z) of the dual program such that\nFor a cut defined by S c V, we define zi = 0 for i E S, zi = 1for i $ S,\nyij = 1for i E S,j $ S, (i,j ) E E and yij = 0 otherwise. Obviously, (y ,z)\nis a feasible solution and its value is\n(c) Given any (not necessarily integral) optimal solution y*, z* of the\ndual linear program and an optimal solution x* of the primal\nlinear program, show how to construct from r* a cut separating\ns from t of value equal to the maximum flow.\n(Hint: Consider the cut defined by S = {i : zi 5 0) and use\ncomplementary slackness conditions.)\nWe divide the vertices into two sets defined as follows:\n\nEvery edge (i, j) with i E S and j\nS satisfies 2,' - 2; + ytj 2 0. Since\n2: 5 0 and zj > 0 we have that ytj > 0, which by complementary slackness\nimplies that x,*,= u:~.Every edge (j,i) with i E S and j $ S satisfies\nzj - z,' +~5~ > 0, since zj > 0 and z,' 5 0. By complementary slackness\nwe have that xji= 0. Thus, we can write\nwhich is the value of the maximum flow.\n(d) Deduce the max-flow-min-cut theorem: the value of the maxi-\nmum flow from s to t is equal to the value of the minimum cut\nseparating s from t.\nFrom (b) and weak duality, the value of any cut is greater or equal to the\nmaximum flow value. By the analysis above, we can find a cut which is\nequal to the maximum flow. Thus, the minimum cut value must be the\nsame as the maximum flow value.\n4. Consider the following property of vector sums.\nTheorem 2 Let vl, .. . ,vn be d-dimensional vectors such that llvi1 1 5 1\nfor i = 1,...,n (where 11.11 denotes any norm) and\nThen there exists a permutation a of (1,. . . ,n) such that\nfor k = 1,...,n.\nIn this problem, you are supposed to prove this theorem by using\nlinear programming techniques.\n(a) Suppose we have a nested sequence of sets\nwhere lVkl= k for k = d , d + l , .. . ,n. Suppose further that we have\nnumbers Xki satisfying:\n\nfor k = d, . . . ,n. Define a permutation n as follows: set n(l),.. . ,n ( d )\nto be elements of Vd in any order, and set n ( k ) to be the unique\nelement in Vk\\ Vk-1 for k = d + 1,... ,n.\nShow that this permutation satisfies the conditions of Theorem\n2.\nFor k 5 d, the theorem is trivial. By the definition of n and Xki, for k > d\nwe have\n(b) Show that there exist Xni, i = 1 ...n, satisfying (I), (2) and (3)\nfor k = n.\nWe choose simply\nThen\nand\n(c) Suppose we have constructed V,, . . .,\nand Xji for j = k + l , .. . ,n\nand i E V, satisfying (I), (2) and (3) for k +1,. .. ,n (where k 2 d).\nProve that the following system of d+1 equalities ((4)contains d\nequalities), k + 1 inequalities and k + 1 nonnegativity constraints\nhas a solution with at least one Pi = 0:\nDeduce the existence of the nested sequence and the X's as de-\nscribed in (a).\n\nBy the induction hypothesis, we have\nk - d\nPi= k + l - d\nXk+l,i\nsatisfying our inequalities, so the polytope of feasible solutions is nonempty.\nWe want to find a solution with at least one zero coordinate. Consider a\nvertex of the polytope and suppose that for each i, Pi > 0. There are k +1\ncoordinates summing up to k - d, so at most k - d - 1 of them can be\nequal to 1. (If k -d coordinates are equal to 1,the rest is zero.) Therefore\nwe have at least d + 2 coordinates pi,0 < Pi < 1. Let's denote this set of\ncoordinates by J. The corresponding vectors vj, j E J cannot be affinely\nindependent, so there exists a linear combination with y # 0 such that\nFor a small enough t > 0, we obtain two feasible solutions by replacing the\ncoordinates of pj for j E J by Pj lj CTj, which contradicts the assumption\nthat ,Biis a vertex. Therefore a vertex has always a zero coordinate and by\nremoving this coordinate we obtain the subset Vk and the corresponding\ncoefficients Xki = Pi which completes the induction.\n5. Consider the following optimization problem with \"robust condi-\nt ions\" :\nmin{cTx : x E Rn;Ax > b for any A E F),\nwhere b E Rm and F is a set of m x n matrices:\nF = {A : Qi, j ; amin < aij 5 acax).\n2J\n-\n(a) Considering F as a polytope in RmXn,what are the vertices of\nF?\nF is an (rn x n)-dimensional product of intervals. It has 2\"\" vertices A@)\nwhere each coordinate a:;)\nis either a;'\"\nor a;=.\n(b) Show that instead of the conditions for all A E F, it is enough to\nconsider the vertices of F. Write the resulting linear program.\nWhat is its size?\n\nSuppose that x satisfies\nfor every vertex A(').\nAny A E F can be written as a convex linear\ncombination of the vertices:\nBy taking the corresponding linear combination of inequalities (with non-\nnegative coefficients), we get\nwhich is\nTherefore x is a feasible solution if and only if it satisfies the condition for\nevery vertex of F. We can write the optimization problem in the following\nform:\nrnin{cTx : Vk;A@)X 2 b).\nThis is a linear program; however, it has an exponential number of inequal-\nities, namely m2mn,in n variables.\n(c) Derive a more efficient description of the linear program: Write\nthe condition on x given by one row of A, for all choices of A.\nFormulate this condition as a linear program. Use duality and\nformulate the original problem as a linear program. What is the\nsize of this one?\nLet us consider a fixed vector x. It is feasible if the following condition is\nsatisfied for each row ai of the matrix A:\nWe can regard this condition as a linear programming problem:\nNote that the variables are now aij, while x is fixed! By duality, we get an\nequivalent condition for a linear program with variables pi j 7qij:\n\nThis means that x is feasible if there exist pij 2 0, qij 2 0 such that\npij - qij = X j\nand\nAll together, we can write our optimization problem as the following:\nwhich is a linear program in variables X j , pij, qij. It has 2mn +n variables,\nmn equalities, m inequalities and 2mn nonnegativity constraints. The\nlinear program from part (b) has size which is exponential in the size of\nthis one."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/349667bfcb9a02721d104e645926b5d5_ps2.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set 2\nIf you have any doubt about the collaboration policy, please check the\ncourse webpage.\nProblems:\n1. Given a directed graph G = (V, E), a source s ∈ V , a sink t ∈ V and a length\nfunction l : E → R, the fattest path problem is to find a simple path P from s\nto t which maximizes min(v,w)∈P l(v, w).\n(a) Give a modification of Dijkstra's algorithm for the shortest path problem\nwhich solves the fattest path problem. Argue correctness of your algorithm.\n(b) Suppose that all arcs lengths are integer-valued between 1 and m where\nm = |E|. Can you provide an implementation of your algorithm that runs\nin O(m) time? (Hint: Do not use any fancy priority queue.)\n2. In this problem, you will show that the fattest augmenting path algorithm for\nthe maximum flow problem can be implemented to run in O(m) time per itera\ntion after some basic preprocessing. Remember that in the fattest augmenting\npath algorithm, the augmenting path with largest minimum residual capacity\nis chosen at every iteration.\n(a) Show that if we have a total ordering of the residual capacities then the\nfattest augmenting path can be found in O(m) time.\n(b) Show that, this total ordering of the residual capacities can be maintained\nin O(m) time after pushing flow along one augmenting path (how do the\nresidual capacities change)?\n(c) What is the running time ofthe resulting inplementation of the fattest\naugmenting path algorithm?\n3. Consider a directed graph G = (V, E) with a length function l : E → Z and\na specified source vertex s ∈ V . The Bellman-Ford shortest path algorithm\ncomputes the shortest path lengths d(v) between s and every vertex v ∈ V ,\nassuming that G has no directed cycle of negative length (otherwise the problem\nis NP-hard). Here is a description of this algorithm:\nThe Bellman-Ford algorithm computes d(v) by computing dk(v) = the shortest\nwalk1 between s and v using exactly k edges. dk(v) can be computed by the\n1A walk is like a path except that vertices might be repeated.\nPS2-1\n\nP\nrecurrence\ndk(v) = min [dk-1(w) + l(w, v)] .\n(w,v)∈E\nLet hl(v) = min dk(v). It can be shown that if the graph has no negative cycle\nk=1,...,l\nthen hn-1(v) = d(v) for all v ∈ V . Moreover, the graph has no negative cycle\niff, for all v, dn(v) ≥ hn-1(v).\n(You are not required to prove any of the above facts.)\n(a) Let μ ∗ be the minimum average length of a directed cycle C of G, i.e.,\nl(u, v)\nμ ∗(G) =\nmin\nμ(C) = min\n(u,v)∈C\n.\ndirected cycles C\nC\n|C|\nUsing the Bellman-Ford algorithm, show how to compute μ ∗ in O(nm)\ntime.\n(Hint: Use the fact that if we decrease the length of each edge by μ the\naverage length of any cycle decreases by μ.)\n∗\n(b) Can you find the cycle C with μ(C) = μ using only O(n2) additional time?\n(In other words, suppose you are given all the values that the Bellman-\nFord algorithm computes. Can you find a minimum mean cost cycle using\nthis information in O(n2)?)\n4. In this problem, we will propose another way to solve the minimum mean cost\ncycle problem. The resulting algorithm will be quite slow, but the technique\nis widely applicable (and for other problems, this will give the fastest known\napproach). The problem of finding μ ∗ is equivalent to the problem of finding\nthe largest value of μ such that the graph with lengths lμ(u, v) = l(u, v) -μ has\nno negative cycles.\n(a) Argue that for a given value of μ, we can decide whether μ ∗ ≥ μ or\nμ ∗ < μ by performing at most O(A(m, n)) additions of 2 numbers and\nO(C(m, n)) comparisons involving 2 numbers (and no other operations\nexcept control statements).\nPlease state the values you can obtain for\nA(m, n) and C(m, n). Observe that, as we are performing only additions\nand comparisons, all the numbers involved are linear functions of the input\nlengths and μ.\n(b) Now suppose we run the above algorithm with μ equal to the unknown\nvalue μ ∗ . We can easily perform the additions provided that we store all\nthe numbers (including the inputs) as linear functions of μ ∗ (i.e. of the\nform a + bμ∗). Explain how we can resolve the comparisons (even though\nwe do not know μ ∗). (It is normal if the solution requires a fair amount of\ntime to resolve each comparison.) As a function of A(m, n) and C(m, n),\nwhat is the total running time of your algorithm to compute μ ∗?\nPS2-2\n\n5. We argued in lecture that for the maximum flow problem, there always exists a\nmaximum flow which is integer-valued if the capacities are integral. Prove that\na corresponding statement for minimum cost circulations also holds, namely\nthat if the capacities and the costs are integer-valued then (i) the minimum\ncost circulation can be chosen to be integer-valued and (ii) the vertex potentials\nproving optimality can also be chosen to be integer-valued.\n6. In this problem, we will add a time dimension to network flows. Suppose we\nhave a network G = (V, E) in which each arc has unit capacity (u(v, w) = 1 for\nall arcs (v, w)), and we have two special vertices, a source s and a sink t. Our\nnetwork for example could be a computer network and our unit of flow could be\na packet. Each arc also has an integer-valued transit time τ(v, w) ∈ Z+ which\nrepresents the time it takes (a unit of flow or packet) to travel through the arc.\nAt every unit of time, say at time d, only one packet can enter the arc (there\nmight be several packets already travelling through the arc since there could\nhave been packets injected in it at times d-1, d-2, etc.). We can assume that\nvertices can instantaneously accept packets on its incoming arcs and also inject\none packet (if available) on each of its outgoing arcs (and if there are remaining\npackets, they can be queued at the vertex).\nThe first problem we consider is, given a deadline D, to find the maximum\nnumber k(D) of packets that can enter the network at s at time 1 or later and\nleave the network at vertex t at time D or earlier. As an example, suppose that\nour graph has only 3 arcs (s, a), (a, t) and (s, t) each with a transit time of 2.\nThen, if D = 5, the answer should be k(5) = 4 packets. Indeed, we can send 3\npackets along the arc (s, t), entering at times 1, 2 and 3 and leaving at time 3,\n4 and 5 ≤ D. We can also send a 4th packet, along the path (s, a) and (a, t);\nit will enter the arc (s, a) at time 1, arrive at a at time 3 and arrive at t at\ntime 5. (Observe by the way that in this example, no packet had to wait at\nintermediate vertices.)\n(a) Construct a maximum flow instance on a bigger network G ′ = (V ′ , E ′) such\nthat the solution of this maximum flow instance allows you to find k(D)\nand the scheduling (when they travel through each arc) of the packets in\nthe original network G. |V ′| can be of the order of D|V |.\n(b) The solution above is not polynomial when D is part of the input (since\nthe size of the network grows linearly in D). To find a polynomial time\nalgorithm, consider the following circulation problem. Take the original\ngraph G = (V, E) with all arcs of capacity 1 and give arc (v, w) ∈ E a cost\nc(v, w) = τ(v, w). Add one arc (t, s) of infinite capacity and cost equal to\n-D. Let -C∗ be the cost of the minimum cost circulation f ∗ . Prove the\nfollowing claim: C∗ is precisely k(D). Also explain how one can find the\nscheduling of the packets from the minimum cost circulation f ∗ .\nPS2-3\n\n(It might be helpful to first see what happens on the simple example with\n3 arcs given above.)\n(c) Now, suppose that we want to solve the converse problem.\nWe would\nlike to send k packets from s to t so that the last packet arrives at t as\nearly as possible. Propose a polynomial-time algorithm which given k finds\nD(k), the minimum time at which all packets have arrived at t. What is\nthe running time of your algorithm as a function of n = |V |, m = |E|,\nT = max τ(v, w) and k?\n(d) Your algorithm for (c) is probably not strongly polynomial, in the sense\nthat its running time depends on log(T) and/or log(k).\nCan you pro\npose a strongly polynomial-time algorithm? Just sketch it (a few lines are\nenough); do not give all the details. (Kind of hint: this solution will be\nmuch slower than (c) when T and k are small.)\n(By the way, all the results above are still true if the capacities are integers\npossibly greater than 1; in such a case, at every time d, at most u(v, w) packets\ncan be injected in arc (v, w). Arguing about the validity of (b) is slightly more\ndifficult.)\n7. Which question did you like the most (excluded this one...)? Which question\ndid you like the least?\nPS2-4"
    },
    {
      "category": "Assignment",
      "title": "Problem Set 2 (2001)",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/8bf6e97ac73e0c97bedcc48aa086e150_homework2.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/8,854 Advanced Algorithms\nProblem Set 2\n1. The Min s-&Cut problem is the following:\nGiven an undimted gmph G = (V,E), a weight firaction w : E +W, and two\nvertices 8, t E V ,find\nMin 8 -t -Cut(G)=min{w(d(S)) :S C I/,s f S,t$3).\nwhere &($) denote the cut\n(a) Argue (i jwt afew lines) that there is a polynomial-timealgorithmto find\na Min s - t-cut based on linear programming (remember Problem Set 1).\n(Be careful; problem set 1 dehed the Min s-t-cut problem for a directed\ngraph, while this problem considera undirected graphs.) [We will see a\nmuch more efficient algorithm for it (not based on Iinem programming)\nhter this semester,]\nWe are going to develop an algorithm for a generdization of the problem:\nGiven an uadimted graph G = (lr, E), w :E +R? , and an even cardinality\nsubset of vertices T G V ,find\nMzn T-Odd -Cut(G)=min{w(5(S)) :S C V,ISnTI = o d 4\nThat is, we want to optimize over all cuts that separate T into two parts of odd\nsize (since IT1 is even, ISn TI odd implies that T \\ SI odd as well).\n(b) Suppose that IT1= 2,say T = { s , t ) . Wbat is the Min T-Odd-Cut then?\n(c) For a given T\nV,call a cut d(S) T-splitting if 0# SnT # T.\nUsing a s-t-&-Cut\nalgorithm, show how we can 6nd the minimum T-\nsplitting cut in polynomial time. Can you do it in at most IT1 cab to a\nMin s-t-Cut algorithm?\n\n(d) For any lam sets C and D (0# C,D c V ) ,prove the inequally that\n(o) Prove that if 6(C)is sminimum T-splitting cut then there is a minimum\nT-odd-cut 6(D)such that either D E C or C D.\nHint: UE the inquality proved above.\n(f) Use the previous ohemtion to design a recursive algorithm which wlvm\nMin T-Odd-Cut in polynomial time. (Hint: pwibly think about mod-\nm\ng the graph.) HOW many & (inO(.) notation) to s Mig &-Cut\nalgorithm does your algorithm perfom?\n2. Usethe ellipacidmethod to solve the minimummight perfect matchingproblem\n(there: is a more &cient mmbinatorial dgoritlm for it, but here we will use\nthe power of the ellipsoid dgorithm]:\nGiven an undimtai gmph G = (V,E ) and a weight finctiopa w :E -+ N,find a\nset of dga M wvm-nganwy vertex ea;actiy once (a perfect matching) with the\nminim~srntotalweight.\nIn order to formdab this problem w a linear program, we d&e\nthe V-join\npolytope:\nP = catu{XM E {0, lIE :M is a perfect matching}\nwhere XM is the c.har%ter&tic vector of M (xM(e)= 1 if e E M and 0 other-\nwise). ?\"he C Q ~ W\nis d h d =(x,\nE A, & >_ O,Ci& =\nhuU WTIUCA)\nAiai :\n1) (where t b m a t i o n ia finite:).\n(a) Argue that thevertices of P are the chractedsticvector~of perf& match-\ninp. Deduce that if we can optimize\nw,~, over P, we would fud a\nminimumweight perfect matching.\n(b) Suppose now that we can dwide (via linw pro^^ or same other\nway) whether Pi-74~:wTx 5 A) isempty or not, for any dvenX (remember\naJI wei&ts weare integers). Show that by calling an algorithm for this\ndecision problem a polynomial number of fhna(in the &e of the input,\ni.e. IVI, IEl and log(w-) 1, we canfind tbeweight of the mhimm-weight\nperfect matchingp\n(c) With the same amumptiona as in the previous part, can you also fbd\na minimtrm-weight perfect matching (not just its weight, but a h which\nedges are in it) in polynomid time? (There might be merd perfect match-\ning having the m\ne\n\nminFmnm .weightTbut here you need to produce only\none of them. &o,\nthe algorithm does not need to be extremely efficient,\njust palgnomial.)\n\nDue to Jack Edmonds, the perfect matching polytope can be described by the\nfollowing inequalities:\n(d) Show that every vector in P satisfies the above inequalities.\nTake the other implicationfor granted (everyvector satisfying these inequalities\nis in P).\n(e) How many inequalities do we have in this complete description of P? Can\nwe just use any polynomial-time algorithm for linear programming to o p\nt G e over P?\n(f) Show how we can use the ellipsoid method to decide if there exists a perfect\nmatching of weight at most X in polynomial time. How would you select\nthe initial ellipsoid? How would you tike care of the equality constraints\nin the description of P? When can you stop?"
    },
    {
      "category": "Assignment",
      "title": "Problem Set Solution 2",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/7c1e4b71e427918687f9560c1f5fc500_solution2.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set Solution 2\n1. The Min s-t-Cut problem is the following:\nGiven an undirected graph G = (V,E), a weight function w : E +R+,\nand two vertices s, t E V, find\nMin s - t - Cut(G) = min{w(S(S)) : S c V, s E S,t $ S).\nwhere 6(S) denotes the cut\n6(S) = {(i,j) E E : I{i,j) nSI = 1)\nand\n(a) Argue (in just a few lines) that there is a polynomial-time al-\ngorithm to find a Min s - t-cut based on linear programming\n(remember Problem Set 1). (Be careful; problem set 1defined\nthe Min s-t-cut problem for a directed graph, while this problem\nconsiders undirected graphs.) [We will see a much more efficient\nalgorithm for it (not based on linear programming) later this\nsemester.]\nThe description of the (directed) Min s-t-cut problem as a linear program\nfrom Problem 1shows immediately that we can solve it in polynomial time,\nfor example by the ellipsoid algorithm. Given an undirected graph, we can\ntransform it into a directed graph simply by producing two directed edges\n(one each way) for every undirected edge. Then cuts in the directed graph\ncorrespond exactly to cuts in the undirected graph, with the same weight.\nWe are going to develop an algorithm for a generalization of the\nproblem:\nGiven an undirected graph G = (V,E), w : E -t R+, and an even\ncardinality subset of vertices T\nV, find\nMin T - Odd - Cut(G) = min{w (b(S)) : S c V,IS fl TI = odd}\nThat is, we want to optimize over all cuts that separate T into two\nparts of odd size (since IT1 is even, ISnTI odd implies that IT\\ SI odd\nas well).\n\n(b) Suppose that IT1 = 2, say T = {s,t). What is the Min T-Odd-Cut\nthen?\nSince any T-odd cut must split T into two odd parts, there is only one\nway to do that - have s and t on opposite sides of the cut. Thus the Min\nT-odd cut is exactly the Min s -t cut.\n(c) For a given T C V, call a cut 6(S) T-splitting if 0 # SnT # T.\nUsing a s-t-Min-Cut algorithm, show how we can find the min-\nimum T-splitting cut in polynomial time. Can you do it in at\nmost IT1 calls to a Min s-t-Cut algorithm?\nChoose a fixed s E T. For every cut b(S), we can assume s E S (the\ngraph is undirected and V \\ S defines the same cut). Moreover, for every\nT-splitting cut, there exists a vertex t E T\\ S and such a cut is an s-t-cut\nas well. It is sufficient to find the minimum s-t-cut for every t E T \\ {s}\nand take the minimum of all these cuts.\n(d) For any two sets C and D (0 # C,D c V), prove the inequality\nthat\nw(b(C \\ D)) +w(b(D \\ C))5 w(b(C)) +w(b(D)).\nWe apply the definition of a cut - 6(X) is the set of all edges between X\nand V \\ X:\n(e) Prove that if 6(C) is a minimum T-splitting cut then there is a\nminimum T-odd-cut 6(D) such that either D\nC or C & D.\nHint: Use the inequality proved above.\nSuppose b(C) is the minimum T-splitting cut. If IC nTI is odd then C\nis also the minimum T-odd cut and we can choose D = C. Otherwise\nsuppose IC nTI is even, while b(D') is the minimum T-odd cut.\n\nSince I D' nTI is odd, either I (Dl n C)nTI or I (D' \\ C)n TI is odd. Denote\nby C' either C or V \\ C so that I (Dl n C') nTI is odd. Note that in any\ncase 6(C1) = S(C) and IC' n TI is even.\nSince S(C1) is a T-splitting cut, (V \\ C') n T is nonempty which implies\nthat either (D' \\ C') n T is nonempty or ((V \\ D') \\ C') n T is nonempty.\nWithout loss of generality, we can assume that (Dl \\ C') nT is nonempty,\notherwise we rename D' to V \\ D' which doesn't change the cut (and the\nnew D' still satisfies I (Dl n C') n TI is odd since IC' n TI is even).\nBecause 6(C1) is the smallest T-splitting cut and S(D1 \\ C') is a T-splitting\ncut, we have w(S(C1)) 5 w(6(D1\\ C')). Since I(C' n Dl) n TI is odd,\nI (C'\\ Dl) nTI is odd as well. The smallest T-odd cut is 6(D1), so w (6(D1))5\nw(S(C1\\ Dl)). Comparing these two inequalities with the one given in the\nhint, w(6(C1)) + w(S(D1))2 w(S(C1\\ Dl)) + w(S(D1\\ C')), we find that\nthey must all hold with equality and 6(C1 \\ D') is a minimum T-odd cut\nas well.\nIn case C' = C, we can choose D = C1\\D' and we have a minimum T-odd\ncut D c C. In case C' = V \\ C, we can choose D = V \\ (C' \\ D') and we\nhave a minimum T-odd cut such that C c D.\n(f) Use the previous observation to design a recursive algorithm\nwhich solves Min T-Odd-Cut in polynomial time. (Hint: pos-\nsibly think about modifying the graph.) How many calls (in O(*)\nnotation) to a Min s-t-Cut algorithm does your algorithm per-\nform?\nMinOddCut (G, T )\n{\nC = MinCut (G, T ) ;\ni f (IC n TI = odd) return C ;\nG1 = Contract (G, C) ;\nG2 = Contract (G, V \\ C) ;\nC1 = MinOddCut (GI,\n\\ C) ;\nC2 = MinOddCut (G2, T fl C) ;\ni f (weight ( 6 ( 4 ) ) < weight (6(C2)) return C1;\nelse return C2;\n}\nHere, MinCut (G, T) is supposed to return the minimum T-splitting cut in\nG and Contract (G, C) should merge C into a single vertex and update\nthe edges accordingly (i.e. any edge between a vertex u of C and a vertex\n\nv not in C becomes a new edge between the new shrunk vertex and v; if\nthere are multiple edges between two vertices, we can replace them by one\nedge with weight equal to the sum of the weights).\nThe correctness of the algorithm follows from the previous observations.\nEither the minimum cut S(C) is T-odd, or we can assume that the mini-\nmum T-odd-cut is S(D) where C C_ D or D\nC. Cuts 6(D) where C C_ D\nare equivalent to cuts in the graph G1 where C is contracted to a single\nvertex. Cuts 6(D) where D\nC are equivalent to cuts in the graph G2\nwhere V \\ C is contracted to a single vertex. The smaller of the two cuts\nmust be the minimum T-odd cut.\nFinally, let's analyze the running time of this algorithm. The body of the\nfunction (excluding the recursive calls) runs in time polynomial in the size\nof the input graph (MinCut algorithm + elementary graph operations). It\nremains to estimate the number of recursive calls to MinOddCut. Denote\nthe size of the input set T by t. Note that if the function is called with\nparameter T and it produces recursive calls with parameters TI and T2,\nthen IT1 = ITlI +IT21. Since lz 1 2 2 in the leaves of the recursion tree, the\nnumber of leaves is at most i. The tree is binary, so the number of nodes\nis at most t. Therefore the total number of recursive calls to MinOddCut is\nlinear in IT I.\nEach call to MinOddCut will require a number of calls to a Min s - t-cut\nalgorithm less than t = ITI. Hence, the tot a1 number of calls to a Min\ns-t-cut algorithm is O(IT 1 2 ) .\n(By studying the problem, one can actually\nsolve the Min T-odd-cut problem with O(IT1) calls to a Min s - t-cut\nalgorithm.)\n2. Use the ellipsoid method to solve the minimum weight perfect match-\ning problem (there is a more efficient combinatorial algorithm for it,\nbut here we will use the power of the ellipsoid algorithm):\nGiven an undirected graph G = (V,E) and a weight function w :\nE tN, find a set of edges M covering every vertex exactly once (a\nperfect matching) with the minimum total weight.\nIn order to formulate this problem as a linear program, we define the\nperfect matching polytope:\nP = conv{xM E {O, 1IE: M is a perfect matching)\nwhere XM is the characteristic vector of M (xM(e) = 1 if e E M and 0\notherwise). The convex hull conv(A) is defined as {xiXixi : xi E A, Xi >\n0, xiXi = 1) (where the summation is finite).\n\n(a) Argue that the vertices of P are the characteristic vectors of\nperfect matchings. Deduce that if we can optimize Cewexe over\nP, we would find a minimum weight perfect matching.\nAny point in P can be written as\nwhere AM 2 0 and EMAM = 1. Clearly, x can be a vertex only if we\nhave exactly one AM = 1. We will show that all such vectors are indeed\nvertices. For a given M , consider the hyperplane\nwhere n = IVI (an even number). Note that every perfect matching has\nexactly 5 edges. Then for any x E P,\nbecause 0 5 xe 5 1. Equality can hold only if Ve E M;xe = 1but then x\nis the characteristic vector of M. Therefore P n H M = { x M ) which proves\nit is a vertex.\nThe optimum of zewese can be assumed to be a vertex X M which means\nthat for any other perfect matching MI, w(M1) 2 w(M).\n(b) Suppose now that we can decide (via linear programming or some\nother way) whether Pn{x : wTx 5 A ) is empty or not, for any given\nX (remember all weights we are integers). Show that by calling\nan algorithm for this decision problem a polynomial number of\ntimes (in the size of the input, i.e. IVI, I EI and log(w,,,)),\nwe can\nfind the weight of the minimum-weight perfect matching.\nWe can find the minimum weight by binary search. If the graph has n\nvertices and maximum edge weight w,,,,\nthe maximum possible weight\nof a perfect matching is inw,,,.\nFor any X E [O; inw,,,],\nwe are able to\ntest whether there exists a perfect matching of weight at most X (that's\nexactly when Pn{x : wTx 5 A ) # a)). The weights are integers, so we can\npinpoint the smallest such X in O(log(nw,,,))\nsteps.\n(c) With the same assumptions as in the previous part, can you also\nfind a minimum-weight perfect matching (not just its weight, but\nalso which edges are in it) in polynomial time? (There might be\nseveral perfect matching having the same minimum weight, but\n\nhere you need to produce only one of them. Also, the algorithm\ndoes not need to be extremely efficient, just polynomial.)\nFor any edge, we can determine if we need it for the optimal perfect match-\ning. First, find the minimum weight W*. Then pick an edge el, remove\nit from the graph and test if there is still a perfect matching of weight\nW*. If yes, we don't need edge el and we continue on the graph G \\ {el).\nOtherwise we know that el appears in any optimal perfect matching, so\nwe remember it, remove its two vertices from the graph, and continue on\nthe remaining graph with modified optimum weight W' = W* - w(el). In\nI E 1 steps, we determine the optimal perfect matching.\nDue to Jack Edmonds, the perfect matching polytope can be de-\nscribed by the following inequalities:\n(d) Show that every vector in P satisfies the above inequalities.\nSuppose x is the characteristic vector of a perfect matching. Then the first\ntwo inequalities are satisfied by definition. For the last inequality, consider\nand odd-size subset W c V. All vertices of W cannot be covered by edges\ninside W because these edges cover disjoint pairs of vertices. At least one\nvertex must be covered by an edge e E 6(W) and therefore\nSince these inequalities are valid for the vertices of P, they are also valid\nfor any point inside P.\nTake the other implication for granted (every vector satisfying these\ninequalities is in P).\n(e) How many inequalities do we have in this complete description\nof P? Can we just use any polynomial-time algorithm for linear\nprogramming to optimize over P?\nUnfortunately, the third condition generates 2\n\"\n~\n~\ninequalities (one for each\nodd subset, the same equality for W and V \\ W). Therefore a straightfor-\nward linear programming approach would be very inefficient (not polyno-\nmial in n, the number of vertices, and log w,,,).\n\n(f) Show how we can use the ellipsoid method to decide if there exists\na perfect matching of weight at most X in polynomial time. How\nwould you select the initial ellipsoid? How would you take care\nof the equality constraints in the description of P ? When can\nyou stop?\nBy adding the inequality wTx 5 A, we get a polytope PAwhich is nonempty\nexactly if there exists a perfect matching of weight at most A.\nThe ellipsoid algorithm can be used to test whether PA= 0 whenever we\ncan:\na find a suitable bounding ellipsoid to start with,\na have a polynomial-time separation oracle, and\na estimate the minimum volume of PA,if it's nonempty.\nThe bounding ellipsoid here is simple. We can take for example the sphere\nwith center in the origin and radius fl.This contains all characteristic\nvectors of perfect matching.\nIf a point x doesn't lie in PA,it's because it violates some of the conditions.\nThe condition wTx 5 X is easy, as well as the first two inequalities in the\ndescription of P . The third inequality seems to require an exponential\nnumber of inequality checks but here's where Problem 1comes into play.\nFor a given x, we can calculate in polynomial time\nbecause this is just a min-V-odd-cut problem. Then we check whether\ny(x) 2 1. In case y(x) < 1,we can report that x violates the inequality for\nW where S(W)is the minimum V-odd cut. Otherwise, we are guaranteed\nthat no such cut exists.\nFinally, we have to make sure that PAhas some volume so we know when\nto stop. We do this by employing the theorem given in class (theorem 2 of\nthe scribe notes of lecture 5) which states that {x : A x 5 b) is nonempty\nif and only if {x : Ax 5 b+ ~ e )is nonempty as well, where E can be chosen\nas 2-L. The value L as defined in class involves the number of rows as\nwell, which is enormous, but this is not needed here. We can simply redo\nthe proof more carefully. We have to consider a vertex of y 2 0, AT = 0,\nand bTy = -1, where the matrix ( t: ) has entries all 0 and I except for\none column containing wj's and X (which can be assumed to be at most\nmw,,,).\nMost of the entries of such a basic feasible solution y will be 0, the\nones that are non-zero (basic, and thus at most m of them) will be at most\nm!mw,,.\nHence, following the proof of Theorem 5, we can choose E to be\n\nsay 2m!m2~,,,\n= 2-Q with Q = O(m log rn + log w,,,),\nwhich happens to\nbe polynomial in rn and log w,,,.\nTherefore we replace each equality by a\npair of inequalities and increase the right-hand size by E = 2-Q. We have\nto slightly modify our separation algorithm (since now we are separating\nover this slightly modified polytope) but this is trivial since we simply\ncompare the value of the minimum V-odd-cut to 1- & instead of 1. In\nsummary, this guarantees that we can stop after a polynomial number of\nsteps (0(m2Q)= 0(m3 log rn +m2 log w,,,)\n) and either find a point in P'\nor declare it empty."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/553efdbb9b04e5eee039d975b9ecccf4_ps3.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n!\n18.415/6.854 Advanced Algorithms\nProblem Set 3\nIf you have any doubt about the collaboration policy, please check the\ncourse webpage.\nProblem 1. Consider the symmetric traveling salesman path problem. We are\ngiven a set V of vertices, a (symmetric) distance d(i, j) = d(j, i) ≥ 0 for every pair\ni, j of vertices, 2 special vertices s and t, and we would like to find a path from s to t\npassing through all vertices (a Hamiltonian path) and of minimum total length. This\nproblem is NP-hard, and so we will not try to solve it exactly in polynomial time.\nOne simple heuristic is known as 2-OPT. Start from any ordering v1 = s, v2,\n, vn =\n· · ·\nt and remove 2 non-consecutive edges of the Hamiltonian path, say (vi, vi+1) and\n(vj , vj+1) where i +1 < j. Now, there is a unique way to form a new Hamilonian path\nby adding 2 other edges, namely (vi, vj ) and (vi+1, vj+1). If this results in a path of\nshorter length, this so-called 2-swap is performed, and this is repeated until no more\nimprovements are possible.\nNow suppose we would like to create a data structure to maintain the ordering\nof the vertices on the path. For any vertex v, we would like to be able to find\nNext(v), the vertex following v on the way to it, and similarly Previous(v), the\nvertex preceding v (i.e. closer to s). The tricky thing is that when we perform a\n2-swap, the vertices vi+1 to vj are now visited in the reverse order (from vi we go\nto vj , then to vj-1 and continue all the way to vi+1, and then continue at vj+1).\nThis means we would also like to have an operation Reverse(v, w) that reverses\nthe ordering from v to w; in our case above we would perform Reverse(vi+1, vj ).\nIf we were maintaining the ordering as a doubly-linked list (with pointers next and\nprevious), a Reverse operation would require Θ(n) time in the worst-case. Show\nhow to use splay trees to maintain the ordering and perform any sequence of m\noperations (Next, Previous, or Reverse) in O((m + n) log n) time.\n(If you augment the splay tree with additional information at every node, you\nmust indicate how this information is maintained while performing operations.)\nProblem 2. In lecture, we argued the static optimality property of splay trees by\nshowing that the time T required for a splay tree to access element i mi ≥ 1 times\nfor i = 1,\n, n is within a constant of the time required by any static BST. In this\n· · ·\nproblem, you need to argue that this time T for splay trees is\nX\nm\nO\nmi 1 + log\n,\nmi\ni\nPS3-1\n\nP\nwhere m =\ni mi is the total number of accesses.\nProblem 3. In the blocking flow problem (which arises in the blocking flow al\ngorithm for the maximum flow problem), we are given a directed acyclic graph\nG = (V, E), 2 vertices s and t and capacities on the edges. The goal is to find a\nflow f such that for every path P in E from s to t at least one of the edges of P is\nsaturated (i.e. f(v, w) = u(v, w)).\n1. Is the following argument correct?\nA blocking flow f is maximum since, if we take S to be the set of\nvertices reachable from s in (V, E) by non-saturated edges, we get\n\na cut (S : S) whose value equals the blocking flow, and hence the\nblocking flow must be optimal.\nIf the argument is fallacious, show a blocking flow which is not maximum.\n2. Show how to find a blocking flow in a graph G = (V, E) with n vertices and m\nedges in O(m log n) time. (Your solution can be quite short.)\n(FYI, Dinitz showed that one can solve a maximum flow problem in a general directed\ngraph by solving at most n blocking flow problems in directed acyclic graphs.)\nProblem 4. A team of n members would like to travel a distance d from A to B\nas quickly as possible. All of them can walk and have also one scooter (which can\ncarry only one person at a time) that they can use. For each person i (1 ≤ i ≤ n), we\nknow his/her walking speed wi and his/her speed si when travelling on the scooter.\nThe goal is to find a way to bring all n people to destination so as to minimize the\ntime at which the last person arrives. The scooter can be left by any member of the\ngroup on the side of the road, and picked up by anyone else of the group. Members\nof the group can also walk or use the scooter backwards (towards A) if that helps.\n1. Consider the case where n = 3, w1 = w2 = 1, s1 = s2 = 6, w3 = 2, s3 = 8 and\nd = 100. Find the fastest way for everyone to travel the distance d.\n2. For a general instance (general n and arbitrary speeds), write a linear program\nwhose value is always a lower bound on the time needed for the n-person team\nto travel a distance d. This should be a small linear program; the number\nof variables and constraints should be O(n) (and not dependent on d, or the\nnumber of 'legs' of the solution).\n3. Use the linear programming routines in matlab (or any LP software like CPLEX)\nto compute your lower bound for the instance given in 1. (You don't have to use\na linear programming software, provided you can exhibit an optimum solution,\nwith a proof of optimality.)\nPS3-2\n\nX\nP\nIf the bound you obtain is not equal to the value you found in 1., either improve\nyour solution to 1., or find a stronger linear program in 2.\nMatlab is available on athena, see http://web.mit.edu/olh/Matlab/Matlab.html\nfor more info. Type help linprog for information on how to use the LP rou\ntine.\nProblem 5. We will rederive the max-flow min-cut theorem from linear program\nming duality. Consider the maximum flow problem on a directed graph G = (V, E)\nwith source s ∈ V , sink t ∈ V and edge capacities u : E → R. The max-flow problem\nis a linear program:\nmax w\nsubject to\n⎧\nX\nX\n⎨ w\ni = s\nxij -\nxji = ⎩ 0\ni =6\ns, t\nj\nj\n-w\ni = t\nxi,j ≤ ui,j\n(i, j) ∈ E\n0 ≤ xi,j\n(i, j) ∈ E.\nThe variables are the xij 's (one per edge) and w.\n1. Show that its dual is equivalent to:\nmin\nuijyij\n(i,j)∈E\nsubject to\nzi - zj + yij ≥ 0\n(i, j) ∈ E\nzs = 0, zt = 1\nyij ≥ 0\n(i, j) ∈ E.\n2. Given a cut (S : S) with s ∈ S and t ∈/ S, show that a feasible solution\ny, z to the dual can be obtained of value equal to the capacity of the cut:\nU(S : S) =\n(i,j)∈E uij yij .\n3. Given any (not necessarily integral!) optimal solution y∗, z∗ of the dual linear\nprogram and an optimal solution x∗ of the primal linear program, show how to\nconstruct from z∗ a cut separating s from t of value equal to the maximum flow.\nThis shows the max-flow-min-cut theorem.\nPS3-3"
    },
    {
      "category": "Assignment",
      "title": "Problem Set 3 (2001)",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/15c42bf554389b217e86310365de7765_homework3.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set 3\n1. Consider the following optimization problem:\nGiven c E Rn, c 2 0, n even, find\nn\nmin{cTx: C,,,xi 2 1 VS c {I,...,n), IS1 = 5,\nIn class, it was shown that this can be solved by the ellipsoid method because\nthere is an efficient separation algorithm. However, this problem has a more\nstraightforward solution.\nDevelop an algorithm which finds the optimum in O(n logn) time. Prove its\ncorrectness.\n2. Fill a gap in the analysis of the interior point algorithm:\nSuppose that (x, y, s) is a feasible vector, i.e. x > 0, s > 0,\nand we perform one Newton step by solving for Ax, Ay, As:\nAAx = O\nA\n~\nA\n~\n\n+ As =\nVjj'; X j S j\nAxjsj + xjA5'j = P\nwhere p > 0. The proximity function is defined as\nX j S j\nI)?\nP\nProve that if\no(x + Ax, s + As, ,!A) < 1\nthen (x + Ax, y + Ay, s + As) is a feasible vector for Ax = b,x > 0 and\nA T Y + s = c , s > 0.\n\n3 b Given a directed graph G = (V,E) and two vertices s and t, we d\nd like to\nfind the maximum number of edge-disjoint paths between s and t (two paths\nare edge-disjoint if they don't share an edge). Denote the number of vertices\nby n and the number of edges by m.\n(a) Argue that this problem can be solved as a m h u m flow problem with\nunit capacitia. Explain.\n(b) Consider now the mashurn ~ Q W\n=\nproMan on directed graphs G (V,E)\nwith unit capacity edges (dthoughsome of the questions below would also\nappIy to the more general case).\nGiven afeasible flow f,we cmconstruct the midual network Gf = (V, E!)\nwhere\nThe residual capacity of an edge (i,j ) E Ef is equal to - fV or to fji\ndepending on the case above. Since we are dealingwith the unit capwits\ncase, an the q*'s are 1 and therefore for 0- 1flows f (i,e. flowsfor which\nthe d u e on any edge is 0 or I),dl residual capwitia wilI be I.\nWe d&e the distance of .a vertex If(v) as the Iengkh of the shortest path\nfrom s to v in Ep (w for vertices which are not reachable from s in Ef).\nFurther, d&e\nthe lmelled residual network as\nd a sabmtingflow g in E) as a flow in E'f (with capacities being the\nresidual capacities) such that every directed s - t path in El has at least\none saturated edge (i.e. an edge whose fiflow equals the residual capacity].\nFor a unit capacity graph and a given 0 -1 flow f,show how we can .find\nthe leverIIed residual network and a saturatisg flaw in O(m)time.\n(c) Prove that ifthe le~1ledreaidualnetwork has no path from s to t (kf (t)=\nm),then the fiow f iis mmhum.\nEd] For a flm f,d&e\n4fl =b($1\n(the distance from s to 5 in the residual network). Prove that if g is a\nsaturating flow far f then\nwhere f+g denotes the b w obtained from f by either incr-\nthe flow\nfij by gij or decreasing the flow fjiiby gg for e m\nedge (i,j ) E GI.\n\n(e) Prove that if f is a feasible 0 - 1flow with distance d = d(f) and f * is an\noptimum flow, then\nand also\n(f) Design a maximum flow algorithm (for unit capacities) which proceeds by\nfinding a saturating flow repeatedly. Try to optimize its running time.\nUsing the observations above, you should achieve a running time bounded\nby o(min (mn2I3, m3I2)).\n(g) Can we now justify that, for 0 - 1capacities, there is always an optimum\nflow that takes values 0 or 1on every edge?"
    },
    {
      "category": "Assignment",
      "title": "Problem Set Solution 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/4004fe272abc830c274663ab7d107daf_solution3.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set Solution 3\n1. Consider the following optimization problem:\nGiven c E Rn,c 2 0, n even, find\nn\nmin{cTx : xi,,\nxi 2 1 VS c (1,. . . ,n), IS1 = 2,\nIn class, it was shown that this can be solved by the ellipsoid method\nbecause there is an efficient separation algorithm. However, this\nproblem has a more straightforward solution.\nDevelop an algorithm which finds the optimum in O(nlog n) time.\nProve its correctness.\nLet\nWe would like to describe the structure of P,which is an unbounded polyhedron.\nWe prove that x E P exactly when x can be written as\nwhere XA denotes the characteristic vector of A, X A 2 0, and additionally\nFirst, suppose x satisfies this and consider S of size n/2. Any set A of size\nIAl > n/2 intersects S in at least IAl - n/2 elements, therefore\nConversely, let x E P . Let 7-r be a permutation such that\nG(1) I G ( 2 ) I . I X,(n).\nPSS3-1\n\nSet\nand\nfor k = 1...n. Then obviously XI, > 0 and\nFinally, we verify condition (*):\nNow we can optimize over P much more easily. First, observe that for any\noptimal solution\nwe can assume X A = 0 for IAI 5 n/2 and\notherwise we decrease the coefficients until the equality holds. This won't in-\ncrease the objective function C cixi, since c > 0. Therefore an optimal solution\nalways exists in the convex hull of {pA : IAl > n/2} where\nWe could evaluate the objective function at all these points but there are still\ntoo many of them. However, we can notice that for a given k = IAl, the\nonly candidate for an optimum pa is the set A which contains the k smallest\ncomponents of c. Therefore the algorithm is the following:\nSort the components of c and let Ak denote the indices of the k smallest\ncomponents of c, for each k > n/2. This takes O(n logn) time.\n\nFor each k > n/2, calculate sk = CiEAk\nck. This can be done in O(n)time,\nbecause the sets Ak form a chain and we can use sk to calculate sk+l in\nconstant time.\nFind the smallest value of\nfor k > n/2. Return this as the optimum.\nThe algorithm runs in 0( nlog n)time and its correctness follows from the anal-\nysis above.\n2. Fill a gap in the analysis of the interior point algorithm:\nSuppose that (x,y, s) is a feasible vector, i.e. x > 0, s > 0,\nAx = b,\nA T y + s = c\nand we perform one Newton step by solving for Ax, Ay, As:\nV\nXjSj+Axj~j+xjAsj =/l\nwhere p > 0. The proximity function is defined as\nProve that if\nthen ( x + Ax, y + Ay, s + As) is a feasible vector for Ax = b, x > 0 and\nATy+s=c,s>O.\nThe equalities are satisfied directly by the assumptions:\nWe have to verify the positivity conditions. First we prove that at least one of\nxj +A x j , sj +Asj is positive. We have xj > 0,sj > 0 and\n\ntherefore either xj +Axj or sj+Asjmust be positive.\nSecond, we use the proximity condition:\nIn particular, for each j\nwhich means that xj + Axj and sj+ Asjhave the same sign. We know they\ncan't be negative so they must be positive.\nGiven a directed graph G = (V,E) and two vertices s and t, we would\nlike to find the maximum number of edge-disjoint paths between s and\nt (two paths are edge-disjoint if they don't share an edge). Denote\nthe number of vertices by n and the number of edges by m.\n(a) Argue that this problem can be solved as a maximum flow prob-\nlem with unit capacities. Explain.\nLet F be a union of k edge-disjoint paths from s to t. We define a flow of\nvalue k in a natural way - an edge gets a flow of value 1if it is contained\nin F and and 0 otherwise. Since each path enters and exits any vertex\n(except s and t) the same number of times, flow conservation holds. The\nvalue of the flow is the number of edges in F leaving s (or entering t) which\nis k.\nConversely, let f be the maximum flow with unit capacities. As we shall\nprove, there is always a 0 -1maximum flow, therefore we can assume that\nf, is either 0 or 1for each edge. Let\nand k be the value of the flow. Then we can decompose F into k edge-\ndisjoint paths in the following way: We start from s and follow a path\nof edges in F until we hit t. (This is possible due to flow conservation.)\nWhen we have found such a path, we remove it from F and consider the\nremaining flow of value k - 1. By induction, we find exactly k such paths.\n(b) Consider now the maximum flow problem on directed graphs G =\n(V,E) with unit capacity edges (although some of the questions\nbelow would also apply to the more general case).\nGiven a feasible flow f , we can construct the residual network\nGf = (V,Ef) where\nEf = {(i,j) : ((i,j) E E & fq < uij) or ((j,i) E E & fji> 0)).\n\nThe residual capacity of an edge (i,j) E Ef is equal to uij - fij or\nto fji depending on the case above. Since we are dealing with\nthe unit capacity case, all the uij's are 1and therefore for 0 - 1\nflows f (i.e. flows for which the value on any edge is 0 or I), all\nresidual capacities will be 1.\nWe define the distance of a vertex lf(v) as the length of the short-\nest path from s to v in Ef (cafor vertices which are not reachable\nfrom s in Ef). Further, define the levelled residual network as\nElf = {(i,j) E Ef : lf(j) = lf (i) + 1)\nand a saturating flow g in Ei as a flow in E; (with capacities\nbeing the residual capacities) such that every directed s -t path\nin Elf has at least one saturated edge (i.e. an edge whose flow\nequals the residual capacity).\nFor a unit capacity graph and a given 0 - 1flow f,show how we\ncan find the levelled residual network and a saturating flow in\nO(m) time.\nFirst, we can find Ef in O(m) time simply by testing each edge and adding\nthe edge or its reverse to Ef,depending on the current flow. Then we can\nlabel the vertices by If (v) by a breadth-first search from s. This takes time\nO(m), also. At the same time we find d(f) as the length of the shortest\npath from s to t.\nThen, we create E$ by keeping only the edges between successive levels.\nThus all paths between s and t in Ei have length d( f). Now we produce\nflow g by finding as many edge-disjoint s-t paths as possible. We start with\nE' = Ei and we perform a depth-first search from s. If we get stuck, we\nbacktrack and remove edges on the dead-end branches since these are not\nin any s-t path anyway. When we find an s-t path, we set gij = 1along\nthat path, and remove it from E'. We continue searching for paths until\nE' is empty. We spend a constant time on each edge before it's removed,\nwhich is O(m) time total. When we are done, there is no s-t path in E$\nwithout a saturated edge, otherwise it would still be in E'.\n(c) Prove that if the levelled residual network has no path from s to\nt (If (t)= co),then the flow f is maximum.\nSuppose there is a flow f * of greater value. Then f*- f (where the dif-\nference is produced by either decreasing flow along an edge and increasing\nflow in the opposite direction) is a feasible flow in the residual network\nwhich has a positive value. This is easy to see because if f; > fij then\n(i,j) appears in Ef and f; - f, 5 uij - fo which is the capacity of this\nedge in Ef. If f; < fq then fu > 0 and therefore the opposite edge (j,i)\nappears in Ef. Also, fi, - f; 5 fij which is the capacity of (j,i) in Ef.\n\nWhen a non-zero flow exists in Ef, there exists a path from s to t using\nonly edges in Ef. The shortest of these paths would appear in Ef,as well,\nwhich is a contradiction.\n(d) For a flow f , define\nd(f = If (t)\n(the distance from s to t in the residual network). Prove that if\ng is a saturating flow for f then\nwhere f +g denotes the flow obtained from f by either increasing\nthe flow f, by gij or decreasing the flow fji by gij for every edge\n(i,j) E Gf.\nConsider Ef and the labeling of vertices if (v). For every edge (i, j) of Ef\nwe have that if (j)5 If (i)+ 1. Since g is a saturating flow in ~ f , ,the only\nedges (u, v) which are in Ef+, and not in Ef are such that (v, u) E E;,\nwhich implies that lf (v) = lf (u) -I. In summary, every edge (i, j ) of Ef+,\nsatisfies if (j)5 If (i)+ 1 and, furthermore, the edges which are not in\nEf actually satisfy the inequality strictly 1 (j)< lf(i) + 1. Consider now\nany path P in Ef+, Adding up lf(j) 5 lf(i) + 1over the edges of P, we\nget that d(f) 5 IPI. Moreover, we can have d(f) = IPI only if all edges\nof P also belong to Ef, which is impossible since g is a saturating flow.\nHence, d(f) < lPl and this is true for any path P of Ef+, implying that\nd(f) < d(f + g).\n(e) Prove that if f is a feasible 0 - 1 flow with distance d = d(f) and\nf * is an optimum flow, then\nand also\nSuppose f has distance d and f * is an optimal flow. As noted before,\ng = f * - f is a feasible flow in the residual network Ef.\nConsider s-t cuts Cl,C2,... Cddefined by\nck = {(i,j) E Ef : lf(i) 5 k,lf(j) > k).\nThere are at most m edges in total and these cuts are disjoint, therefore\n\nSince the value of g cannot be greater than any s-t cut in Ef,\nm\nvalue(f*) - value(f) = value(g) 5 -.d\nSimilarly, define d + 1sets of vertices Vo,V17V2,.. .,Vd:\n= {i E V : lf(i)= k ) .\nBy double counting,\nSuppose that IVk-l 1 = a, IV,I 5 9 - a. Note that the edges of Ck belong\nto Vk-l x Vk. Therefore\n(f) Design a maximum flow algorithm (for unit capacities) which\nproceeds by finding a saturating flow repeatedly. Try to opti-\nmize its running time. Using the observations above, you should\nachieve a running time bounded by 0(min(mn2l3,m3I2)).\nThe algorithm starts with a zero flow f . Then we repeat the following:\nFind the levelled residual network ~ l f .\nFind a saturating flow g.\na Add g to f , reset the residual network and continue.\nEach iteration takes O(m) time. Since d(f) increases every time and it\ncannot reach more than n (the maximum possible distance in G), the\nrunning time is clearly bounded by O(mn). However, we can improve this.\nSuppose we iterate only d times and our flow after d iterations is f . We\nknow d(f) 2 d, and iff* is an optimal flow,\nBecause the flow increases by at least 1 in each iteration, the remaining\nnumber of iterations is bounded by min{y, $). We choose d in order to\noptimize our bound. It turns out that the best choice is dl = m1I2 for the\nbound based on m and d2 = n2I3for the bound based on n. Thus the total\nrunning time is 0(min{m3I2,mn2I3)).\n(g) Can we now justify that, for 0 - 1 capacities, there is always an\noptimum flow that takes values 0 or 1 on every edge?\nOur algorithm finds a 0 -1flow and we have a proof of optimality, therefore\nthere is always a 0 - 1 optimal flow. This justifies our reasoning in part\n(a)."
    },
    {
      "category": "Assignment",
      "title": "Problem Set 4",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/18565004ba2bba2e5974156903dd320e_ps4.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set 4\nProblem 1. In a 0-sum 2-player game, Alice has a choice of n so-called pure strate\ngies and Bob has a choice of m pure strategies. If Alice picks strategy i and Bob picks\nstrategy j, then the payoff is aij , meaning aij dollars are transfered from Alice to\nBob. So Bob makes money if aij is positive, but Alice makes money if aij is negative.\nThus, Alice wants to pick a strategy that minimizes the payoff while Bob wants a\nstrategy that maximizes the payoff. The matrix A = (aij ) is called the payoff matrix.\nIt is well known that to play these games well, you need to use a mixed strategy--\na random choice from among pure strategies. A mixed strategy is just a particular\nprobability distribution over pure strategies: you flip coins and then play the selected\npure strategy. If Alice has mixed strategy x, meaning he plays strategy i with prob\nability xi, and Bob has mixed strategy y, then it is easy to prove that the expected\npayoff in the resulting game is xT Ay. Alice wants to minimize this expected payoff\nwhile Bob wants to maximize it. Our goal is to understand what strategies each\nplayer should play.\nWe'll start by making the pessimal assumption for Alice that whichever strategy\nshe picks, Bob will play best possible strategy against her. In other words, given\nAlice's strategy x, Bob will pick a strategy y that achieves maxy xT Ay. Thus, Alice\nwants to find a distribution x that minimizes maxy xT Ay. Similarly, Bob wants a y\nto maximize minx xT Ay. So we are interested in solving the following 2 problems:\nP min\nPmax\nx T Ay\nx:\nxi=1,x≥0 y:\nyj =1,y≥0\nPmax\nP min\nx T Ay\ny:\nyj =1,y≥0 x:\nxi=1,x≥0\nUnfortunately, these look like nonlinear programs!\n1. Show that if Alice's mixed strategy is known, then Bob has a pure strategy\nserving as his best response.\n2. Show how to convert each program above into a linear program, and thus find\nan optimal strategy for both players in polynomial time.\n3. Use strong duality (applied to the LP you built in the previous part) to argue\nthat the above two quantities are equal.\nPS4-1\n\nX\nP\nThe second statement shows that the strategies x and y, besides being optimal, are\nin Nash Equilibrium: even if each player knows the other's strategy, there is no point\nin changing strategies. This was proven by Von Neumann and was actually one of\nthe ideas that led to the discovery of strong duality.\nProblem 2. Consider the linear program\nmin\nxj\nj\nsubject to\nX\naij xj ≥ 1 ∀i\nj\nxj ≥ 0 ∀j\nand its dual\nX\nmax\nyi\ni\nsubject to\nX\naijyi ≤ 1\ni\nxi ≥ 0.\nAssume that A = [aij ] is m × n and has only nonnegative entries.\nIn this problem, you'll have to show that a continuous algorithm solves (almost\nmiraculously) the above pair of dual linear programs. We shall define a series of\nfunctions whose argument is the \"time\" and you'll show that some of these functions\ntend to the optimal solution as time goes to infinity. (For simplicity of notation, we\ndrop the dependence on the time.)\n- Initially, we let sj = 0 for j = 1, . . . , n and LB = 0. The vector s will (sort\nof) play the role of primal solution, and LB the role of a lower bound on the\nobjective function.\n- At any time, let\nP\nti = e-\nj aij sj\nfor i = 1, . . . , m. Also, let dj =\ni aij ti for j = 1, . . . , n, D = maxj dj and k\nbe an index j attaining the maximum in the definition of D. The algorithm\ncontinuously increases sk.\nObserve that when sk is increased, the vectors t and d as well as D change also,\nimplying that the index k changes over time.\nPS4-2\n\nP\nP\n1. Let α = mini(\nj aij sj ). Let xj = sj /α for j = 1, . . . , n, yi = ti/D for i =\nP\n1, . . . , m and LB = max(LB, D\nti ). Show that x is primal feasible, y is dual\nfeasible and LB is a lower bound on the optimal value of both primal and dual.\n2. Prove that\nm\nX\nP n\nti ≤ me-\nj=1 sj /LB .\ni=1\nHint: Show that initially the inequality holds and that it is also maintained\nwhenever we have equality.\n3. Deduce from (b) that\ni ti tends to 0 as time goes to infinity.\n4. Using (b), give an upper bound on the value of the primal solution x, and\nusing (c), show that this upper bound tends to LB as time goes to infinity.\nThis shows that as time goes to infinity, both x and y tend to primal and dual\noptimal solutions!\nProblem 3. We would like to find a function f(n) such that, given any set of\nn (possibly negative) numbers, c1,\n, cn, one cannot find more than f(n) subsums\n· · ·\nof these numbers which decrease in absolute value by a factor of at least 2. More\nformally:\nLemma 1 Let c ∈ Rn and yk ∈{0, 1}n for k = 1, . . . , q such that 2|yT\n| ≤|yk\nT c|\nk+1c\nfor k = 1, . . . , q - 1. Assume that yT c = 1. Then q ≤ f(n).\nq\nUsing linear programming, you are asked to prove that f(n) = O(n log n).\n1. Given a vector c and a set of q subsums satisfying the hypothesis of the Lemma,\nwrite a set of inequalities in the variables xi ≥ 0, i = 1 . . . n, such that xi = |ci|\nis a feasible vector, and for any feasible vector x0 there is a corresponding vector\nc0 satisfying the hypothesis of the Lemma for the same set of subsums.\n2. Prove that there must exist a vector c0 satisfying the hypothesis of the Lemma,\nwith c0 of the form (d1/d, d2/d, . . . , dn/d) for some integers d , d1 , . . . , dn =\n2O(n log n) .\n| | |\n|\n|\n|\n3. Deduce that f(n) = O(n log n).\n4. (Not part of the problem set; only for those who find the problem sets too\neasy...) Show that f(n) = Ω(n log n) (as a tiny step, can you find a set of\nnumbers such that f(n) > n?).\nPS4-3\n\nProblem 4. Let K be a bounded convex set in Rn . In this problem, you'll prove\nthat there exists an ellipsoid E contained within K such that if you blow it up by\na factor of n (the dimension) then the corresponding ellipoid contains K; in short,\nE ⊆ K and K ⊆ nE.\n1. Suppose that we have an ellipsoid E(a, A) = {x ∈ Rn : (x - a)T A-1(x - a) ≤ 1}\nand we have a point b /∈ nE(a, A). Argue that the convex hull of b and E(a, A),\nconv({b}, E(a, A)), contains an ellipsoid E0 of larger volume than E(a, A).\n(You do not need to explicitly give a0 and A0 corresponding to E0 = E(a0, A0),\nif that helps. It might be easier to deal with a particular case for a, A and b,\nand argue why you can.)\n2. Argue that the maximum volume ellipsoid E contained in K (it is actually\nunique, although you do not need this) is such that nE ⊇ K.\n3. (Optional. Assume that K = {x ∈ Rn : Cx ≤ d} is bounded, where C is m × n\nand d ∈ Rm . Formulate the problem of finding the largest volume ellipsoid\ncontained within K as a convex program (minimizing a convex function over\na convex set, or maximizing a concave function over a convex set. One could\ntherefore use the ellipsoid algorithm to find (a close approximation to) this\nmaximum volume ellipsoid.)\nProblem 6. Given an undirected graph G = (V, E), a set T ⊆ V with |T | even\nand a weight function w : E\nQ+, the minimum (weight) T -cut problem is to find\nS ⊆ with |S ∩ T | odd1 such that d(S) := w(S : S ) :=\ne∈(S:S ) we is minimized.\n→\nP\n\nHere (S : S) denotes the set of edges of E with exactly one endpoint in S (since our\ngraph is undirected, observe that d(S) = d(S )). For T = {s, t}, the minimum T -cut\nproblem reduces to the minimum s - t cut problem (in an undirected graph). In this\nproblem, you will show that the minimum T -cut problem can be solved efficiently.\n1. Argue that the minimum s - t cut problem in an undirected graph G can be\nsolved efficiently by using an algorithm for a minimum s - t cut problem in a\ndirected graph H.\n\n2. A T - T cut is a cut (S : S) with S ∩ T 6= ∅ and S ∩ T 6= ∅. Show that the\nminimum weight T - T cut can be obtained by solving a polynomial number of\nminimum s - t cut problems. Can you do it with O(|T |) such minimum s - t\ncut computations?\n3. Prove that for any A, B ⊆ V , we have\nd(A) + d(B) ≥ d(A ∩ B) + d(A ∪ B).\n1thus, S ∩ T | = |T \\ S| is also odd.\nPS4-4\n\n4. To solve the minimum T -cut problem, suppose we first solve the minimum\n\nT - T cut problem and obtain the cut (S : S). If |S ∩ T | is odd, we are done\n(right?). If |S ∩ T | is even, use the previous inequality to argue that there exists\n\na minimum T -cut (C : C) such that C ⊆ S or C ⊆ S. Deduce from this an\nefficient algorithm for solving the minimum T -cut problem. How many calls to\nyour minimum T - T cut algorithm are you using?\nPS4-5"
    },
    {
      "category": "Resource",
      "title": "Approximaion Algorithms: MAXCUT",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/0eef690adf10b98bffd65d5415516412_lec18.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nX\nX\n18.415/6.854 Advanced Algorithms\nNovember 19, 2008\nApproximaion Algorithms: MAXCUT\nLecturer: Michel X. Goemans\nMAX-CUT problem\nMAX-CUT Problem: Given a graph G = (V, E) and weights on the edges w : E\nR+, find a\ncut (S : S ), S ⊆ V that maximizes w(S : S ) = P\nS) w(e).\n→\ne∈(S:\nMIN-CUT Problem: find a cut (S : S ) that minimizes w(S : S ).\nThere is a polynomial algorithm for the MIN-CUT problem: use the min s - t cut algorithm\non each pair of vertices (or, better, for a fixed s), and take the smallest of them. However, the\nMAX-CUT problem is NP-hard, and we'll try several ways of designing approximation algorithms\nfor it.\nIdea #1: Local Search\nAlgorithm: Start from any cut (S : S ). Define the neighborhood N(S : S ) of the cut to be the\nMOVE neighborhood: all the cuts that result from moving one vertex from one side of the cut to\nthe other side. Consider a locally maximum cut for this neighborhood.\nLemma 1 If (S : S ) is a local maximum for the MOVE neighborhood, then w(S : S ) ≥ 2 w(E) ≥\n1 OPT .\nProof of lemma 1:\nLook at a vertex i ∈ V . Let Ci be the set of all edges (i, j) ∈ E that are\npart of the cut (S : S ) (that is if i ∈ S then j ∈ S and vice versa). Let Ai be the set of all edges\n(i, j) ∈ E that are not part of the cut (S : S ). Since moving any single vertex i to the other side of\nthe cut does not improve the weight of the cut, we know that:\nw(Ci) ≥ w(Ai).\nSumming over all vertices i, we get:\nw(Ci) ≥\nw(Ai),\ni∈V\ni∈V\nor 2w(S : S ) ≥ 2w(E\\(S : S )). Rearranging, we get:\n4w(S : S ) ≥ 2w(E)\nor\nw(S :\nS) ≥ 1\n2 w(E) ≥ 1\n2 OP T.\n\nRemarks:\n(a) The bound of 1/2 cannot be improved for this MOVE neighborhood: Consider a k-vertex\ncycle, where k is a multiple of 4, as the graph G (with unit weights). The best cut will include\nLec18-1\n\nall edges. However, if we start from a cut in which the edges of the cycle alternate in and out\nof the cut, we have a locally optimum solution with only k/2 edges in the cut.\n(b) The local search algorithm based on the MOVE neighborhood for MAX-CUT takes expo\nnentially many steps in the worst-case. This is true even for graphs that are 4-regular (each\nvertex has exactly 4 neighbors) (Haken and Luby [1]). For 3-regular graphs the algorithm is\npolynomial (Poljak [4]).\n(c) To capture the complexity of local search, Johnson, Papadimitriou and Yannakakis [3] have\ndefined the class PLS (Polynomial Local Search). Members of this class are optimization\nproblems of the form max{f(x) : x ∈ S} together with a neighborhood N : S\n2S . We say\n→\nthat v ∈ S is a local optimum if c(v) = max{c(x) : x ∈ N(v)}. To be in PLS, we need to\nhave polynomial-time algorithms for (i) finding a feasible solution, (ii) deciding if a solution is\nfeasible and if so computing its cost, and (iii) deciding if a better solution in the neighborhood\nN(v) of a solution v exists and if so finding one. They introduce a notion of reduction, and\nthis leads to PLS-complete problems for which any problem in PLS can be reduced to it. Their\nnotion of reduction implies that if, for one PLS-complete problem, one has a polynomial-time\nalgorithm for finding a local optimum then the same true for all PLS problems. In particular,\nMAX-CUT with the MOVE neighborhood is PLS-complete [5]. Furthermore, it follows from\nJohnson et al. [3] that the obvious local search algorithm is not an efficient way of finding\na local optimum for a PLS-complete problem; indeed, for any PLS-complete problem, there\nexist instances for which the local search algorithm of repeatedly finding an improved solution\ntakes exponential time. The result of Haken and Luby above is thus just a special case. Still,\nthis does not preclude other ways of finding a local optimum.\nIdea #2: Random Cut\nAlgorithm: There are 2|V | possible cuts. Sample a cut randomly using a uniform distribution over\nall possible cuts in the graph: ∀v ∈ V, Pr(v ∈ S) = 1 , independently for all vertices v ∈ V .\nLemma 2 This randomized algorithm gives a cut with expected weight that is ≥ 1 OPT .\nProof of lemma 2:\nE[w(S :\nS)] = E[\nX\nw(e)I(e ∈ (S :\nS))] =\nX\nw(e) · P r(e ∈ (S :\nS))\n=\ne∈E\nX\nw(e) · 1\n2 = 1\n2 w(E).\ne∈E\ne∈E\n\nUsing the method of conditional expectations, we can transform this randomized algorithm into\na deterministic algorithm. The basic idea is to use the following identity for a random variable f\nand event A:\nE[f]\n= E[f A]Pr(A) + E[f A ]Pr(A ) = E[f|A]Pr(A) + E[f|A ](1 - Pr(A))\n≤ max\n|\n{E[f|A], E[f|A ]\n|\n}.\nIn our setting, we consider the vertices in a specific order, say v1, v2,\n, and suppose we have\n· · ·\nalready decided/conditioned on the position (i.e. whether or not they are in S) of v1,\n, vi-1.\n· · ·\nNow, condition on whether vi ∈ S. Letting f = w(S : S ), we get:\nE[f|{v1, · · · , vi-1} ∩ S = Ci-1]\n≤ max(E[f|{v1, · · · , vi-1} ∩ S = Ci-1, vi ∈ S], E[f|{v1, · · · , vi-1} ∩ S = Ci-1, vi ∈/ S]).\nLec18-2\n\nX\nX\nX\nX\nBoth terms in the max can be easily computed and we can decide to put vi on the side of the cut\nwhich gives the maximum, i.e. we set Ci to be either Ci-1 or Ci-1 ∪{vi} in such a way that:\nE[f|{v1, · · · , vi-1} ∩ S = Ci-1 ≤ E[f|{v1, · · · , vi} ∩ S = Ci].\nWhen we have processed all inequalities, we get a cut (Cn : C n) such that\n2 w(E) ≤ E[f] ≤ w(Cn : C n),\nand this provides a deterministic 0.5-approximation algorithm.\nExamining this derandomized version more closely, we notice that we will place vi on the side\nof the cut that maximizes the total weight between vi and the previous vertices {v1, v2,\n, vi-1}.\n· · ·\nThis is therefore a simple greedy algorithm.\nRemarks:\n(a) The performance guarantee of the randomized algorithm is no better than 0.5; just consider\nthe complete graph on n vertices with unit weights. Also, the performance guarantee of the\ngreedy algorithm is no better than 0.5 int he worst-case.\nIdea #3: LP relaxation\nAlgorithm: Start from an integer-LP formulation of the problem:\nSince we have a variable xe for each edge (if xe = 1 than e ∈ (S : S)), we need the second type of\nmax\nw(e)xe\ne∈E\ns.t.\nxe ∈ {0, 1} ∀e ∈ E\nX\nX\ne∈F\ne∈C\\F\nxe +\n(1 - xe) ≤ |C| - 1\nX\nX\n∀cycle C ⊆ E ∀F ⊆ C, |F | odd\n⇔\ne∈F\ne∈C\\F\nxe -\nxe ≤ |F | - 1 ∀cycle C ⊆ E ∀F ⊆ C, |F | odd\n\nconstraints to guarantee that S is a legal cut. The validity of these constraints comes from the fact\nthat any cycle and any cut must intersect in an even number of edges. even number of edges that\nare in the cut.\nNext, we relax this integer program into a LP:\nmax\nw(e)xe\ne∈E\ns.t.\n0 ≤ xe ≤ 1 ∀e ∈ E\nxe -\nxe ≤|F | - 1 ∀cycle C ⊆ E ∀F ⊆ C, |F | odd.\ne∈F\ne∈C\\F\nThis isa relaxation of the maximum cut problem, and thus provides an upper bound on the value\nof the optimum cut. We could try to solve this linear program and devise a scheme to \"round\" the\npossibly fractional solution to a cut.\nRemarks:\nLec18-3\n\n(a) This LP can be solved in a polynomial time. One possibility is to use the ellipsoid algorithm\nas the separation problem over these inequalities can be solved in polynomial time (this is not\ntrivial). Another possibility is to view the feasible region of the above linear program as the\nprojection of a polyhedral set Q ⊆ Rn 2 with O(n3) number of constraints; again, this is not\nobvious.\n(b) If the graph G is planar, then all extreme points of this linear program are integral and\ncorrespond to cuts. We can therefore find the maximum cut in a planar graph in polynomial\ntime (there is also a simpler algorithm working on the planar dual of the graph).\n(c) There exist instances for which OP T ∼ 1 (or ∃G = (V, E), w(e) = 1, OPT ≤ n( 1 + ), LP ≥\nLP\nn(1 - )), which means that any rounding algorithm we could come up with will not guarantee\na factor better than 1 .\nIdea #4: SDP relaxation\nThe idea is to use semidefinite programming to get a more useful relaxation of the maximum cut\nproblem. This is due to Goemans and Williamson [2].\nInstead of defining variables on the edges as we did in the previous section, let's use variables on\nthe vertices to denote which side of the cut a given vertex is. This leads to the following quadratic\ninteger formulation of the maximum cut problem:\nmax\nX\nw(i, j)1 - yiyj\n(i,j)∈E\ns.t.\nyi ∈{1, -1}n ∀i ∈ V.\nHere we have defined a variable yi for each vertex i ∈ V such that yi = 1 if i ∈ S and yi = -1\notherwise. We know that an edge (i, j) is in the cut (S : S ) iff yiyj = -1, and this explains the\nquadratic term in the objective function.\nWe can rewrite the objective function in a slightly more convenient way using the Laplacian of\nthe graph. The Laplacian matrix L is defined as follows:\n⎧\n⎪0\n(i, j) /\n⎨\n∈ E\nlij =\n-w(i, j)\ni = j, (i, j) ∈ E\n⎪P\n⎩\n=i w(i, k) i = j.\nk:k\nthat is, the off-diagonal elements are the minus the weights, and the diagonal elements correspond\nto the sum of the weights incident to the corresponding vertex. Using the Laplacian matrix, we can\nrewrite equivalently the objective function in the following way:\nn\nX n\nX\nn\nX\nX\nX\ny T Ly =\nyiyj lij =\ny 2\ni\nw(i, k) -\nyiyj w(i, j)\ni=1 j=1\ni=1\nk6=i\n⎛\n(i,j)∈E\n⎞\n= 2w(E) -\nX\nyiyj w(i, j) = 4 ⎝ X\nw(i, j) 1 - yiyj\n⎠ ,\n(i,j)∈E\n(i,j)∈E\nand thus\nX\nw(i, j) 1 - yiyj\n= 1\n4y T Ly.\n(i,j)∈E\nLec18-4\n\nThus the maximum cut value is thus equal to\nmax{ 4 y T LY : y ∈{0, 1}n}.\nIf the optimization was over all y ∈ Rn with ||y||2 = n then we would get that\nn\nmax{ 4y T LY : y ∈ Rn , ||y||2 = n} = 4 λmax(L),\nwhere λmax(L) is the maximum eigenvalue of the matrix L. This shows that OPT ≤ n λmax(L);\nthis is an eigenvalue bound introduced by Delorme and Poljak.\nUsing semidefinite programming, we will get a slightly better bound. Using the Frobenius inner\nproduct, we can again reformulate the objective function as:\ny T Ly = L (yy T ),\n4 -\nor as\n1 L\nY\n4 -\nif we define Y = yyT . Observe that Y 0, Y has all 1's on its diagonal, and its rank is equal to\n1. It is easy to see that the coverse is also true: if Y 0, rank(Y ) = 1 and Yii = 1 for all i then\nY = yyT where y ∈ {-1, 1}n . Thus we can reformulate the problem as:\nmax\nL\nY\n4 -\ns.t.\nrank(Y ) = 1,\n∀i ∈ V : Yii = 1,\nY 0.\nThis is almost a semidefinite program except that the rank condition is not allowed. By removing\nthe condition that rank(Y ) = 1, we relax the problem to a semidefinite program, and we get the\nfollowing SDP:\nSDP = max\nL\nY\n4 -\ns.t.\n∀i ∈ V : Yii = 1,\nY 0.\nObviously, by removing the condition that rank(Y ) = 1 we only increase the space on which we\nmaximize, and therefore the value (simply denoted by SDP ) to this semidefinite program is an\nupper bound on the solution to the maximum cut problem.\nWe can use the algorithms we described earlier in the class to solve this semidefinite program\nto an arbitrary precision. Either the ellipsoid algorithm, or the interior-point algorithms for conic\nprogramming. Remember that semidefinite programs were better behaved if they satisfied a regular\nity condition (e.g., they would satisfy strong duality). Our semidefinite programming relaxation of\nMAXCUT is particularly simple and indeed satisfies both the primal and dual regularity conditions:\n(a) Primal regularity conditions ∃Y 0 s.t. Yii = 1 ∀i. This condition is obviously satisfied\n(consider Y = I).\nLec18-5\n\nX\nX\n(b) Dual regularity condition: First consider the dual problem\nmin\nzi\ni∈V\nz1\n...\nz2\n...\n. . .\n. . .\n.. .\n. . .\n...\nzn\n⎛\n⎞\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎠- L 0,\ns.t.\nwhere zi ∈ R for all i\nz1\n...\nz2\n...\n⎞\n⎛\n∈ V . The regulation condition is that there exist zi's such that\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎠- L 0. This is for example satisfied if, for all i, zi > λmax(L).\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\nzn\nRemark: If we add the condition that z1 = z2 = ... = zn to the dual then the smallest value zi\ncan take is equal to λmax(L), and we derive that:\nn\nOPT ≤ SDP ≤ 4 λmax(L),\nand therefore this SDP bound improves upon the eigenvalue bound.\nWe will start the next lecture by proving the following theorem.\nTheorem 3 ([2]) For all w ≥ 0, we have that OP T ≥ 0.87856.\nSDP\nIn order to prove this theorem, we will propose an algorithm which derives a cut from the solution\nto the semidefinite program. To describe this algorithm, we first need some preliminaries. From the\nCholesky's decomposition, we know that:\nY 0 ⇔\n∃V ∈ Rk×n , k = rank(Y ) ≤ n, s.t. Y = V T V\n⇔\n∃v1, ..., vn s.t. Yij = v T vj , vi ∈ Rn .\ni\nTherefore, we can rewrite the SDP as a 'vector program':\nmax\nw(i, j)1 - vi\nT vj\nTo be continued...\ns.t.\n(i,j)∈E\n∀i ∈ V :\n∀i ∈ V :\nkvik = 1\nvi ∈ Rn .\nReferences\n[1] A. Haken and M. Luby, \"Steepest descent can take exponential time for symmetric connection\nnetworks\", Complex Systems, 1988.\n[2] M.X. Goemans and D.P. Williamson, Improved Approximation Algorithms for Maximum Cut\nand Satisfiability Problems Using Semidefinite Programming, J. ACM, 42, 1115-1145, 1995.\n[3] D.S. Johnson, C.H. Papadimitriou and M. Yannakakis, \"How easy is local search\", Journal of\nComputer and System Sciences, 37, 79-100, 1988.\nLec18-6\n\n[4] S. Poljak, \"Integer Linear Programs and Local Search for Max-Cut\", SIAM J. on Computing,\n24, 1995, pp. 822-839.\n[5] A.A. Sch affer and M. Yannakakis, \"Simple local search problems that are hard to solve\", SIAM\nJournal on Computing, 20, 56-87, 1991.\nLec18-7"
    },
    {
      "category": "Resource",
      "title": "Fibonacci heaps",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/16cda5a1eae18c7a26ec0c0498966dde_lec1.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 3rd, 2008\nFibonacci heaps\nLecturer: Michel X. Goemans\nIntroduction\nToday we will describe Fibonacci heaps, a data structure that provides a very ecient implementa-\ntion of a priority queue. By priority queue we mean a data structure that stores a set S of elements\nwhere with each element s we associate a key k(s) being priority of that element. Now, we want\nthe queue to handle three operations on set S:\ninsert Adding a new element s0 with a key k(s0) to S\n-\nextract-min Returning an element s∗ of S having minimal key and removing s∗ from S\n-\ndecrease-key Replacing the value of a key of some element s by a new, smaller value.\n-\nThe motivation behind the search for fast implementation of priority queues can be observed on\nthe example of two classical graph problems: Single-source Shortest Paths and Minimum Spanning\nTree.\n1.1\nSingle-source Shortest Paths problem\nWe are given a directed graph G = (V, E), some vertex s ∈ V and a length function l : E\nR+ on\n→\nthe arcs. Observe that we impose that the lengths are nonnegative. Now, for each vertex v ∈ V we\nwant to compute the length ds(v) of the shortest path from s to v.\nThe classical solution for this problem is Dijkstra's algorithm. The algorithm is:\n1. Maintain a priority queue containing some subset S of vertices of G with keys k(v). Initially,\nS = V , k(s) = 0 and k(v) = +inf.\n2. As long as S is nonempty:\n- Extract a vertex u from S with minimum key. Output k(u) as the value of ds(u).\nFor each out-neighbor v ∈ S of u, we update (i.e. possibly decrease) the key k(v) of v to\n-\nbe min{k(v), k(u) + l((u, v))}.\nIn this algorithm, k(v) represents the length of the shortest path from s to v using only intermediate\nvertices not in S, and represents ds(v) when extracted. The algorithm can be adapted to output\nthe shortest paths.\n1.2\nMinimum Spanning Tree problem\nGiven an undirected graph G = (V, E) and a weight function w : E\nR on edges, we would like a\n→\nspanning tree of G of minimal weight. Surprisingly, one of the classical solutions of this problem -\nPrim's algorithm - is very similar to the approach of Dijkstra's algorithm for Single-source Shortest\nPath problem. The algorithm is as follows:\nlect-1\n\n-\n1. Maintain a priority queue containing some subset S of vertices of G with keys k(v) and a tree\nT spanning V \\S. Initially, T = ∅, S = V , k(s) = 0 for some arbitrary vertex s and k(v) = +inf\nfor v = s.\n2. As long as S is nonempty:\nExtract a vertex u from S with minimum key. If u = s (rst iteration), add to T the\ncorresponding edge (i.e. the minimum-weight edge connecting u to T of weight k(u)).\n- For each neighbor v /∈ S of u, we update (i.e. possibly decrease) k(v) to be min{k(v), w((u, v))}.\n1.3\nNumber of priority queue operations\nWe will not prove the correctness of the algorithms. However, for the sake of the running time\nanalysis that we will do later, we notice that in both cases the algorithm uses |V | insert operations,\n|V | extract-min operations and, since each edge can enforce at most one decrease-key operation, at\nmost |E| decrease-key operations.\nBinary heaps\nThe classical implementation1 of priority queues are binary heaps. A binary heap T is a binary tree\nwhose nodes correspond to elements of the set S and has two properties:\nit is almost complete i.e. if T has depth h then it has exactly 2i vertices on depth i if i < h\n-\nand the last level is lled from the left.\nheap-ordering: the key of every child is not smaller than the key of its parent.\n-\nKeeping this properties in mind it is relatively easy (see [CLRS]) to develop procedures for\ninserting, extracting the minimal element and decreasing the key that execute any of these operations\nin O(log n) time where n is the number of items in the priority queue. Therefore, since the number\nof elements is at most |V | in our applications, we obtain the total running time of both algorithms to\nbe O((|V |+|E|) log |V |). Obviously, |E| ≥|V | in case of connected graphs and therefore the running\ntime is dominated by the O(|E| log |V |) term corresponding to decrease-key operations. The question\nis: can we do better ?\nd-ary heaps\nOne of the ideas to get a better running time is increasing the arity of the tree that we are using. If we\nuse a d-ary tree instead of a binary one then we reduce the depth of our tree and thus our inserts and\nbottlenecking decrease-key operations execute in O(logd |S|) time. On the other hand, the execution\nof extract-min operation requires O(d logd |S|) time. So, by choosing the best possible d = d|E|/|V |e\nwe get the total running time of our algorithms to be O((|E|+d|V |) logd |V |) = O(|E|logd|E|/|V |e|V |),\nwhich is a signicant improvement for dense graphs. However, it turns out that we can do even\nbetter. Namely, we can implement priority queue in such a way that from the point of view of running\ntime analysis of our algorithms the cost of decrease-key will be constant and costs of insert and\nextract-min will be logarithmic. This leads to the essentially optimal O(|E| + |V | log |V |) running\ntime of both algorithms and for some present-day applications (think graphs with billions of edges)\nthis improvement can make a huge dierence.\n1 A comprehensive coverage of binary heaps (as well as Fibonacci heaps) can be found in [CLRS].\nlect-2\n\nFibonacci heaps\nThe Fibonacci heaps were proposed by Fredman and Tarjan in 1984 giving a very ecient imple-\nmentation of the priority queues. The main motto of this construction is laziness - we do work only\nwhen we must, and then use it to simplify the structure as much as possible so that the future work\nis easy. This way, we enforce that any sequence of operations has to contain a lot of cheap ones\nbefore we need to do something computationally expensive - the formalization of this intuition will\nbe given later.\n4.1\nConstruction\nA Fibonacci heap consists of a collection of heap-ordered trees (of variable arity) with following\nproperties:\n1. nodes of the trees correspond to elements being stored in the queue,\n2. roots of heap-ordered trees are arranged in a doubly-linked list,\n3. we keep a pointer to the root of a tree that corresponds to the element with minimum key\n(note that heap-ordering of the trees implies that such minimum element has to be a root of\nsome tree),\n4. for each node we keep track of its rank (degree), i.e. the number of its children, as well as\nwhether it is marked (the purpose of marking will be dened later on),\n5. size requirement: if a node u has rank k then the subtree rooted at u has at least Fk+2 nodes,\nwhere Fi is the i-th Fibonacci number, i.e. F0 = 0, F1 = 1 and Fi = Fi-1 + Fi-2 for i ≥ 2.\nWe proceed now to describing how do we perform priority queue operation on our Fibonacci\nheap.\n4.1.1\ninsert\nInserting is very simple. We just add the new element s as a new heap-ordered tree to our collection\nand check whether k(s) is smaller that the current minimum for the queueif so then we change\nthe pointer to the minimum accordingly (see Figure 1).\n4.1.2\ndecrease-key\nWhen we decrease the key of an element s, if the heap-ordering is still satised then we do not need\nto do anything else. Otherwise, we just cut s out of the tree in which it resides and put it as a root\nof a new tree in our collection (note that all the descendants of s are now in this new tree as well).\nWe compare the new key of s and the previously minimum key and change the pointer accordingly\n(see Figure 1).\nThis way we end up with something that looks like a desired Fibonacci heap. However, the\nproblem with simply cutting each such s is that, when we perform in this manner many decrease-\nkey operations, we may end up violating the size requirement that we wanted to preserve. Therefore,\nto alleviate this issue we introduce an additional rule that when we cut s we check whether its parent\nis marked. If so then we cut the parent as well (and we unmark it). Otherwise, we just mark the\nparent. Note that we do this cutting recursively, so if the parent of marked parent of s is also marked\nthen we cut it as well, and so on. Obviously, if we cut a root we are not doing anything, and so it\nis useless to mark a root. This (potentially cascading) cutting procedure therefore always ends.\nlect-3\n\nFigure 1: Illustration of: (left side) inserting a new element to the Fibonacci heap; (right side)\ncutting a vertex in the rst step of decrease-key operation. In both examples we assumed that\nthe newly created root has smaller key than the keys of all the other elements.\n4.1.3\nextract-min\nFinally, we can describe extracting the minimum element s∗. We start with removing s∗ (recall that\nwe stored the pointer to it) and putting all the children of s∗ as roots of new trees in our collection.\nNext, we scan the entire list of roots in our collection to nd the new minimum element and we set\nthe relevant pointer accordingly.\nIn principle at this point we could be done, because we obtain once again a valid Fibonacci\nheap. However, it is not hard to see that so far executing of any of our queue operations makes the\nlist longer and longer. So, going through the whole list of roots during extract-min can be very\nexpensive computationally. Therefore, in the spirit of laziness, if we have to do this work anyway\nthen we can use this opportunity to do some cleaning as well, and avoid in this way the necessity of\ndoing the whole work again when doing the next extract-min . What we do is, as long as there\nare two trees whose roots have the same rank, say k, we merge these trees to obtain one tree of rank\nk + 1. Merging consist of just comparing the keys of the roots and setting the root of the tree with\nlarger key as a new child of the other root (see Figure 2). Note that since merging can introduce a\nsecond tree of rank k + 1 in the collection, one root can take part in many merges.\n4.2\nRunning-time Analysis\nNow we want to analyze the worst-case performance of the described Fibonacci heap data structure.\n4.2.1\nA worst-case example\nLet's imagine the following scenario: We do n consecutive insert operations into the Fibonacci\nheap such that it is a circular linked list containing all elements as singleton heaps. If we perform an\nextract-min operation on this Fibonacci heap, this operation will have to go through the entire\nlist to determine the new minimum. This takes O(n) time an unbearable performance for just\none operation.\nlect-4\n\nFigure 2: Illustration of merging of two trees of the same rank.\n4.2.2\nAre Fibonacci heaps useless?\nDoes this mean that Fibonacci heaps are inecient? No! Intuitively such heavy operations can occur\nonly very rarely and make no big contribution to the overall running time of an algorithm using the\nheap. Being not able to give worst-case performance guarantees for each individual operation, we\nwant to consider a sequence of operations and give a proof that, for any such sequence, the total\nrunning time is small, in the sense that this running time can be apportioned between the individual\noperations so that each has a small contribution. This type of analysis is called amortized analysis\n[CLRS]. More precisely, if we have ` dierent types of operations and we claim that the amortized\nrunning time of an operation of type j is at most tj , this means that for any sequence of operations\ncomposed of kj operations of type j for all j = 1,\n, ` (with operations of dierent types interlaced\n· · ·\nP\nin any way), the total running time is upperbounded by\nj kj tj .\n4.2.3\nExcursion: Amortized Analysis via the Potential Method\nThe most common way to perform amortized analysis is using the potential method. The idea of\nthe potential method allows cheap operations to save up time for the use of heavy operations. This\nfunctions like a bank account with time deposited in it. The potential function Φ represents the\nbalance in the account. Initially, the balance is zero, and remains nonnegative during the whole\nsequence. Now operations are performed having costs (i.e. running times) of c1, c2, c3, ..., ck. Every\noperation is allowed to either pay more than its actual cost ci thereby increasing its amortized cost,\nplacing the credit/savings in the bank account thus increasing the balance Φ, or pay less than the\nactual cost by withdrawing the dierence from Φ. This gives the amortized cost.\nOften one can think of the potential function as a measurement of the complexity of the data\nstructure or conguration within an algorithm. In this case cheap operations are allowed to increase\nthe internal complexity, while operations which simplify or clean up the data are allowed to take\nmore time.\nMaking this formal, a potential function, Φ, maps a conguration Di of an evolving algorithm\nor data structure D into a nonnegative number. The start conguration is normalized to have the\nvalue 0: Φ0 = Φ(D0) = 0. Consider a sequence of operations o1, o2, o3, ..., ok and let Di be the\nconguration of the data structure after performing the ith operation. We impose that the potential\nlect-5\n\nX\nX\nX\nX\nX\nX\nX\nfunction remains nonnegative throughout:\n∀t : Φt = Φ(Dt) ≥ 0.\nIf operation oi has cost (running time) ci then its amortized cost is dened by:\nai = ci + ΔΦi = ci + Φi - Φi-1.\nGiven this, it is easy to see that the sum of the amortized costs upperbounds the original total cost:\nk\nk\nk\nk\nai =\n(ci + Φi - Φi-1) =\nci + Φk - Φ0 ≥\nci.\ni=1\ni=1\ni=1\ni=1\nThus amortized analysis provides an upper bound on the worst-case cost of any sequence of opera-\ntions.\nThe diculty in performing amortized analysis is in choosing the right potential function.\n4.2.4\nFibonacci heaps obey the size requirement\nThe rst important observation regarding the heap-ordered trees in a Fibonacci heap is that the\nrestriction to cut o at most one child prevents cutting down the nice binomial tree like structure\nbuilt up through the combination steps. This guarantees that the size requirement we want to have\nis preserved.\nLemma 1 Consider a node x with rank (number of children) d. Let y1, y2, ..., yd be those children\nin the order they were added to the tree. Then every child yi has rank at least i - 2.\nProof:\nWhen yi was added to x, at least the i - 1 children y1 to yi-1 were present. Since only\nroots of the same rank get combined, yi had at least i - 1 children at this time. At most, one of\nthese children could have been cut away since otherwise yi would have qualied for a cascading cut.\nThus yi has at least i - 2 children.\n\nA simple counting argument given in the next lemma reveals that the number of nodes in a\nsubtree rooted at a node of rank d is at least Fd+2. This exponential growth upperbounds the heap\ndegrees to be logarithmic.\nLemma 2 Let N (d) be the smallest possible number of nodes in a subtree rooted at a node of rank\nd. Then N(d) ≥ Fd+2. Thus, the rank of any node in a Fibonacci heap with n elements is O(log n).\nProof:\nFor N, it holds that N(0) = 1, N(1) = 2 and we have the recurrence relation:\nd\nN(d) ≥ 2 +\nN(i - 2)\ni=2\nbecause of Lemma 1 (counting one for the root, one for the rst child y1 and N(i - 2) for each\nremaining child yi). Proceeding by induction on d (thus assuming that N(j) ≥ Fj+2 for j < d), we\nget that\nd\nd\nN(d) ≥ 2 +\nFi = 1 +\nFi.\ni=2\ni=0\nThe right-hand-side is Fd+2; this can be shown again by induction on d: 1+ P\ni\nd\n=0 Fi = Fd+1 + Fd =\nFd+2. Thus we have shown the rst part of the lemma that N (d) ≥ Fd+2.\nUsing the closed-form expression for the Fibonacci numbers, we get that\n⎛\n⎞\n⎝\n⎠\nN(d) ≥ Fd+2 = √1\n\n1 +\n√\n!d+2\n-\n\n1 -\n√\n!d+2\n≥ 1.61d .\nSince N(d) ≤ n, all ranks of nodes in the heap are at most log1.61 n.\n\nlect-6\n\n4.2.5\nAmortized Analysis of the Fibonacci heap operations\nEach individual adding, combining and cutting step takes only O(1) time. Thus the only two critical\nsituations occur when we have to search through many roots for nding the minimum and when we\nhave a long chain of cascading cuts. The length of a cascading cut corresponds to the number of\nnodes being unmarked. With this intuition, we choose the potential function to be\nΦt = rt + 2mt\nwhere rt is the number of roots and mt the number of marked nodes at time t. The reason for the\nfactor of 2 will become clear in the analysis. Here is the amortized analysis of each operation.\ninsert\n-\nInserting a new root in the list takes ct = O(1) time and increases the number of roots\nrt = rt-1 + 1 by one. Thus the amortized cost for an insert operation is also constant:\nat = ct + (rt - rt-1) + 2(mt - mt-1) = O(1) + 1 + 0 = O(1).\nextract-min\n-\nDuring an extract-min operation, we start with rt roots, cut away the minimum root (say\nof rank d) leaving rt-1 + d - 1 roots in the list. These get combined to rt roots, having\ndierent ranks. Since, by Lemma , the maximum possible rank is O(log n), there are in the\nend only rt = O(log n) roots left. Since the cut and each of the combining steps takes O(1)\ntime and eliminates one root the actual time spend on an extract-min operation is at most\nct = rt-1+d-1 units (where the 'unit' may need to be redened to take into account constants).\nPutting this together the amortized cost for an extract-min operation is logarithmic:\nat = ct + (rt - rt-1) + 2(mt - mt-1) = (rt-1 + d - 1) + (rt - rt-1) + 0 = rt + d - 1 = O(log n).\ndecrease-key\n-\nLet's assume that during a decrease-key operation we do k cuts, k ≥ 1. Each (but the rst)\ncut unmarks a node and each cut introduces a new root. Thus the increase in the number\nof roots, rt - rt-1, is equal to the number k of cuts performed. The decrease mt-1 - mt of\nmarked nodes is either k - 1 or k (depending on whether the node itself was marked); thus, in\nany case, mt-1 - mt ≥ k - 1. The key decreasing, cutting and reinserting takes 1 + k units of\ntime (redening the unit, if needed), and thus its amortized cost is:\nat = ct + (rt - rt-1) + 2(mt - mt-1) ≤ 1 + k + k - 2(k - 1) = O(1).\nThis last relation justies the constant 2 in the denition of the potential function.\nSummarizing, in a Fibonacci heap, every insert and decrease-key takes O(1) amortized time,\nand every extract-min takes O(log n) amortized time.\n4.2.6\nUsing Fibonacci heaps to speed up Prim's and Dijkstra's algorithm\nUsing Fibonacci heaps in the two algorithms mentioned in the introduction leads to improved running\ntimes of O(|E| + |V | log |V |).\nlect-7\n\nReferences\n[CLRS] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Cli Stein. Introduction\nto Algorithms (Second Edition). MIT Press and McGraw-Hill.\nlect-8"
    },
    {
      "category": "Resource",
      "title": "Goldberg-Tarjan Min-Cost Circulation Algorithm",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/6c53462d9af606e71fa8950a51dfccfa_lec4.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 15, 2008\nGoldberg-Tarjan Min-Cost Circulation Algorithm\nLecturer: Michel X. Goemans\nIntroduction\nIn this lecture we shall study Klein's cycle cancelling algorithm for finding the circulation of minimum\ncost in greater detail. We will pay particular attention to the choice of cycle to cancel and we will\nrigorously prove two bounds on the number of iterations required, the first of which depends on the\nmagnitude of the cost and is valid only for integer-valued costs, and the second of which is strongly\npolynomial and works even for irrational costs.\nRecall from last time that for a given circulation f, the following are equivalent:\ni. f is of minimum cost\nii. There is no negative cost cycle in the residual graph Gf\niii. There exist potentials p : V ! R such that the reduced costs\ncp(v, w) = c(v, w) + p(v) - p(w) 0\nfor all (v, w) ≥ Ef , where Ef = {e : uf (e) > 0}.\nKlein's cycle cancelling algorithm\nAlgorithm 1 Kleins-Cycle-Cancel(Gf )\nLet f be any circulation (e.g., f = 0)\nwhile there exists a negative cost cycle ≥ Gf do\nPush (f) = min uf (v, w) along\n(v,w)2\nend while\nIt is important to note that the Ford-Fulkerson algorithm for the maximum flow problem is a\nspecial case of Klein's cycle cancelling algorithm, by defining zero costs for all edges in the original\ngraph and by adding an extra edge from the sink to the source with cost -1.\n2.1\nChoice of cycle\nAs in the Ford-Fulkerson algorithm, the question is which negative-cost cycle to choose.\n1. (Weintraub 1972). One idea is to try choosing the maximum improvement cycle, where\nthe difference in cost is as large as possible. One can show that the number of iterations is\npolynomial for rational costs, but finding such a cycle is NP-hard. For irrational costs, one\ncan show that this algorithm may never terminate (Queyranne 1980) even for the maximum\nflow problem (the fattest augmenting path algorithm of Edmonds and Karp), although the\nsolution converges to a minimum cost flow.\nlect-1\n\n||\n||\n2. (Goldberg-Tarjan 1986). Alternatively, we can choose the cycle of minimum mean cost,\ndefined as follows:\nμ(f) =\nmin\ndirected cycles ≥ Gf\nc()\n||\nP\nwhere c() =\n(v,w)2 c(v, w) and || is the number of edges in the cycle.\nNotice that there exists a negative cost cycle in Gf if and only if μ(f) is negative.\nTo see that we can indeed find the minimum mean-cost cycle efficiently, suppose we replace the\ncosts c with c0 such that c0(v, w) = c(v, w) + for each edge (v, w). Then μ0(f) = μ(f) + ,\nso if = -μ(f) then we would have μ0(f) = 0. In particular,\nμ(f) = - inf{ : there is no negative cost cycle in Gf with respect to costs c + }.\nFor any , we can decide if there is a negative cost cycle by using the Bellman-Ford algorithm.\nNow, perform binary search to find the smallest for which no such cycle exists. In the next\nproblem set we will show a result by Karp, which finds the cycle of minimum mean cost in\nO(nm) time by using a variant of Bellman-Ford.\n2.2\nBounding the number of iterations\nWe will give two bounds on the number of iterations for the algorithm. The first depends on the\nmagnitude of the cost and is valid only for integer-valued costs; it is polynomial but not strongly\npolynomial. The second bound is strongly polynomial and works even for irrational costs.\nWe first need a measure of 'closeness' to the optimal circulation. The following definition gives\nsuch a measure, and will be key in quantifying the progress of the algorithm.\nDefinition 1 (Relaxed optimality) A circulation f is said to be -optimal if there exists a po\ntential p : V ! R such that cp(v, w) - for all edges (v, w) ≥ Ef .\nNote that an 0-optimal circulation is of minimum cost.\nDefinition 2 For a circulation f, let\n(f) = min{ : f is -optimal}.\nOne important thing about this that we will prove soon is that when we push some flow in a\ncirculation f along some cycle and obtain a new circulation f 0, we get that (f 0) (f). This means\nthat is monotonically non-increasing in general. First, we need the following strong relationship\nbetween (f) and μ(f), and this really justifies the choice of cycle of Goldberg and Tarjan.\nTheorem 1 For all circulations f, (f) = -μ(f).\nProof:\nWe first show that μ(f) -(f). From the definition of (f) there exists a potential\np : V ! R such that cp(v, w) -(f) for all (v, w) ≥ Ef . For any cycle ≤ Ef the cost c() is\nequal to the reduced cost cp() since the potentials cancel. Therefore c() = cp() -||(f) and\nc()\nso || -(f) for all cycles . Hence μ(f) -(f).\nNext, we show that μ(f) -(f). For this, we start with the definition of μ(f). For every\nc()\ncycle ≥ Ef it holds that || μ(f). Let c0(v, w) = c(v, w) - μ(f) for all (v, w) ≥ Ef . Then,\nc ()\nc()\n=\n- μ(f) 0 for any cycle . Now define p(v) as the cost of the shortest path from an\nadded source s to v with respect to c0 in Gf (see Fig. 1); the reason we add a vertex s is to make sure\nthat every vertex can be reached (by the direct path). Note that the shortest paths are well-defined\nsince there are no negative cost cycles with respect to c0 . By the optimality property of shortest\nlect-2\n\nc'(v,w)\ns\nv\nw\nFigure 1: p(v) is the length of the shortest path from s to v.\npaths, p(w) p(v) + c0(v, w) = p(v) + c(v, w) - μ(f). Therefore cp(v, w) μ(f) for all (v, w) ≥ Ef\nwhich implies that f is -μ(f)-optimal and thus (f) -μ(f).\nBy combining μ(f) -(f) and (f) -μ(f) we conclude (f) = -μ(f) as required.\n\nThe nature of the algorithm is to push flow along negative cost cycles. We would like to know if\nthis actually gets us closer to optimality. This is shown in the following remark.\nRemark 1 (Progress) Let f be a circulation. If we push flow along the minimum mean cost cycle\nin Gf and obtain circulation f 0 then (f) (f 0).\ncp()\nc()\nProof: By definition\n|| = || = μ(f). Now, (f) = -μ(f) implies that there exists a potential\np such that cp(v, w) μ(f) for all (v, w) ≥ Ef . Furthermore for all (v, w) ≥ the reduced cost\ncp(v, w) = μ(f) = -(f). If flow is pushed along some arcs may be saturated and disappear from\nthe residual graph. On the other hand, new edges may be created with a reduced cost of +(f). More\nformally, Ef 0 ≤ Ef →{(w, v) : (v, w) ≥ }. So for all (v, w) ≥ Ef 0 it holds that cp(v, w) -(f).\nThus we have that (f 0) (f).\n\n2.3\nAnalysis for Integer-valued Costs\nWe now prove a polynomial bound on the number of iterations for an integer cost function c : E ! Z.\nAt the start, for any circulation, the following holds for all (v, w) ≥ E:\n(f) C = max |c(v, w)|.\n(v,w)2E\nNow we can continue with the rest of the analysis.\nLemma 2 If costs are integer valued and (f) < n\n1 then f is optimal.\nProof: Consider -(f) = μ(f) > - n\n1 . For any cycle ≥ Gf we have c() = cp() > - n\n1 || -1.\nSince the cost is an integer, c() 0. By the optimality condition, if there is no negative cycle in\nthe graph, the circulation is optimal.\n\nLemma 3 Let f be a circulation and let f 0 be the circulation after m iterations of the algorithm.\nThen (f 0) (1 - n\n1 )(f).\nProof:\nLet p be the potential such that cp(v, w) -(f) for all (v, w) ≥ Ef and let i and fi\nbe the cycle that is cancelled and the circulation obtained at the ith iteration, respectively. Let\nA be the set of edges in Efi such that cp(v, w) < 0 (we should emphasize that this is for the p\ncorresponding to the circulation f we started from). We now show that as long as i ≤ A, then\n|A| strictly decreases. This is because cancelling a cycle removes at least one arc with a negative\nreduced cost from A and any new arc added to Efi must have a positive reduced cost. Hence after\nlect-3\n\n||\nk m iterations we will find an edge (v, w) ≥ k+1 such that cp(v, w) 0. So by Theorem 1, -(fk)\nis equal to the mean cost of k+1 and thus\nc(k+1)\ncp(k+1)\n(fk) = -μ(fk) = -\n= -\n|k+1|\n|k+1|\n0 + (-(f))(|k+1| - 1)\n\n-\n|k+1|\n\n1 -\n(f).\nn\nCorollary 4 If the costs are integer, then the number of iterations is at most mn log(nC).\nProof:\nWe have that\n\nn log(nC)\n(fend) 1 - 1\n(f = 0) < e- log(nC)|C| =\n1 |C| = 1 ,\nn\nnC\nn\nand thus the resulting circulation is optimal.\n\nThe time per iteration will be shown to be O(nm) (see problem set), hence the total running\ntime of the algorithm is O(m2n2 log(nC)).\n2.4\nStrongly Polynomial Analysis\nIn this section we will remove the dependence on the costs. We will obtain a strongly polynomial\nbound for the algorithm for solving the minimum cost circulation problem. In fact we will show\nthat this bound will hold even for irrational capacities. The first strongly polynomial-time analysis\nis due to Tardos; the one here is due to Goldberg-Tarjan. This result was very significant, since it\nwas the most general subclass of Linear Programming (LP) for which a strongly polynomial-time\nalgorithm was shown to exist. It remains a big open problem whether a strongly polynomial-time\nalgorithm exists for general LP.\nDefinition 3 An edge e is -fixed if for all -optimal circulations f we have that f(e) maintains the\nsame value.\nNote that (v, w) is -fixed if and only if (w, v) is -fixed, by skew-symmetry of edge-costs.\nTheorem 5 Let f be a circulation and p be a potential such that f is (f)-optimal with respect to\np. Then if |cp(v, w)| 2n for some edge (v, w) ≥ E, the edge (v, w) is -fixed.\nProof:\nSuppose (v, w) is not (f)-fixed. There exists an f 0 that is (f)-optimal and f 0(v, w) =∪\nf(v, w); without loss of generality assume f 0(v, w) < f(v, w). Let E< = {(x, y) : f 0(x, y) < f(x, y)}.\nWe can see that E< ≤ Ef 0 by definition of Ef 0 . Furthermore, from flow conservation, we know that\nthere exists a cycle ≥ Ef 0 containing the edge (v, w). Indeed, by flow decomposition, we know\nthat the circulation f - f 0 can be decomposed into (positive net) flows along cycles of Ef 0 , and thus\none of these cycles must contain (v, w)\nNow we have the following,\nc() = cp() -2n(f) + (n - 1)(f) < -n(f).\nConsequently, c() < - and so μ(f 0) < -. As a result, f 0 is not (f)-optimal and thus we have a\ncontradiction.\n\nlect-4\n\n||\n||\nLemma 6 After O(mn log n) iterations, another edge becomes fixed.\nProof: Let f be a circulation and f 0 be another circulation after application of mn log(2n) iterations\nof the Goldberg-Tarjan algorithm. Also suppose that is the first cycle cancelled and p, p0 are the\npotentials for f, f 0 respectively. From the previous lemma, we have that (f 0) (1- n\n1 )n log(2n)(f) <\ne- log(2n) = 2\nn (f). Now from the definition of μ we get the following,\ncp0 ()\nc()\n=\n= μ(f) = -(f) < -2n(f 0)\nThis means that there exists an edge (v, w) ≥ such that cp0 (v, w) < -2n(f 0) which means that it\nwas not (f)-fixed. Thus (v, w) becomes (f 0)-fixed and the claim is proven.\n\nNotice that if e is fixed, it will remain fixed as we iterate the algorithm. An immediate con\nsequence of the above lemma then is a bound on the number of iterations in the Goldberg-Tarjan\nalgorithm.\nCorollary 7 The number of iterations of the Goldberg-Tarjan algorithm, even with irrational costs,\nis O(m2n log n).\nlect-5"
    },
    {
      "category": "Resource",
      "title": "Network Flows",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/10ad543e5ab70c642a82341b3fb0d5f3_lec2.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nX\n18.415/6.854 Advanced Algorithms\nSeptember 8th, 2008\nNetwork Flows\nLecturer: Michel X. Goemans\nIntroduction\nIn the previous lecture, we introduced Fibonacci heaps, which is a data structure that provides an\necient implementation of priority queues. In this lecture, we switch our attention from eciency\nto algorithm design. In particular, for the next few lectures we study Network Flows.\nNetwork ows are a family of problems that are concerned with a directed graph and properties of\nfunctions dened on the graph. A ow is an abstraction of elements which typically do not disappear\nwhile travelling through the edges of the directed graph; it could be current in an electrical network,\npackets in a computer network, cars/trains in a transportation network, or some purely abstract\nobject. In the maximum ow problem, we try to obtain a ow on the graph such that the ow going\nfrom a given source vertex to a given sink vertex is maximized.\nIn today's lecture, we focus on two instances of network ow problems: the Shortest Path Problem\nand the Maximum Flow Problem. There are other variants of network ow problems that we\ncover later in this class. For example, we will talk about the minimum cost ow or minimum cost\ncirculation problem, which is a generalization of both the shortest path problem and the maximum\now problem. We will also cover the bipartite matching problem, which has two versions: cardinality\nbipartite matching (a special case of the maximum ow problem) and weighted bipartite matching\n(a special case of the minimum cost ow problem). There are still other network ow problems that\nwe do not discuss such as the multi-commodity ow problem. Figure 1 illustrates how these network\now problems are related to one another.\nShortest Path Problem\nLet G = (V, E) be a directed graph, where V denotes the set of vertices and E denotes the set of\nedges. Let `: E\nR be a length function dened on the edges of G.1 Given two vertices s and t in\n→\nV , the s - t shortest path problem is the problem of nding a simple directed path on G from s to\nt of minimum total length. The length of a path P is dened to be the sum of the lengths of all the\nedges in P :\n`(P ) =\n`(v, w).\n(v,w)∈P\nIn this problem, we refer to s as the source vertex and t as the sink vertex.\nWe note that if the length function `(e) is non-negative for every edge e ∈ E, then Dijkstra's\nalgorithm using Fibonacci heaps provides a O(m + n log n) solution to this problem, where m = |E|\nand n = |V |. On the other hand, if some edges of G have negative lengths, but the graph has\nthe property that for every cycle C the total length of the cycle is non-negative, then we can use\nthe Bellman-Ford algorithm to solve the s - t shortest path problem in polynomial time. For more\ninformation on Dijkstra's and the Bellman-Ford algorithm, see Chapter 24 in [CLRS].\n1 For v, w ∈ V , we use the notation `(v, w) to mean the length of the edge e = (v, w). In these notes, we use the\ntwo notations `(e) and `(v, w) interchangeably.\nlect-1\n\nFigure 1: Some instances of network ow problems and how they are related to one another, where\nthe arrow indicates is a special case of. In this lecture we only cover the shaded boxes: the shortest\npath problem and the maximum ow problem.\nRemark 1: In this lecture, we consider directed graphs only. For undirected graphs with non-\nnegative edge lengths, we can still apply Dijkstra's algorithm by transforming every (undirected)\nedge into two edges of opposite directions with the same length, as illustrated in Figure 2.\n(a) Original undirected edge.\n(b) Directed edges after the transfor-\nmation.\nFigure 2: Transformation of an undirected edge into two directed edges to apply Dijkstra's algorithm.\nHowever, the same trick does not apply for the Bellman-Ford algorithm, because even if the original\nundirected graph satises the constraint that every cycle has non-negative length, the new directed\ngraph resulting from the transformation might violate this constraint. An example of this case is\ngiven in Figure 3.\nThe problem of nding the shortest path between two vertices in an undirected graph where every\ncycle has non-negative length is still solvable in polynomial time, but it is a much harder problem.\nWe will discuss this problem later in the class if time permits.\nRemark 2: In directed graphs with non-negative, given a shortest path P between two vertices, the\npath between any two vertices in P is also the shortest path between those two vertices. However, this\nis not necessarily true in the case of undirected graphs (and this prevents the use of a transformation\nto a directed graph). For example, in the graph given in Figure 3(a), the shortest path between\nv and w is P = {(v, z), (z, w)} with length 0. However, the shortest path between v and z is not\n{(v, z)} as it appears in P , but rather {(v, w), (w, z)} with length 0.\nlect-2\n\nX\nX\nX\nX\n(a) Original undirected graph. Every\n(b) Directed graph after the transfor-\ncycle has non-negative length.\nmation. The cycle {(w, z), (z, w)} has\nnegative length.\nFigure 3: An example where the given transformation creates a negative cycle so that the Bellman-\nFord algorithm cannot be applied.\nMaximum Flow Problem\nThe second instance of network ow problems that we study in this lecture is the maximum ow\nproblem. In this problem, we want to nd a ow from a source vertex to a sink vertex with maximum\now value.\nMore precisely, we dene the problem framework as follows. Let G = (V, E) be a directed graph,\nwhere V is the set of vertices and E is the set of edges of G. Let n denote the cardinality of V and\nm denote the cardinality of E. Given a vertex v ∈ V , let N +(v) (resp. N -(v)) denote the set of\nendpoints of edges coming out (resp. into) v:\nN +(v) = {w ∈ V : (v, w) ∈ E},\nN -(v) = {w ∈ V : (w, v) ∈ E}.\nFurthermore, let u: E\nR+ be a capacity function that limits the amount of ow that we can send\n→\nthrough each edge of G. We refer to the graph G and the capacity function u collectively as the\nnetwork G. Given a source vertex s ∈ V and a sink vertex t ∈ V , we are interested in determining\nhow much ow we can push from s to t through this network.\n3.1\nNotions of Flow\nLoosely speaking, a ow is an assignment of quantity to the edges of G under certain constraints.\nThere are two notions of ow that we use in this class: raw ow and net ow.\nDenition 1 A raw ow on a network G is a function r : E\nR satisfying the following properties:\n→\n1. Capacity constraint: For all (v, w) ∈ E, 0 ≤ r(v, w) ≤ u(v, w).\n2. Conservation constraint: For all v ∈ V \\ {s, t},\nr(v, w) -\nr(w, v) = 0.\nw∈V :(v,w)∈E\nw∈V :(w,v)∈E\nGiven a raw ow r, the ow value of r is dened to be the total excess of ow at the source vertex\ns, i.e.\n|r| =\nr(s, w) -\nr(w, s).\nw∈N+(s)\nw∈N- (s)\nlect-3\n\nX\nP\nWe now give the second denition of ow, which is the one we primarily use for the rest of these\nnotes.\nDenition 2 Given a raw ow r on a network G, the net ow f with respect to r is the function\nf : E → R given by f(v, w) = r(v, w) - r(w, v).\nAn example of raw ow and the corresponding net ow is illustrated in Figure 4.\n(a) Raw ow.\n(b) Net ow.\nFigure 4: An example of a raw ow and its corresponding net ow.\nBefore we go any further, we rst note that from the denition given above, to compute f(v, w) we\nneed both r(v, w) and r(w, v). However, there is a slight diculty because even if (v, w) ∈ E, (w, v)\nmight not be an edge of G. To resolve this issue, we assume that the graph G has the property that\nif (v, w) ∈ E then (w, v) ∈ E. Given a directed graph G, we can achieve this property by modifying\nG as follows:\n1. Consider the set E0 = {(v, w) ∈ E : (w, v) ∈/ E}.\n2. For every (v, w) ∈ E0, create a new edge (w, v) with edge capacity 0 and add it to E.\nSimilar to the denition of the ow value of raw ow, the ow value of f is dened to be the total\namount of net ow that comes out from the source vertex s:\n|f| =\nf(s, w),\n(1)\nw∈N(s)\nwhere we now use N(s) to denote N +(s) = N -(s), the common set of out-neighbors and in-neighbors\nof s.\nFrom the denition of net ow, it is easy to check that the net ow f satises the following properties:\n1. Skew symmetry: For all (v, w) ∈ E, f(v, w) = -f(w, v).\n2. Capacity constraint: For all (v, w) ∈ E, f(v, w) ≤ u(v, w).\n3. Flow conservation: For all v ∈ V \\ {s, t},\nw∈N (v) f(v, w) = 0.\nNote that, unlike r, the ow f has no restriction on being negative. In fact, f will be negative for\nsome edges, unless it is the 0 ow everywhere. For example, if the original graph G has an edge\n(v, w) with positive raw ow r(v, w) such that (w, v) is not an edge, then in the modied graph, the\nedge (w, v) has negative net ow f(w, v) = -r(v, w). Note that this does not violate the capacity\nconstraint since f(w, v) ≤ u(w, v) = 0. Figure 5 illustrates an example of a net ow.\nFor the maximum ow problem, we use the notion of net ow. For the rest of these notes, unless\nspecied otherwise, the term ow refers to net ow. We can now dene the maximum ow problem\nproperly.\nDenition 3 (Maximum Flow Problem) Given a network G, a source vertex s ∈ V , and a sink\nvertex t ∈ V , the maximum ow problem is the problem of nding a ow through G of maximum\now value.\nNotice that modifying G by adding to E the new edges needed to dene the net ow does not aect\nthe maximum ow problem, since the new edges all have zero capacity.\nlect-4\n\nX\nFigure 5: An example of a ow of a network. The label x/y on each edge e is such that x = f(e)\nand y = u(e). Here the ow value is |f| = 3.\n3.2\ns - t Cut\nWe now dene the notion of cut, which helps us to construct the solution of the maximum ow\nproblem.\nDenition 4 Suppose that we have a network G with source vertex s and sink vertex t. Let S be a\nsubset of V such that s ∈ S and t /∈ S, and let S = V \\ S. Then the s - t cut with respect to S is\ndened to be\n(S : S) = {(v, w) ∈ E : v ∈ S and w ∈ S}.\nWe can also denote an s - t cut by δ+(S) or δ-(S), but in this class the preferred notation is (S : S)\nas introduced above. Figure 6 shows an example of an s - t cut.\nFigure 6: An example of an s - t cut. The solid arrows represent the edges in (S : S).\nDenition 5 Given an s - t cut (S : S), then its cut capacity is dened to be the total capacity\nof the edges across the cut:\nu(S : S) =\nu(v, w).\n(v,w)∈(S:S)\n3.3\nConnection between Flows and Cuts\nWe have the following lemma that connects ows and cuts.\nlect-5\n\nX\nX\nX\nX\nX\nX\nX\nX X\nX\nX\nX\nLemma 1 Let G be a network with source s and sink t. Then for every ow f and every s - t cut\n(S : S), we have\nX\n|f| =\nf(v, w).\n(2)\n(v,w)∈(S:S)\nIn particular, this implies that |f| ≤ u(S : S).\nProof:\nFrom the ow conservation property of f, for every vertex v ∈ S \\ {s}, we have\nf(v, w) = 0.\nw∈N(v)\nTaking the sum over all vertices v ∈ S \\ {s} gives us\nf(v, w) = 0.\nv∈S\\{s} w∈N(v)\nAdding the denition of the ow value of f (Eq. (1)) to the equation above yields\n|f| =\nf(s, w) +\nf(v, w).\nw∈N (s)\nv∈S\\{s} w∈N(v)\nNow notice that if an edge (v, w) appears in either of the summations above and w ∈ S, then (w, v)\nalso appears in the summations. Therefore, we can rewrite the equation above in a slightly dierent\nway:\n|f| =\nf(v, w) +\nf(v, w).\n(v,w)∈(S:S)\nv∈S w∈S\nBy the skew-symmetry property of f, the second summation in the equation above is equal to 0\nsince f(v, w) and f(w, v) cancel each other out. Therefore, we conclude that\n|f| =\nf(v, w),\n(v,w)∈(S:S)\nas desired.\nFurthermore, by the capacity constraint of f, we can write\n|f| =\nf(v, w) ≤\nu(v, w) = u(S : S).\n(v,w)∈(S:S)\n(v,w)∈(S:S)\nThis completes the proof of the lemma.\nIn particular, if we take S = V \\ {t} and S = {t}, then Eq. (2) from Lemma 1 tells us that the\now coming from s is equal to the ow going to t. In other words, there is no loss in the ow of the\nnetwork.\nAn important corollary to Lemma 1 comes from the observation that since the value of any ow f is\nalways less than equal to the capacity of any s - t cut (S : S), then it also holds for the case when f\nis a maximum ow and (S : S) is a minimum cut. This fact is known as the Weak-Duality Lemma.\nCorollary 2 (Weak-Duality Lemma) Let G be a network with source vertex s and sink vertex\nt. Then\nmax f ≤ min u(S : S),\nf | |\n(S:S)\nwhere the maximum is taken over all possible ows and the minimum is taken over all possible s - t\ncuts in G.\nlect-6\n\nThe Max-Flow and Min-Cut Theorem\nIn this section, we show that the inequality in the Weak-Duality Lemma is actually an equality, that\nis, the maximum value of a net ow is equal to the minimum value of an s - t cut. This fact was\nrst discovered in 1956 by by Elias, Feinstein, and Shannon (see [EFS]), and independently by Ford\nand Fulkerson in the same year.\nTheorem 3 (Duality Theorem/Maxow Mincut Theorem) In a network G, the following\nequality holds:\nmax f = min u(S : S).\nf | |\n(S:S)\nIn order to prove the theorem, we rst have to introduce some new denitions. The rst one is\nresidual capacity, which denotes the extent to which a ow on some edge is less than the capacity\non that edge.\nDenition 6 The residual capacity of G with respect to f is the function uf : E\nR dened by\n→\nuf (v, w) = u(v, w)- f(v, w) for all (v, w) in E. Hence, the residual capacity on the edge (v, w) is the\namount of additional ow that we can push from v to w, without violating the capacity constraint.\nWe observe that the capacity constraint implies that uf (v, w) = u(v, w) - f(v, w) = u(v, w) +\nf(w, v) ≤ u(v, w) + u(w, v). Moreover, since f is a ow, u(v, w) ≥ f(v, w), so that uf (v, w) ≥ 0.\nHence, the following inequality holds for any edge (v, w) in E:\n0 ≤ uf (v, w) ≤ u(v, w) + u(w, v).\nAll the edges with positive residual capacities are members of a set that we call the residual arcs.\nDenition 7 The residual arcs Ef of G with respect to f is the set given by Ef = {(v, w) ∈ E :\nuf (v, w) > 0}. Intuitively, the residual arcs is the subset of E that contains those edges through\nwhich we can push a non-zero additional ow.\nGiven the vertices of a network G, its residual arcs, and its residual capacity, we can make a new\nnetwork, the residual network.\nDenition 8 The residual network Gf of the network G with respect to f is the network given\nby the graph Gf = (V, Ef ) together with the capacity function uf .\nThe residual network is used to understand to what extent a ow is not maximal, and we do that\nby dening a certain kind of path in the residual network that we call augmenting path.\nDenition 9 An augmenting path of G with respect to f is a directed simple path from the source\ns to the sink t in the residual network Gf .\nIn fact, the existence of an augmenting path in a residual network for a given ow indicates that\nthe ow is not maximal, as we prove in the following lemma.\nLemma 4 If a residual network Gf has at least one augmenting path P , then f is not a maximum\now.\nProof: By denition, the residual network Gf includes only edges with non-zero residual capacity\nwith respect to f. Therefore, an augmenting path P of Gf is a path through which we can push\nmore ow in the original network G, and the additional amount of ow is upper bounded by the\nbottleneck of P .\nlect-7\n\nX\nX\nMore precisely, consider the quantity given by\n(P ) = min uf (v, w).\n(v,w)∈P\nObserve that (P ) > 0, because P ⊂ Ef so that P is a nite set of positive real numbers.\nThen, construct the ow f 0 given by\nf 0(v, w) =\n⎧\n⎪\n⎨\n⎪\n⎩\nf(v, w) + (P ) if (v, w) ∈ P ,\nf(v, w) - (P ) if (w, v) ∈ P ,\nf(v, w)\notherwise.\nNote that f 0 is satises all the ow constraints for G. Moreover, |f 0| = |f| + (P ) > |f|, so that the\now f is not a maximum ow.\nUsing Lemma 4 and the Weak-Duality Lemma, we prove now the Maxow Mincut Theorem.\nProof of Theorem 3: Let f be a ow of maximal value for G = (V, E). By Lemma 4, the residual\nnetwork Gf has no augmenting path, since, if it did, then f would not be of maximal value.\nConsider the set S of vertices v ∈ V such that there exists a directed path from the source s to v in\nGf . By denition, s ∈ S. Moreover, Gf has no augmenting path, so that t 6∈ S. Therefore, (S : S)\nis an s - t cut.\nNow notice that uf (v, w) = 0 for any (v, w) ∈ (S : S). By denition, uf (v, w) = u(v, w) - f(v, w),\nso that f(v, w) = u(v, w) for any (v, w) ∈ (S : S). Thus, we can compute that\n|f| =\nf(v, w) =\nu(v, w) = u(S : S).\n(v,w)∈(S:S)\n(v,w)∈(S:S)\nThe Weak-Duality Lemma tells us that the value of any ow is upper bounded by the capacity of\nany s - t cut, so we can conclude that\nmax f = min u(S : S).\nf | |\n(S:S)\nWe summarize all of the results in the following theorem.\nTheorem 5 (Max-Flow Min-Cut Theorem) Let G be a network and f be a ow on G. Then,\nthe following statements are equivalent:\n1. f is a ow of maximal value;\n2. Gf has no augmenting path; and\n3. |f| = u(S : S) for some s - t cut (S : S).\nProof:\nWe prove the equivalence of the statements by showing that (1)\n(2)\n(3)\n(1), that\n⇒\n⇒\n⇒\nis:\n(1)\n(2): This implication is the contrapositive of the implication proved in Lemma 4.\n-\n⇒\n(2)\n(3): This implication follows from the proof of the Maxow Mincut Theorem.\n-\n⇒\n(3)\n(1): This implication follows from the Weak Duality Lemma.\n-\n⇒\nlect-8\n\nThe Ford-Fulkerson Algorithm\nIn 1956 Ford and Fulkerson used the Max-Flow Min-Cut Theorem to design an algorithm, called the\nFord-Fulkerson algorithm, to compute the maximal ow of a network (see [FF]). The idea of their\nalgorithm is very simple: as long as there is an augmenting path in the residual network we push\nmore ow along that path in the original network. This idea is illustrated as pseudocode below.\nFord-Fulkerson(G)\n1 start with a zero ow f (or any feasible ow)\n2 while Gf has an augmenting path P\ndo push (P ) more units of ow through P , so that |f| ←|f| + (P )\nBefore we declare the idea above an algorithm, there are two issues that need to be addressed:\n1. Does the algorithm ever halt?\n2. If there is more than one augmenting path in the residual network, which one should we choose?\nAnd how does our decision aect the correctness and running time of the algorithm?\nWe consider three cases.\nCase 1: Assume that the capacity function u of G is integer valued. Then we can make the\nfollowing observations:\n1. At every iteration of Ford-Fulkerson, the ow f is integer valued, and therefore so are the\nresidual capacities. Indeed, this is the case at the beginning when f = 0, and by induction,\nthis is maintained since (P ) is the minimum of a set of positive integers and thus a positive\ninteger, and therefore the resulting ow after an augmentation is also integer valued.\nFurthermore, since (P ) ≥ 1 (being a positive integer) and since the minimum-cut value (and\nthus the maximum ow value) is nite, it follows that the Ford-Fulkerson always halts.\n2. Since the algorithm halts and every intermediate ow is integer valued, the maximum ow\noutput will also be integer valued. That is, if the capacities of a network are integral then\nthere is a maximum ow that is also integral. This is a very useful property that has many\napplications. One such application is the cardinality bipartite problem, as we will see in the\nnext lecture.\n3. The number of iterations is bounded by |f| ≤|N(s)|U ≤ nU , where U = max{u(s, w) : w ∈\nN(v)}. Note that U may not be polynomial in the size of G. In fact, Figure 7 shows an example\nof a graph where Ford-Fulkerson takes exponential time to halt. The dotted and dashed\nlines represent paths from the source to the sink. The algorithm might choose alternatively\nand repeatedly the two paths as augmenting paths. In such a case, the algorithm will take\nO(2L) time to terminate. Thus, we need a better policy to choose the augmenting path.\nCase 2: Assume that the capacity function u of G is rational valued. Then, a similar discussion\nas the one carried out in Case 2 shows that Ford-Fulkerson always halts, that the value of the\nmaximal ow is rational, and that there exists an example of a network for which the running time\nis exponential. The arguments are similar because the rational capacities behave like integers if we\nconsider them all as written with the same least common multiple.\nCase 3: Assume that the capacity function u of G is real valued. In the general case (i.e. u(E) ⊂ Q+\nis not necessarily true) there exist instances of networks such that Ford-Fulkerson never halts.\nMoreover, in such cases, the value of |f| may converge to a sub-optimal value.\nlect-9\n\nFigure 7: An example of a network for which the Ford-Fulkerson algorithm may not halt in polyno-\nmial time (the reverse edges and the corresponding ows are not shown for clarity).\nFixing the Ford-Fulkerson Algorithm\nThe problems of the Ford-Fulkerson algorithm that we examined at the end of Section 5 can be\naddressed, at least in part, by specifying a policy for choosing the augmenting path at every iteration.\nA good policy must satisfy two properties:\n1. It is possible to eciently (e.g. in polynomial time) nd the augmenting path specied by the\npolicy; and\n2. The maximum number of augmentations (and thus the total time) is polynomial.\nIn fact, we should be precise when we say that a running time is polynomial, because it means\ndierent things depending on the model of computation. Also, ideally, we would like algorithms for\nwhich the number of operations does not depend on the size of the numbers involved in the input\n(e.g. the capacities in a maximum ow instance); such algorithms could be used even if the data was\nirrational (provided our model allows (arithmetic) operations on irrational data).\nGiven an instance I of a number problem (a computational problem involving numbers as input),\nlet size(I) denote the number of bits needed to represent the input and number(I) denote the\nnumber of numbers involved in the input. For example, for a maximum ow instance, number(I)\ncorresponds to the number m of edges while size(I) corresponds to the number of bits needed to\nrepresent all edge capacities. For the solution of an n × n system of linear equations, number(I) will\nbe n2 + n (n2 for the matrix and n for the right-hand-side) while size(I) is the sum of the binary\nsizes of all the entries of the matrix and the right-hand-side.\nWe say that an algorithm A running on an instance I is (weakly) polynomial if\n- the number of operations performed by A is at most polynomial in size(I) and\n- the size of any number obtained during the execution of A is at most polynomial in size(I).\nFor an algorithm to be strongly polynomial, we require that\n- the number of operations performed by A is at most polynomial in number(I) and\n- the size of any number obtained during the execution of A is at most polynomial in size(I).\nThus, the two notions dier only in whether the number of operations performed depends on the\nsize of the numbers in the input. For example, Gaussian elimination can be shown to be strongly\npolynomial for solving a system of equations (it is clear that the number of operations is at most\nO(n3), but one can also show that the size of the numbers obtained through the algorithm are\npolynomially bounded in the size of the input). On the other hand, Euclid's algorithm for computing\nthe gcd is clearly not strongly polynomial (as only 2 numbers are involved), but is polynomial.\nlect-10\n\nWe now consider two policies for choosing the augmenting path in the Ford-Fulkerson algorithm.\nBoth were proposed by Edmonds and Karp in 1972 [EK]. Both lead to polynomial algorithms, while\nthe second leads to a strongly polynomial algorithm.\nPick the Fattest: Suppose that, in the case of integral capacities, at every iteration of the Ford-\nFulkerson algorithm, we pick the fattest augmenting path, that is, a path P such that (P ) is\nmaximized. Given this policy:\nBy adapting Dijkstra's algorithm to nd this bottleneck path rather than the shortest path, it\n-\nis possible to nd the augmenting path that maximizes (P ) in O(m + n log n) time;\n- It can be shown that the number of iterations is O(m log U), where U is a bound for the\ncapacity function, yielding a running time for this fattest augmenting path algorithm of O((m+\nn log n)m log U).\nA similar argument works for rational capacities as well. However, for irrational capacities, the time\ncomplexity given above does not apply, and this analysis does not even show whether the algorithm\nterminates.\nPick the Shortest: Suppose that, in the case of integral capacities, at every iteration of the Ford-\nFulkerson algorithm, we pick the shortest augmenting path, that is, a path P such that its number\nof edges is minimized. Given this policy, we observe that:\nUsing breadth-rst search, it is possible to nd the augmenting path with a minimum number\n-\nof edges in O(m) time (by breadth-rst-search);\n- It can be shown that the number of iterations is O(nm), yielding a running time for the\nalgorithm of O(nm2). Thus this shortest augmenting path algorithm is strongly polynomial\nand therefore halts even if capacities are irrational.\nNext time we will discuss more network ow problems.\nReferences\n[CLRS] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Cliord Stein, Introduction\nto Algorithms, Second Edition, MIT Press and McGraw-Hill, 2001.\n[EFS] P. Elias, A. Feinstein, and C. E. Shannon, Note on maximum ow through a network, IRE\nTransactions on Information Theory IT-2, 117119, 1956.\n[EK] Jack Edmonds, and Richard M. Karp, Theoretical improvements in algorithmic eciency for\nnetwork ow problems, Journal of the ACM 19 (2): 248264, 1972.\n[FF] L. R. Ford, D. R. Fulkerson, Maximal ow through a network, Canadian Journal of Mathematics\n8: 399404, 1956.\nlect-11"
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 3",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/84ea6cf153a2841c6fb7d25812bd69ff_lec3.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 10, 2008\nLecture 3\nLecturer: Michel X. Goemans\nIntroduction\nToday we continue our discussion of maximum flows by introducing the fattest path augmenting\nalgorithm, an improvement over the Ford-Fulkerson algorithm, to solve the max flow problem. We\nalso discuss the minimum cost circulation problem.\nMaximum Flow\nIn a maximum flow problem, the goal is to find the greatest rate (flow) at which material can be\nsent from a source s to a sink t. Several problems can be modeled as a max-flow problem, including\nbipartite matching, which will be discussed today. We will also discuss flow decomposition and the\nfattest augmenting path algorithm.\n2.1\nMaximum Cardinality Matching in Bipartite Graphs\nA bipartite graph is a graph G = (V, E) whose vertex set V can be partitioned into two disjoint sets,\nA and B, such that every edge connects a vertex in A to one in B. A matching M is a subset of E\nsuch that the endpoints of all the edges in M are distinct. In other words, two edges in M cannot\nshare a vertex. We are interested in solving the following problem: Given an undirected bipartite\ngraph G = (V, E) where V = A ∪ B, find a matching M of maximum cardinality.\nWe can formulate this maximum cardinality matching problem as a max-flow problem. To do\nthat, consider the network shown in Figure 1.\nFigure 1: The figure on the left represents a matching in a bipartite graph. The figure on the right\nshows how the bipartite graph can be converted into a max-flow network by imposing a capacity of\n1 on arcs out of s and into t.\n3-1\n\nThe network is constructed as follows: We orient each edge in G from A to B and assign them a\ncapacity of 1 (any capacity greater than 1 works too). We also add two new vertices, s and t, and\narcs from s to every vertex in A, and from every vertex in B to t. All the new arcs are given unit\ncapacity.\nTheorem 1 Let G = (V, E) be a bipartite graph with vertex partition V = A ∪ B, and let G0 =\n(V 0, E0) be the capacitated network constructed as above. If M is a matching in G, then there is an\ninteger-valued flow f in G0 with value |f| = |M|. Conversely, if f is an integer-valued flow in G0,\nthen there is a matching M in G with cardinality |M| = |f|.\nProof:\nGiven M, define a flow f in G0 as follows: if (u, v) ∈ M, then set f(s, u) = f(u, v) =\nf(v, t) = 1 and f(u, s) = f(v, u) = f(t, v) = -1. For all other edges (u, v) ∈ E0, let f(u, v) = 0.\nEach edge (u, v) ∈ M corresponds to 1 unit of flow in G0 that traverses the path s → u → v → t.\nThe paths in M have distinct vertices, aside from s and t. The net flow across the cut (A ∪ s : B ∪ t)\nis equal to |M|. We know that the net flow across any cut is the same, and equals the value of the\nflow. Thus, we can conclude that |M| = |f|. To prove the converse, let f be an integer-valued flow\nin G0. By flow conservation and the choice of capacities, the net flow in each arc must be -1, 0 or\n1. Let M be the set of edges (u, v), with u ∈ A, v ∈ B for which f(u, v) = 1. It is easy to see, by\nflow conservation again, that M is indeed a matching and, using the same argument as before, that\n|M| = |f|.\n\nSince all the capacities of this maximum flow problem are integer valued, we know that there\nalways exists an integer-valued maximum flow, and therefore the theorem shows that this maximum\nflow formulation correctly models the maximum cardinality bipartite matching.\n2.2\nFlow Decomposition\nIn an (raw) s-t flow, we have the following building blocks:\n- Unit flow on an s-t directed path.\n- Unit flow on a directed cycle.\nAny (raw) s-t flow can be written as a linear combination of these building blocks.\nTheorem 2 Any (raw) s-t flow r can be decomposed into at most m flows along either paths from s\nto t or cycles, where m is the number of edges in the network. More precisely, it can be decomposed\ninto at most |{e : r(e) > 0}| ≤ m paths and cycles.\nProof:\nBy tracing back the flow on an edge e and tracing forward the flow on e, we either get an\ns-t path T , or a cycle T with r(e) > 0 for all e ∈ T . Denote the min flow on T by Δ(T ):\nΔ(T ) = min r(e).\ne∈T\nWe want to decrease the flow on T such that at least one edge goes to 0 (by subtracting out Δ(T )),\nand keep doing that until there are no more edges with non-zero flows. More precisely, the following\nalgorithm extracts at most m paths and cycles.\n(i) While there is a directed cycle C with positive flow:\n(a) Decrease the flow on this cycle by Δ(C)\n(b) Add this cycle as an element of the flow decomposition\n(ii) (The set of arcs with positive flow now form an acyclic graph.) While there is a path P from\ns to t with positive flow:\n3-2\n\n(a) Decrease the flow on this path by Δ(P ).\n(b) Add this path as an element of the flow decomposition.\nEach time we decrease the flow on a path or a cycle T , we zero out the flow on some edge.\nWhen we do this, the new raw flow is rnew(e) = r(e) - Δ(T ) if e ∈ T , or r(e) otherwise. Since\nthere are |{e : r(e) > 0}| ≤ m edges with positive flow in the graph, there will be at most that\nnumber of decreases in the flow, and consequently, at most that number of paths or cycles in the\nflow decomposition.\n\n2.3\nFattest Augmenting Path Algorithm (Edmonds-Karp '72)\nFlow decomposition is a key tool in the analysis of network flow algorithms, as we will illustrate\nnow.\nAs we saw in the last lecture, the Ford-Fulkerson algorithm for finding a maximum flow in a\nnetwork may take exponential time, or even not terminate at all, if the augmenting path is not\nchosen appropriately. We proposed two specific choices of augmenting paths, both due to Edmonds\nand Karp, that provide a polynomial running time. One was the shortest augmenting path, the\nother was the fattest augmenting path or maximum-capacity augmenting path: the augmenting path\nthat increases the flow the most. This is the variant we analyze now.\nFor an augmenting s-t path P ∈ Gf , define\nε(P ) = min uf (v, w)\n(v,w)∈P\nwhere the uf are the residual capacities. The minimum residual capacity ε(P ) (the bottleneck) is\nthe maximum flow that can be pushed along the path P . We wish to find the fattest augmenting\npath P such that ε(P ) is maximized. The fattest augmenting path P can be efficiently found with\nDijkstra's algorithm in O(m + n log n) time 1 .\nTheorem 3 Assuming that capacities are integral and bounded by U, the optimal flow for a network\ncan be found in O(m log(mU )) = O(m log(nU )) iterations of augmenting along the fattest path.\nProof: Start with a zero flow, f = 0. Consider a maximum flow f ∗. Its value is at most the value\nof any cut, which is bounded by mU:\n|f ∗| ≤ mU.\nConsider the flow f ∗ - f (this is, f ∗(e) - f(e) for all edges e) in the residual graph Gf with residual\ncapacities uf = u - f.\nWe can decompose f ∗ - f into ≤ m flows using flow decomposition. As a result, at least one of\nthese paths carry a flow of value at least 1 (|f ∗| -|f|). Suppose now that we push ε(P ) units of\nm\nflow along the fattest path in the residual graph Gf and obtain a new flow f new of value:\n|f new| = |f| + ε(P ).\nSince the fattest path provides the greatest increase in flow value, we must have that ε(P ) ≥\nm (|f ∗| -|f|). Thus we have the following inequality\n|f new| ≥|f| + m (|f ∗| -|f|),\n1Actually, it can be found in O(m) time under the condition that we have the capacities sorted beforehand, see\nthe forthcoming problem set.\n3-3\n\n|\n\nP\nwhich implies\n|f ∗| -|f new| = |f ∗| -|f + |f| -|f new|\n≤\n1 - m (|f ∗| -|f|) .\nAfter k iterations, we get a flow fˆ such that\n\nk\n|f ∗| -|fˆ| ≤ 1 - m\nmU.\nEventually f ∗\nfˆ < 1 which implies f ∗ = fˆ since, for integral capacities, all intermediate flows\n|\n| -| |\nwill be integral. Since (1 - m )m ≤ e for all m ≥ 2, the number of iterations required for the\ndifference to go below 1 is\nk = m log(mU).\nCombining the results mentioned above we have the following corollary.\nCorollary 4 We can find a maximum flow in an integer-capacitated network with maximum capacity\nU in O((m + n log n)m log(nU )) time 2 .\nMinimum Cost Circulation Problem (MCCP)\nA circulation is simply a flow where the net flow into every vertex (there are no sources or sinks) is\nzero. Notice that we can easily transform an s - t flow to a circulation by adding one arc from t to\ns (with infinite capacity) which carries a flow equal to the s - t flow value.\nDefinition 1 A circulation f satisfies\n(i) Skew-Symmetry: ∀ (v, w) ∈ E, f(v, w) = -f(w, v).\n(ii) Flow Conservation: ∀ v ∈ V ,\nf(v, w) = 0.\nw\n(iii) Capacity Constraints: ∀ (v, w) ∈ E, f(v, w) ≤ u(v, w).\nDefinition 2 A cost function c : E 7→ R assigns a cost per unit flow to each edge. We assume the\ncost function satisfies skew symmetry: c(v, w) = -c(w, v). For a set of edges C (e.g. a cycle), we\ndenote the total cost of C by :\nX\nc(C) =\nc(v, w).\n(v,w)∈C\nDefinition 3 The goal of the Minimum Cost Circulation Problem (MCCP) is to find a circulation\nf of minimum cost c(f) where\nX\nc(f) =\nc(v, w)f(v, w).\n(v,w)\nThe MCCP is a special case of a Linear Programming (LP) problem (an optimization problem\nwith linear constraints and a linear objective function). But while no strongly polynomial time\nalgorithms are known for linear programming, we will be able to find one for MCCP.\n2Using the previous footnote, we can do this in O(m2 log(nU)) time.\n3-4\n\nX\nX\nX\nX\nX\n3.1\nVertex Potentials\nBefore we can solve MCCP, it is necessary to introduce the concept of vertex potentials, or simply\npotentials.\nDefinition 4 A vertex potential is a function p : V 7→ R that assigns each vertex a potential. The\nvertex potential defines a reduced cost function cp such that\ncp(v, w) = c(v, w) + p(v) - p(w).\nProposition 5 The function cp satisfies the following properties:\n(i) Skew-Symmetry: cp(v, w) = -cp(w, v).\n(ii) Cycle Equivalence: for a cycle C, c(C) = cp(C); i.e., the reduced cost function agrees with\nthe cost function.\n(iii) Circulation Equivalence: for all circulations, the reduced cost function agrees with the cost\nfunction, c(f) = cp(f).\nProof:\nThe first property is trivial. The second property follows since all the potential terms\ncancel out. And we'll prove the third property. By definition\ncp(f)\n=\n(c(v, w) + p(v) - p(w))(f(v, w))\n(v,w)\n= c(f) +\np(v)\nf(v, w) -\np(w)\nf(v, w).\nv\nw:(v,w)∈E\nw\nv:(w,v)∈E\nNow by flow conservation, the inner sums are all zero. Hence cp(f) = c(f). (The third property also\nfollows easily from flow decomposition, as the decomposition of a circulation only contains cycles\nand thus the cost and the reduced cost of a circulation are the same because of (ii).)\n\n3.2\nKlein's Cycle-Cancelling Algorithm\nWe present a pseudo-algorithm for removing negative-cost cycles. While there exists a negative-cost\ncycle C in Gf , push a flow ε along the cycle C, where ε is the minimum residual flow:\nε = min uf (v, w).\n(v,w)∈C\nOf course, this doesn't lead to a straight-forward implementation, since we haven't specified which\nnegative-cost cycle to select or how to find them. We should also consider whether the algorithm is\nefficient and whether it will terminate. We'll answer these questions in the next lecture. However,\nwe will show now that if it terminates, then the circulation output is of minimum cost.\n3.3\nOptimality Conditions\nWe now present a theorem that specifies the conditions required for f to be a minimum cost circu\nlation.\nTheorem 6 (Optimality Condition) Let f be a circulation. The following are equivalent:\n(i) f is of minimum cost.\n(ii) There exists no negative-cost cycle in the residual graph Gf .\n3-5\n\nX\nX\n(iii) There exists a potential function p such that for all (v, w) ∈ Ef , cp(v, w) ≥ 0.\nProof: To show that (i) implies (ii), we'll prove the contrapositive. Suppose there exists a negative\ncost cycle C in the residual graph Gf where f is the optimal circulation. Denote by C0 the reverse\ncycle (i.e. following the arcs in the reverse order). We define a new circulation f 0 for any edge e as\nfollows. If e ∈ C, f 0(e) = f(e)+ ε. And if e ∈ C0, then f 0(e) = f(e) - ε. Otherwise, let f 0(e) = f(e).\nThen we compute the cost of this new flow as\nc(f 0)\n= c(f) + (ε)(c(C)) + (-ε)(-c(C))\n= c(f) + 2εc(C)\n<\nc(f),\nwhere the last step follows since C is a negative cost cycle. Thus we've shown that f is indeed not\noptimal. Hence (i) implies (ii).\nNow we show that (ii) implies (iii). Add zero-cost (or of arbitrary cost) arcs from a new vertex\ns to every vertex in Gf (this is to make sure that s can reach every vertex in V ). Define a potential\np such that p(v) is the length of the shortest simple path from s to v. Then, since there are no\nnegative cost cycle, we have the optimality conditions for the shortest-path lengths:\np(w) ≤ p(v) + c(v, w) ∀ (v, w) ∈ Ef ,\nas one way to go from s to w is to go to v by a shortest path and then go directly to w.\nHere, we have implicitly used the fact that Gf has no negative cost cycles. For if the shortest\npath from s to v already goes through w then adding (v, w), we create a cycle C (and the resulting\npath is not simple). However, this cycle can't be of negative cost by assumption. Thus, by removing\nit, we obtain a simple path to w of cost less or equal to p(v) + c(v, w). Rearranging the inequality\ngives the desired result\ncp(v, w) ≥ 0 ∀ (v, w) ∈ Ef .\nNow we prove that (iii) implies (i) by showing the contrapositive. Suppose we have an optimal\ncirculation f ∗ and a suboptimal one f: c(f ∗) < c(f). Consider the cost of the circulation f ∗ - f:\nc(f ∗ - f)\n= cp(f ∗ - f)\n=\ncp(v, w)[f ∗(v, w) - f(v, w)]\n(v,w)∈E\n=\ncp(v, w)[f ∗(v, w) - f(v, w)]\n(v,w):f ∗-f>0\n≥ 0\nby (iii). Note that in the second to last step, we utilized the skew-symmetry of the cost of reverse\narcs (with flows of opposite parity). But since f ∗ is supposed to be strictly better than f, we have\na contradiction.\n\nReferences\n[EK72] Jack Edmonds, and Richard M. Karp, Theoretical improvements in algorithmic efficiency\nfor network flow problems, Journal of the ACM 19 (2): 248-264, 1972.\n[Klein67] Klein, M. A primal method for minimum cost flows with application to the assignment and\ntransportation problem. Management Science 14: 205-220, 1967.\n3-6"
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 5",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/0d3338683064d96b5174095829043b93_lec5.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 17, 2008\nLecture 5\nLecturer: Michel X. Goemans\nToday, we continue the discussion of the minimum cost circulation problem. We first review the\nGoldberg-Tarjan algorithm, and improve it by allowing more flexibility in the selection of cycles.\nThis gives the Cancel-and-Tighten algorithm. We also introduce splay trees, a data structure which\nwe will use to create another data structure, dynamic trees, that will further improve the running\ntime of the algorithm.\nReview of the Goldberg-Tarjan Algorithm\nRecall the algorithm of Golberg and Tarjan for solving the minimum cost circulation problem:\n1. Initialize the flow with f = 0.\n2. Repeatedly push flow along the minimum mean cost cycle Γ in the residual graph Gf , until\nno negative cycles exist.\nWe used the notation\nc(Γ)\nμ(f) =\nmin\ncycle Γ⊆Ef\nΓ\n| |\nto denote the minimum mean cost of a cycle in the residual graph Gf . In each iteration of the\nalgorithm, we push as much flow as possible along the minimum mean cost cycle, until μ(f) ≥ 0.\nWe used (f) to denote the minimum such that f is -optimal. In other words\n(f) = min{ : ∃ potential p : V\nR such that cp(v, w) ≥- for all edges (v, w) ∈ Ef }.\n→\nWe proved that for all circulations f,\n(f) = -μ(f).\nA consequence of this equality is that there exists a potential p such that any minimum mean cost\ncycle Γ satisfies cp(v, w) = -(f) = μ(f) for all (v, w) ∈ Γ, since the cost of each edge is bounded\nbelow by mean cost of the cycle.\n1.1\nAnalysis of Goldberg-Tarjan\nLet us recall the analysis of the above algorithm. This will help us to improve the algorithm in order\nto achieve a better running time. Please refer to the previous lecture for the details of the analysis.\nWe used (f) as an indication of how close we are to the optimal solution. We showed that (f)\nis a non-increasing quantity, that is, if f 0 is obtained by f after a single iteration, then (f 0) ≤ (f).\nIt remains to show that (f) decreases \"significantly\" after several iterations.\nLemma 1 Let f be any circulation, and f 0 be the circulation obtained after m iterations of the\nGoldberg-Tarjan algorithm. Then\n\n(f 0) ≤ 1 -\n(f).\nn\nWe showed that if the costs are all integer valued, then we are done as soon as we reach (f) < 1 .\nn\nUsing these two facts, we showed that the number of iterations of the above algorithm is at most\nO(mn log(nC)). An alternative analysis using -fixed edges provides a strongly polynomial bound\nof O(m2n log n) iterations. Finally, the running time per a single iteration is O(mn) using a variant\nof Bellman-Ford (see problem set).\n5-1\n\n1.2\nTowards a faster algorithm\nIn the above algorithm, a significant amount of time is used to compute the minimum cost cycle.\nThis is unnecessary, as our goal is simply to cancel enough edges in order to achieve a \"significant\"\nimprovement in once every several iterations.\nWe can improve the algorithm by using a more flexible selection of cycles to cancel. The idea of\nthe Cancel-and-Tighten algorithm is to push flows along cycles consisting entirely of negative cost\nedges. For a given potential p, we push as much flow as possible along cycles of this form, until no\nmore such cycles exist, at which point we update p and repeat.\nCancel-and-Tighten\n2.1\nDescription of the Algorithm\nDefinition 1 An edge is admissible with respect to a potential p if cp(v, w) < 0. A cycle Γ is\nadmissible if all the edges of Γ are admissible.\nCancel and Tighten Algorithm (Goldberg and Tarjan):\n1. Initialization: f ← 0, p ← 0, ← max(v,w)∈E c(v, w), so that f is -optimal respect to p.\n2. While f is not optimum, i.e., Gf contains a negative cost cycle, do:\n(a) Cancel: While Gf contains a cycle Γ which is admissible with respect to p, push as much\nflow as possible along Γ.\n(b) Tighten: Update p to p0 and to 0, where p0 and 0 are chosen such that cp0 (v, w) ≥-0\nfor all edges (v, w) ∈ Ef and 0 ≤ 1 - n\n1 .\nRemark 1 We do not update the potential p every time we push a flow. The potential p gets updated\nin the tighten step after possibly several flows are pushed through in the Cancel step.\nRemark 2 In the tighten step, we do not need to find p0 and 0 such that 0 is as small as possible;\nit is only necessary to decrease by a factor of at least 1 - 1 . However, in practice, one tries to\nn\ndecrease by a smaller factor in order to obtain a better running time.\nWhy is it always possible to obtain improvement factor of 1 - 1 in each iteration? This is\nn\nguaranteed by the following result, whose proof is similar to the proof used in the analysis during\nthe previous lecture.\nLemma 2 Let f be a circulation and f 0 be the circulation obtained by performing the Cancel step.\nThen we cancel at most m cycles, and\n(f 0) ≤ 1 - n\n(f).\nProof:\nSince we only cancel admissible edges, after any cycle is canceled in the Cancel step:\n- All new edges in the residual graph are non-admissible, since the edge costs are skew-symmetric;\n- At least one admissible edge is removed from the residual graph, since we push the maximum\npossible amount of flow through the cycle.\n5-2\n\nSince we begin with at most m admissible edges, we cannot cancel more than m cycles, as each cycle\ncanceling reduces the number of admissible edges by at least one.\nAfter the cancel step, every cycle Γ contains at least one non-admissible edge, say (u1, v1) ∈ Γ\nwith cp(u1, v1) ≥ 0. Then the mean cost of Γ is\nc(Γ)\nX\ncp(u, v) ≥-(|Γ| - 1) (f) = - 1 - 1\n1 - 1\n(f).\n|Γ| ≥|Γ| (u1,v1 )=(u,v)∈Γ\n|Γ|\n|Γ| (f) ≥-\nn\nTherefore, (f 0) = -μ(f 0) ≤ 1 - n\n1 (f).\n2.2\nImplementation and Analysis of Running Time\n2.2.1\nTighten Step\nWe first discuss the Tighten step of the Cancel-and-Tighten algorithm. In this step, we wish to find\na new potential function p0 and a constant 0 such that cp0 (v, w) ≥-0 for all edges (v, w) ∈ Ef\nand 0 ≤ 1 - n\n1 . We can find the smallest possible 0 in O(mn) time by using a variant of the\nBellman-Ford algorithm. However, since we do not actually need to find the best possible 0, it is\npossible to vastly reduce the running time of the Tighten step to O(n), as follows.\nWhen the Cancel step terminates, there are no cycles in the admissible graph Ga = (V, A), the\nsubgraph of the residual graph with only the admissible edges. This implies that there exists a\ntopological sort of the admissible graph. Recall that a topological sort of a directed acyclic graph\nis a linear ordering l : V →{1, . . . , n} of its vertices such that l(v) < l(w) if (v, w) is an edge of the\ngraph; it can be achieved in O(m) time using a standard topological sort algorithm (see, e.g., CLRS\npage 550). This linear ordering enables us to define a new potential function p0 by the equation\np0(v) = p(v) - l(v)/n. We claim that this potential function satisfies our desired properties.\nClaim 3 The new potential function p0(v) = p(v)-l(v)/n satisfies the property that f is 0-optimal\nwith respect to p0 for some constant 0 ≤ (1 - 1/n).\nProof:\nLet (v, w) ∈ Ef , then\ncp0 (v, w) = c(v, w) + p0(v) - p0(w)\n= c(v, w) + p(v) - l(v)/n - p(w) + l(w)/n\n= cp(v, w) + (l(w) - l(v))/n.\nWe consider two cases, depending on whether or not l(v) < l(w).\nCase 1: l(v) < l(w). Then\ncp0 (v, w) = cp(v, w) + (l(w) - l(v))/n\n≥- + /n\n= -(1 - 1/n).\nCase 2: l(v) > l(w), so that (v, w) is not an admissible edge. Then\ncp0 (v, w) = cp(v, w) + (l(w) - l(v))/n\n≥ 0 - (n - 1)/n\n= -(1 - 1/n).\nIn either case, we see that f is 0-optimal with respect to p0, where 0 ≤ (1 - 1/n).\n5-3\n\n2.2.2\nCancel Step\nWe now shift our attention to the implementation and analysis of the Cancel step. Na ıvely, it takes\nO(m) time to find a cycle in the admissible graph Ga = (V, A) (e.g., using Depth-First Search) and\npush flow along it. Using a more careful implementation of the Cancel step, we shall show that each\ncycle in the admissible graph can be found in an \"amortized\" time of O(n).\nWe use a Depth-First Search (DFS) approach, pushing as much flow as possible along an ad\nmissible cycle and removing saturated edges, as well as removing edges from the admissible graph\nwhenever we determine that they are not part of any cycle. Our algorithm is as follows:\nCancel(Ga = (V, A)): Choose an arbitrary vertex u ∈ V , and begin a DFS rooted at u.\n1. If we reach a vertex v that has no outgoing edges, then we backtrack, deleting from A the\nedges that we backtrack along, until we find an ancestor r of v for which there is another child\nto explore. (Notice that every edge we backtrack along cannot be part of any cycle.) Continue\nthe DFS by exploring paths outgoing from r.\n2. If we find a cycle Γ, then we push the maximum possible flow through it. This causes at\nleast one edge along Γ to be saturated. We remove the saturated edges from A, and start\nthe depth-first-search from scratch using G0\na = (V, A0), where A0 denotes A with the saturated\nedges removed.\nEvery edge that is not part of any cycle is visited at most twice (since it is removed from the\nadmissible graph the second time), so the time taken to remove edges that are not part of any cycle\nis O(m). Since there are n vertices in the graph, it takes O(n) time to find a cycle (excluding the\ntime taken to traverse edges that are not part of any cycle), determine the maximum flow that\nwe can push through it, and update the flow in each of its edges. Since at least one edge of A is\nsaturated and removed every time we find a cycle, it follows that we find at most m cycles. Hence,\nthe total running time of the Cancel step is O(m + mn) = O(mn).\n2.2.3\nOverall Running Time\nFrom the above analysis, we see that the Cancel step requires O(mn) time per iteration, whereas\nthe Tighten step only requires O(m) time per iteration. In the previous lecture, we determined\nthat the Cancel-and-Tighten algorithm requires O(min(n log(nC), mn log n)) iterations. Hence the\noverall running time is O(min(mn2 log(nC), m2n2 log n)).\nOver the course of the next few lectures, we will develop data structures that will enable us to\nreduce the running time of a single Cancel step from O(mn) to O(m log n). Using dynamic trees, we\ncan reduce the running time of the Cancel step to an amortized time of O(log n) per cycle canceled.\nThis will reduce the overall running time to O(min(mn log(nC) log n, m2n log2 n)).\nBinary Search Trees\nIn this section, we review some of the basic properties of binary search trees and the operations\nthey support, before introducing splay trees. A Binary Search Tree (BST) is a data structure that\nmaintains a dictionary. It stores a collection of objects with ordered keys. For an object (or node)\nx, we use key[x] to denote the key of x.\nProperty of a BST. The following invariant must always be satisfied in a BST:\n- If y lies in the left subtree of x, then key[y] ≤ key[x]\n- If z lies in the right subtree of x, then key[z] ≥ key[x]\n5-4\n\nOperations on a BST. Here are some operations typically supported by a BST:\n- Find(k): Determines whether the BST contains an object x with key[x] = k; if so, returns the\nobject, and if not, returns false.\n- Insert(x): Inserts a new node x into the tree.\n- Delete(x): Deletes x from the tree.\n- Min: Finds the node with the minimum key from the tree.\n- Max: Finds the node with the minimum key from the tree.\n- Successor(x): Find the node with the smallest key greater than key[x].\n- Predecessor(x): Find the node with the greatest key less than key[x].\n- Split(x): Returns two BSTs: one containing all the nodes y where key[y] < key[x], and the\nother containing all the nodes z where key[z] ≥ key[x].\n- Join(T1, x, T2): Given two BSTs T1 and T2, where all the keys in T1 are at most key[x], and\nall the keys in T2 are at least key[x], returns a BST containing T1, x and T2.\nFor example, the procedure Find(k) can be implemented by traversing through the tree, and\nbranching to the left (resp. right) if the current node has key greater than (resp. less than) k. The\nrunning time for many of these operations is linear in the height of the tree, which can be as high\nas O(n) in the worst case, where n is the number of nodes in the tree.\nA balanced BST is a BST whose height is maintained at O(log n), so that the above operations\ncan be run in O(log n) time. Examples of BSTs include Red-Black trees, AVL trees, and B-trees.\nIn the next lecture, we will discuss a data structure called splay trees, which is a self-balancing\nBST with amortized cost of O(log n) per operation. The idea is that every time a node is accessed,\nit gets pushed up to the root of the tree.\nThe basic operations of a splay tree are rotations. They are illustrated the following diagram.\nA\nB\nC\nx\ny\nA\nB\nC\nx\ny\nzig (right rotation)\nzag (left rotation)\n5-5"
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 6 - Splay Trees",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/921232cb9a69015c50002ff5ea6a9824_lec6.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 24, 2008\nLecture 6 - Splay Trees\nLecturer: Michel X. Goemans\nIntroduction\nIn this lecture, we investigate splay trees, a type of binary search tree (BST) first formulated by\nSleator and Tarjan in 1985. Splay trees are self-adjusting BSTs that have the additional helpful\nproperty that more commonly accessed nodes are more quickly retrieved. They have good behavior\nwhen compared to many other types of self-balancing BSTs, even when the operations are unknown\nand non-uniform. While in the worst case, operations can take O(n) time, splay trees maintain\nO(log n) amortized cost for basic BST operations, and are within a constant factor to the cost of\nany static BST.\nWe first give an overview of the operations used in splay trees, then give an amortized analysis of\nits behavior. We conclude by noting its behavior relative to other Binary Search Trees.\nSplay Tree Structure\nA splay tree is a dynamic binary search tree, meaning that it performs additional operations to\noptimize behavior. Because they are BSTs, given a node x in a splay tree and a node y in the left\nsubtree of x, we have key(y) < key(x). Similarly, for a node z in the right subtree of x, we have\nkey(x) < key(z). This is the binary search tree property. A well-balanced splay tree will have height\nΘ(log(n), where n is the number of nodes.\nSplay trees achieve their efficiency through use of the following operations:\n2.1\nRotation\nThe basic operation used in splay trees (or any other dynamic BST) is the rotation. A rotation\ninvolves rearranging the nodes of a subtree rooted at y so that one of the children x of y becomes\nthe new root of the subtree, while maintaining the binary search tree property. This is illustrated\nin Figure 1.\nWhen the left child becomes the new root, the rotation is a right rotation. When the right child\nbecomes the new root, the rotation is a left rotation. We call a right rotation a zig and a left rotation\na zag.\nThe key idea of the splay tree is to bring node x to the root of the tree when accessing x via rotations.\nThis brings the most recently accessed nodes closer to the top of the tree.\nHowever, there are many ways of bringing a node to the root via rotations, and we must therefore\nspecify in which order we perform them. Consider a linear tree (effectively a linked list) of the values\n1, . . . , n, rooted at n. Suppose we access the value 1. If we use the naive (and most natural) method\nof repeatedly performing a zig to bring 1 at the top, we proceed as illustrated in Figure 2. The\nresulting tree has the same height as the original tree, and is clearly not better balanced. We must\ntry a more clever approach than successive, single rotations.\n6 - Splay Trees-1\n\nFigure 1: Rotation via zigs and zags.\nFigure 2: When we access node 1 and try to bring it up via pure rotations, the result is a tree that\nis just as unbalanced as before.\n2.2\nSplay-Step\nWe now define an operation called splay-step. In one splay-step on a node x, x is brought up 2\nlevels with rotations (or just 1 level if x's parent is the root). When some node x is accessed in the\nsplay tree, we bring x up with a series of splay-steps until it is the root.\nWe separate the actions performed for the splay-step into the following categories. Call the node\nthat we are trying to access x, its parent y, and y's parent z.\n- Case 0: x is the root. Do nothing in this case.\n- Case 1: y is the root. If x is the left child of the root, perform a zig on x and y. If not,\nperform a zag.\n- Case 2: x and y are both left children (or both right children). Let us look at the case when\nboth x and y are left children. We first do a zig on the y-z connection. Then, we do a zig on\nthe x-y connection. If x and y are right children, we do the same thing, but with zags instead.\n(See Figure 3.)\n- Case 3: x is a left child and y is a right child, or vice versa. Consider the case where x is a\nright child, and y is a left child. We first do a zag on the x-y edge, and then a zig on the x-z\nedge. In the case where x is a left child and y a right child, we do the same thing, but with a\nzig on the first move, followed by a zag. (See Figure 4.)\n6 - Splay Trees-2\n\nFigure 3: Case 2 of the splay-step is when x and y are the same type of children. In this figure, we\nfirst do a zig on y - z, and then a zig on x - z.\nFigure 4: In Case 3, x and y are not the same type of children. In this case, we do a zag on the\nx - y edge, and then a zig on the x - z edge.\nNote that in the case of the earlier example with the chain of nodes, using splay-step instead of\ndirect rotations results in a much more balanced tree, see Figure 5.\n2.3\nSplay\nWith the splay-step operation, we can bring the node x to the root of the splay tree with the\nprocedure:\nsplay(x):\nWHILE x=root:\nDO splay-step(x)\nThe described procedure performs the splay operation in a bottom-up order. It is possible to perform\nthe splay operation in a top down fashion, which would result in the same running time.\n6 - Splay Trees-3\n\nX\nFigure 5: When splaying node 1, the resulting tree has half its original height.\nRunning-Time Analysis\n3.1\nPotential Function\nWe define a class of potential functions for the amortized analysis of operations on a splay tree. The\npotential function depends on weights that we can choose. For each node x in the tree, make the\nfollowing definitions:\n- T (x) is the subtree rooted at x (and it includes teh node x itself),\n- weight function: w(x) > 0 is the weight of node x (we can choose what this is; we'll often take\nw(x) = 1 for all nodes x)\n- weight-sum function: s(x) =\nw(y),\ny∈T (x)\n- rank function: r(x) = log2 s(x).\n6 - Splay Trees-4\n\nX\n\nThen we define the potential function as:\nφ =\nr(x).\nx∈T (root)\n3.2\nAmortized Cost of Splay(x)\nUsing the potential function described above, we can show that the amortized cost of the splay\noperation is O(log n). For the purposes of cost analysis, we assume a rotation takes 1 unit of time.\nLemma 1 For a splay-step operation on x that transforms the rank function r into r0, the amortized\ncost is ai ≤ 3(r0(x) - r(x)) + 1 if the parent of x is the root, and ai ≤ 3(r0(x) - r(x)) otherwise.\nProof of Lemma 1: Let the potential before the splay-step be φ and the potential after the splay-\nstep be φ0. Let the worst case cost of the operation be ci. The amortized cost ai is ai = ci + φ0 - φ.\nWe consider the three cases of splay-step operations.\nCase 1: In this case, the parent of x is the root of the tree. Call it y. After the splay-step, x\nbecomes the root and y becomes a child of x. The operation involves exactly one rotation, so ci = 1.\nThe splay step only affects the rank for x and y. Since y was the root of the tree and x is now the\nroot of the tree, r0(x) = r(y). Additionally, since y is now a child of x, (the new) T (x) contains (the\nnew) T (y), so r0(y) ≤ r0(x). Thus the amortized cost is:\nai\n= ci + φ0 - φ\n= 1 + r0(x) + r0(y) - r(x) - r(y)\n= 1 + r0(y) - r(x)\n≤ 1 + r0(x) - r(x)\n≤ 1 + 3(r0(x) - r(x)),\nsince r0(x) ≥ r(x).\nCase 2: In this case, we perform two zigs or two zags, so ci = 2. Let the parent of x be y and the\nparent of y be z. Node x takes the place of z after the splay-step, so r0(x) = r(z). Also, we see in\nFigure 3 that r(y) ≥ r(x) (since y was the parent of x) and r0(y) ≤ r0(x) (since y is now a child of\nx). Then the amortized cost is:\nai\n= ci + φ0 - φ\n= 2 + r0(x) + r0(y) + r0(z) - r(x) - r(y) - r(z)\n= 2 + r0(y) + r0(z) - r(x) - r(y)\n≤ 2 + r0(x) + r0(z) - r(x) - r(x).\nNext, we use the fact that the log function is concave, or log a+log b ≤ log ( a+b ). If the splay-step\noperation transforms the weight-sum function s into s0, we have:\nlog2 (s(x)) + log2 (s0(z))\ns(x) + s0(z)\n≤ log2\n.\n6 - Splay Trees-5\n\nThe left side is equal to r(x)+\nr0 (z) . On the right side, note that\ns(x) + s0(z) ≤ s0(x);\nindeed the old subtree T (x) and the new subtree T 0(z) cover all nodes of T 0(x), except y (thus\ns(x) + s0(z) = s0(x) - w(y)). Thus, we have:\nr(x) + r0(z)\nlog2 (s0(x))\n≤\n= r0(x) - 1,\nor\nr0(z) ≤ 2r0(x) - r(x) - 2.\nTherefore, the amortized cost is:\nai\n≤ 2 + r0(x) + 2r0(x) - r(x) - 2 - r(x) - r(x)\n= 3(r0(x) - r(x)).\nCase 3: In this case, we perform a zig followed by a zag, or vice versa, so ci = 2. Let the parent\nof x be y and the parent of y be z. Again, r0(x) = r(z) and r(y) ≥ r(x). Then the amortized cost is:\nai\n= ci + φ0 - φ\n= 2 + r0(x) + r0(y) + r0(z) - r(x) - r(y) - r(z)\n≤ 2 + r0(y) + r0(z) - r(x) - r(x).\nNote in Figure 4 that s0(y) + s0(z) ≤ s0(x). Using the fact that the log function is concave as before,\nwe find that r0(y) + r0(z) ≤ 2r0(x) - 2. Then we conclude\nai\n≤ 2 + 2r0(x) - 2 - r(x) - r(x)\n≤ 2(r0(x) - r(x))\n≤ 3(r0(x) - r(x)).\nLemma 2 The amortized cost of the splay operation on a node x in a splay tree is O(1+log s(root) ).\ns(x)\nProof of Lemma 2: The amortized cost a(splay(x)) of the splay operation is the sum of all of the\nsplay-step operations performed on x. Suppose that we perform k splay-step operations on x. Let\nr0(x) be the rank of x before the splay operation. Let ri(x) be the rank of x after the ith splay-step\noperation. Then we have rk(x) = r0(root) and:\na(splay(x)) ≤ 3(rk(x) - rk-1(x)) + 3(rk-1(x) - rk-2(x)) + ... + 3(r1(x) - r0(x)) + 1\n= 3(rk(x) - r0(x)) + 1\n= 3(r0(root) - r0(x)) + 1.\nThe added 1 comes from the possibility of a case 1 splay-step at the end. The definition of r gives\nthe result.\n\nThe above lemma gives the amortized cost of a splay operation, for any settings of the weights. To\nbe able to get good bounds on the total cost of any sequence of operations, we set w(x) = 1 for all\nnodes x. This implies that s(root) ≤ n where n is the total number of nodes ever in the BST, and\nby Lemma 2, the amortized cost of any splay operation is a(splay(x)) = O(log n).\n6 - Splay Trees-6\n\n3.3\nAmortized Cost of BST operations\nWe now need to show how to implement the various BST operations, and analyze their (amortized)\ncost (still with the weights set to 1).\n3.3.1\nFind\nFinding an element in the splay tree follows the same behavior as in a BST. After we find our node,\nwe splay it, which is O(log n) amortized cost. The cost of going down the tree to find the node can\nbe charged to the cost of splaying it. Thus, the total amortized cost of Find is O(log n). (Note: if\nthe node is not found, we splay the last node reached.)\n3.3.2\nFind-Min\nThis operation will only go down the left children, until none are left, and this cost will be charged\nto the subsequent splay operation. After we find the min node, we splay it, which takes O(log n)\namortized cost. The total amortized cost is then O(log n).\n3.3.3\nFind-Max\nThe process for this is the same as for Find-Min, except we go down the right child. The total\namortized cost of this is O(log n) as well.\n3.3.4\nJoin\nGiven two trees T1 and T2 with key(x) < key(y) ∀x ∈ T1, y ∈ T2, we can join T1 and T2 into one tree\nwith the following steps:\n1. Find-Max(T1). This makes the max element of T1 the new root of T1.\n2. Make T2 the right child of this.\nThe amortized cost of the first step is O(log n). For the second step, the actual cost is 1, but we\nneed to take into account in the amortized cost the increase in the potential function value. Before\nstep 2, T1 and T2 had a potential function value of φ(T1) and φ(T2). After it, the resulting tree has\na potential function value ≤ φ(T1) + φ(T2) + log n, since the rank of the new root is ≤ log(n). So\nthe amortized cost of Join is O(log n).\n3.3.5\nSplit\nGiven a tree T and a pivot i, the split operation partitions T into two BSTs:\nT1 : {x | key(x) ≤ i},\nT2 : {x | key(x) > i}.\nWe split the tree T by performing Find(i). This Find will then splay on a node, call it x, which\nbrings it to the root of the tree. We can then cut the tree; everything on the right of x belongs to\n6 - Splay Trees-7\n\nX\nX\nX\nX\nX\nX\n\nT2, and everything on the left belongs to T1. Depending on its key, we add x to either T1 or T2.\nThus, we either make the right child or the left child of x a new root by simply removing its pointer\nto its parent.\nThe amortized cost of the Find operation is O(log n). The actual cost of creating the second BST\n(by cutting off one of the children) is just O(1), and the potential function does not increase (as the\nrank of the root does not increase). Thus the total amortized time of a Split is also O(log n) time.\nJoin and Split make insertion and deletion very simple.\n3.3.6\nInsert\nLet i be the value we want to insert. We can first split the tree around i. Then, we let node i be\nthe new root, and make the two subtrees the left and right subtrees of i respectively. The amortized\ncost again is O(logn).\n3.3.7\nDelete\nTo delete a node i from a tree T , we first Find(i) in the tree, which brings node i to the root.\nWe then delete node i, and are left with its left and right subtrees. Because everything in the left\nsubtree has key less than everything in the right subtree, we can then join them. It is easy to see\nthat this has amortized cost O(log n) as well.\n3.3.8\nTotal cost of m operations\nThe next theorem shows that the cost of any sequence of operations on a splay tree has worst-case\ntime similar to any balanced BST (unless the number of operations m is o(n) where n is the number\nof keys).\nTheorem 3 For any sequence of m operations on a splay tree containing at most n keys, the total\ncost is O((m + n) log n).\nProof of Theorem 3:\nLet ai be the amortized cost of the ith operation. Let ci be the real cost\nof the ith operation. Let φ0 be the potential before and φm be the potential after the m operations.\nThe total amortized cost of m operations is:\nm\nm\nai =\nci + φm - φ0.\ni=1\ni=1\nThen we have:\nm\nm\nci =\nai + φ0 - φm.\ni=1\ni=1\nSince we chose w(x) = 1 for all x, we have that, for any node x, r(x) ≤ log n. Thus φ0 -φm ≤ n log n,\nso we conclude:\nm\nm\nci =\nai + O(n log n) = O(m log n) + O(n log n) = O((m + n) log n).\ni=1\ni=1\n6 - Splay Trees-8\n\nX\nX\nX\n\n!\nP\nP\nComparison to other BSTs\n4.1\nStatic Optimality Property\nWe will show that splay trees are competitive against any binary search tree that does not involve\nany rotations. We consider BSTs containing n keys, and sequences of operations that contain only\nFind operations (thus, no Insert or Delete for example).\nTheorem 4 Define a static binary search tree to be one that uses no rotation operations. Let mi\nbe the number of times element i is accessed for i = 1, . . . , n. We assume mi ≥ 1 for all i. Then the\ntotal cost for accessing every element i mi times is at most a constant times the total cost of any\nstatic binary search tree.\nProof of Theorem 4:\nConsider any binary search tree T rooted at t. Let l(i) be the height\nof of i in T , or the number of nodes on the path from i to the root of T , so l(t) = 1. In T , the\ncost for accessing an element i is l(i), so the total cost for accessing every element i mi times is\nl(i)mi. We want to show that the total cost of operations on a splay tree, irrespective of the\ni\nX\nstarting configuration, is O(\nl(i)mi).\ni\nWe choose a different weight function that earlier. Here, we define the weights to be w(i) = 3-l(i)\nfor all i. Note that s(t) ≤ 1 + 2( 3\n2 ) + 22( 3\n3 ) + . . . = 1. Then, by Lemma 2, the amortized cost of\nfinding i is:\ns(t)\na(i) = O(1 + log2\n) = O(1 + log2\n) = O(1 + l(i)).\ns(i)\n3-l(i)\nThe total amortized cost of accessing every element i mi times on a splay tree is thus:\nO(m +\nl(i)mi) = O\nl(i)mi\n.\ni\ni\nThis is the amortized cost, we now need to argue about the actual cost. Let φ be the potential\nbefore the beginning of the sequence, and φ' be the potential after the sequence of operations. For\na node i, let r(i) be the rank of i before and r0(i) be the rank after the operations. Note that (since\nr(i) ≤ log2 1 and r0(i) ≥ log2 w(i)):\n\n!\nφ - φ0 =\nX\ni\nr(i) - r0(i) ≤\nX\ni\nlog2\n3-l(i) = O\nX\ni\nl(i) .\nThen we have:\n\n!\n\n!\n\n!\nX\nX\nX\nX\nX\n=\nci\nai + φ - φ0 = O\nl(i)mi\n+ O\nl(i)\n= O\nl(i)mi\n,\ni\ni\ni\nsince our assumption mi ≥ 1 implies that\ni l(i) ≤\ni l(i)m(i).\n\n4.2\nDynamic Optimality Conjecture\nThe Dynamic Optimality Conjecture claims that Splay Trees are efficient up to a constant factor to\nany self-adjusting Binary Search Tree (allowing an arbitrary number of (arbitrary) rotations between\naccesses). This conjecture was first put forth in the Tarjan and Sleater's original Splay Tree paper\nin 1985, and has withstood attempts to prove or disprove it since.\n6 - Splay Trees-9\n\n4.3\nScanning Theorem\nThe scanning theorem states that, for a splay tree that contains the values [1, 2, . . . , n], accessing\nall of those elements in sequential order takes O(n) time, regardless of the initial arrangement of\nthe tree. An interesting point is that, even though the Scanning Theorem has been proved, if the\nDynamic Optimality Conjecture were true, then it would follow directly from the fact that one can\ncreate dynamic BST's that perform sequential access in linear time.\n6 - Splay Trees-10"
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 7 - Dynamic Trees",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/ef1228eac504ad517af51a2a3e8e1a93_lec7.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 29, 2008\nLecture 7 - Dynamic Trees\nLecturer: Michel X. Goemans\nOverview\nIn this lecture, we discuss dynamic trees, a sophisticated data structure introduced by Sleator and\nTarjan. Dynamic trees allow to provide the fastest worst-case running times for many network flow\nalgorithms. In particular, it will allow us to efficiently perform the Cancel operation in the Cancel\nand Tighten algorithm. Dynamic trees build upon splay trees, which we introduced in the previous\nlecture.\nDynamic trees manage a set of node-disjoint (not necessarily binary) rooted trees. With each\nnode v is associated a cost. In our use of dynamic trees, the cost will be coming from the edge\n(p(v), v), where p(v) denotes the parent of v; the cost of the root in that case will be set arbitrarily\nlarge (larger than the cost of any other node), say +inf.\nFigure 1: Example of Dynamic Tree.\nDynamic trees will support the following operations:\n- make-tree(v): Creates a tree with a single node v, whose cost is +inf.\n- find-root(v): Finds and returns the root of the tree containing the node v.\n- find-cost(v): Returns the cost of node v. (This may sound like a trivial operation, but\nin fact there is real work to be done, because we will not explicitly maintain the costs of all\nnodes.)\n- find-min(v): Finds and returns the ancestor of w of v with minimum cost. Ties go to the\nnode closest to the root.\n- add-cost(v, x): Adds x to the cost of all nodes w on the path from find-root(v) to v.\n- cut(v): Breaks the rooted tree in two by removing the link to v from its parent. The node v\nis now the root of a new tree, and its cost is set to +inf.\n- link(v, w, x): Assumes that (1) w is a root, and (2) v and w are not in the same tree, i.e.\nfind-root(v)\nw. Combines two trees by adding an edge (v, w), i.e. p(w) = v.\n=\nSets the cost\nof w equal to x.\nWe will later show that all of these operations run in O(log n) amortized time.\n7 - Dynamic Trees-1\n\nv\nv\nFigure 2: cut(v) operation.\nLINK\nW\nV\nCOST(W) = X\nFigure 3: link(v, w, x) operation.\nTheorem 1 The total running time of any sequence of m dynamic tree operations is O((m +\nn) log n), where n is the number of nodes.\nWe defer the proof of this theorem until the next lecture.\nImplementation of Cancel with dynamic trees\nRecall the setting for the Cancel step in the algorithm Cancel and Tighten for the minimum cost\nflow problem. We have a circulation f and node potentials p in an instance defined on graph G.\nRecall that an edge (v, w) is admissible if cp(v, w) < 0, and the admissible graph (V, Ea), is the\nsubgraph of Ef (the residual graph corresponding to our circulation) containing only the admissible\nedges. Our aim is to repeatedly find a cycle in the admissible graph and saturate it. Each time we\ndo this, all of the saturated edges disappear from the graph. Also recall that no edges are added\nto the admissible graph during this process, because any new edge in the residual graph must have\npositive reduced cost and are therefore is not admissible.\nWe represent the problem with dynamic trees, where the nodes in the dynamic trees correspond\nto nodes in G and the edges of the dynamic trees are a subset of the admissible edges. We maintain\ntwo (disjoint) sets of admissible edges: those which are currently in the dynamic tree, and those\nwhich still need to be considered. The cost of a node v will correspond to the residual capacity\nuf (p(v), v) of the edge (p(v), v), unless v is a root node, in which case it will have cost +inf. We\nwill also mark some of the roots (denoted graphically with a (∗)) to indicate that we dealt with\nthem and concluded they can't be part of any cycle. For the edges not in the dynamic tree, we also\nmaintain the flow value. (We don't need to maintain the flow explicitly for the edges in the trees,\nsince we can recover the flow from the edge capacities in G and the residual capacity.)\nTo summarize, we begin with a set of n singleton trees. All of the edges start out in the remaining\npool. In each iteration, we try to find an admissible edge leading to the root r of one of the dynamic\ntrees. If we fail to find such an edge, this implies there are no admissible cycles which include r,\n7 - Dynamic Trees-2\n\nand so we mark it and remove it from consideration. Suppose, on the other hand, that we do find\nan edge (w, r) leading into the root. If w is in a different tree, we join the two trees by adding an\nedge connecting w and r. On the other hand, if w and r are part of the same tree, it means we have\nfound a cycle. In this case, we push flow along the cycle and remove the saturated edges from the\ndata structure.\nIn more detail, we keep repeating the following procedure as long as there still exist unmarked\nroots:\n⊲\nChoose an unmarked root r.\n⊲\nAmong admissible edges, try to find one which leads to r.\n⊲\nCASE 1: there is no such (v, r) ∈ Ea.\n⊲\nMark r, since we know it cannot possibly be part of a cycle.\n⊲\nCut all the children v of r.\n⊲\nSet\nf(r, v)\n← u(r, v) - uf (r, v)\n=\nu(r, v) - find-cost(v)\n⊲\nCASE 2: there is an admissible edge (w, r) from a different tree, i.e.\nfind-root(w) =\nr.\n⊲\nLink the two trees: link(w, r, u(w, r) - f(w, r))\n⊲\nCASE 3: there is an admissible edge (w, r) from the same tree, i.e.\nfind-root(w) =\nr.\n⊲\nWe've found a cycle, so push flow along the cycle. The amount we can push is\nδ = min(u(w, r) - f(w, r), find-cost(find-min(w)))\n⊲\nadd-cost(w, -δ)\n⊲\nIncrease f(w, r) by δ\n⊲\nIf f(w, r) = u(w, r), then (u, r) is inadmissible, so we get rid of it.\n⊲\nWhile find-cost(find-min(w)) = 0:\n⊲\nz ← find-min(w)\n⊲\nf(p(z), z) ← u(p(z), z)\n⊲\ncut(z)\nThe last while loop is to delete all the edges that became inadmissible along the path from r to w.\n2.1\nRunning time\nIn a cancel step, we end up cancelling at most O(m) cycles, where m is the number of edges. In\naddition, each edge gets saturated at most once (if it does, it becomes inadmissible); therefore the\nnumber of cut(z) and find-min(w) over all cases 3 is O(m). Thus the total number of dynamic\ntree (and also other arithmetic or control) operations is at most O(m). Hence, by Theorem 1, the\nrunning time of each Cancel operation is O((m + n) log n) = O(m log n). The overall running time\nof Cancel-and-Tighten is therefore O(m2n log2 n) (strongly polynomial running time bound) or\nO(mn log n log(nC)).\nDynamic trees implementation\nWe now turn to the implementation of dynamic trees. Here we present the definitions; we will cover\nthe running time analysis in the next lecture. The dynamic trees data structure is a collection of\nrooted trees. We decompose each rooted tree into a set of node-disjoint (directed) paths, as shown\nin Figure 4. Each node is in precisely one path (possibly containing that node only). We will refer\n7 - Dynamic Trees-3\n\nFigure 4: Decomposition of rooted tree.\nto the edges on these paths as solid edges, and we will refer to the remaining edges as dashed\nedges, or middle edges. Each path is directed from its tail (highest in the tree) to its head lowest\nin the tree).\nThere are many possible ways to partition a tree into solid paths. For instance, if we are given a\nsolid edge and a dashed edge which are both children of a single parent, we can swap the solid and\ndashed edges. This follows from the basic observation that, for any middle edge (v, w), w is the tail\nof a solid path. This operation is known as splicing as shown in Figure 5.\nSplicing\nFigure 5: Splicing in the rooted tree.\nIn a dynamic tree, each solid path is represented by a splay tree, where the nodes are sorted\nin increasing order from the head to the tail, as shown in Figure 6. In other words, the node with\nsmallest key is the head (the lowest in the tree), and the node with largest key is the tail (the highest\nin the tree)\nIn addition, we will maintain links between the different splay trees. The root of each splay\ntree is attached to the parent of the tail of the path in the rooted tree, as shown in Figure 7. For\nexample, the edge (e, f) in the original rooted tree becomes the edge (e, i) linking e to the root i of\nthe splay tree corresponding to the solid path f → i. The entire data structure -- with the splay\ntrees corresponding to the same rooted tree being connected to each other -- forms what is called\na virtual tree. Any given node of the virtual tree may have at most one left child and at most one\nright child (of a splay tree), as well as any number of children attached by dashed edges. Children\nattached by dashed edges are known as middle children, and we draw them in between the left\nand right children.\nNotice that we can reconstruct the rooted tree from the virtual tree. Each splay tree corresponds\nto a solid path from the node of lowest key to the node of highest key. In addition, for any middle\n7 - Dynamic Trees-4\n\nf\ne\nd\nc\nb\na\nc\nb\na\ne\nf\nd\nHEAD\nTAIL\nFigure 6: Representation of solid path from head to tail in BST (Splay Tree).\na\nb\nc\nd\ne\nf\ni\ng\nh\nb\na\nc\ne\ni\nf\ng\nh\nd\nFigure 7: Rooted tree on the left and corresponding virtual tree on the right.\nedge, we get an edge of the original rooted tree; for example, to (e, i) in the virtual tree, corresponds\nthe edge (e, f) in the original tree where f is the node with highest key in the splay tree in which i\nresides.\nNote that there are many different ways to represent rooted trees as virtual trees, and we can\nmodify virtual trees in various ways which don't affect the rooted trees.\nIn particular, we define the Expose(v) operation, which brings a given node v to the root of the\nvirtual tree. This operation involves three main steps:\n1. Make sure that the path from v to the root only uses roots of splay trees. This can be done\nby performing splay operations whenever we enter a new splay tree.\n2. Make sure that the path from v to the root consists entirely of solid edges. We can ensure this\nthrough repeated splicing.\n3. Do the splay operation to bring v to the top of the resulting splay tree. This is justified since\nv is now in the same splay tree as the root of the original rooted tree.\n7 - Dynamic Trees-5"
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 8",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/5cccdb7dca0aa81810781ba75d0a1d46_lec8.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nOctober 1, 2008\nLecture 8\nLecturer: Michel X. Goemans\nPreviously, we introduced the dynamic tree data structure and the operations that dynamic\ntrees must support. Today, we take a more detailed look at dynamic trees and describe the efficient\nimplementation of the operations. In doing so, much of our focus will be on the Expose method,\nan extended splay operation that is essential in all these operations. We show that any sequence of\nm operations on a dynamic tree with n nodes takes O((m + n) log n) time.\nDynamic Trees\nDynamic trees (also known as link-cut trees) introduced by Sleator and Tarjan are a data structure\nintended to maintain a representation of a set of rooted trees. We will be able to perform various\noperations on these trees, to be discussed later. Figure 1 shows an example tree as a virtual tree\n(left) and a rooted tree (right).\n1.1\nRooted Trees\nWe view rooted trees as unions of node-disjoint (directed) paths. This divides the edges of the tree\ninto two sets. Solid edges are those that are on the node-disjoint paths that the tree is composed\nof, and dashed edges are those that are not on these paths. Note that each path consisting of solid\nedges is a directed path (we omit the arrows here) from top to bottom.\n1.2\nVirtual Trees\nThe union of disjoint paths described above can be used to represent virtual trees. In a virtual tree,\neach solid path is represented by a splay tree such that the following conditions hold:\nA successor node in a splay tree is an ancestor in the rooted tree.\n-\nFor each splay tree, its largest node is linked to the parent of the root in the rooted tree.\n-\nIn the virtual tree, each node has at most one left child, at most one right child, and any\n-\nnumber of middle (virtual) children.\nThere are three kinds of edges in a virtual tree, corresponding to the three types of children a\nnode can have. Left and right children of a node are connected to the node by solid edges, and\nmiddle children of a node are connected to it by dashed edges. Note that there can be many virtual\ntrees corresponding to a rooted tree, because there are two different degrees of freedom involved in\nconstructing a virtual tree -- the union of disjoint paths could be different, as could the structure\nof the splay trees corresponding to the paths.\nAn important consequence of this setup is that rotations in a splay tree do not affect the structure\nof the rooted tree.\nThe Expose Operation\nThe Expose(v) operation is an extended splay operation that brings v to the root of the virtual\ntree without changing the structure of the rooted tree. The important parts of this operation are to\n8-1\n\nFigure 1: Virtual tree (left) and corresponding rooted tree (right).\nmake sure that the path from v to the root is solid and that the splay tree representing the path to\nwhich v belongs is rooted at v. We can describe this operation in three steps. In our example, we\nrun Expose on node 15.\n2.1\nStep 1\nStep 1 consists of walking from v to the root of the virtual tree. Whenever the walk enters a splay\ntree (solid edges) at some node w, a Splay(w) operation is performed, bringing w to the root of\nthat tree. Middle children are not affected in this step. For instance, we splay nodes 11 and 5 in\nour example tree as in figure 2. Note that at the end of step 1 of an Expose(v) operation, v will be\nconnected to the root of the virtual tree only by dashed edges.\n2.2\nStep 2: Splicing\nStep 2 consists of walking from v to the root of the virtual tree exchanging along the way each\nmiddle edge with the left subtree of the parent. This is illustrated in Figure 3 and called splicing.\nA middle child of a node w and its left child can be exchanged (without changing the rooted tree)\nonly if w is the root of its splay tree. This justifies our execution of step 1 first since at the end of\nstep 1 all edges from v to the root are middle edges.\nSplicing is a valid operation on virtual trees. Indeed, referring to Figure 3, the left subtree of\nw in the splay tree corresponds to the part of the solid path that is below w in the rooted tree;\nthis is because w is the root of its splay tree. Exchanging that solid subpath with the solid path\ncorresponding to the splay tree rooted at v still leaves the rooted tree decomposed into a node-disjoint\nunion of paths.\nNote that after performing this operation on every edge to the root of the virtual tree, there will\nbe a solid path from the root of the rooted tree to the node being exposed.\n8-2\n\nFigure 2: Walking Up and Splaying. The virtual tree after splaying 15 and 11 is shown on the left.\nThe virtual tree on the right is at the end of step 1, after splaying also node 5.\nFigure 3: Splicing. w needs to be the root of its splay tree.\n8-3\n\nFigure 4: Left virtual tree is after first splicing, the right virtual tree is the one at the end of step 2.\nThe result of splicing every node on the path to the root for our example is illustrated in Figure\n4.\n2.3\nStep 3\nStep 3 consists of walking from v to the root in the virtual tree, splaying v to the root. Note that\nin the analysis, we can charge the entire cost of step 2 to the final splaying operation in step 3.\nFigure 5 shows the relevant splay tree before and after this step.\nOperations on Dynamic Trees\nWe will now describe the desired operations on a dynamic tree and how to implement them efficiently\nusing the Expose method just defined. Some of these operations require keeping track of different\ncosts in the tree, so we first consider an efficient way of doing this.\n3.1\nMaintaining Cost Information\nWhen performing operations on the dynamic tree, we need to keep track of cost(x) for each node x,\nand we need to be able to find the minimum cost along paths to the root of the rooted tree. If such\na path is the prefix of a path corresponding to a splay tree, it seems that, knowing the minimum\ncost in any subtree of any our splay trees might be helpful. So, in addition to cost(x), we would like\nto keep track of the value mincost(x), given by\nmincost(x) = min{cost(y) | y in the subtree rooted at x of x's splay tree}.\nWe'll see that, instead of maintaining cost(x) and mincost(x), that it will be easier to maintain the\nfollowing two quantities for every node x:\nΔ min(x) = cost(x) - mincost(x)\n8-4\n\nFigure 5: Splaying on Virtual Tree.\nv\nv\nc\nw\na\nb\nb\nc\nw\na\nFigure 6: Rotation.\nand\n(\ncost(x)\nif x is the root of a splay tree,\nΔ cost(x) =\ncost(x) - cost(p(x)) otherwise.\nObserve that, if x is the root of a splay tree, then cost(x) = Δ cost(x) and mincost(x) = Δ cost(x) -\nΔ min(x). This fact, combined with the Expose operation, shows that we can find cost(x) and\nmincost(x) given Δ min(x) and Δ cost(x), so it is sufficient to maintain the latter.\nWe now claim that we can update Δ min(x) and Δ cost(x) in O(1) time after a rotation or a\nsplice, which will allow us to maintain cost(x) and mincost(x) in O(1) time.\nWe first consider a rotation, see Figure 6 for the labelling of the nodes. Let Δ cost(x) and\nΔ cost0(x) correspond to before and after the rotation, respectively. Similarly define Δ min(x) and\nΔ min0(x). Observe that during a rotation, only the nodes b, w and v have their Δ cost(x) change.\nOne can check that the updates are as follows:\nΔ cost0(v)\n= Δ cost(w) + (cost(v) - cost(w))\n= Δ cost(w) + Δ cost(v),\nΔ cost0(w)\n= -Δ cost(v),\nΔ cost0(b)\n= Δ cost(b) + (cost(v) - cost(w)) = Δ cost(b) + Δ cost(v).\nBefore showing the corresponding updates for Δ min(x), observe that Δ min(x) and Δ cost(x)\n8-5\n\nsatisfy the following equation; here x is any node and l is its left child and r is its right child:\nΔ min(x) = cost(x) - mincost(x)\n= cost(x) - min(cost(x), mincost(l), mincost(r))\n= max(0, cost(x) - mincost(l), cost(x) - mincost(r))\n= max(0, Δ min(l) - Δ cost(l), Δ min(r) - Δ cost(r)).\n(1)\nFurthermore, the minimum of the subtree can be located by knowing which term attains the maxi\nmum in the last expression.\nBack to the updates for Δ min(x). The only subtrees that change are those of w and v, and so\nonly those Δ min values change. Using (1), one can see that\nΔ min0(w) = max(0, Δ min(b) - Δ cost0(b), Δ min(c) - Δ cost(c))\nΔ min0(v)\n= max(0, Δ min(a) - Δ cost(a), Δ min0(w) - Δ cost0(w)).\nNotice that Δ min0(v) depends on Δ min0(w) that was just computed.\nSimilar when we perform the splicing step given in Figure 3, Δ cost only change for v and u and\nonly Δ min(w) changes. The updates are:\nΔ cost0(v)\n=\nΔ(cost(v)) - Δ(cost(w)),\nΔ cost0(u)\n= Δ cost(u) + Δ cost(w),\nΔ min0(w) = max(0, Δ min(v) - Δ cost0(v), Δ min(z) - Δ cost(z)).\n3.2\nImplementation of Operations\nWe now describe the implementation of each of the desired operations on a dynamic tree, making\nextensive use of the Expose operation.\n- make-tree(v)\nSimply create a tree with the single node v.\n- find-root(v)\nFirst, run Expose(v). Then follow right children until a leaf w of the splay tree containing v\nis reached. Now, splay(w), and then return w.\n- find-cost(v)\nFirst, run Expose(v). Now v is the root, so return Δ cost(v) = cost(v). Note that the actual\ncomputations here were done by the updates of Δ cost(v) and Δ min(x) within the splay and\nsplice operations.\n- find-min(v)\nFirst, run Expose(v). Now, let's rewrite (1):\nΔ min(v) = max{0, -Δ cost(left(v)) + Δ min(left(v)), -Δ cost(right(v)) + Δ min(right(v))}.\nIf Δ min(v) = 0, then splay(v) and then return v, as the minimum is achieved at v. Else,\nif -Δ cost(left(v)) + Δ min(left(v)) > -Δ cost(right(v)) + Δ min(right(v)), then the minimum\nis contained in the left subtree and we walk down it recursively. Otherwise, the minimum is\ncontained in the right subtree, so we recurse down the right. Once we have found the minimum,\nwe splay it.\n8-6\n\nX\nX\n- add-cost(v, x)\nFirst, run Expose(v). Add x to Δ cost(v) and subtract x from Δ cost(left(v)). Also update\nΔ min(v) (using (1)). (The Δ min value of other nodes is unchanged.)\n- cut(v)\nFirst, run Expose(v). Add Δ cost(v) to Δ cost(right(v)). Remove the edge (v, right(v)).\n- link(v, w, x)\nFirst, run Expose(v) and Expose(w). Then, add the root w as a middle child of v. Add\nΔ cost(w) - x to Δ cost(right(v)) and to Δ cost(left(v)). Also update Δ min(w).\nAnalysis of Dynamic Trees\nWe now give an amortized analysis of cost of operations in these dynamic trees. We will see that\nany sequence of m dynamic tree operations on n nodes will take O((m + n) log n) time.\n4.1\nPotential Function\nWe will use the following potential function in our analysis, motivated by our analysis of splay trees.\nFor each node x, let w(x) = 1 be the weight assigned to x, and define\ns(x) =\nw(y),\ny∈Tx\nwhere Tx is the entire virtual tree subtree attached at x. Then, consider r(x) = log2 s(x) and take\nour final potential function to be\nX\nφ(T ) = 3\nr(x).\nx∈T\nThis differs from the potential function for splay trees in 2 ways. First Tx is defined over the entire\nvirtual tree and secondly we have this additional factor 3. We will see later why the constant factor\nof 3 was chosen here.\n4.2\nRuntime of the Expose Operation\nWe first analyze the runtime of Expose(v), since it is used in all other operations. We look at each\nstep of Expose(v) separately. Let k be the number of middle edges separating v from the root of\nthe entire virtual tree. Equivalently, k is the number of splay operations performed during Step 1.\n- Step 1: Let t(v) be the root of the splay tree containing v. Recall that the amortized cost of\nsplay(v) was 3(r(t(v)) - r(v)) + 1 when we used the potential function\nφsplay(T ) =\nr(x).\nx∈T\nWe now have the potential function φ(T ) = 3φsplay(T ), so the 3(r(t(v)) - r(v)) term here\nshould be multiplied by 3 to obtain an amortized runtime of 9(r(t(v)) - r(v)) + 1 for each call\nof splay(v) (the +1 corresponds to the cost of the last zig, if any, and so we do not need to\nmultiply it by 3).\n8-7\n\nX\nX\nWe are using the splay operation on the k nodes v, p(t(v)), . . . , (p\nt)k-1(v) in this step,\nmeaning that we get a total amortized runtime of\n*\nkX\n-1\n\n9 r(t((p * t)i(v))) - r((p * t)i(v)) + 1 ≤ 9[r(root) - r(v)] + k,\ni=0\nsince we have that r(t(p * t)i-1(v)) ≤ r((p * t)i(v)), so the sum telescopes. The amortized cost\nof step 1 is therefore O(log n) + k (since r(root) - r(v) ≤ log n).\n- Step 2: Splicing does not change the value of φ(T ), so the amortized cost for this step is the\nsame as its actual cost of k.\n- Step 3: We are using the splay operation once on node v at distance k from the root, so this\nhas an actual cost of k. Using the fact that our potential φ has an additional factor 3 in its\ndefinition compared to the splay tree version, we get from the amortized analysis of splaying\nthat:\nk + 3Δφ(T ) ≤ 3[r(root) - r(v)] + 1 = O(log n).\nMultiplying by 3, we see that we can also account for the additional cost of 2k from steps 1\nand 2, and have an amortized time of O(log n).\n- Total: We get O(log n) + k in step 1, k in step 2, and these 2k plus step 3 gives O(log n), for\na total of O(log n).\n4.3\nRuntimes of all Operations\nWe can now briefly summarize the runtimes of all other operations in terms of Expose.\n- find-cost, find-root, find-min, add-cost\nEach of these operations requires at most one use of Expose, at most one run of splay, and\nat most one search of the tree which can be charged to the last splay. Therefore, they each\nrun in O(log n) amortized time.\ncut\n-\nWe again use Expose once. We now consider the effect of the other actions on the poten\ntial function. Removing the edge (v, right(v)) decreases s(v) by s(right(v)) and leaves s(x)\nunchanged for all other x, so it decreases φ(T ), which we can safely ignore. This gives an\namortized runtime of O(log n).\nlink\n-\nWe use Expose twice. Now, when we link w to v, we see that r(v) increases by O(log n), and\nall other r(x) remain unchanged. Hence, this operation increases φ(T ) by O(log n), giving a\ntotal amortized runtime of O(log n).\nWith this analysis, we see that every operation has amortized time O(log n). A sequence of m\noperations has therefore amortized time O(m log n). Furthermore, the potential function satisfies\nφ(T ) =\nr(x) ≤\nlog n ≤ n log n,\nx∈T\nx∈T\nmeaning that any increase in potential is at most O(n log n), implying that the total cost is at most\nO((m + n) log n). We now have the following theorem.\nTheorem 1 Any m operations on a dynamic tree with n nodes run in O((m + n) log n) time.\n8-8"
    },
    {
      "category": "Lecture Notes",
      "title": "Lecture 9",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/c61e649bf05c7123da984519955da0bd_lec9.pdf",
      "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nOctober 6, 2008\nLecture 9\nLecturer: Michel X. Goemans\nLinear Programming\nLinear programming is the class of optimization problems consisting of optimizing the value of a\nlinear objective function, subject to linear equality or inequality constraints. These constraints are\nof the form\na1x1 + · · · + anxn\n{≤, =, ≥} b,\nwhere ai, b ∈ R, and the goal is to maximize or minimize an objective function of the form\nc1x1 +\n+ cnxn.\n· · ·\nIn addition, we constrain the variables xi to be nonnegative.\nThe problem can be expressed in matrix form. Given these constraints\nAx\nb\n{≤, =, ≥}\nx\n0,\n≥\nmaximize or minimize the value of\nT\nc x,\nwhere x ∈ Rn , A ∈ Rm×n , b ∈ Rm , c ∈ Rn .\nLinear programming has many applications and can also be used as a proof technique. In\naddition, it is important from a complexity point-of-view, since it is among the hardest of the class\nof polynomial-time solvable problems.\n9.1\nAlgorithms\nResearch in linear programming algorithms has been an active area for over 60 years. In this class,\nwe will discuss three major (classes of) algorithms:\nSimplex method (Dantzig 1947).\n-\n- Fast in practice.\n- Still the most-used LP algorithm today.\n- Can be nonpolynomial (exponential) in the worst case.\nEllipsoid algorithm (Shor, Khachian 1979).\n-\n- Polynomial time; this was the first polynomial-time algorithm for linear programming.\n- Can solve LP (and other more general) problems where the feasible region P = {x : Ax =\nb, x ≥ 0} is not explicitly given, but instead, given a vector x, one can efficiently decide\nwhether x ∈ P or if not, find an inequality satisfied by P but not by x.\n- Very useful for designing polynomial time algorithms for other problems.\n- Not fast in practice.\n9-1\n\n- Interior-point algorithms (Karmarkar 1984).\n- This is a class of algorithms which maintain a feasible point in the interior of P ; many\nvariants (by many researchers) have been developed.\n- Polynomial time.\n- Fast in practice.\n- Can beat the simplex method for larger problems.\n9.2\nEquivalent forms\nA linear programming problem can be modified to fit a preferred alternate form by changing the\nobjective function and/or the linear constraints. For example, one can easily transform any linear\nprogram into teh standard form: min{cT x : Ax = b, x ≥ 0}. One can use the following simple\ntransformations.\nMaximize to minimize\nmax{cT x}\nmin{-cT x}\n-→\naT\nEquality to inequality\nai\nT x = bi\na\ni\nT\nx ≤ bi\n-→\ni x ≥ bi\nInequality to nonnegativity constraint\nai\nT x ≤ bi\n-→\ns\nai\nT\n≥\nx\n+ s = bi (s ∈ Rn)\n⎧ replace xj everywhere by x + - x-\n⎨\nj\nj\n+\nVariables unrestricted in sign\nxj unrestricted in sign -→ ⎩ xj ≥ 0\nx-\nj ≥ 0\n9.3\nDefinitions\nHere is some basic terminology for a linear program.\nDefinition 1 A vector x is feasible for an LP if it satisfies all the constraints.\nDefinition 2 An LP is feasible if there exists a feasible solution x for it.\nDefinition 3 An LP is infeasible if there is no feasible solution x for it.\nDefinition 4 An LP min{cT x : Ax = b, x ≥ 0} is unbounded if, for all λ ∈ R, ∃x ∈ Rn such that\nAx = b\nx ≥ 0\ncT x ≤ λ.\n9.4\nFarkas' lemma\nIf we have a system of equations Ax = b, from linear algebra, we know that either Ax = b is\nsolvable, or the system AT y = 0, bT y = 0 is solvable.\nIndeed, since Im(A) = ker(AT )⊥, either b\nis orthogonal to ker(AT ) (in which case it is in the image of A, i.e. Ax = b is solvable) or it is\nnot orthogonal to it in which case one can find a vector y ∈ ker(AT ) with a non-zero inner product\nwith b (i.e. AT y = 0, bT y = 0 is solvable).\nFarkas' lemma generalizes this when we have also linear inequalities:\nLemma 1 ((Farkas' lemma)) Exactly one of the following holds:\n1. ∃x ∈ Rn : Ax = b, x ≥ 0,\n9-2\n\n2. ∃y ∈ Rm : AT y ≥ 0, bT y < 0.\nClearly, both cannot simultaneously happen, since the existence of such an x and a such a y\nwould mean:\nyT Ax = yT (Ax) = y T b < 0,\nwhile\nyT Ax = (AT y)T x ≥ 0,\nas the inner product of two nonnegative vectors is nonnegative. Together this gives a contradiction.\n9.4.1\nGeneralizing Farkas' Lemma\nBefore we provide a proof of the (other part of) Farkas' lemma, we would like to briefly mention\nother possible generalizations of the solvability of system of equations.\nFirst of all, consider the case in which we would like the variables x to take integer values,\nbut don't care whether they are nonnegative or not. In this case, the natural condition indeed is\nnecessary and sufficient. Formally, suppose we take this set of constraints:\nAx = b\nx\nZn\n∈\nThen if yT Ax = yT b, and we can find some yT A ∈ Zn and some yT b that is not integral, then the\nsystem of constraints is infeasible. The converse is also true.\nTheorem 2 Exactly one of the following holds:\n1. ∃x ∈ Zn : Ax = b,\n2. ∃y ∈ Rm : AT y ∈ Zn and bT y ∈/ Z.\nOne could try to combine both nonnegativity constraints and integral restrictions but in that case,\nthe necessary condition for feasibility is not sufficient. In fact, for the following set of constraints:\nAx = b\nx\n≥\nx\nZn ,\n∈\ndetermining feasibility is an NP-hard problem, and therefore we cannot expect a good characteriza\ntion (a necessary and sufficient condition that can be checked efficiently).\n9.4.2\nProof of Farkas' lemma\nWe first examine the projection theorem, which will be used in proving Farkas' lemma (see Figure\n1).\nTheorem 3 (The projection theorem) If K is a nonempty, closed, convex set in Rm and b ∈/\nK, define\np = projK (b) = arg min\n(1)\nz∈K kz - bk2.\nThen, for all z ∈ K : (z - p)T (b - p) ≤ 0.\n9-3\n\nFigure 1: The projection theorem.\nProof of Lemma 1:\nWe have seen that both systems cannot be simultaneously solvable.\nSo, now assume that @x : Ax = b, x ≥ 0 and we would like to show the existence of y satisfying\nthe required conditions. Define\nK = {Ax : x ∈ Rn , x ≥ 0} ⊆ Rm .\nBy assumption, b ∈/ K, and we can apply the projection theorem. Define p = projK (b). Since\np ∈ K, we have that p = Ax for some vector x ≥ 0. Let y = p - b ∈ Rm . We claim that y satisfies\nthe right conditions.\nIndeed, consider any point z ∈ K. We know that ∃w ≥ 0 : z = Aw. By the projection theorem,\nwe have that (Aw - Ax)T y ≥ 0, i.e.\n(w - x)T AT y ≥ 0,\n(2)\nfor all w ≥ 0. Choosing w = x + ei (where ei is the ith unit vector), we see that AT y ≥ 0. We still\nneed to show that bT y < 0. Observe that bT y = (p - y)T y = pT y - yT y < 0 because pT y ≤ 0\nand yT y > 0. The latter follows from y = 0 and the former from (2) with\nw = 0: -xT AT y ≥ 0,\ni.e. -pT y ≥ 0.\n\n9.4.3\nCorollary to Farkas' lemma\nFarkas' lemma can also be written in other equivalent forms.\nCorollary 4 Exactly one of the following holds:\n1. ∃x ∈ Rn : Ax ≤ b,\n2. ∃y ∈ Rm : y ≥ 0, AT y = 0, bT y < 0.\nAgain, x and y cannot simultaneously exist. This corollary can be either obtained by massaging\nFarkas' lemma (to put the system of inequalities in the right form), or directly from the projection\ntheorem.\n9.5\nDuality\nDuality is one of the key concepts in linear programming. Given a solution x to an LP of value z,\nhow do we decide whether or not x is in fact an optimum solution? In other words, how can we\ncalculate a lower bound on min cT x given that Ax = b, x ≥ 0?\n9-4\n\nSuppose we have y such that AT y ≤ c. Then observe that yT b = yT Ax ≤ cT x for any feasible\nsolution x. Thus yT b provides a lower bound on the value of our linear program. This conclusion\nis true for all y satisfying AT y ≤ c, so in order to find the best lower bound, we wish to maximize\nyT b under the constraint of AT y ≤ c.\nWe can see that this is in fact itself another LP. This new LP is called the dual linear program\nof the original problem, which is called the primal LP.\n- Primal LP: min cT x, given Ax = b, x ≥ 0,\n- Dual LP: max bT y, given AT y ≤ c.\n9.5.1\nWeak Duality\nThe argument we have just given shows what is known as weak duality.\nTheorem 5 If the primal P is a minimization linear program with optimum value z, then it has a\ndual D, which is a maximization problem with optimum value w and z ≥ w.\nNotice that this is true even if either the primal or the dual is infeasible or unbounded, provided\nwe use the following convention:\ninfeasible min. problem -→ value = +inf\nunbounded min. problem -→ value = -inf\ninfeasible max. problem -→ value = -inf\nunbounded max. problem -→ value = +inf\n9.5.2\nStrong Duality\nWhat is remarkable is that one even has strong duality, namely both linear programs have the same\nvalues provided at least one of them is feasible (it can happen that both the primal and the dual\nare infeasible).\nTheorem 6 If P or D is feasible, then z = w.\nProof:\nWe assume that P is feasible (the argument if D is feasible is analogous; or one could also\nargue that the dual of the dual is the primal and therefore one can exchange the roles of primal and\ndual).\nIf P is unbounded, z = -inf, and by weak duality, w ≤ z. So it must be that w = -inf and thus\nz = w.\nOtherwise (if P is not unbounded), let x∗ be the optimum solution to P, i.e.:\nz = cT x∗\nAx∗\n= b\nx∗\n≥\nWe would like to find a dual feasible solution with the same value as (or no worse than) x∗. That\nis, we are looking for a y satisfying:\nAT y ≤ c\nbT y ≥ z\nIf no such y exists, we can use Farkas' lemma to derive: ∃x ∈ Rn , x ≥ 0, and ∃λ ∈ R, λ ≥ 0 :\nAx - λb = 0 and cT x - λz < 0.\nWe now consider two cases.\n9-5\n\n-\n\nIf λ = 0, we can scale by λ, and therefore assume that λ = 1. Then we get that\n∃x ∈ Rn :\n⎧\n⎨\n⎩\nAx = b,\nx ≥ 0\ncT x < z.\nThis result is a contradiction, because x∗ was the optimum solution, and therefore we should\nnot be able to further minimize z.\nIf λ = 0 then\n-\n∃x ∈ Rm :\n⎧\n⎨\n⎩\nx ≥ 0\nAx = 0\ncT x < 0.\nConsider now x∗ + μx for any μ > 0. We have that\nx∗ + μx ≥ 0\nA(x∗ + μx)\n= Ax∗ + μAx = b + 0 = b.\nThus, x∗ + μx is feasible for any μ ≥ 0. But, we have that\ncT (x∗ + μx) = cT x∗ + μcT x < z,\na contradiction.\n9-6"
    }
  ]
}