{
  "metadata": {
    "timestamp": "2025-07-08 20:56:33",
    "task_id": "bcc4031f",
    "total_courses": 10,
    "subjects": [
      {
        "url": "https://ocw.mit.edu/search/?d=Mathematics",
        "name": "Mathematics"
      }
    ]
  },
  "courses": [
    {
      "course_name": "Linear Partial Differential Equations: Analysis and Numerics",
      "course_description": "This course provides students with the basic analytical and computational tools of linear partial differential equations (PDEs) for practical applications in science engineering, including heat / diffusion, wave, and Poisson equations. Analytics emphasize the viewpoint of linear algebra and the analogy with finite matrix problems. Numerics focus on finite-difference and finite-element techniques to reduce PDEs to matrix problems. The Julia Language (a free, open-source environment) is introduced and used in homework for simple examples.",
      "topics": [
        "Mathematics",
        "Applied Mathematics",
        "Differential Equations",
        "Linear Algebra",
        "Mathematics",
        "Applied Mathematics",
        "Differential Equations",
        "Linear Algebra"
      ],
      "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nPrerequisites\n\n18.06 Linear Algebra\n,\n18.700 Linear Algebra\nor equivalent.\n\nDescription\n\nThis course provides students with the basic analytical and computational tools of linear partial differential equations (PDEs) for practical applications in science engineering, including heat/diffusion, wave, and Poisson equations.\n\nAnalytics emphasize the viewpoint of linear algebra and the analogy with finite matrix problems including operator adjoints and eigenproblems, series solutions, Green's functions, and separation of variables.\n\nNumerics focus on finite-difference and finite-element techniques to reduce PDEs to matrix problems, including stability and convergence analysis and implicit/explicit time-stepping.\n\nJulia programming language\n(a MATLAB(r)-like environment) is introduced and used in homework for simple examples. Julia is a high-level, high-performance dynamic language for technical computing, with syntax that is familiar to users of other technical computing environments. It provides a sophisticated compiler, distributed parallel execution, numerical accuracy, and an extensive mathematical function library.\n\nTextbook\n\nThere is no required text for this course, though the following books are recommended:\n\nStrang, Gilbert.\nComputational Science and Engineering\n. Wellesley-Cambridge Press, 2007. ISBN: 9780961408817.\n\n(emphasizing more the numerical part of the course). More information, including online chapters, can be found on\nProf. Strang's CSE website\n.\n\nOlver, Peter.\nIntroduction to Partial Differential Equations\n. Springer, 2013. ISBN: 9783319020983. [Preview with\nGoogle Books\n] (free online book)\n\nRequirements\n\nThere will be five problem sets and a mid-term exam. There is a final project instead of a final exam. Late problem sets are not accepted, however the lowest problem set score will be dropped at the end of the term.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nAssignments\n\n45%\n\nMidterm exam\n\n25%\n\nFinal project\n\n30%\n\nCalendar\n\nSES #\n\nTOPICS\n\nKEY DATES\n\nL1\n\nOverview of linear PDEs and analogies with matrix algebra\n\nL2\n\nPoisson's equation and eigenfunctions in 1d: Fourier sine series\n\nL3\n\nFinite-difference methods and accuracy\n\nL4\n\nDiscrete vs. continuous Laplacians: Symmetry and dot products\n\nOptional\n\nJulia Tutorial\n\nL5\n\nDiagonalizability of infinite-dimensional Hermitian operators\n\nProblem set 1 due\n\nL6\n\nStart with a truly discrete (finite-dimensional) system, and then derive the continuum PDE model as a limit or approximation\n\nL7\n\nStart in 1d with the \"Sturm-Liouville operator\", generalize Sturm-Liouville operators to multiple dimensions\n\nL8\n\nMusic and wave equations, Separation of variables, in time and space\n\nProblem set 2 due\n\nL9\n\nSeparation of variables in cylindrical geometries: Bessel functions\n\nL10\n\nGeneral Dirichlet and Neumann boundary conditions\n\nL11\n\nMultidimensional finite differences\n\nL12\n\nKronecker products\n\nProblem set 3 due\n\nL13\n\nThe min-max theorem\n\nL14\n\nGreen's functions with Dirichlet boundaries\n\nL15\n\nReciprocity and positivity of Green's functions\n\nL16\n\nDelta functions and distributions\n\nL17\n\nGreen's function of ∇\nin 3d for infinite space, the method of images\n\nProblem set 4 due\n\nL18\n\nThe method of images, interfaces, and surface integral equations\n\nL19\n\nGreen's functions in inhomogeneous media: Integral equations and Born approximations\n\nL20\n\nDipole sources and approximations, Overview of time-dependent problems\n\nL21\n\nTime-stepping and stability: Definitions, Lax equivalence\n\nL22\n\nVon Neumann analysis and the heat equation\n\nProblem set 5 due\n\nL23\n\nAlgebraic properties of wave equations and unitary time evolution, Conservation of energy in a stretched string\n\nL24\n\nStaggered discretizations of wave equations\n\nL25\n\nTraveling waves: D'Alembert's solution\n\nL26\n\nGroup-velocity derivation, Dispersion\n\nMidterm Exam\n\nL27\n\nMaterial dispersion and convolutions\n\nL28\n\nGeneral topic of waveguides, Superposition of modes, Evanescent modes\n\nL29\n\nWaveguide modes, Reduced eigenproblem\n\nL30\n\nGuidance, reflection, and refraction at interfaces between regions with different wave speeds\n\nL31\n\nNumerical examples of total internal reflection\n\nL32\n\nPerfectly matched layers (PML)\n\nL33\n\nPerturbation theory and Hellman-Feynman theorem\n\nL34\n\nFinite element methods: Introduction\n\nL35\n\nGalerkin discretization\n\nL36\n\nConvergence proof for the finite-element method, Boundary conditions and the finite-element method\n\nL37\n\nFinite-element software\n\nL38\n\nSymmetry and linear PDEs\n\nFinal project due",
      "files": [
        {
          "category": "Resource",
          "title": "Linear Partial Differential Equations, Problem 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/59217fe4512ea9f506b08f4e0376da44_MIT18_303F14_pset1.pdf",
          "content": "18.303 Problem Set 1\nDue Friday, 12 September 2014.\nNote: For computational (Julia-based) homework problems in 18.303, turn in with your solu\ntions a printout of any commands used and their results (please edit out extraneous/irrelevant stuff),\nand a printout of any graphs requested; alternatively, you can email your notebook (.ipynb) file\nto the grader . Always label the axes of your graphs (with the xlabel and ylabel commands),\nadd a title with the title command, and add a legend (if there are multiple curves) with the\nlegend command. (Labelling graphs is a good habit to acquire.) Because IJulia notebooks let\nyou combine code, plots, headings, and formatted text, it should be straighforward to turn in\nwell-documented solutions.\nProblem 1: 18.06 warmup\nHere are a few questions that you should be able to answer based only on 18.06:\n(a) Suppose that B is a Hermitian positive-definite matrix. Show that there is a unique matrix\n√\n√\nB which is Hermitian positive-definite and has the property (\nB)2 = B. (Hint: use the\ndiagonalization of B.)\n(b) Suppose that A and B are Hermitian matrices and that B is positive-definite.\n(i) Show that B-1A is similar (in the 18.06 sense) to a Hermitian matrix. (Hint: use your\nanswer from above.)\n(ii) What does this tell you about the eigenvalues λ of B-1A , i.e. the solutions of B-1Ax =\nλx?\n(iii) Are the eigenvectors x orthogonal?\n(iv) In Julia, make a random 5 × 5 real-symmetric matrix via A=rand(5,5); A = A+A'\nand a random 5 × 5 positive-definite matrix via B = rand(5,5); B = B'*B ...\nthen\ncheck that the eigenvalues of B-1A match your expectations from above via lambda,X\n= eigvals(B\\A) (this will give an array lambda of the eigenvalues and a matrix X whose\ncolumns are the eigenvectors).\n(v) Using your Julia result, what happens if you compute C = XT BX via C=X'*B*X? You\nshould notice that the matrix C is very special in some way. Show that the elements Cij\nof C are a kind of \"dot product\" of the eigenvectors i and j, but with a factor of B in\nthe middle of the dot product.\n√\n′′\n′\n(1+ 1+c)t\n(c) The solutions y(t) of the ODE y\n- 2y - cy = 0 are of the form y(t) = C1e\n+\n√\nC2e(1-\n1+c)t for some constants C1 and C2 determined by the initial conditions. Suppose that\nA is a real-symmetric 4×4 matrix with eigenvalues 3, 8, 15, 24 and corresponding eigenvectors\nx1, x2, . . . , x4, respectively.\nd\n(i) If x(t) solves the system of ODEs d2\nx = Ax with initial conditions x(0) = a0 and\ndt2 x-2 dt\nx ′ (0) = b0, write down the solution x(t) as a closed-form expression (no matrix inverses\nor exponentials) in terms of the eigenvectors x1, x2, . . . , x4 and a0 and b0. [Hint: expand\nx(t) in the basis of the eigenvectors with unknown coefficients c1(t), . . . , c4(t), then plug\ninto the ODE and solve for each coefficient using the fact that the eigenvectors are\n_________.]\n(ii) After a long time t ≫ 0, what do you expect the approximate form of the solution to\nbe?\n\nProblem 2: Les Poisson, les Poisson\nIn class, we considered the 1d Poisson equation\nd2\n= f(x) for the vector space of functions\ndx2 u(x)\nu(x) on x ∈ [0, L] with the \"Dirichlet\" boundary conditions u(0) = u(L) = 0, and solved it in terms\nd2\nof the eigenfunctions of dx2 (giving a Fourier sine series). Here, we will consider a couple of small\nvariations on this:\n(a) Suppose that we we change the boundary conditions to the periodic boundary condition\nu(0) = u(L).\n(i) What are the eigenfunctions of\nd2 now?\ndx2\n(ii) Will Poisson's equation have unique solutions? Why or why not?\n(iii) Under what conditions (if any) on f(x) would a solution exist? (You can restrict yourself\nto f with a convergent Fourier series.)\nd2\n(b) If we instead consider\n= g(x) for functions v(x) with the boundary conditions v(0) =\ndx2 v(x)\nv(L) + 1, do these functions form a vector space? Why or why not?\n(c) Explain how we can transform the v(x) problem of the previous part back into the original\nd2\n= f(x) problem with u(0) = u(L), by writing u(x) = v(x) + q(x) and f(x) =\ndx2 u(x)\ng(x) + r(x) for some functions q and r. (Transforming a new problem into an old, solved one\nis always a useful thing to do!)\nProblem 3: Finite-difference approximations\nFor this question, you may find it helpful to refer to the notes and reading from lecture 3. Consider\na finite-difference approximation of the form:\n-u(x + 2Δx) + c · u(x + Δx) - c · u(x - Δx) + u(x - 2Δx)\nu ′ (x) ≈\n.\nd · Δx\n(a) Substituting the Taylor series for u(x+Δx) etcetera (assuming u is a smooth function with a\nconvergent Taylor series, blah blah), show that by an appropriate choice of the constants c and\nd you can make this approximation fourth-order accurate: that is, the errors are proportional\nto (Δx)4 for small Δx.\n(b) Check your answer to the previous part by numerically computing u ′(1) for u(x) = sin(x), as\na function of Δx, exactly as in the handout from class (refer to the notebook posted in lecture\n3 for the relevant Julia commands, and adapt them as needed). Verify from your log-log plot\nof the |errors| versus Δx that you obtained the expected fourth-order accuracy.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Linear Partial Differential Equations, Solution to Problem 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/4ddac74f26070aa38bf2b99e00940db3_MIT18_303F14_pset1sol.pdf",
          "content": "18.303 Problem Set 1 Solutions\nProblem 1: (5+(2+2+2+2+2)+(10+5) points)\nNote that I don't expect you to rederive basic linear-algebra facts. You can use things derived in\n18.06, like the existence of an orthonormal diagonalization of Hermitian matrices.\n(a) Since it is Hermitian, B can be diagonalized: B = QΛQ∗, where Q is the matrix whose\ncolumns are the eigenvectors (chosen orthonormal so that Q-1 = Q∗) and Λ is the diagonal\n√\nmatrix of eigenvalues. Define Λ as the diagonal matrix of the (positive) square roots of the\neigenvalues, which is possible because the eigenvalues are > 0 (since B is positive-definite).\n√\n√\n√\n√\nThen define B = Q ΛQ∗ , and by inspection we obtain ( B)2 = B. By construction, B\nis positive-definite and Hermitian.\n√\nIt is easy to see that this B is unique, even though the eigenvectors X are not unique, because\n√\nany acceptable transformation of Q must commute with Λ and hence with Λ. Consider for\nsimplicity the case of distinct eigenvalues: in this case, we can only scale the eigenvectors by\n(nonzero) constants, corresponding to multiplying Q on the right by a diagonal (nonsingular)\nmatrix D. This gives the same B for any D, since QDΛ(QD)-1 = QΛDD-1Q-1 = QΛQ-1\n√\n(diagonal matrices commute), and for the same reason it gives the same B. For repeated\neigenvalues λ, D can have off-diagonal elements that mix eigenvectors of the same eigenvalue,\nbut D still commutes with Λ because these off-diagonal elements only appear in blocks where\nΛ is a multiple λI of the identity (which commutes with anything).\n(b) Solutions:\n(i) From 18.06, B-1A is similar to C = MB-1AM -1 for any invertible M. Let M = B1/2\nfrom above. Then C = B-1/2AB-1/2, which is clearly Hermitian since A and B-1/2 are\nHermitian. (Why is B-1/2 Hermitian? Because B1/2 is Hermitian from above, and the\ninverse of a Hermitian matrix is Hermitian.)\n(ii) From 18.06, similarity means that B-1A has the same eigenvalues as C, and since C is\nHermitian these eigenvalues are real.\n(iii) No, they are not (in general) orthogonal. The eigenvectors Q of C are (or can be chosen)\nto be orthonormal (Q∗Q = I), but the eigenvectors of B-1A are X = M -1Q = B-1/2Q,\nand hence X∗X = Q∗B-1Q\n= I.\n= I unless B\n(iv) Note that there was a typo in the pset. The eigvals function returns only the eigenval\nues; you should use the eig function instead to get both eigenvalues and eigenvectors,\nas explained in the Julia handout.\nThe array lambda that you obtain in Julia should be purely real, as expected. (You might\nnotice that the eigenvalues are in somewhat random order, e.g. I got -8.11,3.73,1.65,\n1.502,0.443. This is a side effect of how eigenvalues of non-symmetric matrices are com\nputed in standard linear-algebra libraries like LAPACK.) You can check orthogonality\nby computing X∗X via X'*X, and the result is not a diagonal matrix (or even close to\none), hence the vectors are not orthogonal.\n(v) When you compute C = X∗BX via C=X'*B*X, you should find that C is nearly diago\nnal: the off-diagonal entries are all very close to zero (around 10-15 or less). They would\nbe exactly zero except for roundoff errors (as mentioned in class, computers keep only\naround 15 significant digits). From the definition of matrix multiplication, the entry Cij\nis given by the i-th row of X∗ multiplied by B, multiplied by the j-th column of X.\n∗\nBut the j-th column X is the j-th eigenvector xj , and the i-th row of X∗ is x . Hence\ni\n∗\nCij = xi Bxj, which looks like a dot product but with B in the middle. The fact that C\n\n∗\nis diagonal means that x Bxj = 0 for i = j, which is a kind of orthogonality relation.\ni\n[In fact, if we define the inner product (x, y) = x ∗By, this is a perfectly good inner\nproduct (it satisifies all the inner-product criteria because B is positive-definite), and\nwe will see in the next pset that B-1A is actually self-adjoint under this inner product.\nHence it is no surprise that we get real eigenvalues and orthogonal eigenvectors with\nrespect to this inner product.]\n(c) Solutions:\n(i) If we write x(t) =\n(t)xn, then plugging it into the ODE and using the eigenvalue\nn=1 cn\nequation yields\n[ cn - 2 cn - λnc] xn = 0.\nn=1\nUsing the fact that the xn are necessarily orthogonal (they are eigenvectors of a Her\nmitian matrix for distinct eigenvalues), we can take the dot product of both sides with\nxm to find that c n - 2 cn - λnc = 0 for each n, and hence\n√\n√\n(1+ 1+λn)t\n(1- 1+λn)t\ncn(t) = αne\n+ βne\nfor constants αn and βn to be determined from the initial conditions. Plugging in the\ninitial conditions x(0) = a0 and x'(0) = b0, we obtain the equations:\n(αn + βn)xn = a0,\nn=1\n\n([αn + βn] +\n1 + λn[αn - βn])xn = b0.\nn=1\nAgain using orthogonality to pull out the n-th term, we find\n∗\nx a0\nn\nαn + βn =\n1xn\n\n∗\n∗\nx b0\nx (b0 - a0)\nn\nn\n[αn + βn] +\n1 + λn[αn - βn] =\n=⇒ αn - βn =\n√\n1xn\n1xn\n1 + λn\n(note that we were not given that xn were normalized to unit length, and this is not\nautomatic) and hence we can solve for αn and βn to obtain:\n\n∗\n∗\nx (b0 - a0)\n√\nx (b0 - a0)\n√\nxn\n∗\nn\n(1+ 1+λn)t\n∗\nn\n(1- 1+λn )t\nx(t) =\nx a0 +\n√\ne\n+ x a0 +\n√\ne\n.\nn\nn\n1 + λn\n1 + λn\n21xn12\nn=1\n(ii) After a long time, this expression will be dominated by the fastest growing term, which\n√\n(1+ 1+λn)t\nis the e\nterm for λ4 = 24, hence:\n\n∗\nx4(b0 - a0)\nx4\n∗\n6t\nx(t) ≈\nx4a0 +\ne\n.\n21x412\n=\n\nProblem 2: ((5+5+10)+5+5 points)\n(a) Suppose that we we change the boundary conditions to the periodic boundary condition\nu(0) = u(L).\n(i) As in class, the eigenfunctions are sines, cosines, and exponentials, and it only remains to\n2πn\napply the boundary conditions. sin(kx) is periodic if k =\nfor n = 1, 2, . . . (excluding\nL\nn = 0 because we do not allow zero eigenfunctions and excluding n < 0 because they\nare not linearly independent), and cos(kx) is periodic if n = 0, 1, 2, . . . (excluding n < 0\nsince they are the same functions). The eigenvalues are -k2 = -(2πn/L)2 .\ni 2πn\nkx\ni 2πn\nx\nL\ne\nis periodic only for imaginary k =\n, but in this case we obtain e\n=\nL\ncos(2πnx/L) + i sin(2πnx/L), which is not linearly independent of the sin and cos eigen\nfunctions above. Recall from 18.06 that the eigenvectors for a given eigenvalue form a\nvector space (the null space of A - λI), and when asked for eigenvectors we only want a\nbasis of this vector space. Alternatively, it is acceptable to start with exponentials and\ni 2πn x\nL\ncall our eigenfunctions e\nfor all integers n, in which case we wouldn't give sin and\ncos eigenfunctions separately.\nSimilarly, sin(φ + 2πnx/L) is periodic for any φ, but this is not linearly independent\nsince sin(φ + 2πnx/L) = sin φ cos(2πnx/L) + cos φ sin(2πnx/L).\n[Several of you were tempted to also allow sin(mπx/L) for odd m (not just the even\nm considered above). At first glance, this seems like it satisfies the PDE and also has\nu(0) = u(L) (= 0). Consider, for example, m = 1, i.e. sin(πx/L) solutions. This can't be\nright, however; e.g. it is not orthogonal to 1 = cos(0x), as required for self-adjoint prob\nlems. The basic problem here is that if you consider the periodic extension of sin(πx/L),\nthen it doesn't actually satisfy the PDE, because it has a slope discontinuity at the\nendpoints. Another way of thinking about it is that periodic boundary conditions arise\nbecause we have a PDE defined on a torus, e.g. diffusion around a circular tube, and in\nthis case the choice of endpoints is not unique--we can easily redefine our endpoints so\nthat x = 0 is in the \"middle\" of the domain, making it clearer that we can't have a kink\nthere. (This is one of those cases where to be completely rigorous we would need to be\na bit more careful about defining the domain of our operator.)]\n(ii) No, any solution will not be unique, because we now have a nonzero nullspace spanned\nby the constant function u(x) = 1 (which is periodic): d2 1 = 0. Equivalently, we have\ndx2\na 0 eigenvalue corresponding to cos(2πnx/L) for n = 0 above.\n(iii) As suggested, let us restrict ourselves to f(x) with a convergent Fourier series. That is,\nas in class, we are expanding f(x) in terms of the eigenfunctions:\ninf\ni 2πn x\nL\nf(x) =\ncne\n.\nn=-inf\n(You could also write out the Fourier series in terms of sines and cosines, but the complex\nexponential form is more compact so I will use it here.) Here, the coefficients cn, by the\nusual orthogonality properties of the Fourier series, or equivalently by self-adjointness of\nL - 2πn\nL\nAˆ, are cn =\ne\nxf(x)dx.\nL\nu\nIn order to solve d2\n= f, as in class we would divide each term by its eigenvalue\ndx2\n-(2πn/L)2, but we can only do this for n = 0. Hence, we can only solve the equation if\nthe n = 0 term is absent, i.e. c0 = 0. Appling the explicit formula for c0, the equation is\n=\n\nsolvable (for f with a Fourier series) if and only if:\nL\nf(x)dx = 0 .\nThere are other ways to come to the same conclusion. For example, we could expand\nu(x) in a Fourier series (i.e. in the eigenfunction basis), apply d2/dx2, and ask what is\nthe column space of d2/dx2? Again, we would find that upon taking the second derivative\nthe n = 0 (constant) term vanishes, and so the column space consist of Fourier series\nmissing a constant term.\nThe same reasoning works if you write out the Fourier series in terms of sin and cos\nsums separately, in which case you find that f must be missing the n = 0 cosine term,\ngiving the same result.\n(b) No. For example, the function 0 (which must be in any vector space) does not satisy\nthose boundary conditions. (Also adding functions doesn't work, scaling them by constants,\netcetera.)\n(c) We merely pick any twice-differentiable function q(x) with q(L) - q(0) = -1, in which case\nu(L) - u(0) = [v(L) - v(0)] + [q(L) - q(0)] = 1 - 1 = 0 and u is periodic. Then, plugging\nv = u - q into d2\ndx2 v(x) = f(x), we obtain\nd2\nd2q\nu(x) = f(x) +\n,\ndx2\ndx2\nwhich is the (periodic-u) Poisson equation for u with a (possibly) modified right-hand side.\nFor example, the simplest such q is probably q(x) = x/L, in which case d2q/dx2 = 0 and u\nsolves the Poisson equation with an unmodified right-hand side.\nProblem 3: (10+10 points)\nWe are using a difference approximation of the form:\n-u(x + 2Δx) + c · u(x + Δx) - c · u(x - Δx) + u(x - 2Δx)\nu ' (x) ≈\n.\nd · Δx\n(a) First, we Taylor expand:\ninf\n4 u(n)(x)\nn\nu(x + Δx) =\nΔx .\nn!\nn=0\nThe numerator of the difference formula flips sign if Δx →-Δx, which means that when you\nplug in the Taylor series all of the even powers of Δx must cancel! To get 4th-order accuracy,\nthe Δx3 term in the numerator (which would give an error ∼ Δx2) must cancel as well, and\nthis determines our choice of c: the Δx3 term in the numerator is\n''' (x)\nu\n\nΔx -23 + c + c - 23 ,\n3!\nand hence we must have c = 23 = 8 . The remaining terms in the numerator are the Δx term\nand the Δx5 term:\n(5)(x)\nu\n\n2 (5)(x)Δx 5\nu ' (x)Δx [-2 + c + c - 2] +\nΔx -25 + c + c - 25 = 12u ' (x)Δx - u\n+ · · · .\n5!\nClearly, to get the correct u ' (x) as Δx → 0, we must have\n(5)(x)Δx\napproximately - 1 u\n4, which is ∼ Δx4 as desired.\nd = 12 . Hence, the error is\n\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n-15\n-14\n-13\n-12\n-11\n-10\n-9\n-8\n-7\n-6\n-5\n∆x\nerror in d(sin)/dx at x=1\n\nerror in du/dx\npredicted ∼ ∆x4\nFigure 1: Actual vs. predicted error for problem 1(b), using fourth-order difference approximation\nfor u ' (x) with u(x) = sin(x), at x = 1.\n(b) The Julia code is the same as in the handout, except now we compute our difference approxi\nmation by the command: d = (-sin(x+2*dx) + 8*sin(x+dx) - 8*sin(x+dx) + sin(x-2*dx))\n./ (12 * dx); the result is plotted in Fig. 1. Note that the error falls as a straight line (a\npower law), until it reaches ∼ 10-15, when it starts becoming dominated by roundoff errors\n(and actually gets worse). To verify the order of accuracy, it would be sufficient to check the\nslope of the straight-line region, but it is more fun to plot the actual predicted error from\nthe previous part, where d5 sin(x) = - cos(x). Clearly the predicted error is almost exactly\ndx5\nright (until roundoff errors take over).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Linear Partial Differential Equations, Problem 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/5f8e4bdfb93d1f0f021409582c216df4_MIT18_303F14_pset2.pdf",
          "content": "18.303 Problem Set 2\nDue Monday, 22 September 2014.\nProblem 2: Modified inner products for column vectors\nConsider the inner product (x, y) = x ∗By from class (lecture 5.5 notes), where the vectors are in\nCN and B is an N × N Hermitian positive-definite matrix.\n(a) Show that this inner product satisfies the required properties of inner products from class:\n(x, y) = (y, x), (x, x) > 0 except for x = 0. (Linearity (x, αy +z) = α(x, y)+(x, z) is obvious\nfrom linearity the of matrix operations; you need not show it.)\n(b) If M is an arbitrary (possibly complex) N × N matrix, define the adjoint M + by (x, My) =\n(M + x, y) (for all x, y). (In this problem, we use + instead of ∗ for the adjoint in order to\navoid confusion with the conjugate transpose: for this inner product, the adjoint M + is not\nthe conjugate transpose M ∗ = M T .) Give an explicit formula for M + in terms of M and B .\n(c) Using your formula from above, show that M + = M (i.e., M is self-adjoint/Hermitian for this\ninner product) if M = B-1A for some A = A∗ .\nProblem 2: Finite-difference approximations\nFor this question you may find it helpful to refer to the notes and readings from lecture 3. Suppose\nthat we want to compute the operation\nd\ndu\nˆAu =\nc\ndx\ndx\nfor some smooth function c(x) (you can assume c has a convergent Taylor series everywhere). Now,\nwe want to construct a finite-difference approximation for Aˆ with u(x) on Ω = [0, L] and Dirichlet\nboundary conditions u(0) = u(L) = 0, similar to class, approximating u(mΔx) ≈ um for M equally\nL\nspaced points m = 1, 2, . . ., M, u0 = uM+1 = 0, and Δx =\n.\nM+1\n(a) Using center-difference operations, construct a finite-difference approximation for ˆAu evalu\nated at mΔx. (Hint: use a centered first-derivative evaluated at grid points m + 0.5, as in\nclass, followed by multiplication by c, followed by another centered first derivative. Do not\n′\n′\n′′\nseparate Au\nˆ\nby the product rule into c u + cu\nfirst, as that will make the factorization in\npart (d) more difficult.)\n(b) Show that your finite-difference expressions correspond to approximating ˆAu by Au where\nu is the column vector of the M points um and A is a real-symmetric matrix of the form\nA = -DTCD (give C, and show that D is the same as the 1st-derivative matrix from lecture).\n(c) In Julia, the diagm(c) command will create a diagonal matrix from a vector c. The function\ndiff1(M) = [ [1.0 zeros(1,M-1)]; diagm(ones(M-1),1) - eye(M) ]\nwill allow you to create the (M +1)×M matrix D from class via D = diff1(M) for any given\nvalue of M. Using these two commands, construct the matrix A from part (d) for M = 100\nand L = 1 and c(x) = e3x via\nL = 1\nM = 100\nD = diff1(M)\ndx = L / (M+1)\nx = dx*0.5:dx:L # sequence of x values from 0.5*dx to <= L in steps of dx\nC = ....something from c(x)...hint: use diagm...\n\nA = -D' * C * D / dx^2\nYou can now get the eigenvalues and eigenvectors by λ, U = eig(A), where λ is an array of\neigenvalues and U is a matrix whose columns are the corresponding eigenvectors (notice that\nall the λ are < 0 since A is negative-definite).\n(i) Plot the eigenvectors for the smallest-magnitude four eigenvalues. Since the eigenvalues\nare negative and are sorted in increasing order, these are the last four columns of U.\nYou can plot them with:\nusing PyPlot\nplot(dx:dx:L-dx, U[:,end-3:end])\nxlabel(\"x\"); ylabel(\"eigenfunctions\")\nlegend([\"fourth\", \"third\", \"second\", \"first\"])\n(ii) Verify that the first two eigenfunctions are indeed orthogonal with dot(U[:,end],\nU[:,end-1]) in Julia, which should be zero up to roundoff errors ; 10-15 .\n(iii) Verify that you are getting second-order convergence of the eigenvalues: compute the\nsmallest-magnitude eigenvalue λM[end] for M = 100, 200, 400, 800 and check that the\ndifferences are decreasing by roughly a factor of 4 (i.e. |λ100 -λ200| should be about 4\ntimes larger than |λ200 -λ400|, and so on), since doubling the resolution should multiply\nerrors by 1/4.\n(d) For c(x) = 1, we saw in class that the eigenfunctions are sin(nπx/L). How do these compare\nto the eigenvectors you plotted in the previous part? Try changing c(x) to some other function\n(note: still needs to be real and > 0), and see how different you can make the eigenfunctions\nfrom sin(nπx/L). Is there some feature that always remains similar, no matter how much\nyou change c?\nProblem 3: Discrete diffusion\nIn this problem, you will examine thermal conduction in a system of a finite number N of pieces,\nand then take the N →inf limit to recover the heat equation. In particular:\n- You have a metal bar of length L and cross-sectional area a (hence a volume La), with a\nvarying temperature T along the rod. We conceptually subdivide the rod into N (touching)\npieces of length Δx = L/N.\n- If Δx is small, we can approximate each piece as having a uniform temperature Tn within the\npiece (n = 1, 2, . . ., N), giving a vector T of N temperatures.\n- Suppose that the rate q (in units of W) at which heat flows across the boundary from piece\nκa\nn to piece n + 1 is given by q =\n(Tn -Tn+1), where κ is the metal's thermal conductivity\nΔx\n(in units of W/m·K). That is, piece n loses energy at a rate q, and piece n + 1 gains energy\nat the same rate, and the heat flows faster across bigger areas, over shorter distances, or for\nlarger temperature differences. Note that q > 0 if Tn > Tn+1 and q < 0 if Tn < Tn+1: heat\nflows from the hotter piece to the cooler piece.\n- If an amount of heat ΔQ (in J) flows into a piece, its temperature changes by ΔT =\nΔQ/(cρaΔx), where c is the specific heat capacity (in J/kg·K) and ρ is the density (kg/m3)\nof the metal.\n- The rod is insulated: no heat flows out the sides or through the ends.\nGiven these assumptions, you should be able to answer the following:\n\n(a) \"Newton's law of cooling\" says that that the temperature of an object changes at a rate (K/s)\nproportional to the temperature difference with its surroundings. Derive the equivalent here:\nn\nshow that our assumptions above imply that dT\n= α(Tn+1 -Tn) + α(Tn-1 -Tn) for some\ndt\nconstant α, for 1 < n < N. Also give the (slightly different) equations for n = 1 and n = N.\n(b) Write your equation from the previous part in matrix form: dT = AT for some matrix A.\ndt\n(c) Let T (x, t) be the temperature along the rod, and suppose Tn(t) = T ([n - 0.5]Δx, t) (the\ntemperature at the center of the n-th piece). Take the limit N →inf (with L fixed, so that\n∂T\nˆ\nΔx = L/N → 0), and derive a partial differential equation\n= AT . What is Aˆ? (Don't\n∂t\nworry about the x = 0, L ends until the next part.)\n(d) What are the boundary conditions on T (x, t) at x = 0 and L? Check that if you go backwards,\nand form a center-difference approximation of Aˆ with these boundary conditions, that you\nrecover the matrix A from above.\n(e) How does your Aˆ change in the N →inf limit if the conductivity is a function κ(x) of x?\n(f) Suppose that instead of a thin metal bar (1d), you have an L × L thin metal plate (2d), with\na temperature T (x, y, t) and a constant conductivity κ. If you go through the steps above\ndividing it into N × N little squares of size Δx × Δy, what PDE do you get for T in the limit\nN →inf? (Many of the steps should be similar to above.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Linear Partial Differential Equations, Solution to Problem 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/9e71970eef48268d82c1a0b29999f296_MIT18_303F14_pset2sol.pdf",
          "content": "18.303 Problem Set 2 Solutions\nProblem 1 (5+5+5 points)\n(a) We have (x, x) = x ∗Bx > 0 for x = 0\n\nby definition of positive-definiteness. We have (x, y) =\n∗\n∗\nx ∗By = (B∗ x) y = (Bx) y = y ∗(Bx) = (y, x) by B = B∗ .\n(b) (x, My) = x ∗BMy = (M +x, y) = x ∗M +∗By for all x, y, and hence we must have BM =\n∗\nM +∗B, or M +∗ = BMB-1 =⇒ M + = (BMB-1)\n= (B-1)∗M ∗B∗ . Using the fact that\n∗\nM + = B-1M ∗ B .\nB∗ = B (and hence (B-1) = B-1), we have\n(c) If M = B-1A where A = A∗, then M + = B-1AB-1B = B-1A = M. Q.E.D.\nProblem 2: (5+5+(3+3+3)+5 points)\n'\num+1-um\n(a) As in class, let u'([m + 0.5]Δx) ≈ u\n=\n. Define cm+0.5 = c([m + 0.5]Δx). Now\nm+0.5\nΔx\n'\nˆ\nwe want to take the derivative of cm+0.5u\nin order to approximate Au at m by a center\nm+0.5\ndifference:\n\num+1-um\num -um-1\ncm+0.5\n- cm-0.5\nΔx\nΔx\n.\nΔx\nˆAu\n\n≈\nmΔx\nThere are other ways to solve this problem of course, that are also second-order accurate.\nˆ\n(b) In order to approximate Au, we did three things: compute u' by a center-difference as in\nclass, multiply by cm+0.5 at each point m + 0.5, then compute the derivative by another\ncenter-difference. The first and last steps are exactly the same center-difference steps as in\nclass, so they correspond as in class to multiplying by D and -DT , respectively, where D is\nthe (M + 1) × M matrix\n⎞\n⎛ 1\n-1\n-1\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\n.\nD =\n.\n.\nΔ\n.\n.\nx\n.\n.\n-1\n-1\nThe middle step, multiplying the (M +1)-component vector u' by cm+0.5 at each point is just\nmultiplication by a diagonal (M + 1) × (M + 1) matrix\n⎞\n⎛ c0.5\nc1.5\nC =\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎠ .\n...\ncM +0.5\nPutting these steps together in sequence, from right to left, means that A = -DT CD\n(c) In Julia, the diagm(c) command will create a diagonal matrix from a vector c. The function\ndiff1(M) = [ [1.0 zeros(1,M-1)]; diagm(ones(M-1),1) - eye(M) ]\nwill allow you to create the (M + 1) × M matrix D from class via D = diff1(M) for any\ngiven value of M. Using these two commands, we construct the matrix A from part (d) for\n3x\nM = 100 and L = 1 and c(x) = e\nvia\nL = 1\nM = 100\nD = diff1(M)\n\n0.20\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\n0.20\n\nd\n3x\nFigure 1: Smallest-|λ| eigenfunctions of Aˆ =\nc(x) d\nfor c(x) = e .\ndx\ndx\ndx = L / (M+1)\nx = dx*0.5:dx:L # sequence of x values from 0.5*dx to <= L in steps of dx\nc(x) = exp(3x)\nC = diagm(c(x))\nA = -D' * C * D / dx^2\nYou can now get the eigenvalues and eigenvectors by λ, U = eig(A), where λ is an array of\neigenvalues and U is a matrix whose columns are the corresponding eigenvectors (notice that\nall the λ are < 0 since A is negative-definite).\n(i) The plot is shown in Figure 1. The eigenfunctions look vaguely \"sine-like\"--they have\nthe same number of oscillations as sin(nπx/L) for n = 1, 2, 3, 4--but are \"squeezed\" to\nthe left-hand side.\n(ii) We find that the dot product is ≈ 4.3 × 10-16, which is zero up to roundoff errors (your\nexact value may differ, but should be of the same order of magnitude).\n(iii) In the posted IJulia notebook for the solutions, we show a plot of |λ2M -λM | as a function\nof M on a log-log scale, and verify that it indeed decreases ∼ 1/M 2 . You can also just\nlook at the numbers instead of plotting, and we find that this difference decreases by a\nfactor of ≈ 3.95 from M = 100 to M = 200 and by a factor of ≈ 3.98 from M = 200 to\nM = 400, almost exactly the expected factor of 4. (For fun, in the solutions I went to\nM = 1600, but you only needed to go to M = 800.)\n(d) In general, the eigenfunctions have the same number of nodes (sign oscillations) as sin(nπx/L),\nbut the oscillations pushed towards the region of high c(x). This is even more dramatic if we\nincrease the c(x) contrast. In Figure xxx, we show two examples. First, c(x) = e20x, in which\nall of the functions are squished to the left where c is small. Second c(x) = 1 for x < 0.3\nand 100 otherwise--in this case, the oscillations are at the left 1/3 where c is small, but the\nfunction is not zero in the right 2/3. Instead, the function is nearly constant where c is large.\nThe reason for this has to do with the continuity of u: it is easy to see from the operator that\n'\n' ) '\n'\ncu must be continuous for (cu\nto exist, and hence the slope u must decrease by a factor of\neigenfunctions\nfourth\nthird\nsecond\nfirst\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx\n\nˆ\n' ) '\nFigure 2: First four eigenfunctions of Au = (cu\nfor two different choices of c(x).\n100 for x > 0.3, leading to a u that is nearly constant. (We will explore some of these issues\nfurther later in the semester.)\nProblem 3: (5+5+5+5+5+5 points)\ndQn\n(a) The heat capacity equation tells us that dTn =\n, where dQn/dt is the rate of change\ndt\ncρaΔx\ndt\n\nof the heat in the n-th piece. The thermal conductivity equation tells us that dQn/dt, in\nturn, is equal to the sum of the rates q at which heat flows from n + 1 and n - 1 into n:\ndTn\ndQn\nκa\n=\n=\n[(Tn+1 - Tn) + (Tn-1 - Tn)] = α(Tn+1 -Tn)+α(Tn-1 -Tn)\ndt\ncρaΔx dt\ncρaΔx Δx\nκ\nwhere α =\n. The only difference for T1 and TN is that they have no heat flow\ncρ(Δx)\ndT1\ndTN\nn - 1 and n + 1, respectively, since the ends are insulated:\n= α(T2 - T1) and\n=\ndt\ndt\nα(TN -1 - TN ).\n(b) We can obtain A in two ways. First, we can simply look directly at our equations above,\ndTn\nwhich give\ndt = α(Tn+1 - 2Tn + Tn-1) for every n except T1 and TN , and read off the\n\n__\n__\ncorresponding rows of the matrix\n-1\n-2\nA = α\n..\n..\n..\n.\n.\n.\n.\n-2\n-1\n⎞\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\n⎛\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\nAlternatively, we can write each of the above steps--differentiating T to get the rate of heat\nflow q to the left at each of the N - 1 interfaces between the pieces, then taking the difference\nof the q's to get dT/dt, in matrix form, to write:\n⎞\n⎛ 1\n⎞\n⎛ -1\n-1\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\n-1\n⎜\n⎜\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎟\n⎟\n⎠\n-1\n\nκ DDT\n.\n.\n= -\nA =\nκa\n.\n.\n,\n.\n.\n.\n.\ncρa Δ\nΔ\ncρ\n.\n.\nx\nx\n.\n.\n-1\n-1\n-1\n\n-1\n_\n\n_\n-DT : (N-1)×N\nD: N×(N-1)\nin terms of the D matrix from class (except with N reduced by 1), which gives the same A\nas above. As we will see in the parts below, this is indeed a second-derivative approximation,\nbut with different boundary conditions--Neumann conditions--than the Dirichlet conditions\nin class.\nBy the way, it is interesting to consider -DDT , compared to the -DT D we had in class.\nClearly, -DDT is real-symmetric and negative semidefinite. It is not, however, negative def\ninite, since DT does not (and cannot) have full column rank (its rank must be ≤ the number\nof rows N - 1, and in fact in class we showed that it has rank N - 1).\nκ Tn+1-2Tn+Tn-1\n(c) Ignoring the ends for the moment, for all the interior points we have dTn =\n,\nκ ∂2T\ndt\ncρ\nΔx\nwhich is exactly our familiar center-difference approximation for\nat the point n (x =\ncρ ∂x2\n∂T\nκ\nT\n[n - 0.5]Δx). Hence, everywhere in the interior our equations converge to\n=\n∂2 , and\n∂T\ncρ ∂x2\nthus\nκ ∂2\nˆA =\n.\ncρ ∂x2\n(d) The boundary conditions are ∂T = 0\n∂x\nat x = 0, L. The easiest way to see this is to\nobserve that our heat flow q is really a first derivative, and zero heat flow at the ends\nTn+1-Tn\nmeans zero derivatives. That is, qn+0.5 = κa\nis really an approximate derivative:\nΔx\nqn+0.5 ≈ κa ∂T\n= κa ∂T\n, while the flows q0.5 and qN+0.5 to/from n = 0 and\n∂x n+0.5\n∂x nΔx\nn = N + 1 is zero, and hence q0.5 = qN+0.5 = 0 ≈ κa ∂T\n.\n∂x 0,L\nˆ\nT\n''\nκ\nWorking backwards, consider AT = ∂2\n= T\n(setting\n= 1 for convenience) with these\n∂x2\ncρ\nboundary conditions and center-difference approximations.\nWe are given Tn = T([n -\n∂T\n'\nTn+1-Tn\n0.5]Δx, t) for n = 1, . . . , N. First, we compute\n≈ T\n=\nfor n =\n∂x nΔx\nn+0.5\nΔx\n1, . . . , N - 1 (-DT T using the D above). Unlike the Dirichlet case in class, we don't com\n'\n'\npute T\nand T\n, since these correspond to ∂T/∂x at x = 0, L, which are zero by the\n0.5\nN+0.5\nT ′\n-T ′\n''\nn+0.5\nn-0.5\nboundary conditions. Then, we compute our approximate 2nd derivatives T =\nn\nΔx\n\n'\n'\nfor n = 1, . . . , N, where we let T\n= T\n= 0 (DT ' using the D from above). This\n0.5\nN+0.5\n''\nT1.5 -0\nT2-T1\n''\n0-TN -0.5\n-TN +TN -1\n''\ngives T\n=\n=\n, T\n=\n=\nat the endpoints, and T\n=\nΔx\nΔx2\nN\nΔx\nΔx2\nn\n(Tn+1-Tn)-(Tn-Tn-1)\nTn+1 -2Tn+Tn-1\n=\nfor 1 < n < N, which are precisely the rows of our A\nΔx\nΔx\nmatrix above.\n(e) If κ(x), then we get a different κ and α factor for each Tn+1 -Tn difference:\ndTn = αn+1/2(Tn+1 -Tn) + αn-1/2(Tn-1 -Tn),\ndt\nκn+1/2\nwhere αn+1/2 =\nand κn+1/2 = κ([n + 1/2]Δx). In the N →inf limit, this gives\ncρ(Δx)2\n1 ∂\n∂\nAˆ =\nκ\n: we differentiated, multiplied by κ, differentiated again, and then divided\ncρ ∂x ∂x\nby cρ. (You weren't asked to handle the case where cρ is not a constant, so it's okay if you\ncommuted cρ with the derivatives.)\n(f) If we discretize to Tm,n = T (mΔx, nΔy), the steps are basically the same except that we have\nto consider the heat flow in both the x and y directions, and hence we have to take differences\nin both x and y. In particular, suppose the thickness of the block is h. In this case, heat\nκhΔy\nwill flow from Tm,n to Tm+1,n at a rate\nΔx (Tm,n -Tm+1,n) where hΔy is the area of the\ninterface between the two blocks. Then, to convert into a rate of temperature change, we will\ndivide by cρhΔxΔy, where hΔxΔy is the volume of the block. Putting this all together, we\nobtain:\n\ndTm,n\nκ\nTm+1,n -2Tm,n + Tm-1,n\nTm,n+1 -2Tm,n + Tm,n+1\n=\n+\n,\ndt\ncρ\nΔx2\nΔy2\nwhere the thing in [· · · ] is precisely the five-point stencil approximation for ∇2 from class.\nHence, we obtain\nˆA =\n∇· κ∇,\ncρ\nwhere for fun I have put the κ in the middle, which is the right place if κ is not a constant\n(you were not required to do this).\n′\n′\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Linear Partial Differential Equations, Problem 3",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/aea96a46126e9a1a7e7c2669265707fd_MIT18_303F14_pset3.pdf",
          "content": "18.303 Problem Set 3\nDue Wednesday, 1 October 2014.\nProblem 1:\nConsider the space of three-component vector fields u(x) on some finite-volume 3d domain Ω ⊂ R3 .\nOne linear operator on these fields is the curl ∇×, which is important in electromagnetism (which\nwe will study in more detail later in 18.303). Define the inner product of two vector fields u and v\n\nby the volume integral (u, v) =\nu · v.\nΩ\n(a) An 18.02 exercise: derive the identity ∇· (u × v) = (∇× u) · v -u · (∇× v).\n(b) Figure out how to do integration by parts with the curl: show that (u, ∇× v) = (∇× u, v) +\n‚\nw · dS, where dS is the usual outward surface-normal area element, and the w appearing\n∂Ω\nin the surface integral over the boundary (∂Ω) is some vector field to be determined. (Hint:\nuse the identity you derived in the previous part, combined with the divergence theorem.)\n(c) Give a possible boundary condition on our space of vector fields such that ∇× is self-adjoint\nwith this inner product. (Boundary conditions can only involve one vector field at a time! No\nfair giving an equation that relates u to v!) You should not have to specify all the components\nof u on the boundary. It may be convenient to define a vector field n(x) on ∂Ω to denote the\noutward normal vector at each point on the boundary.1\n(d) Show that ∇×∇× is self-adjoint for this inner product under either some boundary condition\non u (similar to above) or some boundary condition on derivatives of u. Is it positive or\nnegative definite or semidefinite?\n-∂B\n1 ∂E\n(e) Two of Maxwell's equations in vacuum are ∇× E =\nand ∇× B =\nwhere c is the\n∂t\nc2 ∂t\nspeed of light. Take the curl of both sides of the second equation to obtain a PDE in E alone.\nSuppose that Ω is the interior of a hollow metal container, where the boundary conditions are\nthat E is perpendicular to the metal at the surface (i.e. E × n|\n= 0).2 Combining these\n∂Ω\nfacts with the previous parts, explain why you would expect to obtain oscillating solutions to\nMaxwell's equations (standing electromagnetic waves, essentially light bouncing around inside\nthe container).\n(This kind of system exists, for example, in the microwave regime where metals have high\nconductivity, and such containers are called microwave resonant cavities.)\nProblem 2:\nIn class, we solved for the eigenfunctions of ∇2 in two dimensions, in a cylindrical region r ∈\n[0, R], θ ∈ [0, 2π] using separation of variables, and obtained Bessel's equation and Bessel-function\nsolutions. Although Bessel's equation has two solutions Jm(kr) and Ym(kr) (the Bessel functions),\nthe second solution (Ym) blows up as r → 0 and so for that problem we could only have Jm(kr)\nsolutions (although we still needed to solve a transcendental equation to obtain k).\nIn this problem, you will solve for the 2d eigenfunctions of ∇2 in an annular region Ω that\ndoes not contain the origin, as depicted schematically in Fig. 1, between radii R1 and R2, so that\n1Technically, we are assuming here that the boundary is a differentiable surface so that the normal vector is well\ndefined. Actually, it is sufficient to assume that it is differentiable except for isolated corners and edges (a \"set of\n\"\nmeasure zero\") since those isolated kinks won't contribute anything to the\nsurface integral.\n∂Ω\n2In a perfect conductor, any nonzero component of E parallel to the surface would generate an infinite current\nparallel to the surface, in which charges instantaneously rearrang to cancel the field. For a real conductor with finite\nconductivity, matters are more complicated because we must consider what the fields do inside the conductor, but\na perfect conductor is a good approximation at microwave and lower frequencies where the penetration of the field\ninto the conductor (the skin depth) is much smaller than the wavelength of light.\n\nR1\nR2\nE\ndomain N\nFigure 1: Schematic of the domain Ω for problem 3: an annular region in two dimensions, with\nradii r ∈ [R1, R2] and angles θ ∈ [0, 2π].\nyou will need both the Jm and Ym solutions. Exactly as in class, the separation of variables ansatz\nu(r, θ) = ρ(r)τ(θ) leads to functions τ(θ) spanned by sin(mθ) and cos(mθ) for integers m, and\nfunctions ρ(r) that satisfy Bessel's equation. Thus, the eigenfunctions are of the form:\nu(r, θ) = [αJm(kr) + βYm(kr)] × [A cos(mθ) + B sin(mθ)]\nfor arbitrary constants A and B, for integers m = 0, 1, 2, . . ., and for constants α, β, and k to be\ndetermined.\nFor fun, we will also change the boundary conditions somewhat. We will impose \"Neumann\"\nboundary condition ∂u = 0 at R1 and R2. That is, for a function u(r, θ) in cylindrical coordinates,\n∂r\n∂u |r=R1 = 0 and ∂u |r=R2 = 0. The following exact identities for the derivatives of the Bessel\n∂r\n∂r\nfunctions will be helpful:\n′\nJm-1(x) -Jm+1(x)\n′\nYm-1(x) -Ym+1(x)\nJ (x) =\n,\nY (x) =\nm\nm\n(a) Using the boundary conditions, write down two equations for α, β and k, of the form\n(\n)\nα\nE\n= 0 for some 2 × 2 matrix E.\nThis only has a solution when det E = 0, and\nβ\nfrom this fact obtain a single equation for k of the form fm(k) = 0 for some function fm that\ndepends on m. This is a transcendental equation; you can't solve it by hand for k. In terms\nof k (which is still unknown), write down a possible expression for α and β, i.e. a basis for\nN(E).\n(b) Assuming R1 = 1, R2 = 2, plot your function fm(k) versus k ∈ [0, 20] for m = 0, 1, 2.\nNote that Julia provides the Bessel functions built-in: Jm(x) is besselj(m,x) and Ym(x) is\nbessely(m,x). You can plot a function with the plot command. See the IJulia notebook\nposted on the course web page for lecture 8 for some examples of plotting and finding roots\nin Julia.\n(c) For m = 0, find the first three (smallest k > 0) solutions k1, k2, and k3 to f0(k) = 0. Get a\nrough estimate first from your graph (zooming if necessary), and then get an accurate answer\n\nby calling the scipy.optimize.newton function as in pset 1, and also as illustrated in the lecture\n8 IJulia notebook. (Note that there is also a k = 0 eigenfunction for m = 0, corresponding to\nthe constant function: the nullspace of Aˆ with Neumann boundary conditions, as in class.)\n\n(d) Because ∇2 is self-adjoint under (u, v) =\n\nΩ, that this is\nuv (we showed in class, in general\nΩ\nstill true with these boundary conditions), we know that the eigenfunctions must be orthogo\nnal. From class, this implies that the radial parts must also be orthogonal when integrated via\n\nr dr. Check that your Bessel solutions for k1 and k2 are indeed orthogonal, by numerically\nintegrating their product via the quadgk function in Julia as in pset 2 and as in the lecture-8\nIJulia notebook.\n(e) Let's change the problem. Suppose that the domain is now 0 ≤ r ≤ R2, and the operator is\nAˆ = c(r)∇2 with c(r) = 2 for r < R1 and c(r) = 1 for r ≥ R1. Suppose we impose Dirichlet\nboundary conditions u(R2) = 0.\n(i) What is the form of the eigenfunctions? (Define them in terms of Jm(kr) and Ym(kr)\nwith unknown coefficients in the r < R1 and r ≥ R1 regions; don't try to solve for the\ncoefficients.)\n(ii) If we solve for eigenfunctions ˆ\n=\nAu\nλu, and u is everywhere finite, then what continuity\nconditions must u satisfy at r = R1 in order for ˆAu to be well defined and finite? If\nyou combine these continuity conditions with the boundary condition at R2, you should\nfind that the number of equations that u must satisfy matches the number of unknown\ncoefficients in the previous part.\n(iii) As before, write down a condition fm(k) = 0 that must be satisfied in order for the above\nequations to have a solution. The roots of this function then give the eigenvalues. (You\nwould have to solve it numerically as above, but you need not do this here; just write\ndown fm, which you can leave in the form of a determinant.)\nProblem 3:\nThe Bessel functions u(x) = Jm(kx), from class, solve the eigenproblem:\n′\nu\nm\nˆ\n′′\nAu = u +\n-\nu = -k2 u = λu\nr\nr2\non [0, R] where u(R) = 0 and u(0) = 0 for m > 0.\n(a) Show that this operator Aˆ is of the form of a Sturm-Louville operator (from the class notes),\nand is therefore self-adjoint for an appropriate inner product (which?). (Hint: rewrite the\nˆ\nfirst two terms of A as a single term.)\n(b) Show that Aˆ is negative definite, hence λ < 0 and we are entitled to write λ = -k2 for a real\nk.\n(c) Write down a center-difference discretization of this operator Aˆ for un = u(nΔx) with n =\n1, . . . , R Δx = R/(N +1). Be careful of where you evaluate the 1/r factors (both to maintain\nsecond-order accuracy, and to avoid dividing by zero.)\nˆ\n(d) In Julia, form the matrix approximation A of A for m = 1 (with N = 100 and R = 1)\nusing code similar to problem 2 from pset 2. Compare its smallest-magnitude eigenfunction\nto J1(k1,1r/R) (where k1,1 is the first root of J1), evaluated with the help of the Julia code\nposted in Lecture 9. They should be the same up to some overall scale factor, within the\ndiscretization accuracy.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Linear Partial Differential Equations, Solution to Problem 3",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/b45c1232530dd3ab522dea9e13c31fc5_MIT18_303F14_pset3sol.pdf",
          "content": "18.303 Problem Set 3 Solutions\nProblem 2: (5+5+5+10+5)\n(a) Writing out the derivation in 18.02 fashion, this is tedious but straightforward:\n⎛\n⎞\n\nuyvz - uzvy\n∂\n∂\n∂\n⎝\n⎠\n\\ · (u × v) =\nuz vx - uxvz\n∂x\n∂y\n∂z\nuxvy - uyvx\n∂(uyvz - uz vy)\n∂(uz vx - uxvz )\n∂(uxvy - uyvx)\n=\n+\n+\n∂x\n∂y\n∂z\n∂uz\n∂uy\n∂ux\n∂uz\n∂uy\n∂ux\n=\n-\nvx +\n-\nvy +\n-\nvz\n∂y\n∂z\n∂z\n∂x\n∂x\n∂y\n∂vy\n∂vz\n∂vz\n∂vx\n∂vx\n∂vy\nux\n-\n+ uy\n-\n+ uz\n-\n∂z\n∂y\n∂x\n∂z\n∂y\n∂x\n= (\\ × u) · v - u · (\\ × v).\n(A much more compact derivation is possible using Einstein notation and the Levi-Civita\ntensor, but probably most of you haven't seen this notation.)\n(b) Given the above identity, integration by parts is straightforwards:\nˆ\nˆ\n\n(u, \\ × v) =\nu · (\\ × v) =\n\\ · (u × v) + \\ × u · v\nΩ\nΩ\n‹\n=\n(u × v) · dS + (\\ × u, v),\n∂Ω\n‚\napplying the divergence theorem in the second line. So, the surface term\n∂Ω w · dS is for\nw = u × v .\n(c) We must have (u × v) · dS = 0. Let dS = n dS, where n is the outward unit normal vector\nat each point on ∂Ω. Then we must have u × v ⊥ n, which is true if, for example, both u\nand v are parallel to n at the boundary. i.e. if u × n|\n= 0 . It is not necessary to require\n∂Ω\nu|\n= 0 on the boundary, and that answer will not be accepted as I specifically requested\n∂Ω\nthat you not constrain all the components of u on the boundary.\nAnother way to see this is to write (u ×v) ·dS = n ·(u ×v)dS = v ·(n ×u )dS = u ·(v ×n)dS\nby elementary triple-product identities, and hence we again see that it is sufficient to have\nu × n = 0 on the boundary.\nAlthough I will accept the above answer, it is actually possible to contrive a slightly weaker\ncondition: u and v can have components ⊥ to n on the boundary, as long as those surface-\nparallel components are in the same direction for both u and v (to obtain zero cross product).\nThat is, suppose p(x) is some surface-parallel (p ⊥ n) vector field on ∂Ω. Then it is suf\nficient for the surface-parallel component of u to be ⊥ to p everywhere on the boundary, or\nequivalently u · p|\n= 0.\n∂Ω\n[Actually, there are other possible conditions on u if we allow non-local boundary condi\ntions, where u at one point on the boundary is related to u at another point. For example,\nif Ω is a cube and u is periodic (i.e. u on one face equals u on the opposite face), then the\n1By the \"hairy ball theorem\" of topology, p must vanish somewhere on ∂Ω if Ω is simply connected (no holes), at\nwhich point u must be parallel to n.\n\n‚\n∂Ω vanishes because each face of the cube cancels the opposite face, without requiring any\ncomponent of u to be zero.]\n(d) We just \"integrate by parts\" twice:\nˆ\n‹\nˆ\n\n(u, \\ × \\ × v) =\nu · (\\ × \\ × v) =\n[u × (\\ × v)] · dS +\n(\\ × u ) · (\\ × v)\n\nΩ\n∂Ω\nΩ\n‹\nˆ\n\n=\n[(\\ × u ) × v] · dS +\n(\\ × \\ × u ) · v = (\\ × \\ × u, v),\n\n∂Ω\nΩ\nwhere the surface terms cancel if either u × n|\n= 0 or (\\ × u) × n|\n= 0 , that is if\n∂Ω\n∂Ω\neither u or its curl are normal to the surface. (As in the previous part, one can actually\nweaken this slightly, but this is sufficient for our purposes.)\nTo check definiteness, carry integration by parts \"halfway\" through:\nˆ\n(u, \\ × \\ × u) =\n|\\ × u|2 ≥ 0,\nΩ\nso it is positive semidefinite. It is not positive-definite since \\ × u = 0 for u = \\φ\n=\nfor any non-constant scalar field φ , and we can easily choose such a φ such that \\φ satisfies\nthe boundary conditions (e.g., choose φ so that it is constant in a neighborhood of ∂Ω).\n(e) Taking the curl of both sides of \\×E = - ∂B , we obtain \\×\\×E = -\\× ∂B = - ∂ \\×B =\n∂t\n∂t\n∂t\n- ∂2 E\n∂t2 . That is, we have\n∂2E\nˆ\n= AE\n∂t2\nwhere Aˆ = -\\×\\×. From the previous parts, for E ⊥ n at the surface, this Aˆ is self-adjoint\nand negative semidefinite, and hence we have a hyperbolic equation.\nAs in class, we therefore expect orthogonal eigenfunctions and real λ ≤ 0, and hence os\n√\ncillating \"normal mode\" solutions with eigenfrequencies ω =\n-λ.\n[Technically, we also obtain λ = 0 solutions which are non-oscillatory--from above, these are\nnullspace solutions E = \\φ for some φ, which physically correspond to the time-independent\nfields of fix charge distributions, where -φ is the potential and \\ · E = \\2φ is the charge\ndensity. Everything else, all of the other eigenfunctions, are oscillating solutions.]\nProblem 2: (5+5+5+5+10)\nSee also the IJulia notebook posted with the solutions.\n(a) Setting the slopes to be zero at R1 and R2 simply gives\nαJ: (kr) + βY : (kr) = 0\nm\nm\nα\nat the two radii, or E\n= 0 where\nβ\nJ: (kR1) Y : (kR1)\nm\nm\nE =\n.\nJ: (kR2) Y : (kR2)\nm\nm\nHence, writing fm(k) = det E, we get\nfm(k) = J: (kR1)Y : (kR2) - J: (kR2)Y : (kR1) .\nm\nm\nm\nm\n\nk\n0.4\n0.2\n0.0\n0.2\n0.4\nfm (k)\nf0(k)\nf1(k)\nf2(k)\nFigure 1: Plot of fm(k) for m = 0, 1, 2.\nGiven a k for which fm(k) = 0, then we can solve for the nullspace of E by arbitrarily choosing\nα\na scaling such that α = 1 and solving for β from the first or second rows of E\n= 0:\nβ\nJ: (kR1)\nJ: (kR2)\nm\nm\nβ = -\n= -\n.\nY : (kR1)\nY : (kR2)\nm\nm\n(b) The plot is shown in Figure 1. Note that fm(k) for m > 0 has a divergence as k → 0, so we\nused the ylim command to rescale the vertical axis (otherwise it would be hard to read the\nplot!); see the solution IJulia notebook.\n(c) We'll use the Scilab newton function, similar to class, to find the roots, with initial guesses\nprovided by our plot in Figure 1. We find k1 ≈ 3.196578, k2 ≈ 6.31234951, and k3 ≈\n9.4444649. See the solutions notebook.\n(d) See the IJulia notebook. Using our k1 and k2 from part (c) and our α and β from part (a),\nR2\nwe find that\nru0,1(r)u0,2(r)dr ≈ 10-15, which is zero up to roundoff errors.\nR1\n(e) Here, we have Aˆ = c\\2, and obtain:\n(i) At the origin, we can't blow up, and therefore we only have J for r < R1, but we have\nboth J and Y outside this. Hence\n\nαJm(k1r)\nr < R1\nu(r, θ) = [A cos(mθ) + B sin(mθ)] ×\n.\nβJm(k2r) + γYm(k2r) r > R1\nNote that the angular dependence must be the same for all r in order to have continuity\nat r = R1. However, we are not guaranteed that k1 = k2! In particular, we want\nˆAu = λu for some λ, and plugging in the Bessel functions and the values of c we find\n√\nλ = -2k2 = -k2. Hence, we let k2 = k and k1 = k/ 2.\n\n(ii) To get a finite Au\nˆ , we must have u and ∂u/∂r continuous at r = R1. Combined with\nu = 0 at r = R2, this gives the equations\n⎡\n√\n⎤ ⎛\n⎞\n⎛\n⎞\n⎣\n-Jm(kR1/ 2)\n-J:\nm(kR1/\n√\n2)\nJm(kR1)\nJ:\nm(kR1)\nYm(kR1)\nY :\nm(kR1) ⎦ ⎝\nα\nβ ⎠ = Em(k) ⎝\nα\nβ ⎠ = 0,\nJm(kR2) Ym(kR2)\nγ\nγ\nwhere the first two rows are the continuity conditions and the last row is the Dirichlet\ncondition, and we have defined a matrix Em(k).\n(iii) We simply need fm(k) = det Em(k) to solve for the solutions k and hence the eigenvalues.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Linear Partial Differential Equations, Problem 4",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/c7e3e7af8a9b6425c3e840dfc4ccdc3c_MIT18_303F14_pset4.pdf",
          "content": "18.303 Problem Set 4\nDue Wednesday, 15 October 2014.\nProblem 1:\nConsider the operator Aˆ = -c(x)Y2 in some 2d region Ω ⊆ R2 with Dirichlet boundaries (u|∂Ω = 0),\nwhere c(x) > 0. Suppose the eigenfunctions of Aˆ are un(x) with eigenvalues λn [that is, ˆ\n=\nAun\nλnun] for n = 1, 2, . . ., numbered in order λ1 < λ2 < λ3 < · · · . Let G(x, x') be the Green's function\nof Aˆ.\n\n(a) If f(x) =\nαnun(x) for some coefficients αn =_________________ (expression\nn\no\n'\nin terms of f and un), then\nG(x, x')f(x')d2x =__________________ (in terms\nΩ\nof αn and un).\n(b) The maximum possible value of\no o\n'\nu(x)G(x, x')u(x') d2x d2x\nΩ\nΩ c(x) o\n|u(x00 )|2\n,\nd2x''\nΩ\nc(x00)\nfor any possible u(x), is _____________________ (in terms of quantities men\ntioned above). [Hint: min-max. Use the fact, from the handout, that if Aˆ is self-adjoint then\nAˆ-1 is also self-adjoint.]\nProblem 2:\nIn this problem, we will solve the Laplacian eigenproblem -Y2u = λu in a 2d radius-1 cylinder\nr ≤ 1 with Dirichlet boundary conditions u|r=1Ω = 0 by \"brute force\" in Julia with a 2d finite-\ndifference discretization, and compare to the analytical Bessel solutions. You will find the IJulia\nnotebooks posted on the 18.303 website for Lecture 9 and Lecture 11 extremely useful! (Note: when\nyou open the notebook, you can choose \"Run All\" from the Cell menu to load all the commands in\nit.)\n(a) Using the notebook for a 100 × 100 grid, compute the 6 smallest-magnitude eigenvalues and\neigenfunctions of A with λi, Ui=eigs(Ai,nev=6,which=\"SM\"). The eigenvalues are given by\nλi. The notebook also shows how to compute the exact eigenvalue from the square of the root\nof the Bessel function. Compared with the high-accuracy λ1 value, compute the error Δλ1 in\nthe corresponding finite-difference eigenvalue from the previous part. Also compute Δλ1 for\nNx = Ny = 200 and 400. How fast is the convergence rate with Δx? Can you explain your\nresults, in light of the fact that the center-difference approximation we are using has an error\nthat is supposed to be ∼ Δx2? (Hint: think about how accurately the boundary condition\non ∂Ω is described in this finite-difference approximation.)\n(b) Modify the above code to instead discretize Y·cY, by writing A0 as -GT CgG for some G ma\ntrix that implements Y and for some Cg matrix that multiplies the gradient by c(r) = r2 + 1.\nDraw a sketch of the grid points at which the components of Y are discretized--these will\nnot be the same as the (nx, ny) where u is discretized, because of the centered differences. Be\ncareful that you need to evaluate c at the Y grid points now! Hint: you can make the matrix\nc\nt\nM1\nin Julia by the syntax [M1;M2].\nM2\nHint: Notice in the IJulia notebook from Lecture 11 how a matrix r is created from a column-\nvector of x values and a row-vector of y values. You will need to modify these x and/or y\nvalues to evaluate r on a new grid(s). Given the r matrix rc on this new grid, you can evaluate\n\nc(r) on the grid by c = rc.^2 + 1, and then make a diagonal sparse matrix of these values\nby spdiagm(reshape(c, prod(size(c)))).\n(c) Using this A ≈ Y · cY, compute the smallest-|λ| eigensolution and plot it. Given the eigen\nfunction converted to a 2d Nx × Ny array u, as in the Lecture 11 notebook, plot u(r) as a\nfunction of r, along with a plot of the exact Bessel eigenfunction J0(k0r) from the c = 1 case\nfor comparison.\nplot(r[Nx/2:end,Ny/2], u[Nx/2:end,Ny/2])\nk0 = so.newton(x -> besselj(0,x), 2.0)\nplot(0:0.01:1, besselj(0, k0 * (0:0.01:1))/50)\nHere, I scaled J0(k0r) by 1/50, but you should change this scale factor as needed to make the\nplots of comparable magnitudes. Note also that the r array here is the radius evaluated on\nthe original u grid, as in the Lecture 11 notebook.\nCan you qualitatively explain the differences?\nProblem 3:\nRecall that the displacement u(x, t) of a stretched string [with fixed ends: u(0, t) = u(L, t) = 0]\nu\nu\nsatisfies the wave equation ∂2 + f(x, t) = ∂2 , where f(x, t) is an external force density (pressure)\n∂x2\n∂t2\non the string.\n(a) Suppose that f(x, t) = <[g(x)e-iωt], an oscillating force with a frequency ω. Show that,\ninstead of solving the wave equation with this f(x, t), we can instead use a complex force\nf (x, t) = g(x)e-iωt, solve for a complex u (x, t), and then take u = <u to obtain the solution\nfor the original f(x, t).\n(b) Suppose that f(x, t) = g(x)e-iωt, and we want to find a steady-state solution u(x, t) =\n-iωt\nv(x)e\nthat is oscillating everywhere at the same frequency as the input force. (This\nwill be the solution after a long time if there is any dissipation in the system to allow the\ninitial transients to die away.) Write an equation ˆ\n=\nA self-adjoint?\nAv\ng that v solves. Is ˆ\nPositive/negative definite/semidefinite?\n(c) Solve for the Green's function G(x, x ' ) of this ˆ\n\nA, assuming that ω = nπ/L for any integer n\n(i.e. assume ω is not an eigenfrequency [why?]). [Write down the continuity conditions that\nG must satisfy at x = x ', solve for x =\nx ', and then use the continuity conditions to eliminate\nunknowns.]\n(d) Form a finite-difference approximation A of your Aˆ. Compute an approximate G(x, x ' ) in\nMatlab by A \\ dk, where dk is the unit vector of all 0's except for one 1/Δx at index\nk = x ' /Δx, and compare (by plotting both) to your analytical solution from the previous\n'\npart for a couple values of x and a couple of different frequencies ω (one < π/L and one\n> π/L) with L = 1.\n(e) Show the limit ω → 0 of your G relates in some expected way to the Green's function of - d2\ndx2\nfrom class.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Linear Partial Differential Equations, Solution to Problem 4",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/0ed71068e504a2f481dc01a5d9a2b671_MIT18_303F14_pset4sol.pdf",
          "content": "18.303 Problem Set 4 Solutions\nProblem 1: (10+10)\nn\n(a) Aˆ is self-adjoint under the inner product (u, v)\n\n=\nuv/c, from class (and the 1d version\nΩ\nin homework), and hence the eigenfunctions are orthogonal for distinct eigenvalues (but not\nnecessarily normalized to IunI = 1), hence αn = (un, f)/(un, un). The expression u(x) =\nn\ne\nG(x, x')f(x\nx' is just the solution to ˆ\nˆ\n=\nαn un\n')d2\nAu = f, i.e. it is u = A-1f\n(x).\nΩ\nn λn\n(b) This is just (u, Aˆ-1u)/(u, u), which is a Rayleigh quotient for the self-adjoint operator Aˆ-1 ,\nand hence (thanks to the min-max theorem) it is bounded above by the largest eigenvalue\nof Aˆ-1, which is 1/λ1 (the inverse of the smallest eigenvalue of Aˆ). (Note that by positive-\ndefiniteness 0 < λ1 < λ2 < · · · .)\nProblem 2:\nIn this problem, we will solve the Laplacian eigenproblem -'2u = λu in a 2d radius-1 cylinder\nr ≤ 1 with Dirichlet boundary conditions u|r=1Ω = 0 by \"brute force\" in Julia with a 2d finite-\ndifference discretization, and compare to the analytical Bessel solutions. You will find the IJulia\nnotebooks posted on the 18.303 website for Lecture 9 and Lecture 11 extremely useful! (Note: when\nyou open the notebook, you can choose \"Run All\" from the Cell menu to load all the commands in\nit.)\n(a) Using the notebook for a 100 × 100 grid, compute the 6 smallest-magnitude eigenvalues and\neigenfunctions of A with λi, Ui=eigs(Ai,nev=6,which=\"SM\"). The eigenvalues are given by\nλi. The notebook also shows how to compute the exact eigenvalue from the square of the root\nof the Bessel function. Compared with the high-accuracy λ1 value, compute the error Δλ1 in\nthe corresponding finite-difference eigenvalue from the previous part. Also compute Δλ1 for\nNx = Ny = 200 and 400. How fast is the convergence rate with Δx? Can you explain your\nresults, in light of the fact that the center-difference approximation we are using has an error\nthat is supposed to be ∼ Δx2? (Hint: think about how accurately the boundary condition\non ∂Ω is described in this finite-difference approximation.)\n(b) Modify the above code to instead discretize '·c', by writing A0 as -GT CgG for some G ma\ntrix that implements ' and for some Cg matrix that multiplies the gradient by c(r) = r2 + 1.\nDraw a sketch of the grid points at which the components of ' are discretized--these will\nnot be the same as the (nx, ny) where u is discretized, because of the centered differences. Be\ncareful that you need to evaluate c at the ' grid points now! Hint: you can make the matrix\nc\nt\nM1\nin Julia by the syntax [M1;M2].\nM2\nHint: Notice in the IJulia notebook from Lecture 11 how a matrix r is created from a column-\nvector of x values and a row-vector of y values. You will need to modify these x and/or y\nvalues to evaluate r on a new grid(s). Given the r matrix rc on this new grid, you can evaluate\nc(r) on the grid by c = rc.^2 + 1, and then make a diagonal sparse matrix of these values\nby spdiagm(reshape(c, prod(size(c)))).\n(c) Using this A ≈' · c', compute the smallest-|λ| eigensolution and plot it. Given the eigen\nfunction converted to a 2d Nx × Ny array u, as in the Lecture 11 notebook, plot u(r) as a\nfunction of r, along with a plot of the exact Bessel eigenfunction J0(k0r) from the c = 1 case\nfor comparison.\nplot(r[Nx/2:end,Ny/2], u[Nx/2:end,Ny/2])\nk0 = so.newton(x -> besselj(0,x), 2.0)\nplot(0:0.01:1, besselj(0, k0 * (0:0.01:1))/50)\n\nHere, I scaled J0(k0r) by 1/50, but you should change this scale factor as needed to make the\nplots of comparable magnitudes. Note also that the r array here is the radius evaluated on\nthe original u grid, as in the Lecture 11 notebook.\nCan you qualitatively explain the differences?\nProblem 3: (5+5+10+10+5 points)\nRecall that the displacement u(x, t) of a stretched string [with fixed ends: u(0, t) = u(L, t) = 0]\nu\nu\nsatisfies the wave equation ∂2 + f(x, t) = ∂2 , where f(x, t) is an external force density (pressure)\n∂x2\n∂t2\non the string.\n∂2\nu\nu\n(a) Suppose that u solves ∂2 + f (x, t) =\nand satisfies u (0, t) = u (L, t) = 0. Now, consider\n∂x2\n∂t2\nu +u\nu = <u =\n. Clearly, u(0, t) = u(L, t) = 0, so u satisfies the same boundary conditions. It\nalso satisfies the PDE:\n\n∂2u\n1 ∂2u\n∂2u\n=\n+\n∂t2\n∂t2\n∂t2\n\n1 ∂2u\n∂2u\n=\n+ f +\n+ f\n∂x2\n∂x2\n∂2u\n=\n+ f,\n∂x2\nf+f\nsince f =\n\n= <f . The key factors that allowed us to do this are (i) linearity, and (ii) the\nreal-ness of the PDE (the PDE itself contains no i factors or other complex coefficients).\n(b) Plugging u(x, t) = v(x)e-iωt and f(x, t) = g(x)e-iωt into the PDE, we obtain\n∂2v\n\n-iωt\n-iωt\n-iωt\n\n+ g = -ω2 v\ne\ne\ne\n,\n∂x2\nand hence\nc\nt\n∂2\n-\n- ω2 v = g\n∂x2\n∂2\nand Aˆ = -\n- ω2 . The boundary conditions are v(0) = v(L) = 0, from the boundary\n∂x2\nconditions on u.\nSince ω2 is real, this is in the general Sturm-Liouville form that we showed in class is self\nadjoint.\nSubtracting a constant from an operator just shifts all of the eigenvalues by that constant,\nkeeping the eigenfunctions the same. Thus Aˆ is still positive-definite if ω2 is < the smallest\neigenvalue of -∂2/∂x2, and positive semidefinite if ω2 = the smallest eigenvalue. In this case,\nwe know analytically that the eigenvalues of -∂2/∂x2 with these boundary conditions are\n(nπ/L)2 for n = 1, 2, . . .. So Aˆ is positive-definite if ω2 < (π/L)2, it is positive-semidefinite if\nω2 = (π/L)2, and it is indefinite otherwise.\n(c) We know that ˆ\n' ) = 0 for x\nx '. Also, just as in class and as in the notes, the fact\nAG(x, x\n=\nthat ˆ\n' ) =\n' ) meas that G must be continuous (otherwise there would be a δ '\nAG(x, x\nδ(x - x\nfactor) and ∂G/∂x must have a jump discontinuity:\n\n∂G\n∂G\n-\n= -1\n∂x\n∂x\nx=x1+\nx=x1-\n\nZ\nZ\n\nfor -∂2G/∂x2 to give δ(x - x ' ). We could also show this more explicitly by integrating:\nx 1+0+\nx 1+0+\nˆAG dx =\nδ(x - x ' ) dx = 1\nx1-0+\nx1-0+\nx +0+\nZ x +0+\n= - ∂G\n-\nω2G dx,\n∂x\n1-0+\nx1-0+\nx\nwhich gives the same result.\n'\nNow, similar to class, we will solve it separately for x < x and for x > x ', and then im\n'\npose the continuity requirements at x = x to find the unknown coefficients.\n'\nˆ\nG\nFor x < x , AG = 0 means that ∂2\n= -ω2G, hence G(x, x ' ) is some sine or cosine of\n∂x2\nωx. But since G(0, x ' ) = 0, we must therefore have G(x, x ' ) = α sin(ωx) for some coefficient\nα.\nSimilarly, for x > x ', we also have a sine or cosine of ωx. To get G(L, x ' ) = 0, the sim\nplest way to do this is to use a sine with a phase shift: G(x, x ' ) = β sin(ω[L - x]) for some\ncoefficient β.\nContinuity now gives two equations in the two unknowns α and β:\nα sin(ωx ' ) = β sin(ω[L - x ' ])\nαω cos(ωx ' ) = -βω cos(ω[L - x ' ]) + 1,\nsin(ω[L-x ])\nsin(ωx1)\nwhich has the solution α =\n, β =\n.\nω cos(ωx1) sin(ω[L-x1])+sin(ωx1 ) cos(ω[L-x1])\nω cos(ωx1) sin(ω[L-x1])+sin(ωx1) cos(ω[L-x1])\nThis simplifies a bit from the identity sin A cos B + cos B sin A = sin(A + B), and hence\n_\nsin(ωx) sin(ω[L - x ' ]) x < x '\nG(x, x ' ) =\n,\n'\nω sin(ωL)\nsin(ωx ' ) sin(ω[L - x]) x ≥ x\nwhich obviously obeys reciprocity.\nNote that if ω is an eigenfrequency, i.e. ω = nπ/L for some n, then this G blows up.\nThe reason is that Aˆ in that case is singular (the n-th eigenvalue was shifted to zero), and\nA-1\ndefining ˆ\nis more problematical. (Physically, this corresponds to driving the oscillating\nstring at a resonance frequency, which generally leads to a diverging solution unless there is\ndissipation in the system.)\nd2\n(d) It is critical to get the signs right. Recall that we approximated\nby -DT D for a 1st\ndx2\nderivative matrix D. Therefore, you want to make a matrix A = DT D - ω2I. In Matlab, A\n= D'*D - omega^2 * eye(N) where N is the size of the matrix D = diff1(N) / dx with dx\n= 1/(N+1). We make dk by dk = zeros(N,1); dk(k) = 1/dx. I used N = 100.\nThe resulting plots are shown in figure 1, for ω = 0.4π/L (left) and ω = 5.4π/L (right)\n'\nand x = 0.5 and 0.75. As expected, for ω < π/L where it is positive-definite, the Green's\nfunction is positive and qualitatively similar to that for ω = 0, except that it is slightly curved.\nFor larger ω, though, it becomes oscillatory and much more interesting in shape. The exact\nG and the finite-difference G match very well, as they should, although the match becomes\nworse at higher frequencies--the difference approximations become less and less accurate as\nthe function becomes more and more oscillatory relative to the grid Δx, just as was the case\n\n0.2\n0.4\n0.6\n0.8\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nx\nG(x,x')\nω = 0.4π/L\n\nexact x'=0.5\nexact x'=0.75\nN=100 x'=0.5\nN=100 x'=0.75\n0.2\n0.4\n0.6\n0.8\n-0.06\n-0.04\n-0.02\n0.02\n0.04\n0.06\nx\nG(x,x')\nω = 5.4π/L\n\nexact x'=0.5\nexact x'=0.75\nN=100 x'=0.5\nN=100 x'=0.75\n- ∂2\nFigure 1: Exact (lines) and finite-difference (dots) Green's functions G(x, x ' ) for Aˆ =\n- ω2 ,\n∂x2\n'\nfor two different ω values (left and right) and two different x values (red and blue).\nfor the eigenfunctions as discussed in class.\n[If you try to plug in an eigenfrequency (e.g. 4π/L) for ω, you may notice that the exact\nG blows up but the finite-difference G is finite. The reason for this is simple: the eigenvalues\nof the finite-difference matrix DT D are not exactly (nπ/L)2, so the matrix is not exactly\nsingular, nor is it realistically possible to make it exactly singular thanks to rounding errors.\nIf you plug in something very close to an eigenfrequency, then G becomes very close to an\neigenfunction multipled by a large amplitude. It is easy to see why this is if you look at the\nexpansion of the solution in terms of the eigenfunctions.]\n(e) For small ω, sin(ωy) ≈ ωy for any y, and hence\nG(x, x ' ) → 1\n\nω2L\n_\n\nω2x(L - x ' )\n\nω2x ' (L - x)\nx < x '\nx ≥ x ' ,\nwhich matches the Green's function of - d2 from class. (Equivalently, you get 0/0 as ω → 0\ndx2\nand hence must use L'Hopital's rule or similar to take the limit.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Linear Partial Differential Equations, Problem 5",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/09c1716433c040001efb2c2ddb3ed043_MIT18_303F14_pset5.pdf",
          "content": "V\nV\nn\nFigure 1: A volume V with a surface ∂V , and an outward unit normal vector n at each point on ∂V .\n18.303 Problem Set 5\nDue Monday, 27 October 2014.\nProblem 1: Distributions\nThis problem concerns distributions as defined in the notes: continuous linear functionals f{φ} from test functions\nφ ∈D, where D is the set of infinitely differentiable functions with compact support (i.e. φ = 0 outside some region\nwith finite diameter [differing for different φ], i.e. outside some finite interval [a, b] in 1d).\n\nln |x| x = 0\n\n(a) In this part, you will consider the function f(x) =\nand its (weak) derivative, which is connected\nx = 0\nto something called the Cauchy Principal Value.\n(i) Show that f(x) defines a regular distribution, by showing that f(x) is locally integrable for all intervals\n[a, b].\n\n= 0\nx\n(ii) Consider the 18.01 derivative of f(x), which gives f '(x) =\nx\n. Suppose we just set\nundefined x = 0\n\n= 0\nx\n\"f '(0) = 0\" at the origin to define g(x) =\nx\n. Show that this g(x) is not locally integrable,\nx = 0\nand hence does not define a distribution.\nBut the weak derivative f '{φ} must exist, so this means that we have to do something different from\nthe 18.01 derivative, and moreover f '{φ} is not a regular distribution. What is it?\n-f\ninf\n(iii) Write f{φ} = limf→0+ ff{φ} where ff{φ} =\n-inf ln(-x)φ(x)dx + f ln(x)φ(x)dx, since this limit exists\nand equals f{φ} for all φ from your proof in the previous part.1 Compute the distributional derivative\nf '{φ} = limf→0+ ff\n'{φ}, and show that f '{φ} is precisely the Cauchy Principal Value (google the definition,\ne.g. on Wikipedia) of the integral of g(x)φ(x).\ninf\n(iv) Alternatively, show that f '{φ(x)} = g{φ(x) - φ(0)} =\ng(x)[φ(x) - φ(0)]dx (which is a well-defined\n-inf\nintegral for all φ ∈D).\n(b) In class, we only looked explicitly at 1d distributions, but a distribution in d dimensions Rd can obviously be\ndefined similarly, as maps f{φ} from smooth localized functions φ(x) to numbers. Analogous to class, define the\ndistributional gradient 'f by 'f{φ} = f{-'φ}.\nConsider some finite volume V with a surface ∂V , and assume ∂V is differentiable so that at each point\nit has an outward-pointing unit normal vector n, as shown in figure 1. Define a \"surface delta function\"\n\nδ(∂V ){φ} =\nφ(x)dd-1x to give the surface integral\nof the test function.\n∂V\n∂V\nd\nd\n1More explicitly, f{φ} - fd{φ} =\nln |x|φ(x)dx ≤ (max φ)\nln |x|dx → 0, since you should have done the something like the last\n-d\n-d\nintegral explicitly in the previous part.\n\nf1(x) x ∈ V\nSuppose we have a regular distribution f{φ} defined by the function f(x) =\n, where we may\nf2(x) x ∈/ V\nhave a discontinuity f2 - f1 = 0 at ∂V .\n(i) Show that the distributional gradient of f is\n'f1(x) x ∈ V\n'f = δ(∂V ) [f1(x) - f2(x)] n(x) +\n,\n'f2(x) x ∈/ V\nwhere the second term is a regular distribution given by the ordinary 18.02 gradient of f1 and f2 (assumed\nto be differentiable), while the first term is the singular distribution\n\nδ(∂V ) [f1(x) - f2(x)] n(x){φ} =\n[f1(x) - f2(x)] n(x)φ(x)dd-1 x.\n∂V\n\nYou can use the integral identity that\n'ψddx = ∂V ψndd-1x to help you integrate by parts.\nV\n(ii) Defining '2f{φ} = f{'2φ}, derive a similar expression to the above for '2f. Note that you should have\none term from the discontinuity f1 - f2, and another term from the discontinuity 'f1 -'f2. (Recall how\nwe integrated '2 by parts in class some time ago.)\nProblem 2: Green's functions\nConsider Green's functions of the self-adjoint indefinite operator Aˆ = -'2 - ω2 (κ > 0) over all space (Ω = R3 in\n3d), with solutions that → 0 at infinity. (This is the multidimensional version of problem 2 from pset 5.) As in class,\nthanks to the translational and rotational invariance of this problem, we can find G(x, x ' ) = g(|x - x ' |) for some g(r)\nin spherical coordinates.\n(a) Solve for g(r) in 3d, similar to the procedure in class.\n(i) Similar to the case of Aˆ = -'2 in class, first solve for g(r) for r > 0, and write g(r) = limf→0+ ff(r)\nwhere ff(r) = 0 for r ≤ E. [Hint: although Wikipedia writes the spherical '2g(r) as 1 (r g ' ) ', it may be\nr\nmore convenient to write it equivalently as '2g =\n(rg) '', as in class, and to solve for h(r) = rg(r) first.\nr\nHint: if you get sines and cosines from this differential equation, it will probably be easier to use complex\nexponentials, e.g. eiωr, instead.]\n(ii) In the previous part, you should find two solutions, both of which go to zero at infinity. To choose between\nthem, remember that this operator arose from a e-iωt time dependence. Plug in this time dependence and\nimpose an \"outgoing wave\" boundary condition (also called a Sommerfield or radiation boundary condition):\nrequire that waves be traveling outward far away, not inward.\n(iii) Then, evaluate Ag\nˆ\n=\nAg){q} = g{ ˆ\n= q(0) for an arbitrary (smooth,\nδ(x) in the distributional sense: ( ˆ\nAq}\nlocalized) test function q(x) to solve for the unknown constants in g(r). [Hint: when evaluating g{ ˆAq}, you\nmay need to integrate by parts on the radial-derivative term of '2q; don't forget the boundary term(s).]\n(b) Check that the ω → 0+ limit gives the answer from class.\n(\n=\n(\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Linear Partial Differential Equations, Solution to Problem 5",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/c8778f356ecf49052c9e5e41b1872e60_MIT18_303F14_pset5sol.pdf",
          "content": "18.303 Problem Set 5 Solutions\nProblem 1: ((3+2+5+5)+(5+5) points)\n(a) Solutions:\n(i) f(x) is bounded in every interval except intervals containing x = 0, so local integrability is trivial except\nb\nfor intervals containing x = 0. It is sufficient to consider integrals\n, because any interval [a, b] containing\nb\n0 can be broken up into\n+\n, and f(-x) = f(x) so we only need to show that the latter is finite. But\na\nwe can now just do the integral explicitly:\nˆ b\nˆ b\nln x dx = lim\nln x dx = lim x ln x - x|b\nl = b ln b - b,\nl→0+\nl→0+\nl\nsince\nlim x ln x = 0,\nl→0+\nas can easily be seen e.g. by L'Hopital's rule applied to lim ln x = lim 1/x = lim x = 0.\n1/x\n-1/x2\n(ii) g(x) is not locally integrable for intervals containing the origin. For example\nˆ b\nˆ b dx\nb\n|g(x)|dx = lim\n= lim ln x|l = inf.\nl→0+\nx\nl→0+\nl\nTherefore, f '{φ} = f{-φ'} is a singular distribution.\n(iii) We write f '{φ} = liml→0+ fl\n'{φ}, and integrate by parts in fl\n'{φ} = fl{-φ'}:\n⎡\n⎤\nˆ -l\nˆ inf\n⎣\n⎦\nl→0+\n' -v \" ' -v \"\n' -v \" ' -v \"\nf '{φ} = - lim\nln(-x) φ'(x)dx +\nln(x) φ'(x)dx\n-inf\nl\nu\ndv\nu\ndv\n\nˆ -l\nˆ inf\n-1\n= - lim ln(E)φ(-E) -\nφ(x)dx - ln(E)φ(E) -\nφ(x)dx\nl→0+\nx\nx\nˆ -l\nˆ\n-inf\ninf\n\nl\n\n= lim\nφ(x)dx +\nφ(x)dx + lim [ln(E) (φ(E) - φ(-E))].\n\nx\n→0+\nl→0+\nx\nl\n-inf\nl\n\nIn the last line, the first limit is precisely the Cauchy Principal Value of g(x)φ(x)dx (CPV = remove a\nball of radius E around the singularity, do the integral, and then take the E → 0+ limit). The second term\nvanishes because, since φ(x) is continuous and infinitely differentiable, φ(E) - φ(-E) vanishes at least as fast\nas E as E → 0, so its product with ln E vanishes in the limit as in part (i).1\n(iv) Use f '{φ} = f{-φ'} = f{-[φ - φ(0)]'} = f '{φ - φ(0)}. Then substitute φ - φ(0) into the previous part,\nand note that since the integrand φ(x)-φ(0) is now finite as x → 0 [since φ(x) - φ(0) goes to zero as x → 0,\nx\nat least proportionally to x or faster as in the previous part], we can now just take the limit to write\ninf\nf '{φ} =\ng(x)[φ(x) - φ(0)]dx without using the CPV.\n-inf\n(b) Solutions:\n(i) Let V c denote the complement of V (the exterior region outside V , i.e. V c = Rd\\V ), and note that\n1This last fact is a little subtle. Naively, we could just write the Taylor expansion φ(x) = φ(0) + φ'(0)E + O(E2) to obtain φ(E) - φ(-E) =\n2φ'(0)E+O(E2). However, it turns out that test functions φ(x) are not analytic (do not have a convergent Taylor series) at all x. Nevertheless,\nbecause φ is differentiable with bounded φ', it follows that φ(x) is \"Lipshitz continuous:\" |φ(x) - φ(y)| < K|x - y| for all x, y and for some\nconstant K > 0 (e.g. you can prove this from the mean-value theorem of analysis), so φ(E) - φ(-E) < 2KE and the result follows. But I\ndon't expect you to provide this level of proof; a Taylor argument is acceptable.\n\n∂V c = ∂V (but with the outward-normal vector reversed in sign). We write:\nˆ\nˆ\nVf{φ} = f{-Vφ} = -\nf1Vφ -\nf2Vφ\nV c\nV\nˆ\nˆ\n= -\n[V(f1φ) - φVf1] -\n[V(f2φ) - φVf2]\nV\nV c\n\nˆ\n\nˆ\n= -\nf1φn +\nφVf1 +\nf2φn +\nφVf2\n∂V\nV\n∂V\nV c\n\nVf1(x) x ∈ V\n=\nδ(∂V ) [f2(x) - f1(x)] n(x) +\n{φ}\nVf2(x) x ∈/ V\nas desired.\n(ii) In this case, we will need to integrate by parts twice, but we can just quote the results from the \"notes on\nelliptic operators\" from class (where we integrated by parts twice with -V2 already), albeit keeping the\nboundary terms from ∂V that were zero in the notes:\nˆ\nˆ\nV2f{φ} = f{V2φ} =\nf1V2φ +\nf2V2φ\nV c\nV\n\nˆ\nˆ\n=\n[(f1 - f2)Vφ - φ(Vf1 -Vf2)] · n +\nφV2f1 +\nφV2f2.\nV c\n∂V\nV\nBut the first term is δ(∂V ) [f1(x) - f2(x)] {n·Vφ} = (n·V)δ(∂V ) [f2(x) - f1(x)] {φ} by the definition (note\nthe sign change) of the distributional derivative n ·V (note that this is a scalar derivative in the n direction,\nnot a gradient vector). The second term is a surface delta function weighted by (n · Vf1 - n · Vf2) ,the\ndiscontinuity in the normal derivative. And the last terms are just a regular distribution. So, we have\n\nV2f1(x) x ∈ V\nV2f = (n · V)δ(∂V ) [f2 - f1] + δ(∂V ) [n · Vf1 - n · Vf2] +\n.\nV2f2(x) x ∈/ V\nAs noted in class, n · V of a delta function is a \"dipole\" oriented in the n direction, so the first term is a\n\"dipole layer\".\nProblem 2: ((5+10)+2 points)\n(a) We solve for g(r) in 3d as follows:\nh ''\n(i) For r > 0, -V2g - ω2g = 0 and hence -ω2g = V2g =\n(rg) '' =⇒\n= -ω2h where h(r) = rg(r). The\nr\niωr +de-iωr\niωr + de-iωr\nce\nsolution to this is h(r) = ce\nfor some constants c and d, or g(r) =\nr\nIt is a little more tricky to determine whether we should use the c or the d term than in class, since\nboth decay at the same rate. The ratio c/d will be determined by some kind of boundary condition at\n±iωr\ninfinity, but what might this be? It is acceptable for you to just punt on this here; since e\nare complex\nconjugates of each other, your analysis will apply equally well to either one, and you can arbitrarily pick\none, say ceiωr/r, to analyze.\nHowever, to see why there should be a sensible choice, recall that this operator arose in pset 5 by as\n-iωt\nsuming a time dependence e\nmultiplying the solution, in which case we are looking at wave solutions\niω(r-t) +de-iω(r+t)\nce\n, where the c term describes waves moving out towards r →inf, while the d term describes\nr\nwaves moving in from infinity. In wave problems, we typically impose a boundary condition of outgoing\nwaves at infinity, in which case we must set d = 0. (However, the choice would have been reversed if we\npicked the opposite sign convention, e+iωt, for the time dependence.)\n(ii) Let's focus on g(r) = ceiωr/r. As in class, the 1/r singularity is no problem in 3d (it is cancelled by the\n\nJacobian factor r2dr), so g is a regular distribution. Given an arbitrary test function q(x), we now evaluate\nˆ\n( ˆ\nˆ\nAg){q} = g{Aq\nˆ } =\ngAq\n⎤\n⎡\nˆ inf\nˆ 2π\nˆ π\nr 2dr\ndφ\nsin θ dθ\n⎢⎢⎣-ω2 gq -\n⎥⎥⎦\nintegrate to 0, from class\n∂2\ng\n\"\n(θ, φ derivatives of q)\n-v\n'\n(rq) +\n=\nr ∂r2\n⎞\n⎛\n⎡\ndr\n⎤\n⎥⎥⎥⎦\n⎢⎢⎢⎣\nc\n⎜\n⎜\n⎜\n⎝\nint. by parts\n⎟\n⎟\n⎟\n⎠\nˆ inf\n∂2\n\niωr -\niωr\n-ω2 rqe\ne\nsin θ dθ dφ\n[rq]\n=\n∂r2\n'\n\"\n-v\n⎡\n⎛\n⎜\n⎜\n⎜\n⎝\n⎤\n⎞\n⎢⎢⎢⎣\n⎟\n⎟\n⎟\n⎠ dr\n⎥⎥⎥⎦\nˆ inf\n\n∂\n∂\n[rq]\ninf\n\n+ c\niωr\niωr\n-ω2 rqe\n+ iωeiωr\n-ce\nsin θ dθ dφ\n[rq]\n=\n∂r\n∂r\n'\n\"\n-v\nint. by parts\n\nˆ inf\n\n-\n(0)\n\ncq\nicωeiωr[rq] 0\ninf\niωr\niωr[rq]\n-ω2 rqe\n+ ω2 e\nsin θ dθ dφ\ndr\n+ c\n=\n= 4πcq(0),\nand hence c = 1/4π . Thus\n/|\niω|x-x\ne\nG(x, x ' ) = 4π|x - x '|\nassuming boundary conditions such that d = 0. More generally, since exactly the same result applies to the\nde-iωr/r term, we obtain,\n/|\n/ |\niω|x-x\nce\n+ de-iω|x-x\nG(x, x ' ) =\n|x - x '|\n'\nfor c + d = 1/4π , with the ratio c/d being set by the boundary conditions at inf. The value at x = x\nbeing irrelevant in the distribution sense, e.g. we can assign it to zero, since this is a regular distribution\nwith a finite integral, similar to class.)\n(b) The ω → 0 limit gives 1/4π|x - x ' | as in class, by inspection.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Exam",
          "title": "Linear Partial Differential Equations, Midterm Exam",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/d53f8880eab061419f4de824ae120b4d_MIT18_303F14_midterm.pdf",
          "content": "18.303 Midterm, Fall 2014\nEach problem has equal weight. You have 55 minutes.\nProblem 1: Adjoints (25 points)\nd2\nConsider the operator Au\nˆ\n=\n(cu) on the domain Ω = [0, L] with Dirichlet boundaries u|∂Ω = 0, where c(x) > 0.\ndx2\nShow that Aˆ = Aˆ∗ for an appropriate choice of inner product (u, v).\nProblem 2: Green (25 points)\nConsider the operator Aˆ = -\\2 in 2d, where the domain is the entire x, y plane. In cylindrical (r, φ) coordinates, we\ncan write\n\n1 ∂\n∂u\n1 ∂2u\n\\2 u =\nr\n+\n.\nr ∂r\n∂r\nr2 ∂φ2\nWe want to solve for the Green's function G(x, x' ) = g(|x - x' |) of Aˆ, reduced to a function g(r) by the symmetry\nof the problem. By looking at r > 0, one can quickly show that g(r) = c ln r for some unknown constant c. This g\ndefines a regular distribution, if we just set g(0) = 0, despite the fact that ln blows up [it is an \"integrable\" singularity\nin 2d: the integral exists for any test function ψ(x, y)]. Find c.\nProblem 3: Waves (25 points)\n\nu\n∂/∂x\n''\nIn class, we re-wrote the wave equation u = u in the form Dˆw = ∂w/∂t, where w =\nand Dˆ =\n.\nv\n∂/∂x\nWe showed that Dˆ ∗ = -Dˆ (anti-Hermitian) for appropriate boundary conditions (let's say u = 0 on the boundaries)\ni\n∗\nfor the obvious inner product (w, w' ) =\nw w' , and hence Dˆ has purely imaginary eigenvalues λ = -iω (oscillating\nΩ\nsolutions), and IwI2 = (w, w) was a conserved \"energy.\"\nConsider a new operator:\n\n-σ\n∂/∂x\nˆDσ =\n∂/∂x\n-σ\nwhere σ(x) > 0.\n(a) Show that IwI2 is not conserved; it is _______ in time. Hint: consider ∂(w, w)/∂t as in class.\n(b) Show that if wn is an eigenfunction, i.e. Dˆσwn = λnwn, then the real part of λ is negative (hence the\neigensolutions are ________ in time). Hint: consider (wn, (Dˆσ + Dˆσ\n∗ )wn).\nProblem 4: Discrete Waves (25 points)\nConsider the operator\n\n-σ\n∂/∂x\nˆDσ =\n∂/∂x\n-σ\n\nu\nn\nfrom the previous problem, acting on w =\n. For σ = 0, we discretized this in class via u\n≈ u(mΔx, nΔt) and\nm\nv\nn+0.5\nv\n(a staggered grid in space and time):\nm+0.5\nn+0.5\nn+0.5\nn+1\nn\nu\n- u\nv\n- v\nm\nm\nm+0.5\nm- 0.5\n=\n,\nΔt\nΔx\nn+0.5\nn- 0.5\nn\nn\nv\n- v\nu\nm+0.5\nm+0.5\nm+1 - um\n=\n.\nΔt\nΔx\nWrite down a center-difference (second-order accurate) scheme for ∂w/∂t = Dˆσ w. For simplicity, take σ to be a\nconstant (independent of x or t), and solve for:\nn+1\nu\n=?\nm\nn+0.5\nv\n=?\nm+0.5\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Exam",
          "title": "Linear Partial Differential Equations, Midterm Exam Solution",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/65841b2e9ddd926b7d6450923326531b_MIT18_303F14_midtermsol.pdf",
          "content": "18.303 Midterm Solutions, Fall 2014\nProblem 1:\n\nDefine the inner product (u, v)c =\nuv =\ncuv = (cu, v) where (u, v) =\n\nAv)c = (cu, (cv)\"\" ) =\nc\nuv . Then (u, ˆ\nΩ\nΩ\nΩ\n((cu)\"\"\nˆ\n, cv) = (cAu, v) = ( ˆ\n, where we have used the self-adjointness of d2/dx2 under (·, ·) from class. Therefore,\nAu, v)c\nAˆ = Aˆ∗ under the (u, v)c inner product (which is a proper inner product for real c > 0).\nProblem 2:\nWe need -'2g = δ(x), and we determine this by evaluating both sides with an arbitrary test function ψ, using the\ndistributional derivative (-'2g){ψ} = g{-'2ψ} as in class. In cylindrical coordinates:\ninf\n2π\n\n1 ∂\n∂ψ\n1 ∂2ψ\ng{-'2ψ} = - lim\nr dr\ndφ c ln r\nr\n+\n1→ 0+\nr ∂r\n∂r\nr2 ∂φ2\n(\n2π\ninf\n\ninf\n2π\n∂\n∂ψ\nc ln r\n(\ndr\n;\n∂ψ\n;(;\n\n= -c\ndφ lim\ndr ln r\nr\n- lim\n(\n1→ 0+\n∂r\n∂r\n1→ 0+\nr\n∂φ\n\n(\n\n2π\ninf\ninf\n(\n\n∂ψ\n∂(ln r)\n∂ψ\n(\n= -c\ndφ lim\nr ln r\n(\n-\ndr\nTr\n1→ 0+\n∂r\n∂r\n∂r\n\n2π\nψ|r=inf\n= c\ndφ lim\n= -2πcψ(0).\nr=1\n1→ 0+\nTo get δψ = ψ(0), therefore, we need c = -1/2π .\nProblem 3:\nIt is convenient to write Dˆσ = Dˆ - σI, where I is the 2 × 2 identity matrix. Then it follows from Dˆ ∗ = -Dˆ and\n\n\" ) =\n∗\n\"\n(σI)∗ = σI (since σ is a real scalar and I is obviously self-adjoint) under the usual inner product (w, w\nw w\nΩ\nthat we have Dˆ ∗ = -Dˆ - σI and Dˆσ + Dˆ ∗ = -2σI.\nσ\nσ\n(a) For a solution w of Dˆσ w = ∂w/∂t , we have\n∂(w, w)/∂t = (∂w/∂t, w) + (w, ∂w/∂t)\n= (Dˆσw, w) + (w, Dˆσw)\n\n= (w, Dˆσ\n∗ w) + (w, Dˆσw) = (w, Dˆσ\n∗ + Dˆσ\nw)\n\n= -2(w, σw) = -2\nσ(x)lw(x)l2 < 0\nand hence lwl2 = (w, w) is decreasing in time.\nIf σ(x) ≥ σ0 > 0 for some σ0, then we can go further and say that E(t) = lwl2 is decaying at least expo\n- 2σ0t\nnentially fast in time, since in that case dE/dt ≤-2σ0E and from this one can show that E(t) ≤ E(0)e\n.\n(i) Given an eigensolution Dˆσwn = λnwn , we can consider\n(wn, (Dˆσ + Dˆσ\n∗ )wn) = -2(wn, σwn)\n= (wn, Dˆσwn) + (Dˆσwn, wn)\n= (wn, λnwn) + (λnwn, wn)\n\n= λn + λn (wn, wn) = (wn, wn)2 Re λn.\nNote that we moved Dˆ ∗ to act on the left via its adjoint. It is not in general true that Dˆσ\n∗ wn = λnwn.\nσ\nThen we have:\n(wn, σwn)\nRe λn = -\n< 0\n(wn, wn)\nsince σ > 0. Hence the eigensolutions are decaying exponentially in time (while they oscillate via the\nλnt\nimaginary part of λn), from their time dependence e\n.\n\nProblem 4:\nWe will have ∂u/∂t = ∂v/∂x - σu and ∂v/∂t = ∂u/∂x - σv, so the only new terms are the σ terms. In the discretized\nn\nn+1\nu +u\nn+0.5\nm\nm\n∂u/∂t equation, the left-hand side is evaluated at point m and time n+0.5, so we have to get u\n=\n+O(Δt2)\nm\nby averaging (similarly to how we handled the Crank-Nicolson discretization in class). Similarly for the ∂v/∂t equation.\nHence, we obtain:\nn+0.5\nn+0.5\nn+1\nn\nn+1\nn\nu\n- u\nv\n- v\nu\n+ u\nm\nm\nm+0.5\nm-0.5\nm\nm\n=\n- σ\n,\nΔt\nΔx\nn+0.5\nn-0.5\nn\nn\nn+0.5\nn-0.5\nv\n- v\nu\nv\nm+0.5\nm+0.5\nm+1 - um\nm+0.5 + vm+0.5\n=\n- σ\n.\nΔt\nΔx\nn+1\nn+0.5\nSolving for u\nand v\n, we obtain the \"leap-frog\" equations:\nm\nm+0.5\n-1\nσΔt\nσΔt\nΔt\nn+1\nn\nn+0.5\nn+0.5\nu\n=\n1 +\n1 -\nu +\nv\n- v\n,\nm\nm\nm+0.5\nm-0.5\nΔx\n-1\nσΔt\nσΔt\nΔt\nn+0.5\nn-0.5\nn\nn\nv\n=\n1 +\n1 -\nv\nu\n.\nm+0.5\nm+0.5 +\nm+1 - um\nΔx\nNote that σ > 0, so we are never dividing by zero in 1 + σΔt/2, regardless of Δt, which is comforting.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Lecture Notes",
          "title": "Linear Partial Differential Equations, Lec 1 Handout",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/2b1299540eb0dd84fe45d44ee9a8f72b_MIT18_303F14_Lec1.pdf",
          "content": "A few important PDEs\nconstant coefficients = 1\nvariable coefficients = c(x)\nPoisson's equation:\n∇2u = f\n∇⋅c∇u\n(\n) = f\nexample: f = charge density,\nc = permittivity ε\nu = -electric potential\nexample: f = heat source/sink rate\nc = thermal conductivity\nu = steady-state temperature\nexample: f = solute source/sink rate,\nc = diffusion coefficient\nu = steady-state concentration\nexample: f ~ force on stretched string/drum\nc ~ \"springy-ness\"\nu = steady-state displacement\nlace's equation:\n∇u = 0\n∇⋅(c∇u) = 0\nexamples: as for Poisson, but no sources\nt/diffusion equation:\n∂u\n= ∇u\n∂u = ∇⋅(c∇u)\n∂t\n∂t\nexamples: u = temperature\nc = thermal conductivity\nu = solute concentration\nc = diffusion coefficient\nar wave equation:\n∂2u\n= ∇\n2 = ∇u\n∂u\n⋅(c∇u)\n∂t\n∂t\nexamples: u = displacement of stretched string/drum\nc2 = 1 / wave speed\nu = density of gas/fluid\nLap\nHea\nScal\n+ many, many others...\nMaxwell (electromagnetism)\nSchrodinger (quantum mechanics)\nNavier-Stokes / Stokes / Euler (fluids)\nBlack-Scholes (options pricing)\nLame-Navier (linear elastic solids)\nbeam equation (bending thin solid strips)\nadvection-diffusion (diffusion in flows)\nreaction-diffusion (diffusion+chemistry)\nminimal-surface equation (soap films)\nnonlinear wave equation (e.g. solitary ocean waves)\n\n(one of several possible examples of)\n\n18.06\nfinite-dimensional linear algebra\nlinear algebra w/ functions & derivatives\nvector space of\nvector space of real-valued (or complex)\ncolumn vectors x (or ) in\nunknowns:\nn\nn\nx\n(or\n),\nfunctions u(x) [for x in some domain Ω],\nor possibly x(t) [time-dependent]\nor possibly u(x,t) [time-dependent],\n...\nvector space:\npossibly restricted by some boundary conditions\nwe can add, subtract, &\nat the boundary ∂Ω [e.g. u(x) = 0 on ∂Ω]\nmultiply by constants\n...\nwithout leaving the space\npossibly with vector-valued u(x) [vector fields]\nlinear operators:\nmatrices A\nlinear operators on functions A,\n[ Au = function ]\nusing partial derivatives. examples:\nlinearity:\nA u =\n2u [ Laplacian operator ]\nA(αx+ y) =\n∇\nβ\nαAx + βAy\nA u\nu\nA(αu+ v) = αAu\nβ\n+ β\n\n= 3 [ mult. by constant ]\nAv\nA3u |x = a(x) u(x) [ mult. by function ]\nA = 4A1 + A2 + 7A3 [ linear comb. of ops. ]\ndot product\nx y\nx*\n⋅ =\ny = Σi xiyi\ncomplex x:\n\n*\nu(x) ⋅ v(x) = 〈u,v〉 = ???????? [inner product]\nand transpose:\n\nx ⋅ Ay = x*Ay = (Ax)*y\nxT\nT = x*\n→x\n\n∂\n\n= ???\n∂\n〈u,Av\n[= some integral]\nx\n〉 =\n*\n〈A u,v〉\n(A)*\nji [conjugate & swap rows/cols]\nA* = ???????? (= A+\n⇔\nij = A\n⇒\nin physics) [adjoint]\nbasis:\nset of vectors bi with span = whole space\ninf set of functions b\n) with span = whole space\n⇔\nb\ni(x\nany x = Σi ci\ni for some coefficients ci\n[ e.g.\n⇔ any\n\nu(x) = Σi ci bi(x) for some coefficients ci\n... if orthonormal basis, then c = b *x\nFourier series! ]\ni\n\ni\n... if orthonormal basis, then ci = 〈bi, u〉\nlinear equations:\nsolve Ax = b for x\nsolve Au = f for u(x)\nexistence\nAx = b solvable if b in column space of A.\nAu = f solvable if f(x) in col. space (image) of A.\n& uniqueness:\nSolution unique if null space of A = {0},\nSolution unique if null space (kernel) of A = {0},\nor equivalently if eigenvalues of A are = 0.\nor equivalently if eigenvalues of A are = 0.\neigenvalues/vectors: solve Ax = λx for x and λ.\nsolve Au = λu for u(x) [eigenfunction] and λ.\nFor this x, A acts just like a number (λ).\nFor this u, A acts just like a number (λ).\n[e.g. Anx = λnx, eAx = eλx.]\n[e.g. Anu = λnu, eAu = eλu.]\n2 example:\n∂\nkx\n∂x 2 sin(\n) = ( k 2\n-)sin(kx)\ntime-evolution\nsolve dx/dt = Ax for x(0)=b [system of ODEs]\nsolve ∂u/∂t = Au for u(x,0)=f(x)\ninitial-value\nA constant ]\nu(x,\nAt\n⇒ x\n\n= eAt b [ if\n⇒\nt) = e f(x) [ if A constant ]\nproblem:\n... expand b in eigenvectors, mult. each by eλt\n... expand f in eigenfunctions, mult. each by eλt\nreal-symmetric\nA = A*\nA = A* [??????]\nor Hermitian:\n⇒ real λ, orthogonal eigenvectors, diagonalizable\n⇒ real\n\nλ, orthogonal eigenvectors (???)\ndiagonalizable (???)\npositive definite\nA = A*, x*Ax > 0 for any x = 0 / x*Ax ≥ 0\nA = A*, 〈u,Au >0 / ≥0 for u = 0 (???)\n〉\n/ semi-definite:\n*\n⇔ real λ>0/≥0, A=B B for some B\n⇔ real λ>0/≥0, A=ˆB *Bˆ for some ˆB (???)\nimportant fact: -\n∇ is symmetric positive definite or semi-definite!\ninverses:\nA-1 A = A A-1 = 1 [if it exists]\n∂\n-\n\n???\n∂x =\n\nA-1 = ??????\n[...delta functions\n= A-1b\n& Green's functions]\n⇒ Ax=b solved by x\n-1\n⇒\n... some kind of integral?\nAu = f solved by f = A u ???\n(real) orthogonal\nA-1 = A* ⇔ (Ax) ⋅ (Ax) = x ⋅ x for any x\nA-1 = A* ⇔ 〈Au,Au〉 = 〈u,u〉 for any u\nor unitary:\n⇒ |λ|=1, orthogonal eigenvectors, diagonalizable\n⇒ | λ|=1, orthogonal eigenvectors (???)\ndiagonalizable (???)\n18.303\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Lecture Notes",
          "title": "Linear Partial Differential Equations, Lec 1 Summary",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/92fe67895ebad3ef83866defaa4c6beb_MIT18_303F14_Lecture1.pdf",
          "content": "Lecture 1\nGeneral overview of what a PDE is and why they are important. Discussed examples of some\ntypical and important PDEs (see handout, page 1). With non-constant coefficients (the most\ncommon case in real-world physics and engineering), even the simplest PDEs are rarely solvable\nby hand; even with constant coefficients, only a relative handful of cases are solvable, usually\nhigh-symmetry cases (spheres, cylinders, etc.) solvable. Therefore, although we will solve a few\nsimple cases by hand in 18.303, the emphasis will instead be on two things: learning to think\nabout PDEs by recognizing how their structure relates to concepts from finite-dimensional linear\nalgebra (matrices), and learning to approximate PDEs by actual matrices in order to solve them\non computers.\nWent through 2nd page of handout, comparing a number of concepts in finite-dimensional linear\nalgebra (ala 18.06) with linear PDEs (18.303). The things in the \"18.06\" column of the handout\nshould already be familiar to you (although you may need to review a bit if it's been a while\nsince you took 18.06)--this is the kind of thing I care about from 18.06 for this course, not how\ngood you are at Gaussian elimination or solving 2×2 eigenproblems by hand. The things in the\n\"18.303\" column are perhaps unfamiliar to you, and some of the relationships may not be clear at\nall: what is the dot product of two functions, or the transpose of a derivative, or the inverse of a\nderivative operator? Unraveling and elucidating these relationships will occupy a large part of\nthis course.\nCovered the concept of nondimensionalization: rescaling the units so that dimensionful\nconstants and other odd numbers disappear, making as many things \"1\" as possible. Gave an\nexample of a heat equation κ∇2T = ∂T/∂t in an L×L box in SI units, where we have a thermal\nconductivity κ in m2/s. By rescaling the spatial coordinates to x/L and y/L, and rescaling the time\ncoordinate to κt/L2, we obtained a simplified equation of the form ∇2T = ∂T/∂t in a 1×1 box. Not\nonly does this simplify the equations, but it can also improve our understanding: by rescaling\nwith characteristic times and distances, we are left with distance and time units where 1 is the\ncharacteristic time and distance, and so in these units it is immediately obvious what we should\nconsider \"big\" and \"small\". For example, in the rescaled time units, 0.01 is a small time in which\nprobably not much happens, while 100 is a big time in which the solution has probably changed\na lot. In the original SI units we would have had to explicitly compare to the characteristic time\nL2/κ.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Lecture Notes",
          "title": "Linear Partial Differential Equations, Lec 2 Handout",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/c3fb398ffd7cce7234931f2b0a93f3d9_MIT18_303F14_sines.pdf",
          "content": "Fourier Sine Series Examples\nMarch 16, 2009\nThe Fourier sine series for a function f(x) defined on x ∈[0,1] writes f(x) as\ninf\nf(x) = ∑bn sin(nπx)\nn=1\nfor some coefficients bn. The key point is that these functions are orthogonal, given the\n\"dot product\" f(x)·g(x) =\nR 1\n0 f(x)g(x)dx. It is a simple calculus exercise to show that\nthe dot product of two sine functions is sin(nπx)·sin(\nR\nmπx) =\n0 sin(nπx)sin(mπx) =\n0 [cos((n-m)πx)-cos((n+m)πx)p]/2, which equals 0 if n =\nR\nm and 1/2 if n = m. [If\nwe divide the sin(nπx) functions by\n1/2, they are orthonormal.] Because of orthog-\nonality, we can compute the bn very simply: for any given m, we integrate both sides\nagainst sin(mπx). In the summation, this gives zero for n = m, and\n0 sin2(mπx) = 1/2\nfor n = m, resulting in the equation\nR\nbm = 2\nZ 1\nf(x) sin(mπx)dx.\nFourier claimed (without proof) in 1822 that any function f(x) can be expanded in\nterms of sines in this way, even discontinuous function! That is, these sine functions\nform an orthogonal basis for \"all\" functions! This turned out to be false for various\nbadly behaved f(x), and controversy over the exact conditions for convergence of the\nFourier series lasted for well over a century, until the question was finally settled by\nCarleson (1966) and Hunt (1968): any function f(x) where\n|f(x)|pdx is finite for\nsome p > 1 has a Fourier series that converges almost everywhere to f(x) [except at\nisolated points]. At points where f(x) has a jump discontinuity\nR\n, the Fourier series\nconverges to the midpoint of the jump. So, as long as one does not care about crazy di-\nvergent functions or the function value exactly at points of discontinuity (which usually\nhas no physical significance), Fourier's remarkable claim is essentially true.\nTo illustrate the convergence of the sine series, let's consider a couple of examples.\nFirst, consider the function f(x) = 1, which seems impossible to expand in sines be-\ncause it is not zero at the endpoints, but nevertheless it works...if you don't care about\nthe value exactly at x = 0 or x = 1. From the formula above, we obtain\nbm = 2\nZ 1\nsin(nπx)dx =\n-nπ cos(nπx)\n\n=\n\nnπ\nn odd\nn even ,\n\nFigure 2: Fourier sine series (blue lines) for the triangle function f(x) = 1\n2 -|x -1\n2|\n(dashed black lines), truncated to a finite number of terms (from 1 to 32), showing that\nthe series indeed converges everywhere to f(x).\nthe region:\n1/2\nbm\n= 2\nf(x) sin(mπx)d = 4\nodd\nZ\nx\nZ\nxsin(mπx)dx =\n(mπ)2 (-1)\nm-1\n2 ,\nwhere for the last step one must do some tedious integration by parts, and thus\nf(x) = π2 sin(πx)-\n(3π)2 sin(3πx)+\nsin(5\n(5π)2\nπx)+···.\nThis is plotted in figure. 2 for 1 to 8 terms--it converges faster than for f(x) = 1\nbecause there are no discontinuities in the function to match, only discontinuities in\nthe derivative.\n\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n1 term\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n2 terms\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n4 terms\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n8 terms\nFigure 2: Fourier sine series (blue lines) for the triangle function f(x) = 1\n2 -|x -1\n2|\n(dashed black lines), truncated to a finite number of terms (from 1 to 32), showing that\nthe series indeed converges everywhere to f(x).\nthe region:\n1/2\nbm\n= 2\nf(x) sin(mπx)d = 4\nodd\nZ\nx\nZ\nxsin(mπx)dx =\n(mπ)2 (-1)\nm-1\n2 ,\nwhere for the last step one must do some tedious integration by parts, and thus\nf(x) = π2 sin(πx)-\n(3π)2 sin(3πx)+\nsin(5\n(5π)2\nπx)+···.\nThis is plotted in figure. 2 for 1 to 8 terms--it converges faster than for f(x) = 1\nbecause there are no discontinuities in the function to match, only discontinuities in\nthe derivative.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Lecture Notes",
          "title": "Linear Partial Differential Equations, Lec 2 Summary",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/d2d752d0d3370cc8fee328fcf07afd37_MIT18_303F14_Lecture2.pdf",
          "content": "Lecture 2\nStarted with a very simple vector space V of functions: functions u(x) on [0,L] with u(0)=u(L)=0\n(Dirichlet boundary conditions), and with one of the simplest operators: the 1d Laplacian\nA=d2/dx2. Explained how this describes some simple problems like a stretched string, 1d\nelectrostatic problems, and heat flow between two reservoirs.\nInspired by 18.06, we begin by asking what the null space of A is, and we quickly see that it is\n{0}. Thus, any solution to Au=f must be unique. We then ask what the eigenfunctions are, and\nquickly see that they are sin(nπx/L) with eigenvalues -(nπ/L)2. If we can expand functions in this\nbasis, then we can treat A as a number, just like in 18.06, and solve lots of problems easily. Such\nan expansion is precisely a Fourier sine series (see handout).\nIn terms of sine series for f(x), solve Au=f (Poisson's equation) and Au=∂u/∂t with u(x,0)=f(x)\n(heat equation). In the latter case, we immediately see that the solutions are decaying, and that\nthe high-frequency terms decay faster...eventually, no matter how complicated the initial\ncondition, it will eventually be dominated by the smallest-n nonzero term in the series (usually\nn=1). Physically, diffusion processes like this smooth out oscillations, and nonuniformities\neventually decay away. Sketched what the solution looks like in a typical case.\nAs a preview of things to come later, by a simple change to the time-dependence found a\nsolution to the wave equation Au=∂2u/∂t2 from the same sine series, which gives \"wavelike\"\nbehavior. (This is an instance of what we will later call a \"separation of variables\" technique.)\nFurther reading: Section 4.1 of the Strang book (Fourier series and solutions to the heat equation).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Lecture Notes",
          "title": "Linear Partial Differential Equations, Lec 3 Handout",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/edea18830013f8c73172ae6bc0eeaafb_MIT18_303F14_difference.pdf",
          "content": "18.303 notes on finite differences\nS. G. Johnson\nSeptember 10, 2013\nThe most basic way to approximate a derivative on\na computer is by a difference. In fact, you probably\nlearned the definition of a derivative as being the\nlimit of a difference:\nu(x + ∆x)\n)\nu′(x = lim\n-u(x\n)\n∆x→0\n.\n∆x\nTo get an approximation, all we have to do is to\nremove the limit, instead using a small but non-\ninfinitesimal ∆x.\nIn fact, there are at least three\nobvious variations (these are not the only possibili-\nties) of such a difference formula:\nu(x + ∆x)\nu′(x)\n-u(x)\n≈\n∆x\nforward difference\n≈u(x) -u(x -∆x)\nbackward difference\n∆x\nu(x + ∆x) -u(x -∆x)\n≈\ncenter difference,\n2∆x\nwith all three of course being equivalent in the ∆x →\n0 limit (assuming a continuous derivative). Viewed\nas a numerical method, the key questions are:\n- How big is the error from a nonzero ∆x?\n- How fast does the error vanish as ∆x →0?\n- How do the answers depend on the difference ap-\nproximation, and how can we analyze and design\nthese approximations?\nLet's try these for a simple example: u(x) = sin(x),\ntaking the derivative at x = 1 for a variety of ∆x val-\nues using each of the three difference formulas above.\nThe exact derivative, of course, is u′(1) = cos(1), so\nwe will compute the error |approximation -cos(1)|\n10-8\n10-7\n10-6\n10-5\n10-4\n10-3\n10-2\n10-1\n∆x\n10-18\n10-16\n10-14\n10-12\n10-10\n10-8\n10-6\n10-4\n10-2\nforward\nbackward\ncenter\n2sin(1)∆x\n6cos(1)∆x2\n|error| in derivative\nFigure 1: Error in forward- (blue circles), backward-\n(red stars), and center-difference (green squares) ap-\nproximations for the derivative u′(1) of u(x) = sin(x).\nAlso plotted are the predicted errors (dashed and\nsolid black lines) from a Taylor-series analysis. Note\nthat, for small ∆x, the center-difference accuracy\nceases to decline because rounding errors dominate\n(15-16 significant digits for standard double preci-\nsion).\n\nversus ∆x. This can be done in Julia with the fol-\nlowing commands (which include analytical error es-\ntimates described below):\nx = 1\ndx = logspace(-8,-1,50)\nf = (sin(x+dx) - sin(x)) ./ dx\nb = (sin(x) - sin(x-dx)) ./ dx\nc = (sin(x+dx) - sin(x-dx)) ./ (2*dx)\nusing PyPlot\nloglog(dx, abs(cos(x) - f), \"o\",\nmarkerfacecolor=\"none\",\nmarkeredgecolor=\"b\")\nloglog(dx, abs(cos(x) - b), \"r*\")\nloglog(dx, abs(cos(x) - c), \"gs\")\nloglog(dx, sin(x) * dx/2, \"k--\")\nloglog(dx, cos(x) * dx.^2/6, \"k-\")\nlegend([\"forward\", \"backward\", \"center\",\nL\"$\\frac{1}{2}\\sin(1) \\Delta x$\",\nL\"$\\frac{1}{6}\\cos(1) \\Delta x^2$\"],\n\"lower right\")\nxlabel(L\"$\\Delta x$\")\nylabel(\"|error| in derivative\")\nThe resulting plot is shown in Figure 1. The obvi-\nous conclusion is that the forward- and backward-\ndifference approximations are about the same, but\nthat center differences are dramatically more accu-\nrate--not only is the absolute value of the error\nsmaller for the center differences, but the rate at\nwhich it goes to zero with ∆x is also qualitatively\nfaster. Since this is a log-log plot, a straight line cor-\nresponds to a power law, and the forward/backward-\ndifference errors shrink proportional to ∼∆x, while\nthe center-difference errors shrink proportional to\n∼∆x2! For very small ∆x, the error appears to go\ncrazy--what you are seeing here is simply the effect\nof roundofferrors, which take over at this point be-\ncause the computer rounds every operation to about\n15-16 decimal digits.\nWe can understand this completely by analyzing\nthe differences via Taylor expansions of u(x). Recall\nthat, for small ∆x, we have\n∆x2\nu(x+∆x) ≈u(x)+∆x u′(x)+ 2 u′′(x)+∆x3\n3! u′′′(x)+· · · .\nu(x-∆x) ≈u(x)-∆x u′(x)+∆x2\n2 u′′(x)-∆x3\nu′′′(x)+\n3!\n· · · .\nIf we plug this into the difference formulas, after some\nalgebra we find:\n∆x\nforward difference ≈u′(x)+ 2 u′′(x)+∆x2\nu′′′(x)+\n3!\n· · · ,\n∆x\nbackward difference ≈u′(x)-2 u′′(x)+∆x2\n3! u′′′(x)+· · · ,\ncenter difference ≈u′(x) + ∆x2\nu′′′(x) +\n3!\n· · · .\nFor the forward and backward differences, the error\nin the difference approximation is dominated by the\nu′′(x) term in the Taylor series, which leads to an\nerror that (for small ∆x) scales linearly with ∆x.\nFor the center-difference formula, however, the u′′(x)\nterm cancelled in u(x + ∆x) -u(x -∆x), leaving us\nwith an error dominated by the u′′′(x) term, which\nscales as ∆x2.\nIn fact, we can even quantitatively predict the er-\nror magnitude: it should be about sin(1)∆x/2 for\nthe forward and backward differences, and about\ncos(1)∆x2/6 for the center differences.\nPrecisely\nthese predictions are shown as dotted and solid lines,\nrespectively, in Figure 1, and match the computed er-\nrors almost exactly, until rounding errors take over.\nOf course, these are not the only possible difference\napproximations. If the center difference is devised so\nas to exactly cancel the u′′(x) term, why not also add\nin additional terms to cancel the u′′′(x) term? Pre-\ncisely this strategy can be pursued to obtain higher-\norder difference approximations, at the cost of mak-\ning the differences more expensive to compute [more\nu(x) terms]. Besides computational expense, there\nare several other considerations that can limit one in\npractice. Most notably, practical PDE problems of-\nten contain discontinuities (e.g. think of heat flow or\nwaves with two or more materials), and in the face\nof these discontinuities the Taylor-series approxima-\ntion is no longer correct, breaking the prediction of\nhigh-order accuracy in finite differences.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Lecture Notes",
          "title": "Linear Partial Differential Equations, Lec 3 Handout",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/86787f1c30f1c5a013c7e75531ba00a6_MIT18_303F14_Julia_cheat.pdf",
          "content": "Julia & IJulia Cheat-sheet (for 18.xxx at MIT)\n\nBasics:\nArithmetic and functions of numbers:\njulialang.org\ndocumentation\n3*4, 7+4, 2-6, 8/3 mult., add, sub., divide numbers\ngithub.com/stevengj/julia-mit\ninstallation & tutorial\ncompute 37\n3^7, 3^(8+2im)\nor 38+2i power\nipython notebook --profile-julia start IJulia browser\n\n-5\nshift-return\n\nexecute input cell in IJulia\nsqrt(-5+0im)\nas a complex number\n\nexp(12)\ne\nnatural log (ln), base-10 log (log )\nDefining/changing variables:\nlog(3), log10(100)\nabs(-5), abs(2+3im) absolute value |-5| or |2+3i|\nx = 3 define variable x to be 3\nsin(5pi/3)\ncompute sin(5π/3)\nx = [1,2,3] array/\"column\"-vector (1,2,3)\nbesselj(2,6) compute Bessel function J2(6)\ny = [1 2 3] 1×3 row-vector (1,2,3)\n\nA = [1 2 3 4; 5 6 7 8; 9 10 11 12]\n--set A to 3×4 matrix with rows 1,2,3,4 etc.\nArithmetic and functions of vectors and matrices:\nx * 3, x + 3\nx[2] = 7\nmultiply/add every element of\n\nchange from (1,2,3) to (1,7,3)\nx by 3\nx\nx + y\nA[2,1] = 0\n\nelement-wise addition of two vectors\n\nchange\nfrom 5 to 0\nx and y\nA2,1\nset u=15.03, v=1.2×10-27\nA*y, A*B\nu, v = (15.03, 1.2e-27)\n\nproduct of matrix A and vector y or matrix B\nx * y\nf(x) = 3x\n\nnot defined for two vectors!\n\ndefine a function f(x)\nx .* y\nx -> 3x\n\nelement-wise product of vectors and\n\nan \"anonymous\" function\nx\ny\n\nevery element of is cubed\n\nx .^ 3\nx\nConstructing a few simple matrices:\ncos(x), cos(A)\ncosine of every element of x or A\nexp(A), expm(A)\nexp of each element of A, matrix exp eA\nrand(12), rand(12,4)\nrandom length-12 vector or 12×4 matrix\n\nconjugate-transpose of vector or matrix\nwith uniform random numbers in [0,1)\nxʹ′, Aʹ′\nthree ways to compute x y\nrandn(12)\nGaussian random numbers (mean 0, std. dev. 1)\nx'*y, dot(x,y), sum(conj(x).*y)\n·\nA \\ b, inv(A)\nreturn solution to Ax=b, or the matrix A-1\neye(5)\n5×5 identity matrix I\n\neigenvals λ and eigenvectors (columns of V) of A\nlinspace(1.2,4.7,100)\n100 equally spaced points from 1.2 to 4.7\nλ, V = eig(A)\n\ndiagm(x)\nmatrix whose diagonal is the entries of x\n\nPlotting (type using PyPlot first)\nPortions of matrices and vectors:\nplot(y), plot(x,y) plot y vs. 0,1,2,3,... or versus x\nnd\nth\nloglog(x,y), semilogx(x,y), semilogy(x,y)\nlog-scale plots\nx[2:12]\nthe 2 to 12 elements of x\n\nset labels\nx[2:end]\nthe 2nd to the last elements of x\ntitle(\"A title\"), xlabel(\"x-axis\"), ylabel(\"foo\")\nlegend([\"curve 1\", \"curve 2\"], \"northwest\")\nlegend at upper-left\n\nrow vector of 1st 3 elements in 5th\n\nA[5,1:3]\nrow of A\n\nrow vector of 5th row of A\ngrid(), axis(\"equal\")\nadd grid lines, use equal x and y scaling\nA[5,:]\ntitle with LaTeX equation\ndiag(A)\nvector of diagonals of A\ntitle(L\"the curve $e^\\sqrt{x}$\")\n\nsavefig(\"fig.png\"), savefig(\"fig.eps\") save as PNG or EPS image\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Lecture Notes",
          "title": "Linear Partial Differential Equations, Lec 3 Summary",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/cb356fd55e96f5d879271582df709f55_MIT18_303F14_Lecture3.pdf",
          "content": "Lecture 3\nNow, we will go back to the happy land of finite-ness for a while, by learning to approximate a\nPDE by a matrix. This will not only give us a way to compute things we cannot solve by hand,\nbut it will also give us a different perspective on certain properties of the solutions that may\nmake certain abstract concepts of the PDE clearer. We begin with one of the simplest numerical\nmethods: we replace the continuous space by a grid, the function by the values on a grid, and\nderivatives by differences on the grid. This is called a finite-difference method.\nWent over the basic concepts and accuracy of approximating derivatives by differences; see\nhandout.\nArmed with center differences (see handout), went about approximating the 1d Laplacian\noperator d2/dx2 by a matrix, resulting in a famous tridiagonal matrix known as a discrete\nLaplacian. The properties of this matrix will mirror many properties of the underlying PDE, but\nin a more familiar context. We already see by inspection that it is real-symmetric, and hence we\nmust have real eigenvalues, diagonalizability, and orthogonal eigenvectors--much as we\nobserved for the d2/dx2 operator--and in the next lecture we will show that the eigenvalues are\nnegative, i.e. that the matrix is negative-definite.\nThe negative eigenvalues mean that the discrete Laplacian is negative definite, and also suggest\nthat it can be written in the form -DTD for some D. Reviewed the proof that this means the\nmatrix is negative definite, which also relies on D being full column rank.\nFurther reading: notes on finite-difference approximations from 18.330. See the matrix K\nsection 1.1 (\"Four special matrices\") of the Strang book, and in general chapter 1 of that book.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Lecture Notes",
          "title": "Linear Partial Differential Equations, Lec 4 Summary",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/8c06d9a3e2a80741dfb13c87ffaf6efd_MIT18_303F14_Lecture4.pdf",
          "content": "Lecture 4\nShowed that our A indeed has this -DTD form and hence is negative-definite: we derived the\ndiscrete Laplacian by turning two derivatives into differences, one by one, and now by writing\nthe first step as a matrix we get D, while writing the second step as a matrix shows that it is -DT.\nTo get a negative definite matrix (as opposed to just negative semidefinite), we additionally\nrequire that D be full column rank; showed that this is easy to see from DT since it is upper-\ntriangular.\nTo do a similar analysis of the actual Laplacian, we first have to have a dot product, or inner\nproduct. Defined an abstract ⟨u,v⟩ notation (a map from functions u and v to scalars ⟨u,v⟩) for\ninner products, as well as three key properties. First, ⟨u,v⟩ = complex conjugate of ⟨v,u⟩. Second,\n|u|2=⟨u,u⟩ must be nonnegative, and zero only if u=0. Third, it must be linear:\n⟨u,αv+βw⟩=α⟨u,v⟩+β⟨u,w⟩. (Note: some textbooks, especially in functional analysis, put the\nconjugation on the second argument instead of the first.) For functions, the most common inner\nproduct (though not the only choice and not always the best choice, as we will see next time) is a\nsimple integral ∫uv (conjugating u for complex functions); we will look at this more next time.\nReviewed inner products of functions. A vector space with an inner product (plus a technical\ncriterion called \"completeness\" that is almost always satisfied in practice) is called a Hilbert space. Note that\nwe include only functions with finite norm ⟨u,u⟩ in the Hilbert space (i.e. we consider only\nsquare-integrable functions), which throws out a lot of divergent functions and means that\neverything has a convergent Fourier series. (Another omitted technicality: we have to ignore finite\ndiscrepancies at isolated points, or otherwise you can have ⟨u,u⟩=0 for u(x) nonzero; there is a rigorous way to do\nthis, which we will come back to later.)\nDefined the adjoint A* of a linear operator: whatever we have to do to move it from one side of\nthe inner product to the other, i.e. whatever A* satisfies ⟨u,Av⟩=⟨A*u,v⟩ for all u,v. (Omitted\ntechicality: we must further restrict ourselves to functions that are sufficiently differentiable that ⟨u,Au⟩ is finite,\nwhich is called a Sobolev space for this A, a subset of the Hilbert space.) For matrices and ordinary vector\ndot products, this is equivalent to the \"swap rows and columns\" definition. For differential\noperators, it corresponds to integration by parts, and depends on the boundary conditions as well\nas on the operator and on the inner product.\nShowed that with u(0)=u(L)=0 boundary conditions and this inner product, (d2/dx2)T is real-\nsymmetric (also called \"Hermitian\" or \"self-adjoint\"). [There is an omitted technicality here: technically,\nwe have only showed that the operator is symmetric. To show that it is Hermitian, we must also show that the\nadjoint has the same domain in the Hilbert space. Mostly we can avoid this technical distinction in real applications;\nit doesn't arise explicitly in the proofs here.]\nNot only that, but next time we will show that d2/dx2 is negative-definite on this space, since\n⟨u,u''⟩=-∫|u'|2, and u'=0 only if u=constant=0 with these boundary conditions.\nShowed that the proof of real eigenvalues from 18.06 carries over without modification for\nHermitian operators; similarly for the proof of orthogonal eigenvectors, hence the orthogonality\nof the Fourier sine series. Similarly for the proof of negative eigenvalues.\n\nSo, many of the key properties of d2/dx2 follow \"by inspection\" once you learn how to transpose\noperators (integrate by parts). And this immediately tells us key properties of the solutions, if we\nassume the spectral theorem: Poisson's equation has a unique solution, the diffusion equation has\ndecaying solutions, and the wave equation has oscillating solutions.\nFurther reading: Notes on function spaces, Hermitian operators, and Fourier series that I once\nwrote for 18.06 (slightly different notation). Textbook, section 3.1: transpose of a derivative. The\ngeneral topic of linear algebra for functions leads to a subject called functional analysis; a\nrigorous introduction to functional analysis can be found in, for example, the book Basic Classes\nof Linear Operators by Gohberg et al. There are some technicalities that I omit: a differential\noperator is only called \"self-adjoint\" if it is equal to its adjoint and is \"densely defined\", and\nshowing that an operator equals its adjoint furthermore requires an extra step of showing that A\nand A* act on the same domains.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Lecture Notes",
          "title": "Linear Partial Differential Equations, Lec 5 Summary",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/2300d93d8e2ec4e32dd579ba45d59bc5_MIT18_303F14_Lecture5.pdf",
          "content": "Lecture 5\nFinished negative-definiteness proof from previous lecture.\nDiscussed diagonalizability of infinite-dimensional Hermitian operators. Unlike the proof of real\neigenvalues, etcetera, we cannot simply repeat the proof from the matrix case (where one can\nproceed by induction on the dimension). In practice, however, real-symmetric operators arising\nfrom physical systems are almost always diagonalizable; the precise conditions for this lead to\nthe \"spectral theorem\" of functional analysis.) (One hand-wavy argument: all physical PDEs can\napparently be simulated by a sufficiently powerful computer to any desired accuracy, in\nprinciple. Since the discrete approximation is diagonalizable, and converges to the continuous\nsolution, it would be surprising if the eigenfunctions of the continous problem \"missed\" some\nsolution. In fact, all the counter-examples of self-adjoint operators that lack a spectral theorem\nseem to involve unphysical solutions that oscillate infinitely fast as they approach some point,\nand hence cannot be captured by any discrete approximation no matter how fine.) In 18.303, we\nwill typically just assume that that all functions of interest lie in the span of the eigenfunctions,\nand focus on the consequences of this assumption.\nShowed how this immediately tells us key properties of the solutions, if we assume the spectral\ntheorem: Poisson's equation has a unique solution, the diffusion equation has decaying solutions\n(with larger eigenvalues = faster oscillations = decaying faster, making the solution smoother\nover time), and the wave equation has oscillating solutions.\nNot only do we now understand d2/dx2 at a much deeper level, but you can obtain the same\ninsights for many operators that cannot be solved analytically. For example, showed that the\noperator d/dx [c(x) d/dx], which is the 1d Laplacian operator for a non-uniform \"medium\", is\nalso real-symmetric positive definite if c(x)>0, given the same u(0)=u(L)=0 boundary conditions.\nAs another example, considered the operator c(x)d2/dx2 for real c(x)>0. This is not self-adjoint\nunder the usual inner product, but is self-adjoint if we use the modified inner product ⟨u,v⟩=∫uv/c\nwith a \"weight\" 1/c(x). (This modified inner product satisfies all of our required inner-product\nproperties for positive c(x).) Therefore, c(x)d2/dx2 indeed has real, negative eigenvalues, and has\neigenfunctions that are orthogonal under this new inner product. Later on, we will see more\nexamples of how sometimes you have to change the inner product in order to understand the self-\nadjointness of A.\nFortunately, it's usually pretty obvious how to change the inner product, typically some simple\nweighting factor that falls out of the definition of A. (In fact, for matrices, it turns out that every\ndiagonalizable matrix with real eigenvalues is Hermitian under some modified inner product. I\ndidn't prove this, however.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Lecture Notes",
          "title": "Linear Partial Differential Equations, Lec 6 Handout",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/fb4bc411e5283ba8d642338a2ce2017b_MIT18_303F14_Lec6.pdf",
          "content": "WI.\n~ v;e.vle!\nf~ e..\nhcft-,) ~e-I et\n<..s\n\"'-\"'\"V\"rd\nC./JhAv..\",M\ne 1.-\"\\\"h',,'V\n~ Y1OI/\ni/e vI,'11 J. ~\n~+W-f-\npi {+~ t4(\nJ/)(re,e\nt?('v~ iL\"1\n-I\nl,.\\fI\nU!1+\\\ntJf-'?\n----7\nc~\"\"j:! ,\"\", (UI;,+L\n1\"0-1\nL\n1\"0.1\n~\n~\n(rtoohi)\nl;\\VI\n,\nk lLA..,+-\\~lA.\",)\nk CIA\" -\nlA,,_\\~\nL.A\"\n~\n\"F.\n\"\n~\nntl\n\"\n'-\nF\" _1-\nk tnt\\-\")... lII\"\n\"\n+- 01,,_ \\\n)\n~ \\\n( I\" %\n); ~ '\" ~ '\"i rLo\"l r f~~ CJ>(1. !\n/V\ntL)(.l.\nMore Srsk \",,<,A' (c, / If\n(l}\n\"\nI\"\nse)\n~~~L\n(5\n~~\"'<1.\nk\n( ~ \"~~( \"'~\nVI.., f;,. )\n(I i) St-f\n'let\nt;:r((\n0-0\"'1\nJ'J-kre., w\n/vi\n'F\"tJ )\n..\n\nF1\n~\n.\n0')\nI\"\n-I\nFlJtl\nL\n~\n.I\nF\n~~~\",pu~vrl.J )\n...:>\nF\n-I\n,\n-I 1\n-I\n- -\n-I\n\\\n-I\n,\n,\n\"\n-4\n~lI\\\n. ,\n:: \"A ~\n~\nriU;I -S; r \"'I1Yl e.J.n c\nfi'<G c.JNe\nJe..hl.-, Ik\n::: ~ .A;<1. (-~ ,~~ \"I -', _\n. I\n-'L 1\n1 -'L\n\"'Xl...\n~\n:;\",Me.. d; s(~k\nl.\"pL;c. ;\"\"\n\n-\nA\n,,\nOr+~I\"drMJ :\n->Jl:--'>\nLI\" lA '\"\n::\n)\" 0 nj.tV\n0'\\\"-'\"\neXp <>1.11 J ~(+)\nI'\"\nt~[j 4c.s;.:,:\nN\n~ ( ~) -\n~ ~ L+')\n~V1\n~\nSO~e\nc.oeJ~~c ieFJ+S\nC'1 Lf-')::\n(;~If\n~ (+')\nby\n.r+~o'lo(W!<;li-!y\n. ,\n..:;\nAlA\nVI\n::.\n..\nvJ~e. ~\n>0 ......<:. coeMc,'(r.J\n.1\nr;;j1(\n\"'-\" ) /\"'/\\\ndQ.-l-er,vl;I/€~\n0y\n(I) ih'\" \\ Co(lC )·Ab\".J\n.--\n~(O):: 2: W~f'l ~/\\\n-\n3 (3,,=- -v..,.:\"(;..{O)\no\n\n=.?;l\n&/) e\nI'lO <k)\n01 Co\nW\"\n~ J+'-I:::\n\"\"\n------7\n-7\n(\",,\"ov,'''..'>\ntvje.r-~r')\n----::;>\nE-\nA:::. k (-<- \\ \\\n/VI\n'-l.. J\nw = IK\n\\\n'I! ~\n(\"\"\"\"'~\n0pf>J::'; k ')\n.( vJ,-:: J:?~\n~::J(~)\n-:;:>\n~\n--=:.>\nWI\n~ (). 7b5\" IT\nI\nl..\nI\n.(\n/'\n-\\\n~ ~ I (I)\n-\nE\n1\",\\, I)(.\n' . '-/ I,-{ J ~\nIA 1.. -\n-It.\n...o(\n~\nc;:\n<\n'-'\n-\"')\nC\niN\n~\n~ :: ~ C~)\nI. 'lLf 9 ~\n:>\nt-\\\n\n-\nII Ie.f\nfY\\ =- ,? .6 X\n'I\ndW;';'y\n(w IVi..5r~ )\n,\nI\n-\nbhort-c r\\ ,'(1.5\n~pri ~.r\n1(\\ Cru..sc.J\nk\n~\n(~\ni::. \\\nl<:. '--\n::.\n(-1<,' *,S\n- -\n....:>\n.-:>.\n-\"\nc..\n- -\nk L:.X' ))-0 \\A\n-\n-\n1T 1)\n\\1\\\n::::=3>\nV\\\nM\nP\n(- t;'D\n~ )7.)\n')\n~ )\\,-1\",+1\n+- c\no2..\\A,(x)+)\n--\n~J'.'\nAX\"'> 0\nof\nr>\n() )('\n,\nsc\",kr\nWe-lie\ne...r..'\"c... fl' 0'\"\ns:><J Vld.s:\nq\nLA\n,\nlA(O)t)\nI\n::lAk+)=-O\nf'.A\n;;s;:/ I\nI\n~ r\n-~/Vll-k\n..J.\nse-lf-c;J)o\\~f\n~r lASV' \" I\n<\ne.t A./)\nSv: vi\n:0\nl\n~\n~\",I, 1\\< 0\n~ o>(I'I/c, il\\;lj s\"ls\n\",~tl\ne--J,,:= ~\n\n'\" ) k\nIi\n....::,\n~ (t')\nF\n~\n--\nK b VI\nC:X\n1\\\n!:::tIl.. blL\nCtJ+-1 '»< N+I )\nditA.J 0\" t:, I fVt ... .J!\\X;\nKIJ,J.J\n'-\n~+\nk's\n-.\n-I\n-\"\nlL ~ ')\n=: - M\nh,/,L. b' kD\n..>\n-\n4~\nVI.\n\\;\\\n\n~\n-::.\nN;< rJ d j'lJ~n<-.1\nCv~ --- I J\nof\nIS\n/\"'/J\n\"''' J.r-; )(\n~\"\"\nA - - £>x\nL tJ\\\n-I b\nT\n1< b\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.303 Linear Partial Differential Equations: Analysis and Numerics\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        }
      ],
      "source_url": "https://ocw.mit.edu/courses/18-303-linear-partial-differential-equations-analysis-and-numerics-fall-2014/",
      "course_info": "18.303 | Undergraduate",
      "subject": "General"
    },
    {
      "course_name": "Single Variable Calculus",
      "course_description": "No description found.",
      "topics": [
        "Mathematics",
        "Calculus",
        "Differential Equations",
        "Mathematics",
        "Calculus",
        "Differential Equations"
      ],
      "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nRecitation: 2 sessions / week, 1 hour / session\n\nPrerequisites\n\nThere is no course at MIT which is a prerequisite for this course. The prerequisites are high school algebra and trigonometry. Students may also receive credit for 18.01 by transferring credit from a comparable college course taken elsewhere, or by passing an advanced standing exam.\n\nCourse Goals\n\nThe basic objective of Calculus is to relate small-scale (differential) quantities to large-scale (integrated) quantities. This is accomplished by means of the Fundamental Theorem of Calculus. Students should demonstrate an understanding of the integral as a cumulative sum, of the derivative as a rate of change, and of the inverse relationship between integration and differentiation.\n\nStudents completing 18.01 can:\n\nUse both the definition of derivative as a limit and the rules of differentiation to differentiate functions.\n\nSketch the graph of a function using asymptotes, critical points, and the derivative test for increasing/decreasing and concavity properties.\n\nSet up max/min problems and use differentiation to solve them.\n\nSet up related rates problems and use differentiation to solve them.\n\nEvaluate integrals by using the Fundamental Theorem of Calculus.\n\nApply integration to compute areas and volumes by slicing, volumes of revolution, arclength, and surface areas of revolution.\n\nEvaluate integrals using techniques of integration, such as substitution, inverse substitution, partial fractions and integration by parts.\n\nSet up and solve first order differential equations using separation of variables.\n\nUse L'Hopital's rule.\n\nDetermine convergence/divergence of improper integrals, and evaluate convergent improper integrals.\n\nEstimate and compare series and integrals to determine convergence.\n\nFind the Taylor series expansion of a function near a point, with emphasis on the first two or three terms.\n\nTextbook\n\nSimmons, George F.\nCalculus with Analytic Geometry\n. 2nd ed. New York, NY: McGraw-Hill, October 1, 1996. ISBN: 9780070576421.\n\nCourse Reader\n\n18.01/18.01A\nSupplementary Notes, Exercises and Solutions\n; Jerison, D., and A. Mattuck.\nCalculus 1\n.\n\nHomework\n\nThere will be 8 problem sets, due on Fridays with one exception; returned in recitation. You may turn in one problem set late with no penalty, provided you do so before solutions are given out. Partial credit may be awarded for subsequent late homework, but you must talk with your recitation instructor.\n\nExams\n\nThere will be four in-class 50 minute exams, and one 3 hour final exam.\n\nMake-up Exams\n\nIf you miss or fail an exam, you may take a make-up exam at certain arranged times. You will be notified by e-mail soon after taking an exam if you have failed it, so that you can plan for the make-up. Make-ups for failed exams can boost your grade only up to the lowest passing score (C-), which will be announced. Make-ups for full credit are permitted with a medical excuse. If you must be absent for other reasons, such as team sports, you must arrange to be excused in advance.\n\nGrading\n\nACTIVITIES\n\nWEIGHTS\n\nProblem sets\n\nExams\n\nFinal\n\nTotal",
      "files": [
        {
          "category": "Assignment",
          "title": "Problem Set 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/76c14c031a187866e85422fc6f0ef464_ps1.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\n18.01 Problem Set 1\nDue Friday 9/15/06, 1:55 pm\n18.01/18.01A Supplementary Notes, Exercises and Solutions are for sale .This is where to\nfindthe exercises labeled 1A, 1B, etc. You will need these for the first day's homework.\nPart I consists of exercises given in the Notes and solved in section S of the Notes. It will be\ngraded quickly, checking that all is there and the solutions not copied.\nPart II consists of problems for which solutions are not given; it is worth more points. Some of\nthese problems are longer multi-part exercises posed here because they do not fit conveniently into\nan exam or short-answer format. See the guidelines below for what collaboration is acceptable, and\nfollow them.\nTo encourage you to keep up with the lectures, both Part I and Part II tell you for each problem\non which day you will have the needed background for it.\nPart I\n(30 points)\nNotation: 2.1 = Section 2.1 of the Simmons book;\nNotes G = section G of the Notes;\n1A-3 = Exercise 1A-3 in Section E (Exercises) of the Notes (solved in section S)\n2.4/13; 81/4 = in Simmons, respectively, section 2.4 Problem 13; page 81 Problem 4\nRecitation 0. Wed. Sept. 6\nGraphing functions.\nRead: Notes G, sections 1-4\nWork: 1A-1a, 2a, 3abe, 6a, 7a\nLecture 1. Thurs., Sept. 7\nDerivative; slope, velocity, rate of change.\nRead: 2.1-2.4\nWork: 1C-3abe, 4ab (use 3), 5, 6 (trace axes onto your answer sheet)\nWork: 1B-2, 1C-1a (start from the definition of derivative)\nLecture 2. Fri. Sept. 8\nLimits and continuity; some trigonometric limits\nRead: 2.5 (bottom p.70-73; concentrate on examples, skip the - δ def'n)\nRead: 2.6 to p. 75; learn def'n (1) and proof \"differentiable =\ncontinuous\" at the end.\n⇒\nRead: Notes C\nWork: 1D-1bcefg, 4a; 1C-2, 1D-3ade, 6a, 8a (hint: \"diff =\ncont.\")\n⇒\nLecture 3. Tues. Sept. 12\nDifferentiation formulas: products and quotients;\nDerivatives of trigonometric functions.\nIn the exercises, an antiderivative of f(x) is any F (x) for which F 0(x) = f(x).\nRead: 3.1, 3.2, 3.4\nWork: 1E-1ac, 2b, 3, 4a, 5a; 1J-1e, 2\nLecture 4. Thurs. Sept. 14\nChain rule; higher derivatives.\nRead: 3.3, 3.6\nWork: 1F-1ab, 2, 6, 7bd; 1J- 1abm 1G-1b, 5ab\nLecture 5. Fri. Sept. 15\nImplicit differentiation; inverse functions.\nRead: 3.5, Notes G section 5\nWork: given on Problem Set 2.\n\nPart II\n(40 points)\nDirections and Rules: Collaboration on problem sets is encouraged, but\na) Attempt each part of each problem yourself. Read each portion of the problem before asking\nfor help. If you don't understand what is being asked, ask for help interpreting the problem and\nthen make an honest attempt to solve it.\nb) Write up each problem independently. On both Part I and II exercises you are expected to\nwrite the answer in your own words.\nc) Write on your problem set whom you consulted and the sources you used. If you\nfail to do so, you may be charged with plagiarism and subject to serious penalties.\nd) It is illegal to consult materials from previous semesters.\n0. (not until due date; 3 points)\nWrite the names of all the people you consulted or with whom you collaborated and the resources\nyou used, or say \"none\" or \"no consultation\". This includes visits outside recitation to your\nrecitation instructor. If you don't know a name, you must nevertheless identify the person, as in,\n\"tutor in Room 2-106,\" or \"the student next to me in recitation.\" Optional: note which of these\npeople or resources, if any, were particularly helpful to you.\nThis \"Problem 0\" will be assigned with every problem set. Its purpose is to make sure that you\nacknowledge (to yourself as well as others) what kind of help you require and to encourage you to\npay attention to how you learn best (with a tutor, in a group, alone). It will help us by letting us\nknow what resources you use.\n1. (Wed, 3 pts) Express (x - 1)/(x + 1) as the sum of an even and an odd function. (Simplify\nas much as possible.)\n2. (Thurs, 6 pts: 3 + 3) Sensitivity of measurement: Suppose f is a function of x. If x = x0+Δx,\nthen we define Δf = f(x) - f(x0) and Δf/Δx measures how much changes in x affect the value\nof f.\nThe planet Quirk is flat. GPS satellites hover over Quirk at an altitude of 20, 000 km (unlike\nEarth where the satellites circle twice a day). See how accurately you can estimate the distance L\nfrom the point directly below the satellite to a point on the planet surface knowing the distance h\nfrom the satellite to the point on the surface in two cases. (The letter h is for hypotenuse.)\na) Use a calculator to compute ΔL/Δh for h = h0±Δh = 25, 000±Δh, and Δh = 1, 10-1 , 10-2 .\nWrite an estimate for L in the form\n|L - L0| = |ΔL| ≤ C|Δh|\nchoosing the simplest round number C that works for all three cases.\nb) Do the same for h = 20, 001 ± Δh, Δh = 1, 10-1 , 10-2 . Is the value of L estimated more or\nless accurately than in part (a)? We will revisit this problem more systematically using calculus.\n3. (Thurs, 4pts) On the planet Quirk, a cell phone tower is a 100-foot pole on top of a green\nmound 1000 feet tall whose outline is described by the parabolic equation y = 1000 - x2 . An ant\nclimbs up the mound starting from ground level (y = 0). At what height y does the ant begin to\nsee the tower?\n\n4. (Thurs, 6 pts) 3.1/21 (parabolic mirrors)\n5. (Thurs, 4pts: 2 + 2)\na) A water cooler is leaking so that its volume at time t in minutes is (10 - t)2/5 liters. Find\nthe average rate at which water drains during the first 5 minutes.\nb) At what rate is the water flowing out 5 minutes after the tank begins to drain.\n6. Friday (8 pts: 1 + 1 + 1 + 1 + 1 + 1 + 2) 2.5/19d (put u = 1/x), 19f, 19g, 20c, 20g (show\nwork); 22a (needs a calculator), 22b (see the proof on page 73).\n7. Tuesday (6 pts: 2 + 4)\na) If u, v and w are differentiable functions, find the formula for the derivative of their product,\nD(uvw).\nb) Generalize your work in part (a) by guessing the formula for D(u1u2 · · · un)--the derivative\nof the product of n differentiable functions.\nThen prove your formula by mathematical induction (i.e., prove its truth for the product of\nn + 1 functions, assuming its truth for the product of n functions)."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 2A",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/dc08c2bdfb6ed8ef96e4560d6391f1d6_ps2a.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\n18.01 Problem Set 2A\nDue Friday 9/29/06, 1:55 pm\n2A is the first half of Problem Set 2, all of which is due a week after Exam 1 (the second half, 2B,\nwill be issued at the exam, or the day before). Even though it won't be collected until later, you\nshould do 2A before the exam, to prepare for it.\nPart I\n(15 points)\nLecture 5. Fri. Sept. 15\nImplicit differentiation; inverse functions and their derivatives.\nRead: 3.5, Notes G section 5, 9.5 (bottom p.913 - 915)\nWork: 1F-3,5,8c; 1A-5b; 5A-1a,b,c(just sin, cos, sec); 5A-3f,h\nLecture 6. Tues. Sept, 19\nExponentials and logs: def'n, algebra, applications, derivatives.\nRead: Notes X (8.2 has some of this), 8.3 to middle p. 267; 8.4 to top p. 271\nWork: 1H-1, 2, 3a, 5b; 1I-1c,d,e,f,m; 1I-4a\nLecture 7. Thurs. Sept. 21\nLogarithmic differentiation. Hyperbolic functions (not on exam).\nReview.\nRead: 9.7 to p. 326\nWork: 5A-5abc\nLecture 8.\nFri. Sept. 22\nExam 1 covering 0-7.\nStudents not passing will get e-mail on Friday evening. Make-up exams are offered Monday-\nThursday of the week following at times posted at the web site. (see \"Exams\" on Syllabus sheet).\nPart II\n(30 points)\nDirections: Attempt to solve each part of each problem yourself. If you collaborate, solutions\nmust be written up independently. It is illegal to consult materials from previous semesters. With\neach problem is the day it can be done.\n0. (not until due date; 2 points) Write the names of all the people you consulted or with\nwhom you collaborated and the resources you used, or say \"none\" or \"no consultation\". (See full\nexplanation on PS1).\n1. (now; 4 pts) Graph the even and odd functions you found in Problem 1, Part II of PS1.\nDirectly below, graph their derivatives. Do this qualitatively using your estimation of the slope.\nDo not use the formulas for the derivatives (except to check your work if you want). You can use\na graphing calculator to check your answer, provided that you mention it in Problem 0. (Note,\nhowever, that you may not use books, notes or calculators during tests, so it is unwise to rely on a\ngraphing calculator here.)\n2. (before Fri; 5 pts = 2 + 3) Compute\na) (d/dx) tan3(x4)\n\nb) (d/dy)(sin2 y cos2 y)\n(Do this two ways: first use the product rule, then write it as f(2y). Show that the answers agree.)\n3. (before Fri; 3pts = 1 + 2)\na) If y = uv, show that y00 = u00v + 2u0v0 + uv00\nb) Find y000.\n4. (Fri; 4pts 3 +1)\na) The function cos-1 x is the inverse of the cos θ on 0 ≤ θ ≤ π. Use implicit differentiation to\nderive the formula for (d/dx) cos-1 x. Pay particular attention to the sign of the square root. (See\nthe book or lecture for the case of the inverse of sine.)\nb) Without calculation, explain why (d/dx) cos-1 x + (d/dx) sin-1 x = 0\n5. (Tues + Thurs; 10pts = 2+2+2+2+2) Do 8.2/8ac, 10, 11; 8.4/18,19a.\n6. (Thurs; 2pts) Derive the formula for D(u1u2 · · · un) from PS1, Part II, 7b, using logarithmic\ndifferentiation."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 2B",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/468108e2f758e1d415c6493e485d6478_ps2b.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\n18.01 Problem Set 2B\nDue Friday 9/29/06, 1:55 pm\n2B is the second half of Problem Set 2, all of which is due along with the first half 2A.\nPart I\n(10 points)\nLecture 9. Tues. Sept. 26. Linear and quadratic approximations.\nRead: Notes A\nWork: 2A-2, 3, 7, 11, 12ade\nLecture 10. Thurs. Sept. 28. Curve-sketching.\nRead: 4.1, 4.2\nWork: 2B-1,2: a,e,h; 2B-4, 6ab, 7ab\nLecture 11. Fri. Sept. 29. Maximum-minimum problems.\nRead: 4.3, 4.4\nWork: assigned on PS3\nPart II\n(16 points + 3 extra)\nDirections: Attempt to solve each part of each problem yourself. If you collaborate, solutions\nmust be written up independently. It is illegal to consult materials from previous semesters. With\neach problem is the day it can be done.\n0. (not until due date; 2 points) Write the names of all the people you consulted or with\nwhom you collaborated and the resources you used, or say \"none\" or \"no consultation\". (See full\nexplanation on PS1).\n1. (10 points: 2 + 4 + 4; and 3 extra) Golf balls\nThe area of a section of a sphere of radius R between two parallel planes that are a distance h\napart is 1\narea of a spherical section = 2πhR\nSlice the sphere of radius R by a horizontal plane. The portion of the plane inside the sphere\nis a disk of radius r ≤ R. The portion of the spherical surface above the plane is called a spherical\ncap. For example, if the plane passes through the center, then the disk has radius r = R, its\ncircumference is the equator, and the spherical cap is the Northern Hemisphere. More generally,\na spherical cap is the portion of surface of the Earth north of a latitude line. The formula above\napplies to regions between two latitude lines, and, in particular, to spherical caps.\na) Consider a spherical cap which is the portion of the surface of the sphere above horizontal\nplane that slices the sphere at or above its center. Find the area of the cap as a function of R and\nr. Do this by finding first the formula for the height h of the spherical cap in terms of r and R.\n(This height is the vertical distance from the horizontal slicing plane to the North Pole.) Then use\nyour formula for h and the formula above for the area of spherical sections.\nThis formula will be derived in Unit 4. Two examples may convince you that it is reasonable. For h = R, it gives\nthe area of the hemisphere, 2πR2 . For h = 2R it gives 4πR2, the area of the whole sphere.\n\nb) Express the formula for the area of a spherical cap in terms of R2 and r/R. (This is natural\nbecause the proportional scaling cr and cR changes the area by the factor c2.) Then use the\nlinear and quadratic approximations to (1 + x)1/2 near x = 0 to find a good and an even better\napproximation to the area of the spherical cap, appropriate when the ratio r/R is small. (Hint:\nWhat is x?) Simplify your answers as far as possible: the approximation corresponding to the\nlinear approximation to (1 + x)1/2 should be very familiar.\nc) The following problem appeared on a middle school math contest exam. The numbers have\nbeen changed to protect the innocent. Consider a golf ball that is 3 centimeters in diameter with\n100 hemispherical dimples of diameter 3 millimeters. (Note that this is not a realistic golf ball\nbecause the dimples are too deep). Find the area of the golf ball rounded to the nearest 1/100 of a\nsquare centimeter using the approximation π ≈ 3.14. (The students were given three minutes. We\nare spending more time on it.)\nUnder the rules of the contest, an incorrectly rounded answer was counted as wrong with no\npartial credit, so correct numerical approximation was crucial. Some students objected that they\ncould not figure out the area of portion of the large sphere that is removed when a dimple is\ninserted. A careless examiner had assumed that the students would use the approximation that the\narea removed for each dimple was nearly the same as the area of a flat disk. We are going to figure\nout whether this approximation is adequate or gives the wrong answer according to the rules.\nWrite down formulas for the surface area of the golf ball in the three cases listed below. (Put\nin 100 dimples, but leave r, R, and π as letters.)\ni) the approximation pretending that the removed surface is flat (what is the relationship be\ntween this and the approximations of part (b)?)\nii) the higher order approximation you derived in part (b)\niii) the exact formula\nFinally, evaluate each of the answers for the given values r = .15 and R = 1.5 centimeters, and\nfind the accuracy of the approximations.\nd) (extra credit:2 3pts) Although nobody noticed at the time, the examiner who created this\nproblem made a much bigger mistake. With the diameters actually given, it would have been\nimpossible for the number of dimples given to be placed on the golf ball without overlap. Give a\n(reasonable) estimate for the largest number of dimples that can fit on our golf ball.\n2. (4 points) Draw the graph of f(x) = 1/(1 + x2) and, directly underneath, it the graphs of\nf0(x) and f00(x). Label critical points and inflection points on the graph of f with their coordinates.\nDraw vertical lines joining these special points of the graph of f to the corresponding points on the\ngraphs below.\nExtra credit points are tabulated separately and only added to your final score at the end of the semester so that\nthey do not influence any grading cutoffs."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 3",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/bf58b1abae6a98338bd60431c1fe0f26_ps3.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\n18.01 Problem Set 3\nDue Friday 10/06/06, 1:55 pm\nPart I\n(10 points)\nLecture 11. Fri. Sept. 29. Maximum-minimum problems.\nRead: 4.3, 4.4\nWork: 2C-1, 2, 5, 11, 13.\nLecture 12. Tue. Oct. 3\nRelated rate problems.\nRead: 4.5\nWork: 2E-2, 3, 5, 7\nLecture 13. Thu. Oct. 5\nNewton's method.\nRead: 4.6, (4.7 is optional)\nLecture 14. Fri. Oct. 6\nMean-value theorem. Inequalities.\nRead: 2.6 to middle p. 77, Notes MVT\nWork: assigned on PS4\nPart II\n(31 points + 8 extra credit)\nDirections: Attempt to solve each part of each problem yourself. If you collaborate, solutions\nmust be written up independently. It is illegal to consult materials from previous semesters. With\neach problem is the day it can be done.\n0. (not until due date; 3 pts) Write the names of all the people you consulted or with whom you\ncollaborated and the resources you used, or say \"none\" or \"no consultation\". (See full explanation\non PS1).\n1. (Friday, 6pts: 3 + 3)\na) 4.3/28\n(Use as variable the distance x from the foot of the ladder to the house. Check\nendpoints.)\nb) 4.4/28\n2. (Tuesday, 2pts) Hypocycloid. Show that every tangent line to the curve x 2/3 + y 2/3 = 1\nin first quadrant has the property that portion of the line in the first quadrant has length 1. (Use\nimplicit differentiation; this is the same as problem 45 page 114 of text.)\n3. (Tuesday, 7pts: 3 + 3 + 1) Sensitivity of measurement, revisited.\na) Recall that in problem 2, PS1/Part II, L2 + 20, 0002 = h2 . Use implicit differentiation\nto calculate dL/dh. Compare the linear approximation dL/dh to the error ΔL/Δh computed in\nexamples on PS1. Explain why ΔL/Δh ≤ dL/dh if the derivative is evaluated at the left endpoint\nof the interval of uncertainty (or, in other words, Δh > 0). In what range of values of h is it true\nthat ΔL\nΔh ?\n|\n| ≤ 2|\n|\nb) Suppose that the Planet Quirk is a not only flat, but one-dimensional (a straight line).\nThere are several satellites at height 20, 000 kilometers and you get readings saying that satellite 1\nis directly above the point x1 ± 10-10 and is at a distance h1 = 21, 000 ± 10-2 from you, satellite\n\n2 is directly above x2 ± 10-10 and at a distance h2 = 52, 000 ± 10-2 . Where are you and to what\naccuracy? Hint: Consider separately the cases x1 < x2 and x2 > x1.\nc) Express dL/dh in terms of the angle between the line of sight to the satellite and the horizontal\nfrom the person on the ground. (When expressed using the line-of-sight angle, the formula also\nworks for a curved planet like Earth.)\n4. (Tuesday, 5pts: 3 + 2 + 0) More sensitivity of measurement. Consider a parabolic\nmirror with equation y = -1/4 + x2 and focus at the origin. (See Problem Set 1.) A ray of light\ntraveling down vertically along the line x = a hits the mirror at the point (a, b) where b = -1/4+a2\nand goes to the origin along a ray at angle θ measured from the positive x-axis.\na) Find the formula for tan θ in terms of a and b, and calculate dθ/da using implicit differenti\nation. (Express your answer in terms of a and θ.)\nb) If the telescope records a star at θ = -π/6 and the measurement is accurate to 10-3 radians,\nuse part (a) to give an estimate as to the location of the star in the variable a.\nc) (optional; no credit) Solve for a as a function of θ alone and doublecheck your answers to\nparts (a) and (b).\n5. (Thursday, 8 pts: 3 + 3 + 2) Newton's method.\na) Compute the cube root of 9 to 6 significant figures using Newton's method. Give the general\nformula, and list numerical values, starting with x0 = 2. At what iteration k does the method\nsurpass the accuracy of your calculator or computer? (Display your answers to the accuracy of\nyour calculator or computer.)\nb) For each step xk, k = 0, 1, . . ., say whether the value is i) larger or smaller than 91/3; ii)\nlarger or smaller than the preceding value xk-1. Illustrate on the graph of x3 - 9 why this is so.\nc) Find a quadratic approximation to 91/3, and estimate the difference between the quadratic\napproximation and the exact answer. (Hint: To get a reasonable quadratic approximation, use\n9 = 8(1 + 1/8).)\n6. (extra credit 8 pts: 3 + 2 + 2 + 1) Hypocycloid, again. Here we derive the equation\nfor the hypocycloid of Problem 2 from the sweeping out property directly. This takes quite a bit\nlonger. We will look at the hypocycloid from yet another (easier) point of view later on.\nThink of the first quadrant of the xy-plane as representing the region to the right of a wall with\nthe ground as the positive x-axis and the wall as the positive y-axis. A unit length ladder is placed\nvertically against the wall. The bottom of the ladder is at x = 0 and slides to the right along the\nx-axis until the ladder is horizontal. At the same time, the top of the ladder is dragged down the\ny-axis ending at the origin (0, 0). We are going to describe the region swept out by this motion,\nin other words, the blurry region formed in a photograph of the motion if the eye of the camera is\nopen the whole time.\na) Suppose that L1 is the line segment from (0, y1) to (x1, 0) and L2 is the line segment from\n(0, y2) to (x2, 0). Find the formula for the point of intersection (x3, y3) of the two line segments.\nDon't expect the formula to be simple: It must involve all four parameters x1, x2, y1, and y2. But\nsimplify as much as possible!\nIt's important to make sure you have the right formulas before proceeding further. You can\ndoublecheck your formulas in several ways. (This is optional.)\ni) If y2 = 0, then x3 = x1.\n\nii) When the x's and y's are interchanged the formulas should be the same. What transformation\nof the plane does the exchange of x and y represent?\niii) It is impossible to find x3 and y3 if the lines are parallel, so the denominator in the formula\nmust be zero when L1 and L2 have the same slope.\niv) Rescaling all variables by a factor c leaves the formula unchanged, so the numerator of the\nformula for x3 and y3 should have degree (in all variables) one greater than the denominator.\nb) Write the equation involving x2 and y2 that expresses the property that ladder L2 has length\none. We will suppose that L1 represents the ladder at a fixed position, and L2 tends to L1. Thus\nx2 = x1 + Δx;\ny2 = y1 + Δy\nUse implicit differentiation (related rates) to find\nΔy\nlim\nΔx\n0 Δx\n→\n(Express the limit as a function of the fixed values x1 and y1.)\nc) Substitute x2 = x1 + Δx and y2 = y1 + Δy into the formula in part (a) for x3 and use part\n(b) to compute\nX = lim x3 = lim x3\nx2→x1\nΔx\n→\nSimplify as much as possible. Deduce, by symmetry alone, the formula for\nY = lim y3\nx2→x1\nd) Show that X2/3 +Y 2/3 = 1. (The limit point (X, Y ) that you found in part (c) is expressed as\na function of x1 and y1. This is the unique point of the ladder L1 that is also part of the boundary\ncurve of the region swept out by the family of ladders.)"
        },
        {
          "category": "Assignment",
          "title": "Problem Set 4",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/e13dc193e1f610134a7a76f7fc8a4ee6_ps4.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\n18.01 Problem Set 4\nDue Friday 10/20/06, 1:55 pm\nThis is all of Problem Set 4 (not split into 4A and 4B). Although it is not due until after Exam\n2, you should do all the Part I exercises through Lecture 16 and all the Part II problems through\nProblem 4 before the exam, in order to prepare for it. Practice exam problems and an actual past\nexam will be posted on line as usual.\nPart I\n(20 points)\nLecture 14. Fri. Oct. 6\nMean-value theorem. Inequalities.\nRead: 2.6 to middle p. 77, Notes MVT\nWork: 2G-1b, 2b, 5, 6\n(Columbus Day Holiday. No classes Mon and Tues, Oct 9 and 10)\nLecture 15. Thurs. Oct. 12\nDifferentials and antiderivatives.\nRead: 5.2, 5.3\nWork: 3A-1de, 2acegik, -3aceg\nLecture 16. Fri. Oct. 13\nDifferential equations; separating variables.\nRead: 5.4, 8.5\nWork: 3F-1cd, 2ae, 4bcd, 8b\nLecture 17. Tues. Oct 17\nExam 2 Covers Lectures 8-16.\nLecture 18. Thurs. Oct. 19\nDefinite integral; summation notation.\nRead: 6.3 though formula (4); skip proofs; 6.4, 6.5\nWork: 3B-2ab, 3b, 4a, 5\n4J-1 (set up integral; do not evaluate)\nLecture 19. Fri. Oct. 20\nFirst fundamental theorem. Properties of integrals.\nRead: 6.6, 6.7 to top p. 215 (Skip the proof pp. 207-8, which will be discussed in Lec 20.)\nWork: assigned on PS 5\nPart II\n(36 points + 10 extra credit)\nDirections: Attempt to solve each part of each problem yourself. If you collaborate, solutions\nmust be written up independently. It is illegal to consult materials from previous semesters. With\neach problem is the day it can be done.\n0. (not until due date; 3 pts) Write the names of all the people you consulted or with whom you\ncollaborated and the resources you used, or say \"none\" or \"no consultation\". (See full explanation\non PS1).\n1. (Lec 14, 10pts: 2 + 2 + 2 + 2 + 2))\na) Use the mean value property to show that if f(0) = 0 and f0(x) ≥ 0, then f(x) ≥ 0 for all\nx ≥ 0.\nb) Deduce from part (a) that ln(1 + x) ≤ x for x ≥ 0. Hint: Use f(x) = x - ln(1 + x).\nc) Use the same method as in (b) to show ln(1 + x) ≥ x - x2/2 and ln(1 + x) ≤ x - x2/2 + x3/3\nfor x ≥ 0.\n\nZ\nZ\nd) Find the pattern in (b) and (c) and make a general conjecture.\ne) Show that ln(1 + x) ≤ x for -1 < x ≤ 0. (Use the change of variable u = -x.)\n2. (Lec 15, 4 pts: 2 + 2)\na) Do 5.3/68\nb) Show that both of the following integrals are correct, and explain.\ntan x sec2 xdx = (1/2) tan2 x;\ntan x sec2 xdx = (1/2) sec2 x\n3. (Lec 16, 6 pts: 3 + 3)\na) Do 8.6/5 (answer in back of text)\nb) Do 8.6/6 (optional?)\n4. (Lec 16, 7 pts: 2 + 3 + 2) Do 3F-5abc\nSTOP HERE. DO THE REST AFTER EXAM 2.\nZ 1\n5. (Lec 18, 6 pts) Calculate\ne xdx using lower Riemann sums. (You will need to sum a\ngeometric series to get a usable formula for the Riemann sum. To take the limit of Riemann\nsums, you will need to evaluate lim n(e 1/n - 1), which can be done using the standard linear\nn→inf\napproximation to the exponential function.)\n6. (Lec 16; extra credit: 10 pts: 2 + 2 + 3 + 3) More about the hypocycloid. We use\ndifferential equations to find the curve with the property that the portion of its tangent line in the\nfirst quadrant has fixed length.\na) Suppose that a line through the point (x0, y0) has slope m0 and that the point is in the first\nquadrant. Let L denote the length of the portion of the line in the first quadrant. Calculate L2 in\nterms of x0, y0 and m0. (Do not expand or simplify.)\nb) Suppose that y = f(x) is a graph on 0 ≤ x ≤ L satisfying f(0) = L and f(L) = 0 and such\nthat the portion of each tangent line to the graph in the first quadrant has the same length L. Find\nthe differential equation that f satisfies. Express it in terms of L, x, y and y0 = dy/dx. (Hints:\nThis requires only thought, not computation. Note that y = f(x), y0 = f0(x). Don't take square\nroots, the expression using L2 is much easier to use. Don't expand or simplify; that would make\nthings harder in the next step.)\nc) Differentiate the equation in part (b) with respect to x. Simplify and write in the form\n(something)(xy0 - y)y00 = 0\n(This starts out looking horrendous, but simplifies considerably.)\nd) Show that one solution to the equation in part (c) is x2/3 + y2/3 = L2/3 . What about two\nother possibilities, namely, those solving y00 = 0 and xy0 - y = 0?"
        },
        {
          "category": "Assignment",
          "title": "Problem Set 5",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/11be301cf5dc7040bb15079265463dd6_ps5.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\n18.01 Problem Set 5\nDue Friday 10/27/06, 1:55 pm\nPart I\n(20 points)\nLecture 19. Fri. Oct. 20\nFirst fundamental theorem. Properties of integrals.\nRead: 6.6, 6.7\n(The second fundamental theorem, discussed in Lecture 20, is stated in\nthe text as (13) at the bottom of page 215 and also as Step 1, page 207, of the proof of the first\nfundamental theorem.)\nWork: 3C-1, 2a, 3a, 5a; 3E-6bc; 4J-2\nLecture 20. Tues. Oct. 24\nSecond Fundamental Theorem. Def'n of ln x.\nRead: Notes PI, p.2 [eqn.(7) and example]; Notes FT.\nWork: 3E-1, 3a; 3D-1, 4bc, 5, 8a; 3E-2ac\nLecture 21. Thur. Oct. 26\nAreas between curves. Volumes by slicing.\nRead: 7.1, 7.2, 7.3\nWork1: 4A-1b, 2, 4; 4B-1de, 6, 7\nLecture 22. Fri. Oct. 27\nVolumes by disks and shells.\nRead: 7.4\nWork on PS 6\nPart II\n(30 points + 5 extra credit)\nDirections: Attempt to solve each part of each problem yourself. If you collaborate, solutions\nmust be written up independently. It is illegal to consult materials from previous semesters. With\neach problem is the day it can be done.\n0. (not until due date; 3 pts) Write the names of all the people you consulted or with whom you\ncollaborated and the resources you used, or say \"none\" or \"no consultation\". (See full explanation\non PS1).\n1. (Lec 19, 6 pts: 3 + 3) (+ 5 extra for part (c))\na) Suppose that at the beginning of day 0, some time last summer, the temperature in Boston\nwas y(0) = 65* Fahrenheit and that over a 50-day period, the temperature increased according to\nthe rule y0(t) = y(t)/100, with time t measured in days. Find the formula for y, and draw a graph\nof temperature on days 3 and 4, 3 ≤ t ≤ 5, and label with the correct day and shade in the regions\nwhose areas represent the average temperature each of the two days.2\n1A more colorful way of expressing 4B-6 is in terms of the volume of a tent, as in the textbook problem 7.3/7.\nUnfortunately, the problem is ill-posed and can't be done without pretending that the cross-sections are triangles.\nIn real life, the canvas would have creases. In general, the shapes formed by stretching canvas or nylon over various\narrays of tent poles are quite hard to compute.\n2The continuous average of a function is\nZ b\nf(x)dx\nb - a\na\nIn this case b - a = 1, so the average is the same as integral. For more, see Notes, AV, and Lecture 23.\n\nZ\nZ\nZ\nb) The number of cooling degree days is the sum for each day of the difference between the\naverage temperature for that day and 65*. The number is used to estimate the demand for electricity\nfor air conditioning. Draw a second graph of y for the whole 50 days and shade in the region whose\narea represents the total number of degree days. Write a formula for this total area as the difference\nbetween 65 50 and a definite integral. Evaluate the definite integral using the fundamental theorem\n·\nof calculus. (Alternatively, write the whole quantity as an integral expressing the area between\ncurves as in Lecture 21, and 7.2.)\nc) (extra credit) Compute the definite integral in part (b) directly by evaluating a lower Riemann\nsum and taking a limit. Follow the procedure in Problem 5, PS4, but with different scale factors.\nThis rather elaborate calculation shows how much time and effort we save every time we use the\nfundamental theorem and the change of variable formula in integrals.\nx\n2. (Lec 20, 16 pts: 2 + 2 + 4 + 2 + 6) Consider the function f(x) =\ncos(t2)dt. There is no\nexpression for f(x) in terms of standard elementary functions. It is known as a Fresnel integral,\nalong with the corresponding sine integral.\na) Draw a rough sketch of cos(t2), showing the first positive and negative zeros. What does the\ncurve look like at t = 0? Is the function even or odd?\nb) List the critical points of f(x) in the entire range -inf < x < inf. Which critical points are\nlocal maxima and which ones are local minima?\nc) Sketch the graph of f on the interval -2 ≤ x ≤ 2, with labels for the critical points and\ninflection points. (The drawing should be qualitatively correct, but just estimate the values of f\nat the labelled points.)\nd) Estimate f(0.1) to six decimal places.\ne) Fresnel integrals are sometimes expressed using different scaling of the variables.\nx\ni) Let g(x) =\ncos((π/2)u 2)du. Make a change of variables to show that f(x) = c1g(c2x) for\nsome constants c1 and c2. Why did we choose the factor π/2?\nii) Let h(x) =\nx cos v dv. (This integral is called improper because 1/√v is infinite3 at v = 0.)\n√v\nMake a different change of variable to show that f(x) = ch(x2) for some constant c (assume that\nx > 0).\niii) Let k(x) = √x\nZ 1\ncos(xt2)dt, x > 0. Use the change of variable z = xt2 and part (ii) to find\nthe relationship between the functions k and f. Hint: Which quantities are variable and which are\nconstant? (Not assigned: you can also work out this relationship using a change of variables like\nthe one in part (i).)\n3. (Lec 21, 5 pts: 2 + 3)\na) Do 7.3/22.\nb) Find the volume of the region in 3-space with x > 0, y > 0 and z > 0 given by\nz 2/2 < x + y < z\nHint: First find the area of the horizontal cross-sections.\nAlthough the integrand is infinite, the area under the curve is finite. The function h is continuous, h(0) = 0, but\nits graph has infinite slope at x = 0."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 6",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/1d7ae64272a135e4b8041a3cf5286534_ps6.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\n18.01 Problem Set 6\nDue THURSDAY 11/09/06, 12:55 pm\nWarning: This problem set is due on a THURSDAY not Friday, because of the Veterans'\nDay holiday. It is due before lecture, which is at 1:05 on Thursday.\nEven though this problem set is due two days after Exam 3, you will need to do most of it by\nTuesday, in the process of preparing for Exam 3 -- all except the Part I problems connected to\nLecture 25.\nPart I\n(22 points)\nLecture 22. Fri. Oct. 27\nVolumes by disks and shells.\nRead: 7.4\nWork: 4B-2eg, 5; 4C-1a, 2, 3 4J-3\nLecture 23. Tues. Oct. 31\nWork; average value; probability.\nRead: 7.7, to middle p. 247\nNotes AV.\nWork: 249/5, 6, 15 (solutions posted at web site); 4D-2, 3, 5\nLecture 24. Thurs. Oct. 29\nNumerical Integration.\nRead 10.9\nWork: 3G-1ad, 4\nLecture 25. Fri. Nov. 3\nTrigonometric integrals. Direct substitution.\nRead 10.2, 10.3\nWork: 5B-9, 11, 13, 16; 5C-5, 7, 9, 11 (due after Exam 3)\nLecture 26. Tues. Nov. 7\nExam 3 1:05-1:55 covering lectures 18-24.\nPart II\n(30 points)\nDirections: Attempt to solve each part of each problem yourself. If you collaborate, solutions\nmust be written up independently. It is illegal to consult materials from previous semesters. With\neach problem is the day it can be done.\n0. (not until due date; 3 pts) Write the names of all the people you consulted or with whom you\ncollaborated and the resources you used, or say \"none\" or \"no consultation\". (See full explanation\non PS1).\n1. (Lec 22, 7pts: 3 + 4) Do 7.4/12 and 13.\n2. (Lec 23, 4pts) The voltage V of house current is given by\nV (t) = C sin(120πt)\nwhere t is time, in seconds and C is a constant amplitude. The square root of the average value\nof V 2 over one period of V (t) (or cycle) is called the root-mean-square voltage, abbreviated RMS.\nThis is what the voltage meter on a house records. For house current, find the RMS in terms of\nthe constant C. (The peak voltage delivered to the house is ±C. The units of V 2 are square volts;\nwhen we take the square root again after averaging, the units become volts again.)\n\nZ\n3. (Lec 23, 6 pts: 1 + 2 + 1 + 2)\na) What is the probability that x2 < y if (x, y) is chosen from the unit square 0 ≤ x ≤ 1,\n0 ≤ y ≤ 1 with probability equal to the area.\nb) What is the probability that x2 < y if (x, y) is chosen from the square 0 ≤ x ≤ 2, 0 ≤ y ≤ 2\nwith probability proportional to the area. (Probability = Part/Whole).\nc) Evaluate\nZ\nZ N\nW =\ninf\ne-atdt = lim\ne-atdt\nN →inf 0\nThis is known as an improper integral because it represents the area of an unbounded region. We\nare using the letter W to signify \"whole.\"\nThe probability that a radioactive particle will decay some time in the interval 0 ≤ t ≤ T is\nPART\n1 Z T\nP ([0, T ]) =\n=\ne-atdt\nWHOLE\nW\nNote that P ([0, inf)) = 1 = 100%.\nd) The half-life is the time T for which P ([0, T ]) = 1/2. Find the value of a and W for which\nthe half-life is T = 1. Suppose that a radioactive particle has a half-life of 1 second. What is the\nprobability that it survives to time t = 1, but decays some time during the interval 1 ≤ t ≤ 2?\n(Give an integral formula, and use a calculator to get an approximate numerical answer.)\n4. (Lec 24, 6pts) The basis for Simpson's rule is the following formula. Let x1 be the midpoint\nof the interval [x0, x2], and denote its length by 2h. Consider any three points (x0, y0), (x1, y1),\n(x2, y2). There is a unique quadratic function (parabola)\ny = Ax2 + Bx + C\nwhose graph passes through the three points. Simpson's rule says that the area under the parabola\nabove [x0, x2] is\nh(y0 + 4y1 + y2)\nThis problem is devoted to proving this formula. It is significant because it illustrates how calcu\nlations can be simplified by using symmetry, and by looking ahead to see what you need.\nSince the area will be the same if the parabola is translated to the left or right, we may assume\nthat x0 = -h, x1 = 0, and x2 = h. Then in terms of the rest of the data (i.e., h and the yi)\nmake a sketch and determine C;\nshow, by integrating, that to find the area we need only determine A (or better, 2Ah2);\ndetermine 2Ah2 using the data;\nput the results together to establish the formula for area.\n5. (Lec 24, 4pts) Use a calculator to make a table of values of the integrand and find approx\na\np\nimations to the Fresnel integral\ncos(t2)dt for a =\nπ/2, using Simpson's rule with four and\neight intervals. (The exact answer to five decimal places is 1.22505. Record your approximations\nto six decimal places to compare.)"
        },
        {
          "category": "Assignment",
          "title": "Problem Set 7",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/d7836a022f9355654f4012e9c6a3a2cd_ps7.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\nZ\nZ\n18.01 Problem Set 7\nDue Friday 11/17/06, 1:55 pm\nPart I\n(20 points)\nLecture 25. (We will begin here on Nov 9.)\nTrigonometric integrals. Direct substitution.\nRead 10.2, 10.3\nWork: 5B-9, 11, 13, 16; 5C-5, 7, 9, 11 (moved here from PS6)\nLecture 27. Thurs. Nov. 9\nInverse substitution. Completing the square.\nRead 10.4\nWork: 5D-1, 2, 7, 10\nLecture 28. Tues. Nov. 14 Integrating rational functions; partial fractions.\nRead 10.6, Notes F\nWork: 5E-2, 3, 5, 6, 10h (complete the square)\nLecture 29. Thurs. Nov. 16\nIntegration by parts. Reduction formulas.\nRead 10.7\nWork: 5F-1a, 2d then 2b, 3\nLecture 30. Tues. Nov. 16 Parametric equations; arclength. Surface area\nRead 17.1, 7.5\nWork will be assigned on PS8\nPart II\n(22 points)\nDirections: Attempt to solve each part of each problem yourself. If you collaborate, solutions\nmust be written up independently. It is illegal to consult materials from previous semesters. With\neach problem is the day it can be done.\n0. (not until due date; 3 pts) Write the names of all the people you consulted or with whom you\ncollaborated and the resources you used, or say \"none\" or \"no consultation\". (See full explanation\non PS1).\n1. (Lec 27, 4pts) (from PS6) The voltage V of house current is given by\nV (t) = C sin(120πt)\nwhere t is time, in seconds and C is a constant amplitude. The square root of the average value\nof V 2 over one period of V (t) (or cycle) is called the root-mean-square voltage, abbreviated RMS.\nThis is what the voltage meter on a house records. For house current, find the RMS in terms of\nthe constant C. (The peak voltage delivered to the house is ±C. The units of V 2 are square volts;\nwhen we take the square root again after averaging, the units become volts again.)\n2. (Lec 27, 4pts) The solid torus is the figure obtained by rotating the disk (x - b)2 + y2 ≤ a2\naround the y-axis. Find its volume by the method of shells. (Hint: Substitute for x - b. As noted\np. 229/11, the answer happens to be the area of the disk multiplied by the distance travelled by\nthe center as it revolves.)\n3. (Lec 27, 4pts: 2 + 2)\na) For any integer n ≥ 0, use the substitution tan2 x = sec2 x - 1 to show that\ntann+2 x dx =\ntann+1\ntann x dx\nn + 1\nx -\n\nZ\nZ\nb) Deduce a formula for\ntan4 x dx.\n4. (Lec 28, 4pts: 3 + 1)\ncos x\na) Derive a formula for\nsec x dx by writing sec x =\n(verify this), and then making\n1 - sin2 x\na substitution for sin x and using partial fractions. (Your final answer must be expressed in terms\nof x.)\nb) Convert the formula into the more familiar one by multiplying the fraction in the answer on\nboth top and bottom by 1 + sin x. (Note that (1/2) ln u = ln √u.)\n5. (Lec 29, 3pts) Find the volume under the first hump of the function y = cos x rotated around\nthe y-axis by the method of shells."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 8A",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/7b0ef3784e4eabf239d95286d01ca139_ps8a.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\ns\n18.01 Problem Set 8A\nDue Friday 12/08/06, 1:55 pm\n8A is the first half of Problem Set 8, all of which is due a week after Exam 4. (The second half,\n8B, will be issued at the exam.) Even though it won't be collected until later, you should do 8A\nbefore the exam, to prepare for it.\nPart I\n(15 points)\nLecture 30. Fri. Nov. 17\nParametric equations; arclength. Surface area.\nRead 17.1, 7.5, 7.6\nWork: 4E-2, 3, 8;\n4F-1d, 4, 5, 8;\n4G-2, 5.\nIf a curve is given by x = x(t), y = y(t), to find its arclength, use ds in the form\nds =\nq\n(dx)2 + (dy)2 =\ndx 2\n+\ndy 2\ndt ,\ndt\ndt\nand integrate from start to finish: from t = t0 to t = t1.\nLecture 31. Tues. Nov. 21\nPolar coordinates; area in polar coordinates.\nRead: 16.1, (16.2 lightly, for the pictures), 16.3 to top p.570, 16.5 to middle p.581\nWork: 4H-1bfg; 4H-2a,3f; 4I-2,3\nLecture 32. Tues. Nov 28.\nContinuation and review.\nLecture 33. Thurs. Nov 30.\nExam 4, 1:05-1:55, lectures 25-32.\nPart II\n(30 points)\nDirections: Attempt to solve each part of each problem yourself. If you collaborate, solutions\nmust be written up independently. It is illegal to consult materials from previous semesters. With\neach problem is the day it can be done.\n0. (not until due date; 3 pts) Write the names of all the people you consulted or with whom you\ncollaborated and the resources you used, or say \"none\" or \"no consultation\". (See full explanation\non PS1).\n1. (Lec 30, 7 pts: 3 + 1 + 3)\na) Find the algebraic equation in x and y for the curve\nx = a cosk t,\ny = a sink t.\nDraw the portion of the curve 0 ≤ t ≤ π/2 in the three cases k = 1, k = 2, k = 3.\nb) Without calculation, find the arclength in the cases k = 1 and k = 2.\nc) Find a definite integral formula for the length of the curve for general k. Then evaluate the\nintegral in the three cases k = 1, k = 2, and k = 3. (Your answer in the first two cases should\nmatch what you found in part (b), but the calculation takes more time.)\n2. (Lec 30, 9 pts: 3 + 1 + 3 + 2) The hyperbolic sine and cosine are defined by\ncosh x = ex + e-x\n,\nsinh x = ex - e-x\n\na) Show that\nd\nd\ni)\nsinh x = cosh x and\ncosh x = sinh x.\ndx\ndx\nii) cosh2 x = 1 + sinh2 x and\n1 + cosh 2x\niii) cosh2 x =\nb) What curve is described parametrically by x = cosh t, y = sinh t? (Give the equation and\nits name.)\nc) The curve y = cosh x is known as a catenary. It is the curve formed by a chain whose two\nends are held at the same height.\ni) Sketch the curve\nii) Find its arclength from the lowest point to the point (x1, cosh x1) for a fixed x1 > 0.\nd) Find the area of the surface of revolution formed by revolving the portion of the curve from\npart (c) around the x-axis. This surface is known as a catenoid. It is interesting because it is the\nsurface of least area connecting the two circles that form its edges. If you dip two circles of wire in\na soap solution, then (with some coaxing) a soap film will form in this shape. In general, the soap\nfilms try to span a frame of wires with a surface with the least area possible.\n3. (Lec 31, 11 pts: 3 + 2 + 2 + 4) Area in polar coordinates.\na) Find the area of the right triangle with vertices at (x, y) = (0, 0), (x, y) = (a, 0) and at\n(x, y) = (a, h), using polar coordinates. (One of the less convenient ways to find the area of a\ntriangle.)\nb) Find the equation in (x, y) coordinates for the curve r = 1/(1 + sin θ) and sketch it.\nc) Find the area of the region 0 ≤ r ≤ 1/(1 + sin θ), 0 ≤ θ ≤ π, using the rectangular coordinate\nformula you found in part (b).\nd) Find the area of the region in part (c) using polar coordinates. One way to evaluate the\nintegral is to change variables to u = θ - π/2, and then use the half angle formula\n1 + cos u = 2 cos2(u/2)\nThis area was already computed in part (c), but the polar coordinate formula is still valuable\nbecause it gives the area of any sector, not just the one that is bounded by a horizontal line. The\narea swept out from the viewpoint of the focus of an ellipse, parabola, or hyperbola is related by\nKepler's law to the speed of planets and comets."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 8B",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/87417161803dd6503034989fbe34cc37_ps8b.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\nZ\nZ\n18.01 Problem Set 8B\nDue Friday 12/08/06, 1:55 pm\nPart I\n(20 points)\nLecture 34. Fri. Dec 1\nIndeterminate forms; L'Hospital's rule, growth rate of functions.\nRead: 12.2, 12.3 (Examples 1-3, remark 1)\nWork: 6A-1befgj, 5, 6c\nLecture 35. Tue. Dec. 5\nImproper integrals.\nRead: 12.4, Notes INT\nWork: 6B-1,2,7afkm, 8c\nLecture 36. Thurs. Dec. 7\nInfinite series; simple convergence tests\nGeometric series; harmonic series.\nRead: pp. 439-442(top)\nComparison tests. pp. 451-3 (skip proof in Example 3)\nIntegral test. pp. 455-457(top)\nWork: 7A-1abc;\n7B-1abf\n7B-2acde\nLecture 37. Fri. Dec. 8\nTaylor series.\nRead: 14.4 through p. 498 (bottom); skip everything involving the remainder term Rn(x).\nDifferentiation and integration of series. Read: 14.3-p.490(top); Examples 1-5.\nWork: see handout with remarks about the final exam\nLecture 38. Tues. Dec. 12\nFinal Review.\nPart II\n(17 points + 5 extra)\nDirections: Attempt to solve each part of each problem yourself. If you collaborate, solutions\nmust be written up independently. It is illegal to consult materials from previous semesters. With\neach problem is the day it can be done.\n0. (not until due date; 3 pts) Write the names of all the people you consulted or with whom you\ncollaborated and the resources you used, or say \"none\" or \"no consultation\". (See full explanation\non PS1).\n1. (Lecs 34-36, 6 + 5 pts: 1 + 2 + 2 + 1 + (5 extra))\na) Use L'Hospital's rule to evaluate\nlim x m e-x\nx→inf\nb) Use part (a) and limit comparison to show that the improper integral\ninf\nx n e-xdx converges\nfor n ≥ 0. (Do not integrate by parts.)\nc) Denote A(n) =\ninf\nx n e-x dx. Use integration by parts to find the constant cn for which\nA(n + 1) = cnA(n)\n(Explain what happens at the infinite limit using part (a).)\n\n!\nd) Find A(0) and deduce from part (c) the formula for A(n), n = 0, 1, 2, . . ..\ne) (optional; 5pts extra credit) Show that the improper integral representing A(-1/2) converges\nand evaluate it using a change of variables.\n2. (Review problem using solids and surfaces of revolution; 9pts: 4 + 2 + 3)\na) Show that the volume V of the solid of revolution enclosed on the top by a spherical cap of\nradius r of height h and underneath by a horizontal plane is\nh3\nV = π rh2 - 3\nDraw a picture. Doublecheck this answer in the three cases: the empty set (h = 0); the half ball\n(h = r); the whole ball (h = 2r). We will always assume that 0 ≤ h ≤ 2r. Why?\nb) Find the surface area of the spherical cap of radius r and height h. Doublecheck your answer\nby evaluating it in the three cases h = 0, h = r and h = 2r.\nc) We will now find the shape of a soap bubble sitting on a table. Suppose that the bubble is a\nportion of a sphere as above, that it encloses a fixed volume V , and that its surface area is as small\nas possible (area of the curved surface, not counting the flat bottom = table). Find the minimizing\nshape, and describe it geometrically. (Hint: find h/r.)"
        },
        {
          "category": "Resource",
          "title": "Course Outline",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/25808dab1504a9b2f2a582abee50e112_schedulef06.pdf",
          "content": "18.01 Course Outline -- Fall 2006\nDifferentiation\n0.\nW\nSept\nRecitation: Graphing.\n1.\nR\nSept\nDerivatives, slope, velocity, rate of change.\n2.\nF\nSept\nLimits, continuity. Trigonometric limits.\n3.\nT\nSept\nDerivatives of products, quotients, sine, cosine\n4.\nR\nSept\nChain rule. Higher derivatives.\n5.\nF\nSept\nImplicit differentiation, inverses.\nPS 1 due\n6.\nT\nSept\nExponential and log. Logarithmic differentiation; hyperbolic functions.\n7.\nR\nSept\nContinuation and Review\n8.\nF\nSept\nEXAM 1 covering lectures 1-7.\nM\nSept\nStudent holiday - no recitation\nApplications of Differentiation\n9.\nT\nSept\nLinear and quadratic approximations.\n10.\nR\nSept\nCurve sketching.\n11.\nF\nSept\nMax-min problems.\nPS 2 due\n12.\nT\nOct\nRelated rates.\n13.\nR\nOct\nNewton's method and other applications.\n14.\nF\nOct\nMean value theorem. Inequalities.\nPS 3 due\nM,T\nOct\n9,10\nColumbus Day holiday - no classes\n15.\nR\nOct\nDifferentials, antiderivatives.\n16.\nF\nOct\nDifferential equations, separation of variables.\n17.\nT\nOct\nEXAM 2 covering lectures 8-16.\nIntegration\n18.\nR\nOct\nDefinite integrals.\n19.\nF\nOct\nFirst fundamental theorem of calculus.\nPS 4 due\n20.\nT\nOct\nSecond fundamental theorem. Def'n of log.\n21.\nR\nOct\nAreas between curves, volumes by slicing.\n22.\nF\nOct\nVolumes by disks, shells.\nPS 5 due\n23.\nT\nOct\nWork, average value, probability.\n24.\nR\nNov\nNumerical integration.\nTechniques of Integration\n25.\nF\nNov\nTrigonometric integrals.\n26.\nT\nNov\nEXAM 3 covering lectures 18-24.\n27.\nR\nNov\nIntegration by inverse substitution; completing the square.\nPS 6 due\nF\nNov\nVeterans' Day holiday - no classes. (PS6 due on a Thursday.)\n28.\nT\nNov\nPartial fractions.\n29.\nR\nNov\nIntegration by parts; reduction formulas.\n30.\nF\nNov\nParametric equations, arclength, surface area.\nPS 7 due\n31.\nT\nNov\nPolar coordinates; area in polar coordinates.\nR,F\nNov\n23,24\nThanksgiving holiday - no classes\n32.\nT\nNov\nContinuation and review.\n33.\nR\nNov\nEXAM 4 covering lectures 25-32.\n34.\nF\nDec\nIndeterminate forms; L'Hospital's rule.\n35.\nT\nDec\nImproper integrals.\n36.\nR\nDec\nInfinite series. Convergence tests.\n37.\nF\nDec\nTaylor series.\nPS 8 due\n38.\nT\nDec\nFinal review. (FINAL EXAM date to be announced)"
        },
        {
          "category": "Resource",
          "title": "End of Term",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/d71c273968f16a144ab0cf2bcfa0772f_endoftermf06.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\n18.01 End of Term\nPart I\n(do not hand in)\nLecture 37. Fri. Dec. 8\nTaylor series.\nRead: 14.4 through p. 498 (bottom); skip everything involving the remainder term Rn(x).\nDifferentiation and integration of series. Read: 14.3-p.490(top); Examples 1-5.\n7C/1ac;\n7D/1adg;\n7D/2bde (skip radius of convergence)\nLecture 38. Tues. Dec. 12 Continuation and review.\nFinal Examination: Thursday, Dec. 21\nThe final examination will cover the entire semester's work, including all the material since\nExam 4, namely, L'Hˆopital's rule, improper integrals, infinite series, power series.\nTo prepare for it, make sure you understand the solutions to the problems on the four hour\nexams and to the practice problems given out for each of the hour exams. If you find you still have\ntrouble with some topics, go back to the problem sets which covered those, and practice on the\nrelevant Part I problems that were assigned for them. A practice final exam will be posted.\nThe table of formulas from Exam 4 will be printed with the final exam. Some problems on the\nexam related to techniques of integration may be in a new format, namely, you will be given an\nintegral formula that follows a method of integration, (integration by parts, partial fractions, or\nsubstitution) and asked to find the mistake(s) in it. The final exam will test the notion of weighted\naverage (see Problem 4, Exam 3).\nMakeup 18.01 during IAP\nStudents who receive a D in 18.01 this semester will be eligible to take a review course during\nIAP. It counts as 6 units on the IAP load, so students also in 8.01L can continue with that course.\nStudents who pass this review course will receive the usual 12 units credit for having passed 18.01.\nThe IAP review course consists of daily one or two hour review sessions, going through the\nentire course, with the hour exams occurring at the relevant times (and a make-up hour exam the\nnext day, for those who failed the earlier one). In general, there will be two or three review sessions\npreceding each exam. At the end is a final exam.\nStudents who receive an F in 18.01 this semester will in general not be eligible for the review\ncourse; they should retake 18.01 in the spring. If there are extenuating circumstances which make\nit likely that the student will be able to pass the review course, they can be considered.\nStudents who fail 18.01 will be notified by e-mail the day after the final, so they can make plans\nfor the IAP course if they wish to do so and are eligible for it."
        },
        {
          "category": "Exam",
          "title": "Practice Final Exam",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/7025cfa8c7d4f105856a70f9fdc10db3_prfinal.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\nZ\np\n\n18.01 Practice Final Exam\nThere are 19 problems, totaling 250 points. No books, notes, or calculators. This practice exam should\ntake 3 hours.\nGenerally useful trigonometry:\nsin2 x = 1 - cos 2x ;\ncos 2 x = 1 + cos 2x ;\nsec x = ln(sec x + tan x)\nsec x =\n;\nsin2 x + cos2 x = 1;\ntan2 x + 1 = sec2 x\ncos x\nIn a 30-60-90 right triangle, with hypotenuse 2, the legs are 1 and\np\n3.\np\n2 = 1.41\np\n3 = 1.73\n= 3.14\nln 2 = .69\nln 10 = 2.3\nProblem 1. (15) Evaluate each of the following:\nd ln x\na)\n2 ; simplify your answer.\ndx x\nb)\nd\n3 sin2 u + 2\nc)\ndn\ne kx\n,\nk constant.\ndu\ndxn\nx=0\nProblem 2. (10) Find the equation of the line tangent to the graph of x2y2 + y3 = 2 at the point\n(1, 1) on the graph. (Give the equation in the form y = mx + b.)\nProblem 3.\n(10) Using implicit differentiation, derive the formula for D cos-1 x by using the\nformula for D cos x. (Let y = cos-1 x.)\nx2 + x + a, x 0\nProblem 4. (10) Let f(x) =\n, a and b constants.\nbx + 2,\nx > 0\nFind all values of a and b for which f(x) is differentiable.\nProblem 5. (15) On a night when the full moon is directly overhead, an outdoor Christmas tree 50\nfeet high is falling over. Its top is falling at the rate of 2 feet/sec, at the moment when it is 30 feet from\nthe ground. At that moment, how rapidly is the shadow of the tree cast by the moon lengthening?\nProblem 6. (15) Find the area of the largest rectangle whose base lies along the x-axis and whose\ntop corners lie on the parabola y = 1 - x2 .\nProblem 7. (15: 4,7,4) The graph of y = y(x) has this property: at each point (x, y) on the graph,\nthe normal line at that point passes through the fixed point (1, 0). (The normal is the line perpendicular\nto the tangent line.)\n1 - x\na) Show that y = y(x) satisfies the differential equation y =\n.\ny\nb) Using separation of variables, find all solutions to the differential equation. You can leave the\nsolutions in implicit form, i.e., as equations connecting x and y.\nc) Describe the curves which are their graphs. (You may have to use algebraic processes first (like\ncompleting the square) in order to change the equations into a form where you know what their graphs\nlook like.)\nProblem 8. (15) The cup of a wine-glass has the shape formed by rotating the parabola y = x2\nabout the y-axis; its upper rim is a circle of radius 1. How much wine does it hold?\n\nZ 1\nZ /2\nProblem 9. (10) Using the trapezoidal rule with three subdivisions (n = 3), estimate\nsin2 x dx.\nDo the work systematically, making a table of values first.\nZ x\nProblem 10. (15: 7,8) Let F (x) =\ne -t dt.\na) Find F 0(1) and F 00(1).\nZ 2\nb) Express\ne -u 2 /4du in terms of values of F (x).\nProblem 11. (15: 7,8) Between the two towers of a suspension bridge, each of the two main cables\nhas the shape of the parabola y = 1 x2 (units are kilometers). The two towers are 2 km. apart; the\nvertical cables from the main cable to the horizontal roadway are closely and equally spaced.\na) Set up a definite integral which gives the length of each main cable between the two towers.\nb) What is the average length (to the nearest meter) of the vertical cables?\nProblem 12. (20: 10,10) Evaluate\nZ 1\nZ\ndx\na)\n; (begin by factoring the denominator).\nb)\nx 2 ln x dx\nx2 + 3x + 2\nZ 1\ndx\nProblem 13. (10) Evaluate\nby making the substitution x = tan u; remember the\n(x2 + 1)2\nlimits.\nProblem 14. (15) Starting at the point where r = 1, the point P moves counterclockwise along the\npolar curve r = e/2, in such a way that the line segment OP makes one complete revolution. (Here O\ndenotes the origin.)\nSketch the curve, and find the total area swept out by OP as it makes the revolution.\nProblem 15. (15) Evaluate (showing work):\nsin2 x\n(ln x)2\na) lim\nb) lim\nc) lim x 2 e -x\nx!0 1 - cos x\nx!1 x - 1\nx!1\ndx\nProblem 16. (10) Evaluate\n3/2\nx\nX\nn\nProblem 17. (10) For what values of p does\nconverge? (Indicate reasoning.)\np4 + np\nProblem 18. (15: 10,5)\na) Find by differentiating the function f(x) = p1 + x the first four non-zero terms of its Taylor series\naround x = 0. (Show work.)\nb) Use the correct answer to (a) (or your own answer, if you don't know the correct answer) to calculate\np\n1.2 to four decimal places.\nProblem 19.\n(10) Find the Taylor series for tan-1 x around x = 0 by using term-by-term\ndifferentiation or integration on the appropriate geometric series. Give enough terms to make the pattern\nclear."
        },
        {
          "category": "Exam",
          "title": "Exam 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/f59cd98e31f25c1690accfb8821eef13_exam1.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\nFind the tangent line to y =\n3 x at x = 1\nProblem 1. (10 pts.)\nProblem 2. Find the derivative of the following functions:\n( )\na. (7 pts.)\n(2 )\nb. (8 pts).\nc. (5 pts).\n= g(x)\nd. (5 pts.) ln(sin x)\nf x\nx\nx\ncos\nx\nx\ne\n-\nProblem 3. (15 pts.) Find dy\ndx\nfor the function for the function y defined implicitly\n4+ xy = 4 at x =3, y = l\ny\nProblem 4. (15 pts.) Draw the graph of the derivative of the function\n(qualitatively accurate) directly under the graph of the function.\nby\n\nProblem 5. (15 pts) Let\nax + b x < 1\nx4 + x + 1 x ≥1\nfx=\nFind all a and b such that the function f(x) is differentiable.\nProblem 6. Evaluate these limits by relating them to a derivative.\n(1\n2 )\nlim\nx\nx\nx\n→\n+\n-\na. (5 pts.) Evaluate\ncos\nlim\nx\nx\nx\n→\n-\nb. (5 pts.) Evaluate\nProblem 7. (10 pts.) Derive the formula\n( )\nx\nx\nd a\nM a a\ndx\n=\ndirectly from the\ndefinition of the derivative, and identify\n( )\nM a as a limit."
        },
        {
          "category": "Exam",
          "title": "Exam 1 Review",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/18a28c4c5da3c4fd876e376f1917d7ba_unit1_review.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.01 Fall 2006\nExam 1 Review\nGeneral Differentiation Formulas\n(u + v)0\n=\nu0 + v0\n(cu)0\n=\ncu0\n(uv)0\n=\nu0v + uv0\n(product rule)\nu 0\n=\nu0v - uv0\n(quotient rule)\nv\nv2\nd f(u(x))\n=\nf 0(u(x)) u0(x) (chain rule)\ndx\n·\nYou can remember the quotient rule by rewriting\nu 0\n= (uv-1)0\nv\nand applying the product rule and chain rule.\nImplicit differentiation\nLet's say you want to find y0 from an equation like\ny 3 + 3xy 2 = 8\nd\nInstead of solving for y and then taking its derivative, just take\nof the whole thing. In this\ndx\nexample,\n3y 2 y0 + 6xyy0 + 3y 2\n=\n(3y 2 + 6xy)y0\n=\n-3y 2\ny0\n=\n-3y2\n3y2 + 6xy\nNote that this formula for y0 involves both x and y. Implicit differentiation can be very useful for\ntaking the derivatives of inverse functions.\nFor instance,\ny = sin-1 x\nsin y = x\n⇒\nImplicit differentiation yields\n(cos y)y0 = 1\nand\ny0 =\n=\ncos y\n√\n1 - x2\n\n18.01 Fall 2006\nSpecific differentiation formulas\nYou will be responsible for knowing formulas for the derivatives and how to deduce these formulas\nn\nx\nfrom previous information: x , sin-1 x, tan-1 x, sin x, cos x, tan x, sec x, e , ln x .\nd\nFor example, let's calculate\nsec x:\ndx\nd\nd\n-(- sin x)\nsec x =\n=\n= tan x sec x\ndx\ndx cos x\ncos2 x\nd\nd\nYou may be asked to find\nsin x or\ncos x, using the following information:\ndx\ndx\nsin(h)\nlim\n=\nh\nh\n→\nlim cos(h) - 1\n=\nh\nh\n→\nRemember the definition of the derivative:\nd f(x) = lim f(x + Δx) - f(x)\ndx\nΔx\nΔx\n→\nTying up a loose end\nd\nHow to find\nx r, where r is a real (but not necessarily rational) number? All we have done so far\ndx\nis the case of rational numbers, using implicit differentiation. We can do this two ways:\n1st method: base e\nx\n=\ne ln x\nx r\n=\n\ne ln x r = e r ln x\nd\ndx x r\n=\nd\ndx e r ln x = e r ln x d\ndx (r ln x) = e r ln x r\nx\nd\ndx x r\n=\nx r r\nx\n\n= rx r-1\n2nd method: logarithmic differentiation\nf 0\n(ln f)0\n=\nf\nf\n=\nx r\nln f\n=\nr ln x\nr\n(ln f)0\n=\nx\nf 0 = f(ln f)0\n=\nx r\nr\n= rx r-1\nx\n\n18.01 Fall 2006\nFinally, in the first lecture I promised you that you'd learn to differentiate anything-- even\nsomething as complicated as\nd\nx tan-1 x\ne\ndx\nSo let's do it!\nd\nd\ne uv\n=\ne uv\n(uv) = e uv (u0v + uv0)\ndx\ndx\nSubstituting,\nd e x tan-1 x\n=\ne x tan-1 x tan-1 x + x\ndx\n1 + x2"
        },
        {
          "category": "Exam",
          "title": "Practice Questions for Exam 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/1c540fc9ab7b091636f7bd26896b1f4b_prexam1a.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\np\n\n18.01 Practice Questions for Exam 1\nSolutions will be posted on the 18.01 website.\nNo books, notes, or calculators will be allowed at the exam.\n1.\nEvaluate each of the following, simplifying where possible; for (b) indicate reasoning.\nThe letters a and k represent constants.\nd 3t\n3u\nd3\nd 3\na)\nb)\nlim\nc)\nsin kx\nd)\na + k sin2\ndt ln t\nu!0 tan 2u\ndx3\nd\ne\nd\n2.\nDerive the formula for\nx 3 at the point x = x0\ndirectly from the definition of\ndx\nderivative.\n1 - 3p\n1 + h\n3.\nFind lim\nby relating it to a derivative. (Indicate reasoning.)\nh!0\nh\n4.\nSketch the curve y = sin-1 x,\n-1 x 1, and derive the formula for its\nderivative from that for the derivative of sin x.\n5.\nFor the function\nax + b,\nx > 0\nf(x) =\n,\na and b constants,\n1 - x + x2 ,\nx 0,\na)\nfind all values of a and b for which the function will be continuous;\nb)\nfind all values of a and b for which the function will be differentiable.\n6. For the curve given by the equation\nx 2 y + y 3 + x 2 = 8,\nfind all points on the curve where its tangent line is horizontal.\n7. Where does the tangent line to the graph of y = f(x) at the point (x0, y0) intersect\nthe x-axis?\n8. The volume of a spherical balloon is decreasing at the instantaneous rate of -10 cm3/sec,\nat the moment when its radius is 20 cm.\nAt that moment, how rapidly is its radius\ndecreasing?\n9. Where are the following functions discontinuous?\n1 + x2\nd\na)\nsec x\nb)\nc)\nx\n1 - x2\ndx |\n|\n10. A radioactive substance decays according to a law A = A0e-rt , where A(t) is the\namount in present at time t, and r is a positive constant.\na) Derive an expression in terms of r for the time it takes for the amount to fall to\none-quarter of the initial amount A0.\nb) At the moment when the amount has fallen to 1/4 the initial amount, how rapidly\nis the amount falling? (Units: grams, seconds.)"
        },
        {
          "category": "Exam",
          "title": "Practice Questions for Exam 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/45a20e8aee443709bd03320699145924_prexam1b.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\np\n\n18.01 Practice Questions for Exam 1\nSolutions will be posted on the 18.01 website\nProblem 1. Evaluate each of the following:\nd\npx\na)\ndx 1 + 2x x=1\nd\nb)\n(u ln 2u)\n(simplify your answer)\ndu\nd\nProblem 2.\na) Evaluate\n1 - k cos2 t, where k is constant.\ndt\nb) Check your answer to part (a) by showing that if k = 1, your answer agrees with the\nderivative calculated by a simpler method.\nd\nProblem 3.\nDerive the formula for\ndirectly from the definition of\ndx x2\nderivative.\n(You will need to transform the difference quotient algebraically before taking the\nlimit.)\nd\nProblem 4.\nDerive the formula for\nsin-1 x by solving y = sin-1 x for x and\ndx\nusing implicit differentiation.\n(You may use the known values of D sin x and D cos x in your derivation. Your answer\nmust be expressed in terms of x.)\nProblem 5.\nFind all values of the constants a and b for which the function defined\nby\nax + b,\nx > 1\nf(x) =\nx - 3x + 2,\nx 1\nwill be differentiable.\nProblem 6.\nEvaluate the following, with enough indications to show you are not\njust guessing:\ntan 2u\na)\nlim\nu!0\nu\neh - 1\nb)\nlim\n(relate it to the value of a derivative)\nh!0\nh\nProblem 7.\nA hawk is pursuing a mouse. We choose a coordinate system so the\nmouse runs along the x-axis in the negative direction, and the hawk is flying over the x-axis,\nswooping down along the exponential curve y = ekx, for some positive constant k. The\nhawk in flight is always aimed directly at the mouse. It is noon at the equator, and the sun\nis directly overhead.\nWhen the hawk's shadow on the ground is at the point x0, where is the mouse?"
        },
        {
          "category": "Exam",
          "title": "Exam 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/daea3cad537fab8e3d158209d75fb407_exam2.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\n18.01 Exam 2\nTuesday, Oct. 17, 2006\nProblem 1. (15 pts.) Estimate the following to two decimal places (show work)\na. (8 pts.) sin(\n1/100)\nπ +\nb. (7 pts.) 101\nProblem 2. (20 pts.) Sketch the graph of\ny\nx\nx\n=\n+ +\non\nx\n-inf< <inf\nand label all critical points and infection points with their coordinates on the\ngraph along with the letter \"C\" or \"I\"\nProblem 3. (20 pts.) An architect plans to build a triangular enclosure with a fence\non two sides and a wall on the third side. Each of the fence segments has fixed\nLength L. What is the length x of the third side if the region enclosed has the largest\npossible area? Show work and include an argument to show that your answer really\ngives the maximum area.\nProblem4. (15 pts) A rocket has launched straight up, and its altitude is h = 10t2\nfeet after t seconds. You are on the ground 1000 feet from the launch site. The line\nof sight from you to the rocket makes an angle θ with the horizontal. By how many\nRadians per second is θ changing ten seconds after the launch?\nWrite down on which intervals the function is:\nIncreasing:\n\nDecreasing:\n\nConcave down:\n\nProblem 5. a. (10 pts) Evaluate the following indefinite integrals\ni. ∫cos(3x)dx\n∫xe(\n)\nx2\nii.\ndx\nb. (10 pts) Find y( )\nx such that y1\n= y3\nand\ny(0)=1\nProblem 6. (10 pts.) Suppose that f '(x\ne\n)=\n(\n)\nx2 , and f (0)=10\nOne can conclude from the mean value theorem that\nA\nf\n<\n(1)< B\nfor which numbers A and B?"
        },
        {
          "category": "Exam",
          "title": "Practice Exam 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/1f1292067f4d0302e6516daabe77d7ec_prexam2b.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\nZ\nZ\nZ\np\n18.01 Practice Exam 2\nProblem 1. (20)\nFind the local maxima and minima and points of inflection of\n2x 3 + 3x 2 - 12x + 1 .\nThen use this data to sketch its graph on the given axes, showing also where it is convex (concave up) or\nconcave (down). (Note that different scales are used on the two axes.)\nProblem 2. (20)\nA new junk food -- NoKarb PopKorn -- is to be sold in large cylindrical metal\ncans with a removable plastic lid instead of a metal top. The metal side and bottom will be of uniform\nthickness, and the volume is fixed to be 64 cubic inches.\nWhat base radius r and height h for the can will require the least amount of metal?\nShow work, and include an argument to show your values for r and h really give a minimum.\nProblem 3. (15)\nEvaluate the following indefinite integrals:\na)\ne -3xdx\nb)\ncos 2 x sin x dx\nx dx\nc)\np\n1 - x2\nProblem 4. (15)\nA searchlight L is 100 meters from a prison wall. It is rotating at a constant rate of one revolution\nevery 8 minutes. (How many radians/minute is that?)\nMartha, an escaping prisoner, is running along the wall trying to keep just ahead of the beam of light.\nAt the moment when the searchlight angle is 60 degrees, how fast does she have to run?\nProblem 5. (15: 5, 10)\n-xp\na) What value for the constant c will make the function e\n1 + cx approximately constant, for\nvalues of x near 0?\n(Show work.)\ndx\nb) Find the solution x(t) to the differential equation\n= 2t 1 - x2\ndt\nwhich also satisfies the condition x(0) = 1.\nProblem 6. (15: 8,7)\na) Use the Mean-value Theorem to show that\nln(1 + x) < x,\nif x > 0.\n(You do not have to state the theorem.)\nb) Let c be any constant. Show that the function f(x) = x 3 + x + c cannot have two zeros.\n(Use the Mean-value theorem, or some other argument.)"
        },
        {
          "category": "Exam",
          "title": "Practice Questions for Exam 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/18a4b80044ab85b6d948ba920af54c03_prexam2a.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006\n\nZ\nZ\nZ\n18.01 Practice Questions for Exam 2\nSolutions will be posted on the 18.01 website. No books, notes, calculators. Show work.\n1.\nFor the function 3x5 - 5x3 + 1, sketch the graph over a suitable interval showing\nall the local maximum and minimum points on the graph, the points of inflection, and the\napproximate location of its zeros (show on which intervals of the form [n, n + 1], (n is an\ninteger) they occur. Show work, or indicate reasoning.\n2. Sketch the graph of 4x 2 -\nover an interval showing its interesting features - local\nx\nmaxima and minima, points of inflection, zeros, asymptotes.\n3. A line of negative slope through (1, 2) cuts off a triangle in the first quadrant. For which\nsuch line will the triangle have least area? (Use its slope m as the independent variable.\nShow that you get a minimum.)\n4. The bottom of the legs of a three-legged table are the vertices of an isosceles triangle\nwith sides 5, 5, and 6. The legs are to be braced at the bottom by three wires in the shape\nof a Y. What is the minimum length of wire needed? Show it is a minimum.\n5.\nA 200 foot tree is falling in the forest; the sun is directly overhead. At the moment\nwhen the tree makes an angle of 30o with the horizontal, its shadow is lengthening at the\nrate of 50 feet/sec. How fast is the angle changing at that moment?\n6.\nA container in the shape of a right circular cone with vertex angle a right angle is\npartially filled with water.\na) Suppose water is added at the rate of 3 cu.cm./sec. How fast is the water level\nrising when the height h = 2cm.?\nb) Suppose instead no water is added, but water is being lost by evaporation. Show\nthe level falls at a constant rate. (You will have to make a reasonable physical assumption\nabout the rate of water loss--state it clearly.)\n-x\ne\n7. How should the parameter be chosen so that f(x) =\nremain as close to 1\n1 + 2 sin x\nas possible, when x 0? Using this value of , estimate f(.1) to two decimal places.\n8. State the Mean-value Theorem, and use it to prove that\na) if f(x) is differentiable and f 0(x) > 0 for all x, then f(x) is an increasing function;\nb) e x > 1 + x for all x > 0.\ndx\nln2 x\n9. Evaluate:\na)\nb)\nsin 2x sin x dx\nc)\ndx\n(3x + 2)2\nx\ndy\n10. Find the function y(x) satisfying\n= xy + x,\ny(0) = 1.\ndx\n11.\nThe rate at which a body heats up by conduction is proportional to the difference\nbetween its temperature T and the temperature Te of its surroundings. A fish at room\ntemperature (20o) is cooked by putting it into boiling water at 100o . After 5 minutes its\ntemperature has risen to 30o . How long will it take to be done (60o) ?\na) Set up a differential equation for the fish temperature T (t). (Call the constant of\nproportionality k.)\nb) Find the solution T (t) satisfying the given data, then use it to answer the question."
        },
        {
          "category": "Resource",
          "title": "Practice Final Solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/ddc6b990286fcf509e8e1c53a00848fc_prfinalsol.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006"
        },
        {
          "category": "Exam",
          "title": "Exam 1 Solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/c19bbbeab0d30f010c2d8f52c60b8c8e_exam1sol.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006"
        },
        {
          "category": "Exam",
          "title": "Practice Exam 1 Solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/6b8dc6e9248fac9328da413eb2fb193e_prexam1bsol.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006"
        },
        {
          "category": "Exam",
          "title": "Practice Exam 1 Solutions 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/888be10d7c78f32914aa04d5b9d8e96a_prexam1asolv1.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nCorrection to #3"
        },
        {
          "category": "Exam",
          "title": "Practice Exam 1 Solutions 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/04b2c07bac4167a0b602b643b8172466_prexam1asolv2.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006"
        },
        {
          "category": "Exam",
          "title": "Exam 2 Solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/2b26407260f12f6c1babf3078d1be635_exam2sol.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006"
        },
        {
          "category": "Exam",
          "title": "Practice Exam 2 Solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/de608fc3e0fa7dff2014bff9135e11fb_prexam2asol.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006"
        },
        {
          "category": "Exam",
          "title": "Practice Exam 2 Solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/193066d33989e9c58ae911c4ffc58891_prexam2bsol.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006"
        },
        {
          "category": "Exam",
          "title": "Exam 3 Solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/0ebd4a5db190a7138644784bce3f8cd6_exam3sol.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006"
        },
        {
          "category": "Exam",
          "title": "Practice Exam 3 Solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/204fbfbf46cea3c9af775855a3bec084_prexam3asol.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFall 2006"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 1: Derivatives, Slope, Velocity, and Rate of Change",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/c6a2d4081848972d197c41332e604d49_lec1.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 1\n18.01 Fall 2006\nUnit 1: Derivatives\nA. What is a derivative?\n- Geometric interpretation\n- Physical interpretation\n- Important for any measurement (economics, political science, finance, physics, etc.)\nB. How to differentiate any function you know.\nd\nFor example:\ne x arctan x . We will discuss what a derivative is today. Figuring out how to\n-\ndx\ndifferentiate any function is the subject of the first two weeks of this course.\nLecture 1: Derivatives, Slope, Velocity, and Rate\nof Change\nGeometric Viewpoint on Derivatives\nTangent line\nSecant line\nf(x)\nP\nQ\nx0\nx0+∆x\ny\nFigure 1: A function with secant and tangent lines\nThe derivative is the slope of the line tangent to the graph of f(x). But what is a tangent line,\nexactly?\n\nLecture 1\n18.01 Fall 2006\n- It is NOT just a line that meets the graph at one point.\n- It is the limit of the secant line (a line drawn between two points on the graph) as the distance\nbetween the two points goes to zero.\nGeometric definition of the derivative:\nLimit of slopes of secant lines PQ as Q\nP (P fixed). The slope of PQ:\n→\nP\nQ\n(x0+∆x, f(x0+∆x))\n(x0, f(x0))\n∆x\n∆f\nSecant Line\nFigure 2: Geometric definition of the derivative\nlim Δf\n= lim f(x0 + Δx) - f(x0)\n=\nf 0(x0)\nΔx\n0 Δx\nΔx\nΔx\n| {z }\n→\n→|\n{z\n}\n\"difference quotient\"\n\"derivative of f at x0 \"\nExample 1. f(x) = x\nOne thing to keep in mind when working with derivatives: it may be tempting to plug in Δx = 0\nΔf\nright away. If you do this, however, you will always end up with\n= . You will always need to\nΔx\ndo some cancellation to get at the answer.\nΔf\n1 x0 - (x0 + Δx)\n\n= x0 +Δx - x0 =\n=\n-Δx\n=\n-1\nΔx\nΔx\nΔx\n(x0 + Δx)x0\nΔx (x0 + Δx)x0\n(x0 + Δx)x0\nTaking the limit as Δx\n0,\n→\nlim\n-1\n= -1\nΔx→0 (x0 + Δx)x0\nx2\n\nLecture 1\n18.01 Fall 2006\ny\nx\nx0\nFigure 3: Graph of x\nHence,\nf 0(x0) = -\nx0\nNotice that f 0(x0) is negative -- as is the slope of the tangent line on the graph above.\nFinding the tangent line.\nWrite the equation for the tangent line at the point (x0, y0) using the equation for a line, which you\nall learned in high school algebra:\ny - y0 = f 0(x0)(x - x0)\nPlug in y0 = f(x0) = 1 and f 0(x0) = -\n1 to get:\nx0\nx0\ny - x\n= -\nx2\n1(x - x0)\n\nLecture 1\n18.01 Fall 2006\ny\nx\nx0\nFigure 4: Graph of x\nJust for fun, let's compute the area of the triangle that the tangent line forms with the x- and\ny-axes (see the shaded region in Fig. 4).\nFirst calculate the x-intercept of this tangent line. The x-intercept is where y = 0. Plug y = 0\ninto the equation for this tangent line to get:\n0 - 1\n=\n-\n1(x - x0)\nx0\nx0\n-1\n-1\n=\nx +\nx0\nx0\nx0\nx\n=\nx0\nx0\nx\n=\nx 2\n0(\n) = 2x0\nx0\nSo, the x-intercept of this tangent line is at x = 2x0.\nNext we claim that the y-intercept is at y = 2y0. Since y =\nand x =\nare identical equations,\nx\ny\nthe graph is symmetric when x and y are exchanged. By symmetry, then, the y-intercept is at\ny = 2y0. If you don't trust reasoning with symmetry, you may follow the same chain of algebraic\nreasoning that we used in finding the x-intercept. (Remember, the y-intercept is where x = 0.)\nFinally,\nArea = (2y0)(2x0) = 2x0y0 = 2x0(\n) = 2 (see Fig. 5)\nx0\nCuriously, the area of the triangle is always 2, no matter where on the graph we draw the tangent\nline.\n\nLecture 1\n18.01 Fall 2006\ny\nx\nx0\n2x0\ny0\n2y0\nx-1\nFigure 5: Graph of x\nNotations\nCalculus, rather like English or any other language, was developed by several people. As a result,\njust as there are many ways to express the same thing, there are many notations for the derivative.\nSince y = f(x), it's natural to write\nΔy = Δf = f(x) - f(x0) = f(x0 + Δx) - f(x0)\nWe say \"Delta y\" or \"Delta f\" or the \"change in y\".\nIf we divide both sides by Δx = x - x0, we get two expressions for the difference quotient:\nΔy = Δf\nΔx\nΔx\nTaking the limit as Δx → 0, we get\nΔy\nΔx\n→\ndy\ndx (Leibniz' notation)\nΔf\nΔx\n→\nf 0(x0) (Newton's notation)\nWhen you use Leibniz' notation, you have to remember where you're evaluating the derivative\n-- in the example above, at x = x0.\nOther, equally valid notations for the derivative of a function f include\ndf , f 0, and Df\ndx\n\nLecture 1\n18.01 Fall 2006\nExample 2. f(x) = x n where n = 1, 2, 3...\nd\nWhat is\nx n?\ndx\nTo find it, plug y = f(x) into the definition of the difference quotient.\nn\nn\nΔy = (x0 + Δx)n - x0 = (x + Δx)n - x\nΔx\nΔx\nΔx\n(From here on, we replace x0 with x, so as to have less writing to do.) Since\n(x + Δx)n = (x + Δx)(x + Δx)...(x + Δx) n times\nWe can rewrite this as\n\nx n + n(Δx)x n-1 + O (Δx)2\nO(Δx)2 is shorthand for \"all of the terms with (Δx)2, (Δx)3, and so on up to (Δx)n.\" (This is part\nof what is known as the binomial theorem; see your textbook for details.)\nn\nn\nΔy = (x + Δx)n - x\n= xn + n(Δx)(xn-1) + O(Δx)2 - x\n= nx n-1 + O(Δx)\nΔx\nΔx\nΔx\nTake the limit:\nΔy\nlim\n= nx n-1\nΔx\n0 Δx\n→\nTherefore,\nd\nn\nx = nx n-1\ndx\nThis result extends to polynomials. For example,\nd\n(x 2 + 3x 10) = 2x + 30x\ndx\nPhysical Interpretation of Derivatives\nYou can think of the derivative as representing a rate of change (speed is one example of this).\nOn Halloween, MIT students have a tradition of dropping pumpkins from the roof of this building,\nwhich is about 400 feet high.\nThe equation of motion for objects near the earth's surface (which we will just accept for now)\nimplies that the height above the ground y of the pumpkin is:\ny = 400 - 16t2\nΔy\ndistance travelled\nThe average speed of the pumpkin (difference quotient) =\n=\nΔt\ntime elapsed\nWhen the pumpkin hits the ground, y = 0,\n400 - 16t2 = 0\n\nLecture 1\n18.01 Fall 2006\nSolve to find t = 5. Thus it takes 5 seconds for the pumpkin to reach the ground.\n400 ft\nAverage speed =\n= 80 ft/s\n5 sec\nA spectator is probably more interested in how fast the pumpkin is going when it slams into the\nground. To find the instantaneous velocity at t = 5, let's evaluate y0:\ny0 = -32t = (-32)(5) = -160 ft/s (about 110 mph)\ny0 is negative because the pumpkin's y-coordinate is decreasing: it is moving downward."
        },
        {
          "category": "Resource",
          "title": "Ses #1-7 Complete",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/ba59b9a1c43f19dfc9624078156179e2_u1_l1_7_jl_15_08.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 1\nSept. 7, 2006\n18.01 Fall 2006\nUnit 1: Derivatives\nA. What is a derivative?\n- Geometric interpretation\n- Physical interpretation\n- Important for any measurement (economics, political science, finance, physics, etc.)\nB. How to differentiate any function you know.\nd\nFor example:\ne x arctan x . We will discuss what a derivative is today. Figuring out how to\n-\ndx\ndifferentiate any function is the subject of the first two weeks of this course.\nLecture 1: Derivatives, Slope, Velocity, and Rate\nof Change\nGeometric Viewpoint on Derivatives\nTangent line\nSecant line\nf(x)\nP\nQ\nx0\nx0+∆x\ny\nFigure 1: A function with secant and tangent lines\nThe derivative is the slope of the line tangent to the graph of f(x). But what is a tangent line,\nexactly?\n\nLecture 1\nSept. 7, 2006\n18.01 Fall 2006\n- It is NOT just a line that meets the graph at one point.\n- It is the limit of the secant line (a line drawn between two points on the graph) as the distance\nbetween the two points goes to zero.\nGeometric definition of the derivative:\nLimit of slopes of secant lines PQ as Q\nP (P fixed). The slope of PQ:\n→\nP\nQ\n(x0+∆x, f(x0+∆x))\n(x0, f(x0))\n∆x\n∆f\nSecant Line\nFigure 2: Geometric definition of the derivative\nlim Δf\n= lim f(x0 + Δx) - f(x0)\n=\nf 0(x0)\nΔx\n0 Δx\nΔx\nΔx\n| {z }\n→\n→|\n{z\n}\n\"difference quotient\"\n\"derivative of f at x0 \"\nExample 1. f(x) = x\nOne thing to keep in mind when working with derivatives: it may be tempting to plug in Δx = 0\nΔf\nright away. If you do this, however, you will always end up with\n= . You will always need to\nΔx\ndo some cancellation to get at the answer.\nΔf\n1 x0 - (x0 + Δx)\n\n= x0 +Δx - x0 =\n=\n-Δx\n=\n-1\nΔx\nΔx\nΔx\n(x0 + Δx)x0\nΔx (x0 + Δx)x0\n(x0 + Δx)x0\nTaking the limit as Δx\n0,\n→\nlim\n-1\n= -1\nΔx→0 (x0 + Δx)x0\nx2\n\nLecture 1\nSept. 7, 2006\n18.01 Fall 2006\ny\nx\nx0\nFigure 3: Graph of x\nHence,\nf 0(x0) = -\nx0\nNotice that f 0(x0) is negative -- as is the slope of the tangent line on the graph above.\nFinding the tangent line.\nWrite the equation for the tangent line at the point (x0, y0) using the equation for a line, which you\nall learned in high school algebra:\ny - y0 = f 0(x0)(x - x0)\nPlug in y0 = f(x0) = 1 and f 0(x0) = -\n1 to get:\nx0\nx0\ny - x\n= -\nx2\n1(x - x0)\n\nLecture 1\nSept. 7, 2006\n18.01 Fall 2006\ny\nx\nx0\nFigure 4: Graph of x\nJust for fun, let's compute the area of the triangle that the tangent line forms with the x- and\ny-axes (see the shaded region in Fig. 4).\nFirst calculate the x-intercept of this tangent line. The x-intercept is where y = 0. Plug y = 0\ninto the equation for this tangent line to get:\n0 - 1\n=\n-\n1(x - x0)\nx0\nx0\n-1\n-1\n=\nx +\nx0\nx0\nx0\nx\n=\nx0\nx0\nx\n=\nx 2\n0(\n) = 2x0\nx0\nSo, the x-intercept of this tangent line is at x = 2x0.\nNext we claim that the y-intercept is at y = 2y0. Since y =\nand x =\nare identical equations,\nx\ny\nthe graph is symmetric when x and y are exchanged. By symmetry, then, the y-intercept is at\ny = 2y0. If you don't trust reasoning with symmetry, you may follow the same chain of algebraic\nreasoning that we used in finding the x-intercept. (Remember, the y-intercept is where x = 0.)\nFinally,\nArea = (2y0)(2x0) = 2x0y0 = 2x0(\n) = 2 (see Fig. 5)\nx0\nCuriously, the area of the triangle is always 2, no matter where on the graph we draw the tangent\nline.\n\nLecture 1\nSept. 7, 2006\n18.01 Fall 2006\ny\nx\nx0\n2x0\ny0\n2y0\nx-1\nFigure 5: Graph of x\nNotations\nCalculus, rather like English or any other language, was developed by several people. As a result,\njust as there are many ways to express the same thing, there are many notations for the derivative.\nSince y = f(x), it's natural to write\nΔy = Δf = f(x) - f(x0) = f(x0 + Δx) - f(x0)\nWe say \"Delta y\" or \"Delta f\" or the \"change in y\".\nIf we divide both sides by Δx = x - x0, we get two expressions for the difference quotient:\nΔy = Δf\nΔx\nΔx\nTaking the limit as Δx → 0, we get\nΔy\nΔx\n→\ndy\ndx (Leibniz' notation)\nΔf\nΔx\n→\nf 0(x0) (Newton's notation)\nWhen you use Leibniz' notation, you have to remember where you're evaluating the derivative\n-- in the example above, at x = x0.\nOther, equally valid notations for the derivative of a function f include\ndf , f 0, and Df\ndx\n\nLecture 1\nSept. 7, 2006\n18.01 Fall 2006\nExample 2. f(x) = x n where n = 1, 2, 3...\nd\nWhat is\nx n?\ndx\nTo find it, plug y = f(x) into the definition of the difference quotient.\nn\nn\nΔy = (x0 + Δx)n - x0 = (x + Δx)n - x\nΔx\nΔx\nΔx\n(From here on, we replace x0 with x, so as to have less writing to do.) Since\n(x + Δx)n = (x + Δx)(x + Δx)...(x + Δx) n times\nWe can rewrite this as\n\nx n + n(Δx)x n-1 + O (Δx)2\nO(Δx)2 is shorthand for \"all of the terms with (Δx)2, (Δx)3, and so on up to (Δx)n.\" (This is part\nof what is known as the binomial theorem; see your textbook for details.)\nn\nn\nΔy = (x + Δx)n - x\n= xn + n(Δx)(xn-1) + O(Δx)2 - x\n= nx n-1 + O(Δx)\nΔx\nΔx\nΔx\nTake the limit:\nΔy\nlim\n= nx n-1\nΔx\n0 Δx\n→\nTherefore,\nd\nn\nx = nx n-1\ndx\nThis result extends to polynomials. For example,\nd\n(x 2 + 3x 10) = 2x + 30x\ndx\nPhysical Interpretation of Derivatives\nYou can think of the derivative as representing a rate of change (speed is one example of this).\nOn Halloween, MIT students have a tradition of dropping pumpkins from the roof of this building,\nwhich is about 400 feet high.\nThe equation of motion for objects near the earth's surface (which we will just accept for now)\nimplies that the height above the ground y of the pumpkin is:\ny = 400 - 16t2\nΔy\ndistance travelled\nThe average speed of the pumpkin (difference quotient) =\n=\nΔt\ntime elapsed\nWhen the pumpkin hits the ground, y = 0,\n400 - 16t2 = 0\n\nLecture 1\nSept. 7, 2006\n18.01 Fall 2006\nSolve to find t = 5. Thus it takes 5 seconds for the pumpkin to reach the ground.\n400 ft\nAverage speed =\n= 80 ft/s\n5 sec\nA spectator is probably more interested in how fast the pumpkin is going when it slams into the\nground. To find the instantaneous velocity at t = 5, let's evaluate y0:\ny0 = -32t = (-32)(5) = -160 ft/s (about 110 mph)\ny0 is negative because the pumpkin's y-coordinate is decreasing: it is moving downward.\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\nLecture 2: Limits, Continuity, and Trigonometric\nLimits\nMore about the \"rate of change\" interpretation of the\nderivative\ny = f(x)\ny\nx\n∆x\n∆y\nFigure 1: Graph of a generic function, with Δx and Δy marked on the graph\n3. T\ntemperature gradient\nΔy\nΔx\n→\ndy\ndx as Δx → 0\nAverage rate of change\n→\nInstantaneous rate of change\nExamples\n1. q = charge\ndq\ndt = electrical current\n2. s = distance\nds\ndt = speed\ndT\n= temperature\n=\ndx\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\n4. Sensitivity of measurements: An example is carried out on Problem Set 1. In GPS, radio\nsignals give us h up to a certain measurement error (See Fig. 2 and Fig. 3). The question is\nΔL\nhow accurately can we measure L. To decide, we find\n. In other words, these variables are\nΔh\nrelated to each other. We want to find how a change in one variable affects the other variable.\nL\nh\ns\nsatellite\nyou\nFigure 2: The Global Positioning System Problem (GPS)\nh\ns\nL\nFigure 3: On problem set 1, you will look at this simplified \"flat earth\" model\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\nLimits and Continuity\nEasy Limits\nx2 + x\n32 + 3\nlim\n=\n=\n= 3\nx→3 x + 1\n3 + 1\nWith an easy limit, you can get a meaningful answer just by plugging in the limiting value.\nRemember,\nlim Δf = lim f(x0 + Δx) - f(x0)\nx→x0 Δx\nx→x0\nΔx\nis never an easy limit, because the denominator Δx = 0 is not allowed. (The limit x\nx0 is\ncomputed under the implicit assumption that x =6\nx0.)\n→\nContinuity\nWe say f(x) is continuous at x0 when\nlim f(x) = f(x0)\nx\nx0\n→\nPictures\nx\ny\nFigure 4: Graph of the discontinuous function listed below\nx + 1\nx > 0\nf(x) =\n-x\nx ≥ 0\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\nThis discontinuous function is seen in Fig. 4. For x > 0,\nlim f(x) = 1\nx\n→\nbut f(0) = 0. (One can also say, f is continuous from the left at 0, not the right.)\n1. Removable Discontinuity\nFigure 5: A removable discontinuity: function is continuous everywhere, except for one point\nDefinition of removable discontinuity\nRight-hand limit: lim f(x) means lim f(x) for x > x0.\n+\nx\nx0\n→\nx\nx\n→\nLeft-hand limit:\nlim f(x) means lim f(x) for x < x0.\nx-\nf(x) = lim f(x) but this is not f(x0), or if f(x0) is undefined, we say the disconti\nx\nx0\nx\n→\n→\nIf lim\n+\nx-\n→\nnuity is removable.\nx\nx\nx→\nFor example, sin(\nx\nx) is defined for x = 0. We will see later how to evaluate the limit as\nx → 0.\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\n2. Jump Discontinuity\nx0\nFigure 6: An example of a jump discontinuity\nlim for (x < x0) exists, and lim for (x > x0) also exists, but they are NOT equal.\n+\n-\nx0\nx\nx\nx\n→\n→\n3. Infinite Discontinuity\ny\nx\nFigure 7: An example of an infinite discontinuity: 1\nx\nRight-hand limit: lim\n= inf;\nLeft-hand limit: lim\nx→0+ x\nx→0- x = -inf\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\n4. Other (ugly) discontinuities\nFigure 8: An example of an ugly discontinuity: a function that oscillates a lot as it approaches the origin\nThis function doesn't even go to ±inf -- it doesn't make sense to say it goes to anything. For\nsomething like this, we say the limit does not exist.\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\nPicturing the derivative\nx\ny\nx\ny'\nFigure 9: Top: graph of f (x) = 1 and Bottom: graph of f 0(x) = - 1\nx\nx\nNotice that the graph of f(x) does NOT look like the graph of f 0(x)! (You might also notice\nthat f(x) is an odd function, while f 0(x) is an even function. The derivative of an odd function is\nalways even, and vice versa.)\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\nPumpkin Drop, Part II\nThis time, someone throws a pumpkin over the tallest building on campus.\nFigure 10: y = 400 - 16t2 , -5 ≤ t ≤ 5\nFigure 11: Top: graph of y(t) = 400 - 16t2 . Bottom: the derivative, y0(t)\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\nTwo Trig Limits\nNote: In the expressions below, θ is in radians-- NOT degrees!\nlim sin θ = 1;\nlim 1 - cos θ = 0\nθ\nθ\nθ\nθ\n→\n→\nHere is a geometric proof for the first limit:\nθ\narc\nlength\n= θ\nsinθ\nFigure 12: A circle of radius 1 with an arc of angle θ\nsin θ\narc\nlength\n= θ\nθ\nFigure 13: The sector in Fig. 12 as θ becomes very small\nImagine what happens to the picture as θ gets very small (see Fig. 13). As θ\n0, we see that\nsin θ\n→\n1.\nθ\n→\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\nWhat about the second limit involving cosine?\ncos θ\n1 - cos θ\narc\nlength\n= θ\nθ\nFigure 14: Same picture as Fig. 12 except that the horizontal distance between the edge of the triangle and the\nperimeter of the circle is marked\nFrom Fig. 15 we can see that as θ → 0, the length 1 - cos θ of the short segment gets much\nsmaller than the vertical distance θ along the arc. Hence, 1 - cos θ\n0.\nθ\n→\ncos θ\n1 - cos θ\narc\nlength\n= θ\nθ\nFigure 15: The sector in Fig. 14 as θ becomes very small\n\nLecture 2\nSept. 8, 2006\n18.01 Fall 2006\nWe end this lecture with a theorem that will help us to compute more derivatives next time.\nTheorem: Differentiable Implies Continuous.\nIf f is differentiable at x0, then f is continuous at x0.\nf(x) - f(x0)\nProof:\nxlim\nx0 (f(x) - f(x0)) =\nxlim\nx0\nx - x0\n(x - x0) = f 0(x0) · 0 = 0.\n→\n→\nRemember: you can never divide by zero! The first step was to multiply by x - x0 . It looks as\nx - x0\nif this is illegal because when x = x0, we are multiplying by\n. But when computing the limit as\nx → x0 we always assume x 6= x0. In other words x - x0 6= 0. So the proof is valid.\n\nLecture 3\nSept. 12, 2006\n18.01 Fall 2006\nLecture 3 : Derivatives of Products, Quotients,\nSine, and Cosine\nDerivative Formulas\nThere are two kinds of derivative formulas:\nd\nd\n1. Specific Examples:\nx n or\ndx\ndx\nx\n2. General Examples: (u + v)0 = u0 + v0 and (cu) = cu0 (where c is a constant)\nA notational convention we will use today is:\n(u + v)(x) = u(x) + v(x);\nuv(x) = u(x)v(x)\nProof of (u + v) = u0 + v0. (General)\nStart by using the definition of the derivative.\n(u + v)0(x)\n=\nlim (u + v)(x + Δx) - (u + v)(x)\nΔx\nΔx\n→\n=\nlim u(x + Δx) + v(x + Δx) - u(x) - v(x)\nΔx\nΔx\n→\n\n=\nlim\nu(x + Δx) - u(x) + v(x + Δx) - v(x)\nΔx\nΔx\nΔx\n→\n(u + v)0(x)\n=\nu0(x) + v0(x)\nFollow the same procedure to prove that (cu)0 = cu0.\nDerivatives of sin x and cos x. (Specific)\nLast time, we computed\nsin x\nlim\n=\nx\nx\n→\nd (sin x) x=0\n=\nlim sin(0 + Δx) - sin(0) = lim sin(Δx) = 1\ndx\n|\nΔx→0\nΔx\nΔx→0\nΔx\nd (cos x) x=0\n=\nlim cos(0 + Δx) - cos(0) = lim cos(Δx) - 1 = 0\ndx\n|\nΔx→0\nΔx\nΔx→0\nΔx\nd\nd\nSo, we know the value of\nsin x and of\ncos x at x = 0. Let us find these for arbitrary x.\ndx\ndx\nd sin x = lim sin(x + Δx) - sin(x)\ndx\nΔx\nΔx\n→\n\nLecture 3\nSept. 12, 2006\n18.01 Fall 2006\nRecall:\nsin(a + b) = sin(a) cos(b) + sin(b) cos(a)\nSo,\nd sin x\n=\nlim sin x cos Δx + cos x sin Δx - sin(x)\ndx\nΔx\nΔx\n→\n\n=\nlim\nsin x(cos Δx - 1) + cos x sin Δx\nΔx\nΔx\nΔx\n→\n\n=\nlim sin x cos Δx - 1\n+ lim cos x sin Δx\nΔx\nΔx\nΔx\nΔx\n→\n→\nSince cos Δx - 1\n0 and that sin Δx\n1, the equation above simplifies to\nΔx\n→\nΔx\n→\nd sin x = cos x\ndx\nA similar calculation gives\nd cos x = - sin x\ndx\nProduct formula (General)\n(uv)0 = u0v + uv0\nProof:\n(uv)0 = lim (uv)(x + Δx) - (uv)(x) = lim u(x + Δx)v(x + Δx) - u(x)v(x)\nΔx\nΔx\nΔx\nΔx\n→\n→\nNow obviously,\nu(x + Δx)v(x) - u(x + Δx)v(x) = 0\nso adding that to the numerator won't change anything.\n(uv)0 = lim u(x + Δx)v(x) - u(x)v(x) + u(x + Δx)v(x + Δx) - u(x + Δx)v(x)\nΔx\nΔx\n→\nWe can re-arrange that expression to get\n(uv)0 = lim\nu(x + Δx) - u(x) v(x) + u(x + Δx) v(x + Δx) - v(x)\nΔx\nΔx\nΔx\n→\nRemember, the limit of a sum is the sum of the limits.\n\nlim u(x + Δx) - u(x) v(x) + lim\nu(x + Δx) v(x + Δx) - v(x)\nΔx\nΔx\nΔx\nΔx\n→\n→\n(uv)0 = u0(x)v(x) + u(x)v0(x)\nNote: we also used the fact that\nlim u(x + Δx) = u(x)\n(true because u is continuous)\nΔx\n→\nThis proof of the product rule assumes that u and v have derivatives, which implies both functions\nare continuous.\n\nLecture 3\nSept. 12, 2006\n18.01 Fall 2006\nu\n∆u\n∆v\nv\nFigure 1: A graphical \"proof\" of the product rule\nAn intuitive justification:\nWe want to find the difference in area between the large rectangle and the smaller, inner rectangle.\nThe inner (orange) rectangle has area uv. Define Δu, the change in u, by\nΔu = u(x + Δx) - u(x)\nWe also abbreviate u = u(x), so that u(x + Δx) = u + Δu, and, similarly, v(x + Δx) = v + Δv.\nTherefore the area of the largest rectangle is (u + Δu)(v + Δv).\nIf you let v increase and keep u constant, you add the area shaded in red. If you let u increase\nand keep v constant, you add the area shaded in yellow. The sum of areas of the red and yellow\nrectangles is:\n[u(v + Δv) - uv] + [v(u + Δu) - uv] = uΔv + vΔu\nIf Δu and Δv are small, then (Δu)(Δv) ≈ 0, that is, the area of the white rectangle is very\nsmall. Therefore the difference in area between the largest rectangle and the orange rectangle is\napproximately the same as the sum of areas of the red and yellow rectangles. Thus we have:\n[(u + Δu)(v + Δv) - uv] ≈ uΔv + vΔu\n(Divide by Δx and let Δx\n0 to finish the argument.)\n→\n\nLecture 3\nSept. 12, 2006\n18.01 Fall 2006\nQuotient formula (General)\nTo calculate the derivative of u/v, we use the notations Δu and Δv above. Thus,\nu(x + Δx)\nu(x)\n=\nu + Δu\nu\nv(x + Δx) - v(x)\nv + Δv - v\n=\n(u + Δu)v - u(v + Δv)\n(common denominator)\n(v + Δv)v\n=\n(Δu)v - u(Δv)\n(v + Δv)v\n(cancel uv - uv)\nHence,\nΔx\nu + Δu\nv + Δv - u\nv\n\n=\n( Δu\nΔx )v - u( Δv\nΔx )\n(v + Δv)v\n-→\nv( du\ndx ) - u( dv\ndx )\nv2\nas Δx → 0\nTherefore,\n( u\nv )0 = u0v - uv0\nv2\n.\n\nLecture 4\nSept. 14, 2006\n18.01 Fall 2006\nLecture 4 : Chain Rule, and Higher Derivatives\nChain Rule\nWe've got general procedures for differentiating expressions with addition, subtraction, and multi\nplication. What about composition?\nExample 1. y = f(x) = sin x, x = g(t) = t2 .\nSo, y = f(g(t)) = sin(t2). To find dy , write\ndt\nt = t0 + Δt\nt0 = t0\nx = x0 + Δx\nx0 = g(t0)\ny = y0 + Δy\ny0 = f(x0)\nΔy = Δy Δx\nΔt\nΔx · Δt\nAs Δt\n0, Δx\n0 too, because of continuity. So we get:\n→\n→\ndy\ndy dx\n=\nThe Chain Rule!\ndt\ndx dt ←\nIn the example, dx\ndt = 2t and dy\ndx = cos x.\nSo,\nd\ndt\n\nsin(t2)\n\n=\n( dy\ndx )( dx\ndt )\n=\n=\n(cos x)(2t)\n(2t)\n\ncos(t2)\n\nAnother notation for the chain rule\n\nd\ndt f(g(t)) = f 0(g(t))g0(t)\nor\nd\ndx f(g(x)) = f 0(g(x))g0(x)\nExample 1. (continued)\nComposition of functions f(x) = sin x and g(x) = x2\n(f\ng)(x)\n=\nf(g(x))\n=\nsin(x 2)\n*\n(g\nf)(x)\n=\ng(f(x))\n=\nsin2(x)\n*\nNote:\nf * g\n6=\ng * f.\nNot Commutative!\n\nLecture 4\nSept. 14, 2006\n18.01 Fall 2006\nx\ng\ng(x)\nf(g(x))\nf\nFigure 1: Composition of functions: f\ng(x) = f(g(x))\n*\nd\nExample 2.\ncos\n= ?\ndx\nx\nLet u = x\ndy\n=\ndy du\ndx\ndu dx\ndy\ndu\ndu\n=\n- sin(u);\ndx = - x2\n\nsin\ndy\nsin(u)\nx\n=\n= (- sin u) -1\n=\ndx\nx2\nx2\nx2\nd\nExample 3.\nx-n = ?\ndx\n\nn\nThere are two ways to proceed. x-n =\n, or x-n =\nx\nxn\n1. d\nx-n\n= d 1 n\n= n\n1 n-1 -1\n= -nx-(n-1)x-2 = -nx-n-1\ndx\ndx\nx\nx\nx2\n2. d\nx-n\n= d\n= nx n-1\n-1\n= -nx-n-1 (Think of xn as u)\ndx\ndx\nxn\nx2n\n\nLecture 4\nSept. 14, 2006\n18.01 Fall 2006\nHigher Derivatives\nHigher derivatives are derivatives of derivatives. For instance, if g = f 0, then h = g0 is the second\nderivative of f. We write h = (f 0)0 = f 00.\nNotations\nf 0(x)\nDf\ndf\ndx\nf 00(x)\nD2f\nd2f\ndx2\nf 000(x)\nD3f\nd3f\ndx3\nf (n)(x)\nDnf\ndnf\ndxn\nHigher derivatives are pretty straightforward --- just keep taking the derivative!\nn\nExample.\nDnx\n= ?\nStart small and look for a pattern.\nDx\n=\nD2 x 2\n=\nD(2x) = 2 ( = 1 2)\n·\nD3 x 3\n=\nD2(3x 2) = D(6x) = 6 ( = 1 2 3)\n·\n·\nD4 x 4\n=\nD3(4x 3) = D2(12x 2) = D(24x) = 24 ( = 1 2 3 4)\n·\n·\n·\nDn x n\n=\nn!\nwe guess, based on the pattern we're seeing here.\n←\nThe notation n! is called \"n factorial\" and defined by n! = n(n - 1)\n2 1\n· · · ·\nProof by Induction: We've already checked the base case (n = 1).\nn\nInduction step: Suppose we know Dnx\n= n! (nth case). Show it holds for the (n + 1)st case.\nDn+1 x n+1\n=\nDn Dxn+1\n= Dn ((n + 1)x n) = (n + 1)Dn x n = (n + 1)(n!)\nDn+1 x n+1\n=\n(n + 1)!\nProved!\n\nLecture 5\nSept. 15, 2006\n18.01 Fall 2006\nLecture 5 : Implicit Differentiation and Inverses\nImplicit Differentiation\nExample 1.\nd (x a) = ax a-1 .\ndx\nWe proved this by an explicit computation for a = 0, 1, 2, .... From this, we also got the formula for\na = -1, -2, .... Let us try to extend this formula to cover rational numbers, as well:\nm\nm\na =\n;\ny = x n\nwhere m and n are integers.\nn\nWe want to compute dy . We can say yn = xm\nso\nnyn-1 dy = mx m-1 . Solve for dy :\ndx\ndx\ndx\ndy = m xm-1\ndx\nn yn-1\n( m\nWe know that y = x\nn ) is a function of x.\ndy\n=\nm\nxm-1\ndx\nn\nyn-1\nm\nxm-1\n=\nn\n(xm/n)n-1\nm\nxm-1\n=\nn xm(n-1)/n\nm (m-1)- m(n-1)\n=\nx\nn\nn\nm\nn(m-1)-m(n-1)\n=\nx\nn\nn\nm\nnm-n-nm+m\n=\nx\nn\nn\nm\nm\nn\n=\nx n - n\nn\ndy\nm\nm\nSo,\n=\nx n - 1\ndx\nn\nThis is the same answer as we were hoping to get!\nExample 2.\nEquation of a circle with a radius of 1: x2 +y2 = 1 which we can write as y2 = 1-x2 .\nSo y = ±\n√\n1 - x2. Let us look at the positive case:\np\ny\n=\n+\n1 - x2 = (1 - x 2) 2\ndy\n=\n(1 - x 2)\n-\n(-2x) =\n-x\n= -x\ndx\n√\n1 - x2\ny\n\nLecture 5\nSept. 15, 2006\n18.01 Fall 2006\nNow, let's do the same thing, using implicit differentiation.\nx 2 + y 2\n=\nd\nd\nx 2 + y\n=\n(1) = 0\ndx\ndx\nd\nd\n(x 2) +\n(y 2)\n=\ndx\ndx\nApplying chain rule in the second term,\n2x + 2y dy\n=\ndx\n2y dy\n=\n-2x\ndx\ndy\n=\n-x\ndx\ny\nSame answer!\nExample 3.\ny3 + xy2 + 1 = 0. In this case, it's not easy to solve for y as a function of x. Instead,\nwe use implicit differentiation to find dy .\ndx\n3y 2 dy + y 2 + 2xy dy = 0\ndx\ndx\nWe can now solve for dy in terms of y and x.\ndx\ndy\ndx (3y 2 + 2xy)\n=\n-y 2\ndy\n=\n-y2\ndx\n3y2 + 2xy\nInverse Functions\nIf y = f(x) and g(y) = x, we call g the inverse function of f, f -1:\nx = g(y) = f -1(y)\nNow, let us use implicit differentiation to find the derivative of the inverse function.\ny\n=\nf(x)\nf -1(y)\n=\nx\nd\nd\n(f -1(y))\n=\n(x) = 1\ndx\ndx\nBy the chain rule:\nd\ndy\n(f -1(y))\n=\ndy\ndx\nand\nd\n(f -1(y))\n=\ndy\ndy\ndx\n\np\nLecture 5\nSept. 15, 2006\n18.01 Fall 2006\nSo, implicit differentiation makes it possible to find the derivative of the inverse function.\nExample. y = arctan(x)\ntan y\n=\nx\nd\ndx [tan(y)]\n=\ndx\ndx = 1\nd\ndy [tan(y)]\n\ncos2(y)\n\ndy\ndx\ndy\ndx\n=\n=\ndy\ndx\n=\ncos2(y) = cos2(arctan(x))\nThis form is messy. Let us use some geometry to simplify it.\nx\n(1+x2)1/2\ny\nFigure 1: Triangle with angles and lengths corresponding to those in the example illustrating differentiation using\nthe inverse function arctan\nIn this triangle, tan(y) = x so\narctan(x) = y\nThe Pythagorian theorem tells us the length of the hypotenuse:\nh =\n1 + x2\nFrom this, we can find\ncos(y) = √\n1 + x2\nFrom this, we get\n\ncos2(y) =\n=\n√\n1 + x2\n1 + x2\n\nLecture 5\nSept. 15, 2006\n18.01 Fall 2006\nSo,\ndy =\ndx\n1 + x2\nIn other words,\nd\narctan(x) =\ndx\n1 + x2\nGraphing an Inverse Function.\nSuppose y = f(x) and g(y) = f -1(y) = x. To graph g and f together we need to write g as a\nfunction of the variable x. If g(x) = y, then x = f(y), and what we have done is to trade the\nvariables x and y. This is illustrated in Fig. 2\nf -1(f(x)) = x\nf -1\nf(x) = x\nf(f -1(x)) = x\nf\nf\n*\n-1(x) = x\n*\nf(x)\ng(x)\na=f-1(b)\nb=f(a)\nx\ny\ny=x\nFigure 2: You can think about f -1 as the graph of f reflected about the line y = x\n\nLecture 6\nSept. 19, 2006\n18.01 Fall 2006\nLecture 6: Exponential and Log, Logarithmic\nDifferentiation, Hyperbolic Functions\nTaking the derivatives of exponentials and logarithms\nBackground\nWe always assume the base, a, is greater than 1.\na 0 = 1;\na 1 = a;\na 2 = a a;\n. . .\n·\na x1+x2\n=\na x1 a x2\n(a x1 )x2\n=\na x1 x2\nq\nq\na\np\n=\n√\nap\n(where p and q are integers)\nr\nTo define a for real numbers r, fill in by continuity.\nd\nToday's main task: find\na x\ndx\nWe can write\nd\nax+Δx\nx\na x = lim\n- a\ndx\nΔx\nΔx\n→\nWe can factor out the a x:\na\na\na\nlim\nx+Δx - ax\n= lim a x\nΔx - 1 = a x lim\nΔx - 1\nΔx\nΔx\nΔx\nΔx\nΔx\nΔx\n→\n→\n→\nLet's call\na\nM(a) ≡ lim\nΔx - 1\nΔx\nΔx\n→\nWe don't yet know what M(a) is, but we can say\nd a x = M(a)a x\ndx\nHere are two ways to describe M(a):\nd\nx\n1. Analytically M(a) = dx a\nat x = 0.\nIndeed, M(a) = lim a0+Δx - a0\n= d a x\nΔx\nΔx\ndx\n→\nx=0\n\nLecture 6\nSept. 19, 2006\n18.01 Fall 2006\nM(a)\n(slope of ax at x=0)\nax\nFigure 1: Geometric definition of M(a)\nx\n2. Geometrically, M(a) is the slope of the graph y = a\nat x = 0.\nThe trick to figuring out what M(a) is is to beg the question and define e as the number such\nthat M(e) = 1. Now can we be sure there is such a number e? First notice that as the base a\nx\nincreases, the graph a\ngets steeper. Next, we will estimate the slope M(a) for a = 2 and a = 4\ngeometrically. Look at the graph of 2x in Fig. 2. The secant line from (0, 1) to (1, 2) of the graph\ny = 2x has slope 1. Therefore, the slope of y = 2x at x = 0 is less: M(2) < 1 (see Fig. 2).\n1 1\nNext, look at the graph of 4x in Fig. 3. The secant line from (- 2 , 2) to (1, 0) on the graph of\ny = 4x has slope 1. Therefore, the slope of y = 4x at x = 0 is greater than M(4) > 1 (see Fig. 3).\nSomewhere in between 2 and 4 there is a base whose slope at x = 0 is 1.\n\nLecture 6\nSept. 19, 2006\n18.01 Fall 2006\ny=2x\nslope M(2)\nslope = 1 (1,2)\nsecant line\nFigure 2: Slope M(2) < 1\ny=4x\nsecant line\n(1,0)\n(-1/2, 1/2)\nslope M(4)\nFigure 3: Slope M(4) > 1\n\nLecture 6\nSept. 19, 2006\n18.01 Fall 2006\nThus we can define e to be the unique number such that\nM(e) = 1\nor, to put it another way,\nlim eh - 1 = 1\nh\nh\n→\nor, to put it still another way,\nd (e x) = 1 at x = 0\ndx\nd\nd\nWhat is\n(e x)?\nWe just defined M(e) = 1, and\n(e x) = M(e)e x . So\ndx\ndx\nd (e x) = e x\ndx\nNatural log (inverse function of ex)\nTo understand M(a) better, we study the natural log function ln(x). This function is defined as\nfollows:\nIf y = e x , then ln(y) = x\n(or)\nIf w = ln(x), then e x = w\nx\nNote that e\nis always positive, even if x is negative.\nRecall that ln(1) = 0;\nln(x) < 0 for 0 < x < 1;\nln(x) > 0 for x > 1. Recall also that\nln(x1x2) = ln x1 + ln x2\nLet us use implicit differentiation to find d ln(x).\nw = ln(x). We want to find dw .\ndx\ndx\ne w\n=\nx\nd (e w)\n=\nd (x)\ndx\ndx\nd (e w) dw\n=\ndw\ndx\ne w dw\n=\ndx\ndw\n=\n=\ndx\new\nx\nd\n(ln(x)) =\ndx\nx\n\nLecture 6\nSept. 19, 2006\n18.01 Fall 2006\nd\nFinally, what about\n(a x)?\ndx\nThere are two methods we can use:\nMethod 1: Write base e and use chain rule.\nRewrite a as eln(a). Then,\n\nx\na x = eln(a)\n= e x ln(a)\nThat looks like it might be tricky to differentiate. Let's work up to it:\nd e x\n=\ne x\ndx\nand by the chain rule,\nd e 3x\n=\n3e 3x\ndx\nRemember, ln(a) is just a constant number- not a variable! Therefore,\nd e(ln a)x\n=\n(ln a)e(ln a)x\ndx\nor\nd (a x) = ln(a) a x\ndx\n·\nRecall that\nd (a x) = M (a) a x\ndx\n·\nSo now we know the value of M(a):\nM(a) = ln(a).\nEven if we insist on starting with another base, like 10, the natural logarithm appears:\nd 10x = (ln 10)10x\ndx\nThe base e may seem strange at first. But, it comes up everywhere. After a while, you'll learn to\nappreciate just how natural it is.\nMethod 2: Logarithmic Differentiation.\nd\nd\nThe idea is to find\nf(x) by finding\nln(f(x)) instead. Sometimes this approach is easier. Let\ndx\ndx\nu = f(x).\n\nd\nd ln(u) du\ndu\nln(u) =\n=\ndx\ndu\ndx\nu\ndx\ndu\nSince u = f and\n= f 0, we can also write\ndx\nf 0\n(ln f)0 =\nor f 0 = f(ln f)0\nf\n\nLecture 6\nSept. 19, 2006\n18.01 Fall 2006\nx\nApply this to f(x) = a .\nd\nd\nd\nln f(x) = x ln a =\nln(f) =\nln(a x) =\n(x ln(a)) = ln(a).\n⇒ dx\ndx\ndx\n(Remember, ln(a) is a constant, not a variable.) Hence,\nd\nf 0\nd\nx\nx\n(ln f) = ln(a) =\n= ln(a)\n=\nf 0 = ln(a)f =\na = (ln a)a\ndx\n⇒ f\n⇒\n⇒ dx\nd\nExample 1.\n(x x) = ?\ndx\nWith variable (\"moving\") exponents, you should use either base e or logarithmic differentiation.\nIn this example, we will use the latter.\nf\n=\nx x\nln f\n=\nx ln x\n(ln f)0\n=\n1 (ln x) + x\n= ln(x) + 1\n·\nx\nf 0\n(ln f)0\n=\nf\nTherefore,\nf 0 = f(ln f)0 = x x (ln(x) + 1)\nIf you wanted to solve this using the base e approach, you would say f = ex ln x and differentiate\nit using the chain rule. It gets you the same answer, but requires a little more writing.\n\nk\nExample 2. Use logs to evaluate lim\n1 +\n.\nk→inf\nk\nBecause the exponent k changes, it is better to find the limit of the logarithm.\n\"\nk #\nlim ln\n1 +\nk→inf\nk\nWe know that\n\"\nk #\n\nln\n1 +\n= k ln 1 +\nk\nk\nThis expression has two competing parts, which balance: k →inf while ln 1 + k\n→ 0.\n\"\n1 k #\n\nln\n\n1 + k\nln(1 + h)\nln\n1 +\n= k ln\n1 +\n=\n=\n(with h =\n)\nk\nk\nh\nk\nk\nNext, because ln 1 = 0\n\"\nk #\nln\n1 + 1\n= ln(1 + h) - ln(1)\nk\nh\n\nLecture 6\nSept. 19, 2006\n18.01 Fall 2006\nTake the limit: h = k → 0 as k →inf, so that\nln(1 + h) - ln(1)\nd\n\nlim\n=\nln(x)\n= 1\nh\nh\ndx\nx=1\n→\nIn all,\n\nk\nlim ln 1 +\n= 1.\nk→inf\nk\n\nk\nWe have just found that ak = ln[ 1 + k\n] → 1 as k →inf.\n\nk\nIf bk =\n1 + k\n, then bk = e ak → e 1 as k →inf. In other words, we have evaluated the limit we\nwanted:\n\nk\nlim\n1 +\n= e\nk→inf\nk\nRemark 1. We never figured out what the exact numerical value of e was. Now we can use this\nlimit formula; k = 10 gives a pretty good approximation to the actual value of e.\nRemark 2. Logs are used in all sciences and even in finance. Think about the stock market. If I\nsay the market fell 50 points today, you'd need to know whether the market average before the drop\nwas 300 points or 10, 000. In other words, you care about the percent change, or the ratio of the\nchange to the starting value:\nf 0(t)\nd\n=\nln(f(t))\nf(t)\ndt\n\nLecture 7\nSept. 21, 2006\n18.01 Fall 2006\nLecture 7: Continuation and Exam Review\nHyperbolic Sine and Cosine\nHyperbolic sine (pronounced \"sinsh\"):\nsinh(x) = ex - e-x\nHyperbolic cosine (pronounced \"cosh\"):\nex + e-x\ncosh(x) =\nx\nx\nd sinh(x) = d\ne - e-x\n= e - (-e-x) = cosh(x)\ndx\ndx\nLikewise,\nd cosh(x) = sinh(x)\ndx\nd\n(Note that this is different from\ncos(x).)\ndx\nImportant identity:\ncosh2(x) - sinh2(x) = 1\nProof:\n\nx\ncosh2(x) - sinh2(x)\n=\nex +\ne-x\n-\ne -\ne-x\ncosh2(x) - sinh2(x)\n=\n4 e 2x + 2e x e-x + e-2x - 4 e 2x - 2 + e-2x = 4(2 + 2) = 1\nWhy are these functions called \"hyperbolic\"?\nLet u = cosh(x) and v = sinh(x), then\nu 2 - v 2 = 1\nwhich is the equation of a hyperbola.\nRegular trig functions are \"circular\" functions. If u = cos(x) and v = sin(x), then\nu 2 + v 2 = 1\nwhich is the equation of a circle.\n\nLecture 7\nSept. 21, 2006\n18.01 Fall 2006\nExam 1 Review\nGeneral Differentiation Formulas\n(u + v)0\n=\nu0 + v0\n(cu)0\n=\ncu0\n(uv)0\n=\nu0v + uv0\n(product rule)\nu 0\n=\nu0v - uv0\n(quotient rule)\nv\nv2\nd f(u(x))\n=\nf 0(u(x)) u0(x) (chain rule)\ndx\n·\nYou can remember the quotient rule by rewriting\nu 0\n= (uv-1)0\nv\nand applying the product rule and chain rule.\nImplicit differentiation\nLet's say you want to find y0 from an equation like\ny 3 + 3xy 2 = 8\nd\nInstead of solving for y and then taking its derivative, just take\nof the whole thing. In this\ndx\nexample,\n3y 2 y0 + 6xyy0 + 3y 2\n=\n(3y 2 + 6xy)y0\n=\n-3y 2\ny0\n=\n-3y2\n3y2 + 6xy\nNote that this formula for y0 involves both x and y. Implicit differentiation can be very useful for\ntaking the derivatives of inverse functions.\nFor instance,\ny = sin-1 x\nsin y = x\n⇒\nImplicit differentiation yields\n(cos y)y0 = 1\nand\ny0 =\n=\ncos y\n√\n1 - x2\n\nLecture 7\nSept. 21, 2006\n18.01 Fall 2006\nSpecific differentiation formulas\nYou will be responsible for knowing formulas for the derivatives and how to deduce these formulas\nn\nx\nfrom previous information: x , sin-1 x, tan-1 x, sin x, cos x, tan x, sec x, e , ln x .\nd\nFor example, let's calculate\nsec x:\ndx\nd\nd\n-(- sin x)\nsec x =\n=\n= tan x sec x\ndx\ndx cos x\ncos2 x\nd\nd\nYou may be asked to find\nsin x or\ncos x, using the following information:\ndx\ndx\nsin(h)\nlim\n=\nh\nh\n→\nlim cos(h) - 1\n=\nh\nh\n→\nRemember the definition of the derivative:\nd f(x) = lim f(x + Δx) - f(x)\ndx\nΔx\nΔx\n→\nTying up a loose end\nd\nHow to find\nx r, where r is a real (but not necessarily rational) number? All we have done so far\ndx\nis the case of rational numbers, using implicit differentiation. We can do this two ways:\n1st method: base e\nx\n=\ne ln x\nx r\n=\n\ne ln x r = e r ln x\nd\ndx x r\n=\nd\ndx e r ln x = e r ln x d\ndx (r ln x) = e r ln x r\nx\nd\ndx x r\n=\nx r r\nx\n\n= rx r-1\n2nd method: logarithmic differentiation\nf 0\n(ln f)0\n=\nf\nf\n=\nx r\nln f\n=\nr ln x\nr\n(ln f)0\n=\nx\nf 0 = f(ln f)0\n=\nx r\nr\n= rx r-1\nx\n\nLecture 7\nSept. 21, 2006\n18.01 Fall 2006\nFinally, in the first lecture I promised you that you'd learn to differentiate anything-- even\nsomething as complicated as\nd\nx tan-1 x\ne\ndx\nSo let's do it!\nd\nd\ne uv\n=\ne uv\n(uv) = e uv (u0v + uv0)\ndx\ndx\nSubstituting,\nd e x tan-1 x\n=\ne x tan-1 x tan-1 x + x\ndx\n1 + x2"
        },
        {
          "category": "Resource",
          "title": "Ses #1-7 Complete",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/c80e8c2fae48adb10be578dffac3bb60_unit1_sept08.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 1\n18.01 Fall 2006\nUnit 1: Derivatives\nA. What is a derivative?\n- Geometric interpretation\n- Physical interpretation\n- Important for any measurement (economics, political science, finance, physics, etc.)\nB. How to differentiate any function you know.\nd\nFor example:\ne x arctan x . We will discuss what a derivative is today. Figuring out how to\n-\ndx\ndifferentiate any function is the subject of the first two weeks of this course.\nLecture 1: Derivatives, Slope, Velocity, and Rate\nof Change\nGeometric Viewpoint on Derivatives\nTangent line\nSecant line\nf(x)\nP\nQ\nx0\nx0+∆x\ny\nFigure 1: A function with secant and tangent lines\nThe derivative is the slope of the line tangent to the graph of f(x). But what is a tangent line,\nexactly?\n\nLecture 1\n18.01 Fall 2006\n- It is NOT just a line that meets the graph at one point.\n- It is the limit of the secant line (a line drawn between two points on the graph) as the distance\nbetween the two points goes to zero.\nGeometric definition of the derivative:\nLimit of slopes of secant lines PQ as Q\nP (P fixed). The slope of PQ:\n→\nP\nQ\n(x0+∆x, f(x0+∆x))\n(x0, f(x0))\n∆x\n∆f\nSecant Line\nFigure 2: Geometric definition of the derivative\nlim Δf\n= lim f(x0 + Δx) - f(x0)\n=\nf 0(x0)\nΔx\n0 Δx\nΔx\nΔx\n| {z }\n→\n→|\n{z\n}\n\"difference quotient\"\n\"derivative of f at x0 \"\nExample 1. f(x) = x\nOne thing to keep in mind when working with derivatives: it may be tempting to plug in Δx = 0\nΔf\nright away. If you do this, however, you will always end up with\n= . You will always need to\nΔx\ndo some cancellation to get at the answer.\nΔf\n1 x0 - (x0 + Δx)\n\n= x0 +Δx - x0 =\n=\n-Δx\n=\n-1\nΔx\nΔx\nΔx\n(x0 + Δx)x0\nΔx (x0 + Δx)x0\n(x0 + Δx)x0\nTaking the limit as Δx\n0,\n→\nlim\n-1\n= -1\nΔx→0 (x0 + Δx)x0\nx2\n\nLecture 1\n18.01 Fall 2006\ny\nx\nx0\nFigure 3: Graph of x\nHence,\nf 0(x0) = -\nx0\nNotice that f 0(x0) is negative -- as is the slope of the tangent line on the graph above.\nFinding the tangent line.\nWrite the equation for the tangent line at the point (x0, y0) using the equation for a line, which you\nall learned in high school algebra:\ny - y0 = f 0(x0)(x - x0)\nPlug in y0 = f(x0) = 1 and f 0(x0) = -\n1 to get:\nx0\nx0\ny - x\n= -\nx2\n1(x - x0)\n\nLecture 1\n18.01 Fall 2006\ny\nx\nx0\nFigure 4: Graph of x\nJust for fun, let's compute the area of the triangle that the tangent line forms with the x- and\ny-axes (see the shaded region in Fig. 4).\nFirst calculate the x-intercept of this tangent line. The x-intercept is where y = 0. Plug y = 0\ninto the equation for this tangent line to get:\n0 - 1\n=\n-\n1(x - x0)\nx0\nx0\n-1\n-1\n=\nx +\nx0\nx0\nx0\nx\n=\nx0\nx0\nx\n=\nx 2\n0(\n) = 2x0\nx0\nSo, the x-intercept of this tangent line is at x = 2x0.\nNext we claim that the y-intercept is at y = 2y0. Since y =\nand x =\nare identical equations,\nx\ny\nthe graph is symmetric when x and y are exchanged. By symmetry, then, the y-intercept is at\ny = 2y0. If you don't trust reasoning with symmetry, you may follow the same chain of algebraic\nreasoning that we used in finding the x-intercept. (Remember, the y-intercept is where x = 0.)\nFinally,\nArea = (2y0)(2x0) = 2x0y0 = 2x0(\n) = 2 (see Fig. 5)\nx0\nCuriously, the area of the triangle is always 2, no matter where on the graph we draw the tangent\nline.\n\nLecture 1\n18.01 Fall 2006\ny\nx\nx0\n2x0\ny0\n2y0\nx-1\nFigure 5: Graph of x\nNotations\nCalculus, rather like English or any other language, was developed by several people. As a result,\njust as there are many ways to express the same thing, there are many notations for the derivative.\nSince y = f(x), it's natural to write\nΔy = Δf = f(x) - f(x0) = f(x0 + Δx) - f(x0)\nWe say \"Delta y\" or \"Delta f\" or the \"change in y\".\nIf we divide both sides by Δx = x - x0, we get two expressions for the difference quotient:\nΔy = Δf\nΔx\nΔx\nTaking the limit as Δx → 0, we get\nΔy\nΔx\n→\ndy\ndx (Leibniz' notation)\nΔf\nΔx\n→\nf 0(x0) (Newton's notation)\nWhen you use Leibniz' notation, you have to remember where you're evaluating the derivative\n-- in the example above, at x = x0.\nOther, equally valid notations for the derivative of a function f include\ndf , f 0, and Df\ndx\n\nLecture 1\n18.01 Fall 2006\nExample 2. f(x) = x n where n = 1, 2, 3...\nd\nWhat is\nx n?\ndx\nTo find it, plug y = f(x) into the definition of the difference quotient.\nn\nn\nΔy = (x0 + Δx)n - x0 = (x + Δx)n - x\nΔx\nΔx\nΔx\n(From here on, we replace x0 with x, so as to have less writing to do.) Since\n(x + Δx)n = (x + Δx)(x + Δx)...(x + Δx) n times\nWe can rewrite this as\n\nx n + n(Δx)x n-1 + O (Δx)2\nO(Δx)2 is shorthand for \"all of the terms with (Δx)2, (Δx)3, and so on up to (Δx)n.\" (This is part\nof what is known as the binomial theorem; see your textbook for details.)\nn\nn\nΔy = (x + Δx)n - x\n= xn + n(Δx)(xn-1) + O(Δx)2 - x\n= nx n-1 + O(Δx)\nΔx\nΔx\nΔx\nTake the limit:\nΔy\nlim\n= nx n-1\nΔx\n0 Δx\n→\nTherefore,\nd\nn\nx = nx n-1\ndx\nThis result extends to polynomials. For example,\nd\n(x 2 + 3x 10) = 2x + 30x\ndx\nPhysical Interpretation of Derivatives\nYou can think of the derivative as representing a rate of change (speed is one example of this).\nOn Halloween, MIT students have a tradition of dropping pumpkins from the roof of this building,\nwhich is about 400 feet high.\nThe equation of motion for objects near the earth's surface (which we will just accept for now)\nimplies that the height above the ground y of the pumpkin is:\ny = 400 - 16t2\nΔy\ndistance travelled\nThe average speed of the pumpkin (difference quotient) =\n=\nΔt\ntime elapsed\nWhen the pumpkin hits the ground, y = 0,\n400 - 16t2 = 0\n\nLecture 1\n18.01 Fall 2006\nSolve to find t = 5. Thus it takes 5 seconds for the pumpkin to reach the ground.\n400 ft\nAverage speed =\n= 80 ft/s\n5 sec\nA spectator is probably more interested in how fast the pumpkin is going when it slams into the\nground. To find the instantaneous velocity at t = 5, let's evaluate y0:\ny0 = -32t = (-32)(5) = -160 ft/s (about 110 mph)\ny0 is negative because the pumpkin's y-coordinate is decreasing: it is moving downward.\n\nLecture 2\n18.01 Fall 2006\nLecture 2: Limits, Continuity, and Trigonometric\nLimits\nMore about the \"rate of change\" interpretation of the\nderivative\ny = f(x)\ny\nx\n∆x\n∆y\nFigure 1: Graph of a generic function, with Δx and Δy marked on the graph\n3. T\ntemperature gradient\nΔy\nΔx\n→\ndy\ndx as Δx → 0\nAverage rate of change\n→\nInstantaneous rate of change\nExamples\n1. q = charge\ndq\ndt = electrical current\n2. s = distance\nds\ndt = speed\ndT\n= temperature\n=\ndx\n\nLecture 2\n18.01 Fall 2006\n4. Sensitivity of measurements: An example is carried out on Problem Set 1. In GPS, radio\nsignals give us h up to a certain measurement error (See Fig. 2 and Fig. 3). The question is\nΔL\nhow accurately can we measure L. To decide, we find\n. In other words, these variables are\nΔh\nrelated to each other. We want to find how a change in one variable affects the other variable.\nL\nh\ns\nsatellite\nyou\nFigure 2: The Global Positioning System Problem (GPS)\nh\ns\nL\nFigure 3: On problem set 1, you will look at this simplified \"flat earth\" model\n\nLecture 2\n18.01 Fall 2006\nLimits and Continuity\nEasy Limits\nx2 + x\n32 + 3\nlim\n=\n=\n= 3\nx→3 x + 1\n3 + 1\nWith an easy limit, you can get a meaningful answer just by plugging in the limiting value.\nRemember,\nlim Δf = lim f(x0 + Δx) - f(x0)\nx→x0 Δx\nx→x0\nΔx\nis never an easy limit, because the denominator Δx = 0 is not allowed. (The limit x\nx0 is\ncomputed under the implicit assumption that x =6\nx0.)\n→\nContinuity\nWe say f(x) is continuous at x0 when\nlim f(x) = f(x0)\nx\nx0\n→\nPictures\nx\ny\nFigure 4: Graph of the discontinuous function listed below\nx + 1\nx > 0\nf(x) =\n-x\nx ≥ 0\n\nLecture 2\n18.01 Fall 2006\nThis discontinuous function is seen in Fig. 4. For x > 0,\nlim f(x) = 1\nx\n→\nbut f(0) = 0. (One can also say, f is continuous from the left at 0, not the right.)\n1. Removable Discontinuity\nFigure 5: A removable discontinuity: function is continuous everywhere, except for one point\nDefinition of removable discontinuity\nRight-hand limit: lim f(x) means lim f(x) for x > x0.\n+\nx\nx0\n→\nx\nx\n→\nLeft-hand limit:\nlim f(x) means lim f(x) for x < x0.\nx-\nf(x) = lim f(x) but this is not f(x0), or if f(x0) is undefined, we say the disconti\nx\nx0\nx\n→\n→\nIf lim\n+\nx-\n→\nnuity is removable.\nx\nx\nx→\nFor example, sin(\nx\nx) is defined for x = 0. We will see later how to evaluate the limit as\nx → 0.\n\nLecture 2\n18.01 Fall 2006\n2. Jump Discontinuity\nx0\nFigure 6: An example of a jump discontinuity\nlim for (x < x0) exists, and lim for (x > x0) also exists, but they are NOT equal.\n+\n-\nx0\nx\nx\nx\n→\n→\n3. Infinite Discontinuity\ny\nx\nFigure 7: An example of an infinite discontinuity: 1\nx\nRight-hand limit: lim\n= inf;\nLeft-hand limit: lim\nx→0+ x\nx→0- x = -inf\n\nLecture 2\n18.01 Fall 2006\n4. Other (ugly) discontinuities\nFigure 8: An example of an ugly discontinuity: a function that oscillates a lot as it approaches the origin\nThis function doesn't even go to ±inf -- it doesn't make sense to say it goes to anything. For\nsomething like this, we say the limit does not exist.\n\nLecture 2\n18.01 Fall 2006\nPicturing the derivative\nx\ny\nx\ny'\nFigure 9: Top: graph of f (x) = 1 and Bottom: graph of f 0(x) = - 1\nx\nx\nNotice that the graph of f(x) does NOT look like the graph of f 0(x)! (You might also notice\nthat f(x) is an odd function, while f 0(x) is an even function. The derivative of an odd function is\nalways even, and vice versa.)\n\nLecture 2\n18.01 Fall 2006\nPumpkin Drop, Part II\nThis time, someone throws a pumpkin over the tallest building on campus.\nFigure 10: y = 400 - 16t2 , -5 ≤ t ≤ 5\nFigure 11: Top: graph of y(t) = 400 - 16t2 . Bottom: the derivative, y0(t)\n\nLecture 2\n18.01 Fall 2006\nTwo Trig Limits\nNote: In the expressions below, θ is in radians-- NOT degrees!\nlim sin θ = 1;\nlim 1 - cos θ = 0\nθ\nθ\nθ\nθ\n→\n→\nHere is a geometric proof for the first limit:\nθ\narc\nlength\n= θ\nsinθ\nFigure 12: A circle of radius 1 with an arc of angle θ\nsin θ\narc\nlength\n= θ\nθ\nFigure 13: The sector in Fig. 12 as θ becomes very small\nImagine what happens to the picture as θ gets very small (see Fig. 13). As θ\n0, we see that\nsin θ\n→\n1.\nθ\n→\n\nLecture 2\n18.01 Fall 2006\nWhat about the second limit involving cosine?\ncos θ\n1 - cos θ\narc\nlength\n= θ\nθ\nFigure 14: Same picture as Fig. 12 except that the horizontal distance between the edge of the triangle and the\nperimeter of the circle is marked\nFrom Fig. 15 we can see that as θ → 0, the length 1 - cos θ of the short segment gets much\nsmaller than the vertical distance θ along the arc. Hence, 1 - cos θ\n0.\nθ\n→\ncos θ\n1 - cos θ\narc\nlength\n= θ\nθ\nFigure 15: The sector in Fig. 14 as θ becomes very small\n\nLecture 2\n18.01 Fall 2006\nWe end this lecture with a theorem that will help us to compute more derivatives next time.\nTheorem: Differentiable Implies Continuous.\nIf f is differentiable at x0, then f is continuous at x0.\nf(x) - f(x0)\nProof:\nxlim\nx0 (f(x) - f(x0)) =\nxlim\nx0\nx - x0\n(x - x0) = f 0(x0) · 0 = 0.\n→\n→\nRemember: you can never divide by zero! The first step was to multiply by x - x0 . It looks as\nx - x0\nif this is illegal because when x = x0, we are multiplying by\n. But when computing the limit as\nx → x0 we always assume x 6= x0. In other words x - x0 6= 0. So the proof is valid.\n\nLecture 3\n18.01 Fall 2006\nLecture 3\n\nDerivatives of Products, Quotients, Sine, and\nCosine\nDerivative Formulas\nThere are two kinds of derivative formulas:\nd\nd\n1. Specific Examples:\nx n or\ndx\ndx\nx\n2. General Examples: (u + v)0 = u0 + v0 and (cu) = cu0 (where c is a constant)\nA notational convention we will use today is:\n(u + v)(x) = u(x) + v(x);\nuv(x) = u(x)v(x)\nProof of (u + v) = u0 + v0. (General)\nStart by using the definition of the derivative.\n(u + v)0(x)\n=\nlim (u + v)(x + Δx) - (u + v)(x)\nΔx\nΔx\n→\n=\nlim u(x + Δx) + v(x + Δx) - u(x) - v(x)\nΔx\nΔx\n→\n\n=\nlim\nu(x + Δx) - u(x) + v(x + Δx) - v(x)\nΔx\nΔx\nΔx\n→\n(u + v)0(x)\n=\nu0(x) + v0(x)\nFollow the same procedure to prove that (cu)0 = cu0.\nDerivatives of sin x and cos x. (Specific)\nLast time, we computed\nsin x\nlim\n=\nx→0\nx\nd (sin x) x=0\n=\nlim sin(0 + Δx) - sin(0) = lim sin(Δx) = 1\ndx\n|\nΔx→0\nΔx\nΔx→0\nΔx\nd (cos x) x=0\n=\nlim cos(0 + Δx) - cos(0) = lim cos(Δx) - 1 = 0\ndx\n|\nΔx→0\nΔx\nΔx→0\nΔx\nd\nd\nSo, we know the value of\nsin x and of\ncos x at x = 0. Let us find these for arbitrary x.\ndx\ndx\nd sin x = lim sin(x + Δx) - sin(x)\ndx\nΔx\nΔx\n→\n\nLecture 3\n18.01 Fall 2006\nRecall:\nsin(a + b) = sin(a) cos(b) + sin(b) cos(a)\nSo,\nd sin x\n=\nlim sin x cos Δx + cos x sin Δx - sin(x)\ndx\nΔx\nΔx\n→\n\n=\nlim\nsin x(cos Δx - 1) + cos x sin Δx\nΔx\nΔx\nΔx\n→\n\n=\nlim sin x cos Δx - 1\n+ lim cos x sin Δx\nΔx\nΔx\nΔx\nΔx\n→\n→\nSince cos Δx - 1\n0 and that sin Δx\n1, the equation above simplifies to\nΔx\n→\nΔx\n→\nd sin x = cos x\ndx\nA similar calculation gives\nd cos x = - sin x\ndx\nProduct formula (General)\n(uv)0 = u0v + uv0\nProof:\n(uv)0 = lim (uv)(x + Δx) - (uv)(x) = lim u(x + Δx)v(x + Δx) - u(x)v(x)\nΔx\nΔx\nΔx\nΔx\n→\n→\nNow obviously,\nu(x + Δx)v(x) - u(x + Δx)v(x) = 0\nso adding that to the numerator won't change anything.\n(uv)0 = lim u(x + Δx)v(x) - u(x)v(x) + u(x + Δx)v(x + Δx) - u(x + Δx)v(x)\nΔx\nΔx\n→\nWe can re-arrange that expression to get\n(uv)0 = lim\nu(x + Δx) - u(x) v(x) + u(x + Δx) v(x + Δx) - v(x)\nΔx\nΔx\nΔx\n→\nRemember, the limit of a sum is the sum of the limits.\n\nlim u(x + Δx) - u(x) v(x) + lim\nu(x + Δx) v(x + Δx) - v(x)\nΔx\nΔx\nΔx\nΔx\n→\n→\n(uv)0 = u0(x)v(x) + u(x)v0(x)\nNote: we also used the fact that\nlim u(x + Δx) = u(x)\n(true because u is continuous)\nΔx\n→\nThis proof of the product rule assumes that u and v have derivatives, which implies both functions\nare continuous.\n\nLecture 3\n18.01 Fall 2006\nu\n∆u\n∆v\nv\nFigure 1: A graphical \"proof\" of the product rule\nAn intuitive justification:\nWe want to find the difference in area between the large rectangle and the smaller, inner rectangle.\nThe inner (orange) rectangle has area uv. Define Δu, the change in u, by\nΔu = u(x + Δx) - u(x)\nWe also abbreviate u = u(x), so that u(x + Δx) = u + Δu, and, similarly, v(x + Δx) = v + Δv.\nTherefore the area of the largest rectangle is (u + Δu)(v + Δv).\nIf you let v increase and keep u constant, you add the area shaded in red. If you let u increase\nand keep v constant, you add the area shaded in yellow. The sum of areas of the red and yellow\nrectangles is:\n[u(v + Δv) - uv] + [v(u + Δu) - uv] = uΔv + vΔu\nIf Δu and Δv are small, then (Δu)(Δv) ≈ 0, that is, the area of the white rectangle is very\nsmall. Therefore the difference in area between the largest rectangle and the orange rectangle is\napproximately the same as the sum of areas of the red and yellow rectangles. Thus we have:\n[(u + Δu)(v + Δv) - uv] ≈ uΔv + vΔu\n(Divide by Δx and let Δx\n0 to finish the argument.)\n→\n\nLecture 3\n18.01 Fall 2006\nQuotient formula (General)\nTo calculate the derivative of u/v, we use the notations Δu and Δv above. Thus,\nu(x + Δx)\nu(x)\n=\nu + Δu\nu\nv(x + Δx) - v(x)\nv + Δv - v\n=\n(u + Δu)v - u(v + Δv)\n(common denominator)\n(v + Δv)v\n=\n(Δu)v - u(Δv)\n(v + Δv)v\n(cancel uv - uv)\nHence,\nΔx\nu + Δu\nv + Δv - u\nv\n\n=\n( Δu\nΔx )v - u( Δv\nΔx )\n(v + Δv)v\n-→\nv( du\ndx ) - u( dv\ndx )\nv2\nas Δx → 0\nTherefore,\n( u\nv )0 = u0v - uv0\nv2\n.\n\nLecture 4\nSept. 14, 2006\n18.01 Fall 2006\nLecture 4\n\nChain\n\nRule, and Higher\n\nDerivatives\nChain Rule\nWe've got general procedures for differentiating expressions with addition, subtraction, and multi\nplication. What about composition?\nExample 1. y = f(x) = sin x, x = g(t) = t2 .\nSo, y = f(g(t)) = sin(t2). To find dy , write\ndt\nt = t0 + Δt\nt0 = t0\nx = x0 + Δx\nx0 = g(t0)\ny = y0 + Δy\ny0 = f(x0)\nΔy = Δy Δx\nΔt\nΔx · Δt\nAs Δt\n0, Δx\n0 too, because of continuity. So we get:\n→\n→\ndy\ndy dx\n=\nThe Chain Rule!\ndt\ndx dt ←\nIn the example, dx\ndt = 2t and dy\ndx = cos x.\nSo,\nd\ndt\n\nsin(t2)\n\n=\n( dy\ndx )( dx\ndt )\n=\n=\n(cos x)(2t)\n(2t)\n\ncos(t2)\n\nAnother notation for the chain rule\n\nd\ndt f(g(t)) = f 0(g(t))g0(t)\nor\nd\ndx f(g(x)) = f 0(g(x))g0(x)\nExample 1. (continued)\nComposition of functions f(x) = sin x and g(x) = x2\n(f\ng)(x)\n=\nf(g(x))\n=\nsin(x 2)\n*\n(g\nf)(x)\n=\ng(f(x))\n=\nsin2(x)\n*\nNote:\nf * g\n6=\ng * f.\nNot Commutative!\n\nLecture 4\nSept. 14, 2006\n18.01 Fall 2006\nx\ng\ng(x)\nf(g(x))\nf\nFigure 1: Composition of functions: f\ng(x) = f(g(x))\n*\nd\nExample 2.\ncos\n= ?\ndx\nx\nLet u = x\ndy\n=\ndy du\ndx\ndu dx\ndy\ndu\ndu\n=\n- sin(u);\ndx = - x2\n\nsin\ndy\nsin(u)\nx\n=\n= (- sin u) -1\n=\ndx\nx2\nx2\nx2\nd\nExample 3.\nx-n = ?\ndx\n\nn\nThere are two ways to proceed. x-n =\n, or x-n =\nx\nxn\n1. d\nx-n\n= d 1 n\n= n\n1 n-1 -1\n= -nx-(n-1)x-2 = -nx-n-1\ndx\ndx\nx\nx\nx2\n2. d\nx-n\n= d\n= nx n-1\n-1\n= -nx-n-1 (Think of xn as u)\ndx\ndx\nxn\nx2n\n\nLecture 4\nSept. 14, 2006\n18.01 Fall 2006\nHigher Derivatives\nHigher derivatives are derivatives of derivatives. For instance, if g = f 0, then h = g0 is the second\nderivative of f. We write h = (f 0)0 = f 00.\nNotations\nf 0(x)\nf 00(x)\nf 000(x)\nf (n)(x)\nDf\nD2f\nD3f\nDnf\ndf\ndx\nd2f\ndx2\nd3f\ndx3\ndnf\ndxn\nHigher derivatives are pretty straightforward --- just keep taking the derivative!\nn\nExample.\nDnx\n= ?\nStart small and look for a pattern.\nDx\n=\nD2 x 2\n=\nD(2x) = 2 ( = 1 2)\n·\nD3 x 3\n=\nD2(3x 2) = D(6x) = 6 ( = 1 2 3)\n·\n·\nD4 x 4\n=\nD3(4x 3) = D2(12x 2) = D(24x) = 24 ( = 1 2 3 4)\n·\n·\n·\nDn x n\n=\nn!\nwe guess, based on the pattern we're seeing here.\n←\nThe notation n! is called \"n factorial\" and defined by n! = n(n - 1)\n2 1\n· · · ·\nProof by Induction: We've already checked the base case (n = 1).\nn\nInduction step: Suppose we know Dnx\n= n! (nth case). Show it holds for the (n + 1)st case.\nDn+1 x n+1\n=\nDn Dxn+1\n= Dn ((n + 1)x n) = (n + 1)Dn x n = (n + 1)(n!)\nDn+1 x n+1\n=\n(n + 1)!\nProved!\n\nLecture 5\n18.01 Fall 2006\nLecture\n\nImplicit\n\nDifferentiation and Inverses\nImplicit Differentiation\nd\nExample 1.\n(x a) = ax a-1 .\ndx\nWe proved this by an explicit computation for a = 0, 1, 2, .... From this, we also got the formula for\na = -1, -2, .... Let us try to extend this formula to cover rational numbers, as well:\nm\nm\na =\n;\ny = x n\nwhere m and n are integers.\nn\nWe want to compute dy . We can say yn = xm\nso\nnyn-1 dy = mx m-1 . Solve for dy :\ndx\ndx\ndx\ndy = m xm-1\ndx\nn yn-1\n( m\nWe know that y = x\nn ) is a function of x.\ndy\n=\nm\nxm-1\ndx\nn\nyn-1\nm\nxm-1\n=\nn\n(xm/n)n-1\nm\nxm-1\n=\nn xm(n-1)/n\n=\nx(m-1)- m(n\nn\n-1)\nm\nn\nm\nn(m-1)-m(n-1)\n=\nx\nn\nn\nm\nnm-n-nm+m\n=\nx\nn\nn\nm\nm\nn\n=\nx n - n\nn\ndy\nm\nm\nSo,\n=\nx n - 1\ndx\nn\nThis is the same answer as we were hoping to get!\nExample 2.\nEquation of a circle with a radius of 1: x2 +y2 = 1 which we can write as y2 = 1-x2 .\nSo y = ±\n√\n1 - x2. Let us look at the positive case:\np\ny\n=\n+\n1 - x2 = (1 - x 2) 2\ndy\n=\n(1 - x 2)\n-\n(-2x) =\n-x\n= -x\ndx\n√\n1 - x2\ny\n\nLecture 5\n18.01 Fall 2006\nNow, let's do the same thing, using implicit differentiation.\nx 2 + y 2\n=\nd\nd\nx 2 + y\n=\n(1) = 0\ndx\ndx\nd\nd\n(x 2) +\n(y 2)\n=\ndx\ndx\nApplying chain rule in the second term,\n2x + 2y dy\n=\ndx\n2y dy\n=\n-2x\ndx\ndy\n=\n-x\ndx\ny\nSame answer!\nExample 3.\ny3 + xy2 + 1 = 0. In this case, it's not easy to solve for y as a function of x. Instead,\nwe use implicit differentiation to find dy .\ndx\n3y 2 dy + y 2 + 2xy dy = 0\ndx\ndx\nWe can now solve for dy in terms of y and x.\ndx\ndy\ndx (3y 2 + 2xy)\n=\n-y 2\ndy\n=\n-y2\ndx\n3y2 + 2xy\nInverse Functions\nIf y = f(x) and g(y) = x, we call g the inverse function of f, f -1:\nx = g(y) = f -1(y)\nNow, let us use implicit differentiation to find the derivative of the inverse function.\ny\n=\nf(x)\nf -1(y)\n=\nx\nd\nd\n(f -1(y))\n=\n(x) = 1\ndx\ndx\nBy the chain rule:\nd\ndy\n(f -1(y))\n=\ndy\ndx\nand\nd\n(f -1(y))\n=\ndy\ndy\ndx\n\np\nLecture 5\n18.01 Fall 2006\nSo, implicit differentiation makes it possible to find the derivative of the inverse function.\nExample. y = arctan(x)\ntan y\n=\nx\nd\ndx [tan(y)]\n=\ndx\ndx = 1\nd\ndy [tan(y)]\n\ncos2(y)\n\ndy\ndx\ndy\ndx\n=\n=\ndy\ndx\n=\ncos2(y) = cos2(arctan(x))\nThis form is messy. Let us use some geometry to simplify it.\nx\n(1+x2)1/2\ny\nFigure 1: Triangle with angles and lengths corresponding to those in the example illustrating differentiation using\nthe inverse function arctan\nIn this triangle, tan(y) = x so\narctan(x) = y\nThe Pythagorian theorem tells us the length of the hypotenuse:\nh =\n1 + x2\nFrom this, we can find\ncos(y) = √\n1 + x2\nFrom this, we get\n\ncos2(y) =\n=\n√\n1 + x2\n1 + x2\n\nLecture 5\n18.01 Fall 2006\nSo,\ndy =\ndx\n1 + x2\nIn other words,\nd\narctan(x) =\ndx\n1 + x2\nGraphing an Inverse Function.\nSuppose y = f(x) and g(y) = f -1(y) = x. To graph g and f together we need to write g as a\nfunction of the variable x. If g(x) = y, then x = f(y), and what we have done is to trade the\nvariables x and y. This is illustrated in Fig. 2\nf -1(f(x)) = x\nf -1\nf(x) = x\n*\nf(f -1(x)) = x\nf\nf -1(x) = x\n*\nf(x)\ng(x)\na=f-1(b)\nb=f(a)\nx\ny\ny=x\nFigure 2: You can think about f -1 as the graph of f reflected about the line y = x\n\nLecture 6\n18.01 Fall 2006\nLecture 6: Exponential and Log, Logarithmic\nDifferentiation, Hyperbolic Functions\nTaking the derivatives of exponentials and logarithms\nBackground\nWe always assume the base, a, is greater than 1.\na 0 = 1;\na 1 = a;\na 2 = a a;\n. . .\n·\na x1+x2\n=\na x1 a x2\n(a x1 )x2\n=\na x1 x2\np\nq\nq\na\n=\n√\nap\n(where p and q are integers)\nr\nTo define a for real numbers r, fill in by continuity.\nd\nToday's main task: find\na x\ndx\nWe can write\nd\nax+Δx\nx\nx\na = lim\n- a\ndx\nΔx\nΔx\n→\nWe can factor out the a x:\nx+Δx\nx\nΔx\nΔx\nlim a\n- a\n= lim a x a\n- 1 = a x lim a\n- 1\nΔx\nΔx\nΔx\nΔx\nΔx\nΔx\n→\n→\n→\nLet's call\nM(a) ≡ lim aΔx - 1\nΔx\nΔx\n→\nWe don't yet know what M(a) is, but we can say\nd a x = M(a)a x\ndx\nHere are two ways to describe M(a):\nd\n1. Analytically M(a) =\na x at x = 0.\ndx\nIndeed, M(a) = lim a0+Δx - a0\n= d a x\nΔx\nΔx\ndx\n→\nx=0\n\nLecture 6\n18.01 Fall 2006\nM(a)\n(slope of ax at x=0)\nax\nFigure 1: Geometric definition of M(a)\nx\n2. Geometrically, M(a) is the slope of the graph y = a\nat x = 0.\nThe trick to figuring out what M(a) is is to beg the question and define e as the number such\nthat M(e) = 1. Now can we be sure there is such a number e? First notice that as the base a\nx\nincreases, the graph a\ngets steeper. Next, we will estimate the slope M(a) for a = 2 and a = 4\ngeometrically. Look at the graph of 2x in Fig. 2. The secant line from (0, 1) to (1, 2) of the graph\ny = 2x has slope 1. Therefore, the slope of y = 2x at x = 0 is less: M(2) < 1 (see Fig. 2).\n1 1\nNext, look at the graph of 4x in Fig. 3. The secant line from (- 2 , 2) to (1, 0) on the graph of\ny = 4x has slope 1. Therefore, the slope of y = 4x at x = 0 is greater than M(4) > 1 (see Fig. 3).\nSomewhere in between 2 and 4 there is a base whose slope at x = 0 is 1.\n\nLecture 6\n18.01 Fall 2006\ny=2x\nslope M(2)\nslope = 1 (1,2)\nsecant line\nFigure 2: Slope M(2) < 1\ny=4x\nsecant line\n(1,0)\n(-1/2, 1/2)\nslope M(4)\nFigure 3: Slope M(4) > 1\n\nLecture 6\n18.01 Fall 2006\nThus we can define e to be the unique number such that\nM(e) = 1\nor, to put it another way,\nlim eh - 1 = 1\nh\nh\n→\nor, to put it still another way,\nd (e x) = 1 at x = 0\ndx\nd\nd\nWhat is\n(e x)?\nWe just defined M(e) = 1, and\n(e x) = M(e)e x . So\ndx\ndx\nd (e x) = e x\ndx\nNatural log (inverse function of ex)\nTo understand M(a) better, we study the natural log function ln(x). This function is defined as\nfollows:\nIf y = e x , then ln(y) = x\n(or)\nIf w = ln(x), then e x = w\nx\nNote that e\nis always positive, even if x is negative.\nRecall that ln(1) = 0;\nln(x) < 0 for 0 < x < 1;\nln(x) > 0 for x > 1. Recall also that\nln(x1x2) = ln x1 + ln x2\nLet us use implicit differentiation to find d ln(x).\nw = ln(x). We want to find dw .\ndx\ndx\ne w\n=\nx\nd (e w)\n=\nd (x)\ndx\ndx\nd (e w) dw\n=\ndw\ndx\ne w dw\n=\ndx\ndw\n=\n=\ndx\new\nx\nd\n(ln(x)) =\ndx\nx\n\nLecture 6\n18.01 Fall 2006\nd\nFinally, what about\n(a x)?\ndx\nThere are two methods we can use:\nMethod 1: Write base e and use chain rule.\nRewrite a as eln(a). Then,\n\nx\na x = eln(a)\n= e x ln(a)\nThat looks like it might be tricky to differentiate. Let's work up to it:\nd e x\n=\ne x\ndx\nand by the chain rule,\nd e 3x\n=\n3e 3x\ndx\nRemember, ln(a) is just a constant number- not a variable! Therefore,\nd e(ln a)x\n=\n(ln a)e(ln a)x\ndx\nor\nd (a x) = ln(a) a x\ndx\n·\nRecall that\nd (a x) = M (a) a x\ndx\n·\nSo now we know the value of M(a):\nM(a) = ln(a).\nEven if we insist on starting with another base, like 10, the natural logarithm appears:\nd 10x = (ln 10)10x\ndx\nThe base e may seem strange at first. But, it comes up everywhere. After a while, you'll learn to\nappreciate just how natural it is.\nMethod 2: Logarithmic Differentiation.\nd\nd\nThe idea is to find\nf(x) by finding\nln(f(x)) instead. Sometimes this approach is easier. Let\ndx\ndx\nu = f(x).\n\nd\nd ln(u) du\ndu\nln(u) =\n=\ndx\ndu\ndx\nu\ndx\ndu\nSince u = f and\n= f 0, we can also write\ndx\nf 0\n(ln f)0 =\nor f 0 = f(ln f)0\nf\n\nLecture 6\n18.01 Fall 2006\nx\nApply this to f(x) = a .\nd\nd\nd\nln f(x) = x ln a =\nln(f) =\nln(a x) =\n(x ln(a)) = ln(a).\n⇒ dx\ndx\ndx\n(Remember, ln(a) is a constant, not a variable.) Hence,\nd\nf 0\nd\nx\nx\n(ln f) = ln(a) =\n= ln(a)\n=\nf 0 = ln(a)f =\na = (ln a)a\ndx\n⇒ f\n⇒\n⇒ dx\nd\nExample 1.\n(x x) = ?\ndx\nWith variable (\"moving\") exponents, you should use either base e or logarithmic differentiation.\nIn this example, we will use the latter.\nf\n=\nx x\nln f\n=\nx ln x\n(ln f)0\n=\n1 (ln x) + x\n= ln(x) + 1\n·\nx\nf 0\n(ln f)0\n=\nf\nTherefore,\nf 0 = f(ln f)0 = x x (ln(x) + 1)\nIf you wanted to solve this using the base e approach, you would say f = ex ln x and differentiate\nit using the chain rule. It gets you the same answer, but requires a little more writing.\n\nk\nExample 2. Use logs to evaluate lim\n1 +\n.\nk→inf\nk\nBecause the exponent k changes, it is better to find the limit of the logarithm.\n\"\nk #\nlim ln\n1 +\nk→inf\nk\nWe know that\n\"\nk #\n\nln\n1 +\n= k ln 1 +\nk\nk\nThis expression has two competing parts, which balance: k →inf while ln 1 + k\n→ 0.\n\"\n1 k #\n\nln\n\n1 + k\nln(1 + h)\nln\n1 +\n= k ln\n1 +\n=\n=\n(with h =\n)\nk\nk\nh\nk\nk\nNext, because ln 1 = 0\n\"\nk #\nln\n1 + 1\n= ln(1 + h) - ln(1)\nk\nh\n\nLecture 6\n18.01 Fall 2006\nTake the limit: h = k → 0 as k →inf, so that\nln(1 + h) - ln(1)\nd\n\nlim\n=\nln(x)\n= 1\nh\nh\ndx\nx=1\n→\nIn all,\n\nk\nlim ln 1 +\n= 1.\nk→inf\nk\n\nk\nWe have just found that ak = ln[ 1 + k\n] → 1 as k →inf.\n\nk\nIf bk =\n1 + k\n, then bk = e ak → e 1 as k →inf. In other words, we have evaluated the limit we\nwanted:\n\nk\nlim\n1 +\n= e\nk→inf\nk\nRemark 1. We never figured out what the exact numerical value of e was. Now we can use this\nlimit formula; k = 10 gives a pretty good approximation to the actual value of e.\nRemark 2. Logs are used in all sciences and even in finance. Think about the stock market. If I\nsay the market fell 50 points today, you'd need to know whether the market average before the drop\nwas 300 points or 10, 000. In other words, you care about the percent change, or the ratio of the\nchange to the starting value:\nf 0(t)\nd\n=\nln(f(t))\nf(t)\ndt\n\nLecture 7\n18.01 Fall 2006\nLecture 7: Continuation and Exam Review\nHyperbolic Sine and Cosine\nHyperbolic sine (pronounced \"sinsh\"):\nsinh(x) = ex - e-x\nHyperbolic cosine (pronounced \"cosh\"):\nex + e-x\ncosh(x) =\nx\nx\nd sinh(x) = d\ne - e-x\n= e - (-e-x) = cosh(x)\ndx\ndx\nLikewise,\nd cosh(x) = sinh(x)\ndx\nd\n(Note that this is different from\ncos(x).)\ndx\nImportant identity:\ncosh2(x) - sinh2(x) = 1\nProof:\n\nx\ncosh2(x) - sinh2(x)\n=\nex +\ne-x\n-\ne -\ne-x\ncosh2(x) - sinh2(x)\n=\n4 e 2x + 2e x e-x + e-2x - 4 e 2x - 2 + e-2x = 4(2 + 2) = 1\nWhy are these functions called \"hyperbolic\"?\nLet u = cosh(x) and v = sinh(x), then\nu 2 - v 2 = 1\nwhich is the equation of a hyperbola.\nRegular trig functions are \"circular\" functions. If u = cos(x) and v = sin(x), then\nu 2 + v 2 = 1\nwhich is the equation of a circle.\n\nLecture 7\n18.01 Fall 2006\nExam 1 Review\nGeneral Differentiation Formulas\n(u + v)0\n=\nu0 + v0\n(cu)0\n=\ncu0\n(uv)0\n=\nu0v + uv0\n(product rule)\nu 0\n=\nu0v - uv0\n(quotient rule)\nv\nv2\nd f(u(x))\n=\nf 0(u(x)) u0(x) (chain rule)\ndx\n·\nYou can remember the quotient rule by rewriting\nu 0\n= (uv-1)0\nv\nand applying the product rule and chain rule.\nImplicit differentiation\nLet's say you want to find y0 from an equation like\ny 3 + 3xy 2 = 8\nd\nInstead of solving for y and then taking its derivative, just take\nof the whole thing. In this\ndx\nexample,\n3y 2 y0 + 6xyy0 + 3y 2\n=\n(3y 2 + 6xy)y0\n=\n-3y 2\ny0\n=\n-3y2\n3y2 + 6xy\nNote that this formula for y0 involves both x and y. Implicit differentiation can be very useful for\ntaking the derivatives of inverse functions.\nFor instance,\ny = sin-1 x\nsin y = x\n⇒\nImplicit differentiation yields\n(cos y)y0 = 1\nand\ny0 =\n=\ncos y\n√\n1 - x2\n\nLecture 7\n18.01 Fall 2006\nSpecific differentiation formulas\nYou will be responsible for knowing formulas for the derivatives and how to deduce these formulas\nn\nx\nfrom previous information: x , sin-1 x, tan-1 x, sin x, cos x, tan x, sec x, e , ln x .\nd\nFor example, let's calculate\nsec x:\ndx\nd\nd\n-(- sin x)\nsec x =\n=\n= tan x sec x\ndx\ndx cos x\ncos2 x\nd\nd\nYou may be asked to find\nsin x or\ncos x, using the following information:\ndx\ndx\nsin(h)\nlim\n=\nh\nh\n→\nlim cos(h) - 1\n=\nh\nh\n→\nRemember the definition of the derivative:\nd f(x) = lim f(x + Δx) - f(x)\ndx\nΔx\nΔx\n→\nTying up a loose end\nd\nHow to find\nx r, where r is a real (but not necessarily rational) number? All we have done so far\ndx\nis the case of rational numbers, using implicit differentiation. We can do this two ways:\n1st method: base e\nx\n=\ne ln x\nx r\n=\n\ne ln x r = e r ln x\nd\ndx x r\n=\nd\ndx e r ln x = e r ln x d\ndx (r ln x) = e r ln x r\nx\nd\ndx x r\n=\nx r r\nx\n\n= rx r-1\n2nd method: logarithmic differentiation\nf 0\n(ln f)0\n=\nf\nf\n=\nx r\nln f\n=\nr ln x\nr\n(ln f)0\n=\nx\nf 0 = f(ln f)0\n=\nx r\nr\n= rx r-1\nx\n\nLecture 7\n18.01 Fall 2006\nFinally, in the first lecture I promised you that you'd learn to differentiate anything-- even\nsomething as complicated as\nd\nx tan-1 x\ne\ndx\nSo let's do it!\nd\nd\ne uv\n=\ne uv\n(uv) = e uv (u0v + uv0)\ndx\ndx\nSubstituting,\nd e x tan-1 x\n=\ne x tan-1 x tan-1 x + x\ndx\n1 + x2"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 2: Limits, Continuity, and Trigonometric Limits",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/acebd5cc8fe0315270d486685739d08f_lec2.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 2\n18.01 Fall 2006\nLecture 2: Limits, Continuity, and Trigonometric\nLimits\nMore about the \"rate of change\" interpretation of the\nderivative\ny = f(x)\ny\nx\n∆x\n∆y\nFigure 1: Graph of a generic function, with Δx and Δy marked on the graph\n3. T\ntemperature gradient\nΔy\nΔx\n→\ndy\ndx as Δx → 0\nAverage rate of change\n→\nInstantaneous rate of change\nExamples\n1. q = charge\ndq\ndt = electrical current\n2. s = distance\nds\ndt = speed\ndT\n= temperature\n=\ndx\n\nLecture 2\n18.01 Fall 2006\n4. Sensitivity of measurements: An example is carried out on Problem Set 1. In GPS, radio\nsignals give us h up to a certain measurement error (See Fig. 2 and Fig. 3). The question is\nΔL\nhow accurately can we measure L. To decide, we find\n. In other words, these variables are\nΔh\nrelated to each other. We want to find how a change in one variable affects the other variable.\nL\nh\ns\nsatellite\nyou\nFigure 2: The Global Positioning System Problem (GPS)\nh\ns\nL\nFigure 3: On problem set 1, you will look at this simplified \"flat earth\" model\n\nLecture 2\n18.01 Fall 2006\nLimits and Continuity\nEasy Limits\nx2 + x\n32 + 3\nlim\n=\n=\n= 3\nx→3 x + 1\n3 + 1\nWith an easy limit, you can get a meaningful answer just by plugging in the limiting value.\nRemember,\nlim Δf = lim f(x0 + Δx) - f(x0)\nx→x0 Δx\nx→x0\nΔx\nis never an easy limit, because the denominator Δx = 0 is not allowed. (The limit x\nx0 is\ncomputed under the implicit assumption that x =6\nx0.)\n→\nContinuity\nWe say f(x) is continuous at x0 when\nlim f(x) = f(x0)\nx\nx0\n→\nPictures\nx\ny\nFigure 4: Graph of the discontinuous function listed below\nx + 1\nx > 0\nf(x) =\n-x\nx ≥ 0\n\nLecture 2\n18.01 Fall 2006\nThis discontinuous function is seen in Fig. 4. For x > 0,\nlim f(x) = 1\nx\n→\nbut f(0) = 0. (One can also say, f is continuous from the left at 0, not the right.)\n1. Removable Discontinuity\nFigure 5: A removable discontinuity: function is continuous everywhere, except for one point\nDefinition of removable discontinuity\nRight-hand limit: lim f(x) means lim f(x) for x > x0.\n+\nx\nx0\n→\nx\nx\n→\nLeft-hand limit:\nlim f(x) means lim f(x) for x < x0.\nx-\nf(x) = lim f(x) but this is not f(x0), or if f(x0) is undefined, we say the disconti\nx\nx0\nx\n→\n→\nIf lim\n+\nx-\n→\nnuity is removable.\nx\nx\nx→\nFor example, sin(\nx\nx) is defined for x = 0. We will see later how to evaluate the limit as\nx → 0.\n\nLecture 2\n18.01 Fall 2006\n2. Jump Discontinuity\nx0\nFigure 6: An example of a jump discontinuity\nlim for (x < x0) exists, and lim for (x > x0) also exists, but they are NOT equal.\n+\n-\nx0\nx\nx\nx\n→\n→\n3. Infinite Discontinuity\ny\nx\nFigure 7: An example of an infinite discontinuity: 1\nx\nRight-hand limit: lim\n= inf;\nLeft-hand limit: lim\nx→0+ x\nx→0- x = -inf\n\nLecture 2\n18.01 Fall 2006\n4. Other (ugly) discontinuities\nFigure 8: An example of an ugly discontinuity: a function that oscillates a lot as it approaches the origin\nThis function doesn't even go to ±inf -- it doesn't make sense to say it goes to anything. For\nsomething like this, we say the limit does not exist.\n\nLecture 2\n18.01 Fall 2006\nPicturing the derivative\nx\ny\nx\ny'\nFigure 9: Top: graph of f (x) = 1 and Bottom: graph of f 0(x) = - 1\nx\nx\nNotice that the graph of f(x) does NOT look like the graph of f 0(x)! (You might also notice\nthat f(x) is an odd function, while f 0(x) is an even function. The derivative of an odd function is\nalways even, and vice versa.)\n\nLecture 2\n18.01 Fall 2006\nPumpkin Drop, Part II\nThis time, someone throws a pumpkin over the tallest building on campus.\nFigure 10: y = 400 - 16t2 , -5 ≤ t ≤ 5\nFigure 11: Top: graph of y(t) = 400 - 16t2 . Bottom: the derivative, y0(t)\n\nLecture 2\n18.01 Fall 2006\nTwo Trig Limits\nNote: In the expressions below, θ is in radians-- NOT degrees!\nlim sin θ = 1;\nlim 1 - cos θ = 0\nθ\nθ\nθ\nθ\n→\n→\nHere is a geometric proof for the first limit:\nθ\narc\nlength\n= θ\nsinθ\nFigure 12: A circle of radius 1 with an arc of angle θ\nsin θ\narc\nlength\n= θ\nθ\nFigure 13: The sector in Fig. 12 as θ becomes very small\nImagine what happens to the picture as θ gets very small (see Fig. 13). As θ\n0, we see that\nsin θ\n→\n1.\nθ\n→\n\nLecture 2\n18.01 Fall 2006\nWhat about the second limit involving cosine?\ncos θ\n1 - cos θ\narc\nlength\n= θ\nθ\nFigure 14: Same picture as Fig. 12 except that the horizontal distance between the edge of the triangle and the\nperimeter of the circle is marked\nFrom Fig. 15 we can see that as θ → 0, the length 1 - cos θ of the short segment gets much\nsmaller than the vertical distance θ along the arc. Hence, 1 - cos θ\n0.\nθ\n→\ncos θ\n1 - cos θ\narc\nlength\n= θ\nθ\nFigure 15: The sector in Fig. 14 as θ becomes very small\n\nLecture 2\n18.01 Fall 2006\nWe end this lecture with a theorem that will help us to compute more derivatives next time.\nTheorem: Differentiable Implies Continuous.\nIf f is differentiable at x0, then f is continuous at x0.\nf(x) - f(x0)\nProof:\nxlim\nx0 (f(x) - f(x0)) =\nxlim\nx0\nx - x0\n(x - x0) = f 0(x0) · 0 = 0.\n→\n→\nRemember: you can never divide by zero! The first step was to multiply by x - x0 . It looks as\nx - x0\nif this is illegal because when x = x0, we are multiplying by\n. But when computing the limit as\nx → x0 we always assume x 6= x0. In other words x - x0 6= 0. So the proof is valid."
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 3: Derivatives of Products, Quotients, Sine, and Cosine",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/23c2c1b1ab31c9f10745b18e7b0bf131_lec3.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 3\n18.01 Fall 2006\nLecture 3 (presented by Kobi Kremnizer):\nDerivatives of Products, Quotients, Sine, and\nCosine\nDerivative Formulas\nThere are two kinds of derivative formulas:\nd\n1. Specific Examples: -x\"\nor -\ndx\n2. General Examples: (u +v)' = u1+v1 and (cu) = cul (where c is a constant)\nA notational convention we will use today is:\nProof of (u+ v) = u' +v'. (General)\n$@rt by using the definition af &hederivative.\n(U +v)'(x)\n=\nlirn (U+U)(X+a x ) - (U+v)(x)\nAx-0\na x\nU(X+a x ) +V(X+AX)- U(X)- V(X)\n=\nlirn\nAx-0\na x\n=\nlim { u(x +Ax) - u(x) + v (x +Ax) - v(x)\nAX-O\nAx\nAx\nFollow the same procedure to prove that (cu)' = cu'.\nDerivatives of sin x and cos x. (Specific)\nLast time, we computed\nsinx\nlim - = 1\nx-0\nx\nd\nsin(0 +Ax) - sin(0)\nsin( Ax)\n-(sinx)\nIZEo\n=\nlirn\n= lim -= 1\ndx\nAX-o\nax\nAX-o\nax\nd\nCOS(O +a x ) - COS(O)\nC O S ( ~ X )i\n-\n- ( c o ~ x ) I ~ = ~=\nlim\n= lim\n= 0\ndx\nAX-0\nax\nAZ-O\nax\nd\nd\nSo, we know the value of -sin x and of -cos x at x = 0. Let us find these for arbitrary x.\ndx\ndx\nd\nsin(x +Ax) - sin(x)\n-sin x = lirn\ndx\nAX-0\nax\n\nLecture 3\n18.01 Fall 2006\nRecall:\nsin x cos Ax + cos x sin Ax -sin(%)\n-\nlirn\nAx-0\nAx\nsin x(cos Ax - 1)\ncos x sin Ax\n=\nlim [\nAX-o\nAx\nAx\n+\ncos Ax - 1\nsin Ax\n=\nAX-o\nAx\nlim C\nO\nS\nX\n(\n~\n)\nlim sins(\n) + a s t o\ncos Ax - 1\nsin Ax\nSince\n+0 and that --+ 1,the equation above simplifies to\nAx\nAx\nd\n-sinx\n= cosx\ndx\nA similar calculation gives\nd -cosx = -\ndx\nProduct formula (General)\n(uv)' = ulv + uvl\nProof:\n(uv)(X + AX)- (UV)(x)\nu(x + Ax)v(x + Ax) -u(x)v(x)\n(uv)' = lim\n= lim\nh x t ~\nAx\nA x t o\nAX\nNow obviously,\nso adding that to the numerator won't change anything.\nu(x + Ax)v(x) - u(x)v(x) + U(X + AX)V(X+ Ax) - u(x + Ax)v(x)\n(uv)' = lirn\nAx-0\nAX\nWe can re-arrange that expression to get\n(uv)' = lim\n) V(X)+ U(X + AX)\nAx-0\nRemember, the limit of a sum is the sum of the limits.\nu(x + Ax) - u(x)\nAx-0\nAx\nI >\n(uv)' =ul(x)v(x)+ u(x)vl(x)\nNote: we also used the fact that\nlirn u(x + Ax) = u(x)\n(true because u is continuous)\nAx-0\nThis proof of the product rule assumes that u and v have derivatives, which implies both functions\nare continuous.\n\nLecture 3\n18.01 Fall 2006\nFigure 1: A graphical \"proof\" of the product rule\nAn intuitive justification:\nWe want to find the difference in area between the large rectangle and the smaller, inner rectangle.\nThe inner (orange) rectangle has area uv. Define Au, the change in u, by\nAu = u(x+ Ax) - U ( X )\nWe also abbreviate u = u(x),so that u(x+ Ax) +\n= u + Au, and, similarly, v(x+ Ax) = v + Av.\nTherefore the area of the largest rectangle is (u\nAu) (v + Av).\nIf you let v increase and keep u constant, you add the area shaded in red. If you let u increase\nand keep v constant, you add the area shaded in yellow. The sum of areas of the red and yellow\nrectangles is:\n[u(v+ Av) - uv]+ [v(u+ Au) -uv]= uAv + vAu\nIf Au and Av are small, then (Au)(Av)FZ 0, that is, the area of the white rectangle is very\nsmall. Therefore the difference in area between the laxgest rectangle and the orange rectangle is\napproximately the same as the sum of areas of the red and yellow rectangles. Thus we have:\n[(u+ Au) (v + Av) - uv]w uAv + vAu\n(Divide by Ax and let Ax + 0 to finish the argument.)\n\n--\n-\n- -\nLecture 3\n18.01 Fall 2006\nQuotient formula (General)\nTo calculate the derivative of ulv, we use the notations Au and Av above. Thus,\nu(x + Ax)\nu(x)\nu + A u\nu\n-\nU(X+ Ax)\nV(X)\n-\nv + A v\nv\n--\n(\" +\n- u(v +\n(common denominator)\n(v +Av)v\n( A u ) ~- u(Av)\n-\n(v + Av)v\n(cancel uv - uv)\nHence,\nTherefore.\nu\nu'v - uv'\nu2"
        },
        {
          "category": "Exam",
          "title": "Exam 4 Review",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/0202fa3893049a6a502c4f7079eca657_exam4_review.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\ns\nLecture 32: Exam 4 Review\n18.01 Fall 2006\nExam 4 Review\n1. Trig substitution and trig integrals.\n2. Partial fractions.\n3. Integration by parts.\n4. Arc length and surface area of revolution\n5. Polar coordinates\n6. Area in polar coordinates.\nQuestions from the Students\n- Q: What do we need to know about parametric equations?\n- A: Just keep this formula in mind:\n\ndx\ndy\nds =\n+\ndt\ndt\nExample: You're given\nx(t) = t4\nand\ny(t) = 1 + t\nFind s (length).\np\nds =\n(4t3)2 + (1)2dt\nThen, integrate with respect to t.\n- Q: Can you quickly review how to do partial fractions?\n- A: When finding partial fractions, first check whether the degree of the numerator is greater\nthan or equal to the degree of the denominator. If so, you first need to do algebraic long-\ndivision. If not, then you can split into partial fractions.\nExample.\nx2 + x + 1\n(x - 1)2(x + 2)\nWe already know the form of the solution:\nx2 + x + 1\nA\nB\nC\n=\n+\n+\n(x - 1)2(x + 2)\nx - 1\n(x - 1)2\nx + 2\nThere are two coefficients that are easy to find: B and C. We can find these by the cover-up\nmethod.\n12 + 1 + 1\nB =\n=\n(x\n1)\n1 + 2\n→\n\nLecture 32: Exam 4 Review\n18.01 Fall 2006\nTo find C,\n(-2)2 - 2 + 1\nC =\n=\n(-2 - 1)2\n(x →-2)\nTo find A, one method is to plug in the easiest value of x other than the ones we already used\n(x = 1, -2). Usually, we use x = 0.\nA\n1/3\n=\n+\n+\n(-1)2(2)\n-1\n(-1)2\nand then solve to find A.\nThe Review Sheet handed out during lecture follows on the next page.\n\np\np\np\np\np\np\np\nLecture 32: Exam 4 Review\n18.01 Fall 2006\nExam 4 Review Handout\n1. Integrate by trigonometric substitution; evaluate the trigonometric integral and work\nbackwards to the original variable by evaluating trig(trig-1) using a right triangle:\na) a2 - x2 use x = a sin u, dx = a cos u du.\nb) a2 + x2 use x = a tan u, dx = a sec2 u du\nc) x2 - a2 use x = a sec u, dx = a sec u tan u du\n2. Integrate rational functions P/Q (ratio of polynomials) by the method of partial fractions:\nIf the degree of P is less than the degree of Q, then factor Q completely into linear and quadratic\nfactors, and write P/Q as a sum of simpler terms. For example,\n3x2 + 1\nA\nB1\nB2\nCx + D\n=\n+\n+\n+\n(x - 1)(x + 2)2(x2 + 9)\nx - 1\n(x + 2)\n(x + 2)2\nx2 + 9\nTerms such as D/(x2 + 9) can be integrated using the trigonometric substitution x = 3 tan u.\nThis method can be used to evaluate the integral of any rational function. In practice, the\nhard part turns out to be factoring the denominator! In recitation you encountered two other steps\nrequired to cover every case systematically, namely, completing the square1 and long division.2\n3. Integration by parts:\nZ b\nuv0dx = uv\nb\nZ b\na\n-\nu0vdx\na\na\nThis is used when u0v is simpler than uv0. (This is often the case if u0 is simpler than u.)\n4. Arclength: ds =\ndx2 + dy2. Depending on whether you want to integrate with respect to\nx, t or y this is written\nds =\n1 + (dy/dx)2 dx;\nds =\n(dx/dt)2 + (dy/dt)2 dt;\nds =\n(dx/dy)2 + 1 dy\n5. Surface area for a surface of revolution:\na) around the x-axis: 2πyds = 2πy 1 + (dy/dx)2 dx (requires a formula for y = y(x))\nb) around the y-axis: 2πxds = 2πx (dx/dy)2 + 1 dy (requires a formula for x = x(y))\n6. Polar coordinates: x = r cos θ, y = r sin θ (or, more rarely, r =\nx2 + y2, θ = tan-1(y/x))\na) Find the polar equation for a curve from its equation in (x, y) variables by substitution.\nb) Sketch curves given in polar coordinates and understand the range of the variable θ (often\nin preparation for integration).\n7. Area in polar coordinates:\nZ θ2 1 r 2dθ\nθ1\n(Pay attention to the range of θ to be sure that you are not double-counting regions or missing\nthem.)\n1For example, we rewrite the denominator x2 + 4x + 13 = (x + 2)2 + 9 = u2 + a2 with u = x + 2 and a = 3.\n2Long division is used when the degree of P is greater than or equal to the degree of Q. It expresses P (x)/Q(x) =\nP1(x) + R(x)/Q(x) with P1 a quotient polynomial (easy to integrate) and R a remainder. The key point is that the\nremainder R has degree less than Q, so R/Q can be split into partial fractions.\n\nZ\nZ\nLecture 32: Exam 4 Review\n18.01 Fall 2006\nThe following formulas will be printed with Exam 4\nsin2 x + cos2 x = 1;\nsec2 x = tan2 x + 1\nsin2 x = 1\n2 - 1\n2 cos 2x;\ncos2 x = 1\n2 + 1\n2 cos 2x\ncos 2x = cos2 x - sin2 x;\nsin 2x = 2 sin x cos x\nd\nd\nd\nd\ndx tan x = sec x;\ndx sec x = sec x tan x;\ndx tan-1 x = 1 + x2 ;\ndx sin-1 x = √\n1 - x2\ntan x dx = - ln(cos x) + c;\nsec x dx = ln(sec x + tan x) + c\nSee the next page for a review on integration of rational functions.\n\n|\n|\nZ\nZ\n\nZ\nZ\nLecture 32: Exam 4 Review\n18.01 Fall 2006\nPostscript: Systematic integration of rational functions\nFor a general rational function P/Q, the first step is to express P/Q as the sum of a polynomial\nand a ratio in which the numerator has smaller degree than the denominator.\nFor example,\nx3\n= x + 2 +\n3x - 2\nx2 - 2x + 1\nx2 - 2x + 1\n(To carry out this long division, do not factor the denominator Q(x) = x2 - 2x + 1, just leave it\nalone.) The quotient x + 2 is a polynomial and is easy to integrate. The remainder term\n3x - 2\n(x - 1)2\nhas a numerator 3x - 2 of degree 1 which is less than the degree 2 of the denominator (x - 1)2 .\nTherefore there is a partial fraction decomposition. In fact,\n3x - 2 = (3x - 3) + 1 =\n+\n(x - 1)2\n(x - 1)2\nx - 1\n(x - 1)2\nIn general, if P has degree n and Q has degree m, then long division gives\nP (x)\nR(x)\n= P1(x) +\nQ(x)\nQ(x)\nin which P1, the quotient in the long division, has degree n - m and R, the remainder in the long\ndivision, has degree at most m - 1.\nEvaluation of the \"simple\" pieces\nThe integral\nZ\n(x -\ndx\na)n = n\n-\n-\n1(x - a)1-n + c\nif n = 1 and ln x - a + c if n = 1. On the other hand the terms\nxdx\ndx\nand\n(Ax2 + Bx + C)n\n(Ax2 + Bx + C)n\nare handled by first completing the square:\nB2\nAx2 + Bx + C = A(x - B/2A)2 + C - 4A\nUsing the variable u =\n√\nA(x - B/2A) yields combinations of integrals of the form\nudu\ndu\nand\n(u2 + k2)n\n(u2 + k2)n\nThe first integral is handled by the substitution w = u2 + k2 , dw = 2udu. The second integral can\nbe worked out using the trigonometric substitution u = k tan θ du = k sec2 θdθ. This then leads to\nsec-tan integrals, and the actual computation for large values of n are long.\nThere are also other cases that we will not cover systematically. Examples are below:\n1. If Q(x) = (x - a)m(x - b)n, then the expression is\nA1\nA2\nAm\nB1\nB2\nBn\n+\n+\n+\n+\n+\n+\n+\nx - a\n(x - a)2\n· · ·\n(x - a)m\nx - b\n(x - b)2\n· · ·\n(x - b)n\n\nLecture 32: Exam 4 Review\n18.01 Fall 2006\n2. If there are quadratic factors like (Ax2 + Bx + C)p, one gets terms\na1x + b1\na2x + b2x\napx + bp\n+\n+\n+\nAx2 + Bx + C\n(Ax2 + Bx + C)2\n· · ·\n(Ax2 + Bx + C)p\nfor each such factor. (To integrate these quadratic pieces complete the square and make a\ntrigonometric substitution.)"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 4: Chain Rule, and Higher Derivatives",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/b8051c7c7a28e2cd03667de9dd4865fb_lec4.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 4\nSept. 14, 2006\n18.01 Fall 2006\nLecture 4\n\nChain\n\nRule, and Higher Derivatives\nChain Rule\nWe've got general procedures for differentiating expressions with addition, subtraction, and multi\nplication. What about composition?\nExample 1. y = f(x) = sin x, x = g(t) = t2 .\nSo, y = f(g(t)) = sin(t2). To find dy , write\ndt\nt = t0 + Δt\nt0 = t0\nx = x0 + Δx\nx0 = g(t0)\ny = y0 + Δy\ny0 = f(x0)\nΔy = Δy Δx\nΔt\nΔx · Δt\nAs Δt\n0, Δx\n0 too, because of continuity. So we get:\n→\n→\ndy\ndy dx\n=\nThe Chain Rule!\ndt\ndx dt ←\nIn the example, dx\ndt = 2t and dy\ndx = cos x.\nSo,\nd\ndt\n\nsin(t2)\n\n=\n( dy\ndx )( dx\ndt )\n=\n=\n(cos x)(2t)\n(2t)\n\ncos(t2)\n\nAnother notation for the chain rule\n\nd\ndt f(g(t)) = f 0(g(t))g0(t)\nor\nd\ndx f(g(x)) = f 0(g(x))g0(x)\nExample 1. (continued)\nComposition of functions f(x) = sin x and g(x) = x2\n(f\ng)(x)\n=\nf(g(x))\n=\nsin(x 2)\n*\n(g\nf)(x)\n=\ng(f(x))\n=\nsin2(x)\n*\nNote:\nf * g\n6=\ng * f.\nNot Commutative!\n\nLecture 4\nSept. 14, 2006\n18.01 Fall 2006\nx\ng\ng(x)\nf(g(x))\nf\nFigure 1: Composition of functions: f\ng(x) = f(g(x))\n*\nd\nExample 2.\ncos\n= ?\ndx\nx\nLet u = x\ndy\n=\ndy du\ndx\ndu dx\ndy\ndu\ndu\n=\n- sin(u);\ndx = - x2\n\nsin\ndy\nsin(u)\nx\n=\n= (- sin u) -1\n=\ndx\nx2\nx2\nx2\nd\nExample 3.\nx-n = ?\ndx\n\nn\nThere are two ways to proceed. x-n =\n, or x-n =\nx\nxn\n1. d\nx-n\n= d 1 n\n= n\n1 n-1 -1\n= -nx-(n-1)x-2 = -nx-n-1\ndx\ndx\nx\nx\nx2\n2. d\nx-n\n= d\n= nx n-1\n-1\n= -nx-n-1 (Think of xn as u)\ndx\ndx\nxn\nx2n\n\nLecture 4\nSept. 14, 2006\n18.01 Fall 2006\nHigher Derivatives\nHigher derivatives are derivatives of derivatives. For instance, if g = f 0, then h = g0 is the second\nderivative of f. We write h = (f 0)0 = f 00.\nNotations\nf 0(x)\nf 00(x)\nf 000(x)\nf (n)(x)\nDf\nD2f\nD3f\nDnf\ndf\ndx\nd2f\ndx2\nd3f\ndx3\ndnf\ndxn\nHigher derivatives are pretty straightforward --- just keep taking the derivative!\nn\nExample.\nDnx\n= ?\nStart small and look for a pattern.\nDx\n=\nD2 x 2\n=\nD(2x) = 2 ( = 1 2)\n·\nD3 x 3\n=\nD2(3x 2) = D(6x) = 6 ( = 1 2 3)\n·\n·\nD4 x 4\n=\nD3(4x 3) = D2(12x 2) = D(24x) = 24 ( = 1 2 3 4)\n·\n·\n·\nDn x n\n=\nn!\nwe guess, based on the pattern we're seeing here.\n←\nThe notation n! is called \"n factorial\" and defined by n! = n(n - 1)\n2 1\n· · · ·\nProof by Induction: We've already checked the base case (n = 1).\nn\nInduction step: Suppose we know Dnx\n= n! (nth case). Show it holds for the (n + 1)st case.\nDn+1 x n+1\n=\nDn Dxn+1\n= Dn ((n + 1)x n) = (n + 1)Dn x n = (n + 1)(n!)\nDn+1 x n+1\n=\n(n + 1)!\nProved!"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 5: Implicit Differentiation and Inverses",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/18d6a86a30a4bc046c5f7034d47587f1_lec5.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 5\n18.01 Fall 2006\nLecture\n\nImplicit\n\nDifferentiation\n\nand\n\nInverses\n\nImplicit Differentiation\nd\nExample 1.\n(x a) = ax a-1 .\ndx\nWe proved this by an explicit computation for a = 0, 1, 2, .... From this, we also got the formula for\na = -1, -2, .... Let us try to extend this formula to cover rational numbers, as well:\nm\nm\na =\n;\ny = x n\nwhere m and n are integers.\nn\nWe want to compute dy . We can say yn = xm\nso\nnyn-1 dy = mx m-1 . Solve for dy :\ndx\ndx\ndx\ndy = m xm-1\ndx\nn yn-1\n( m\nWe know that y = x\nn ) is a function of x.\ndy\n=\nm\nxm-1\ndx\nn\nyn-1\nm\nxm-1\n=\nn\n(xm/n)n-1\nm\nxm-1\n=\nn xm(n-1)/n\n=\nx(m-1)- m(n\nn\n-1)\nm\nn\nm\nn(m-1)-m(n-1)\n=\nx\nn\nn\nm\nnm-n-nm+m\n=\nx\nn\nn\nm\nm\nn\n=\nx n - n\nn\ndy\nm\nm\nSo,\n=\nx n - 1\ndx\nn\nThis is the same answer as we were hoping to get!\nExample 2.\nEquation of a circle with a radius of 1: x2 +y2 = 1 which we can write as y2 = 1-x2 .\nSo y = ±\n√\n1 - x2. Let us look at the positive case:\np\ny\n=\n+\n1 - x2 = (1 - x 2) 2\ndy\n=\n(1 - x 2)\n-\n(-2x) =\n-x\n= -x\ndx\n√\n1 - x2\ny\n\nLecture 5\n18.01 Fall 2006\nNow, let's do the same thing, using implicit differentiation.\nx 2 + y 2\n=\nd\nd\nx 2 + y\n=\n(1) = 0\ndx\ndx\nd\nd\n(x 2) +\n(y 2)\n=\ndx\ndx\nApplying chain rule in the second term,\n2x + 2y dy\n=\ndx\n2y dy\n=\n-2x\ndx\ndy\n=\n-x\ndx\ny\nSame answer!\nExample 3.\ny3 + xy2 + 1 = 0. In this case, it's not easy to solve for y as a function of x. Instead,\nwe use implicit differentiation to find dy .\ndx\n3y 2 dy + y 2 + 2xy dy = 0\ndx\ndx\nWe can now solve for dy in terms of y and x.\ndx\ndy\ndx (3y 2 + 2xy)\n=\n-y 2\ndy\n=\n-y2\ndx\n3y2 + 2xy\nInverse Functions\nIf y = f(x) and g(y) = x, we call g the inverse function of f, f -1:\nx = g(y) = f -1(y)\nNow, let us use implicit differentiation to find the derivative of the inverse function.\ny\n=\nf(x)\nf -1(y)\n=\nx\nd\nd\n(f -1(y))\n=\n(x) = 1\ndx\ndx\nBy the chain rule:\nd\ndy\n(f -1(y))\n=\ndy\ndx\nand\nd\n(f -1(y))\n=\ndy\ndy\ndx\n\np\nLecture 5\n18.01 Fall 2006\nSo, implicit differentiation makes it possible to find the derivative of the inverse function.\nExample. y = arctan(x)\ntan y\n=\nx\nd\ndx [tan(y)]\n=\ndx\ndx = 1\nd\ndy [tan(y)]\n\ncos2(y)\n\ndy\ndx\ndy\ndx\n=\n=\ndy\ndx\n=\ncos2(y) = cos2(arctan(x))\nThis form is messy. Let us use some geometry to simplify it.\nx\n(1+x2)1/2\ny\nFigure 1: Triangle with angles and lengths corresponding to those in the example illustrating differentiation using\nthe inverse function arctan\nIn this triangle, tan(y) = x so\narctan(x) = y\nThe Pythagorian theorem tells us the length of the hypotenuse:\nh =\n1 + x2\nFrom this, we can find\ncos(y) = √\n1 + x2\nFrom this, we get\n\ncos2(y) =\n=\n√\n1 + x2\n1 + x2\n\nLecture 5\n18.01 Fall 2006\nSo,\ndy =\ndx\n1 + x2\nIn other words,\nd\narctan(x) =\ndx\n1 + x2\nGraphing an Inverse Function.\nSuppose y = f(x) and g(y) = f -1(y) = x. To graph g and f together we need to write g as a\nfunction of the variable x. If g(x) = y, then x = f(y), and what we have done is to trade the\nvariables x and y. This is illustrated in Fig. 2\nf -1(f(x)) = x\nf -1\nf(x) = x\n*\nf(f -1(x)) = x\nf\nf -1(x) = x\n*\nf(x)\ng(x)\na=f-1(b)\nb=f(a)\nx\ny\ny=x\nFigure 2: You can think about f -1 as the graph of f reflected about the line y = x"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 6: Exponential and Log, Logarithmic Differentiation, Hyperbolic Functions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/f9af0e98490296c99d330faf47389507_lec6.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 6\n18.01 Fall 2006\nLecture 6: Exponential and Log, Logarithmic\nDifferentiation, Hyperbolic Functions\nTaking the derivatives of exponentials and logarithms\nBackground\nWe always assume the base, a, is greater than 1.\na 0 = 1;\na 1 = a;\na 2 = a a;\n. . .\n·\na x1+x2\n=\na x1 a x2\n(a x1 )x2\n=\na x1 x2\np\nq\nq\na\n=\n√\nap\n(where p and q are integers)\nr\nTo define a for real numbers r, fill in by continuity.\nd\nToday's main task: find\na x\ndx\nWe can write\nd\nax+Δx\nx\nx\na = lim\n- a\ndx\nΔx\nΔx\n→\nWe can factor out the a x:\nx+Δx\nx\nΔx\nΔx\nlim a\n- a\n= lim a x a\n- 1 = a x lim a\n- 1\nΔx\nΔx\nΔx\nΔx\nΔx\nΔx\n→\n→\n→\nLet's call\nM(a) ≡ lim aΔx - 1\nΔx\nΔx\n→\nWe don't yet know what M(a) is, but we can say\nd a x = M(a)a x\ndx\nHere are two ways to describe M(a):\nd\n1. Analytically M(a) =\na x at x = 0.\ndx\nIndeed, M(a) = lim a0+Δx - a0\n= d a x\nΔx\nΔx\ndx\n→\nx=0\n\nLecture 6\n18.01 Fall 2006\nM(a)\n(slope of ax at x=0)\nax\nFigure 1: Geometric definition of M(a)\nx\n2. Geometrically, M(a) is the slope of the graph y = a\nat x = 0.\nThe trick to figuring out what M(a) is is to beg the question and define e as the number such\nthat M(e) = 1. Now can we be sure there is such a number e? First notice that as the base a\nx\nincreases, the graph a\ngets steeper. Next, we will estimate the slope M(a) for a = 2 and a = 4\ngeometrically. Look at the graph of 2x in Fig. 2. The secant line from (0, 1) to (1, 2) of the graph\ny = 2x has slope 1. Therefore, the slope of y = 2x at x = 0 is less: M(2) < 1 (see Fig. 2).\n1 1\nNext, look at the graph of 4x in Fig. 3. The secant line from (- 2 , 2) to (1, 0) on the graph of\ny = 4x has slope 1. Therefore, the slope of y = 4x at x = 0 is greater than M(4) > 1 (see Fig. 3).\nSomewhere in between 2 and 4 there is a base whose slope at x = 0 is 1.\n\nLecture 6\n18.01 Fall 2006\ny=2x\nslope M(2)\nslope = 1 (1,2)\nsecant line\nFigure 2: Slope M(2) < 1\ny=4x\nsecant line\n(1,0)\n(-1/2, 1/2)\nslope M(4)\nFigure 3: Slope M(4) > 1\n\nLecture 6\n18.01 Fall 2006\nThus we can define e to be the unique number such that\nM(e) = 1\nor, to put it another way,\nlim eh - 1 = 1\nh\nh\n→\nor, to put it still another way,\nd (e x) = 1 at x = 0\ndx\nd\nd\nWhat is\n(e x)?\nWe just defined M(e) = 1, and\n(e x) = M(e)e x . So\ndx\ndx\nd (e x) = e x\ndx\nNatural log (inverse function of ex)\nTo understand M(a) better, we study the natural log function ln(x). This function is defined as\nfollows:\nIf y = e x , then ln(y) = x\n(or)\nIf w = ln(x), then e x = w\nx\nNote that e\nis always positive, even if x is negative.\nRecall that ln(1) = 0;\nln(x) < 0 for 0 < x < 1;\nln(x) > 0 for x > 1. Recall also that\nln(x1x2) = ln x1 + ln x2\nLet us use implicit differentiation to find d ln(x).\nw = ln(x). We want to find dw .\ndx\ndx\ne w\n=\nx\nd (e w)\n=\nd (x)\ndx\ndx\nd (e w) dw\n=\ndw\ndx\ne w dw\n=\ndx\ndw\n=\n=\ndx\new\nx\nd\n(ln(x)) =\ndx\nx\n\nLecture 6\n18.01 Fall 2006\nd\nFinally, what about\n(a x)?\ndx\nThere are two methods we can use:\nMethod 1: Write base e and use chain rule.\nRewrite a as eln(a). Then,\n\nx\na x = eln(a)\n= e x ln(a)\nThat looks like it might be tricky to differentiate. Let's work up to it:\nd e x\n=\ne x\ndx\nand by the chain rule,\nd e 3x\n=\n3e 3x\ndx\nRemember, ln(a) is just a constant number- not a variable! Therefore,\nd e(ln a)x\n=\n(ln a)e(ln a)x\ndx\nor\nd (a x) = ln(a) a x\ndx\n·\nRecall that\nd (a x) = M (a) a x\ndx\n·\nSo now we know the value of M(a):\nM(a) = ln(a).\nEven if we insist on starting with another base, like 10, the natural logarithm appears:\nd 10x = (ln 10)10x\ndx\nThe base e may seem strange at first. But, it comes up everywhere. After a while, you'll learn to\nappreciate just how natural it is.\nMethod 2: Logarithmic Differentiation.\nd\nd\nThe idea is to find\nf(x) by finding\nln(f(x)) instead. Sometimes this approach is easier. Let\ndx\ndx\nu = f(x).\n\nd\nd ln(u) du\ndu\nln(u) =\n=\ndx\ndu\ndx\nu\ndx\ndu\nSince u = f and\n= f 0, we can also write\ndx\nf 0\n(ln f)0 =\nor f 0 = f(ln f)0\nf\n\nLecture 6\n18.01 Fall 2006\nx\nApply this to f(x) = a .\nd\nd\nd\nln f(x) = x ln a =\nln(f) =\nln(a x) =\n(x ln(a)) = ln(a).\n⇒ dx\ndx\ndx\n(Remember, ln(a) is a constant, not a variable.) Hence,\nd\nf 0\nd\nx\nx\n(ln f) = ln(a) =\n= ln(a)\n=\nf 0 = ln(a)f =\na = (ln a)a\ndx\n⇒ f\n⇒\n⇒ dx\nd\nExample 1.\n(x x) = ?\ndx\nWith variable (\"moving\") exponents, you should use either base e or logarithmic differentiation.\nIn this example, we will use the latter.\nf\n=\nx x\nln f\n=\nx ln x\n(ln f)0\n=\n1 (ln x) + x\n= ln(x) + 1\n·\nx\nf 0\n(ln f)0\n=\nf\nTherefore,\nf 0 = f(ln f)0 = x x (ln(x) + 1)\nIf you wanted to solve this using the base e approach, you would say f = ex ln x and differentiate\nit using the chain rule. It gets you the same answer, but requires a little more writing.\n\nk\nExample 2. Use logs to evaluate lim\n1 +\n.\nk→inf\nk\nBecause the exponent k changes, it is better to find the limit of the logarithm.\n\"\nk #\nlim ln\n1 +\nk→inf\nk\nWe know that\n\"\nk #\n\nln\n1 +\n= k ln 1 +\nk\nk\nThis expression has two competing parts, which balance: k →inf while ln 1 + k\n→ 0.\n\"\n1 k #\n\nln\n\n1 + k\nln(1 + h)\nln\n1 +\n= k ln\n1 +\n=\n=\n(with h =\n)\nk\nk\nh\nk\nk\nNext, because ln 1 = 0\n\"\nk #\nln\n1 + 1\n= ln(1 + h) - ln(1)\nk\nh\n\nLecture 6\n18.01 Fall 2006\nTake the limit: h = k → 0 as k →inf, so that\nln(1 + h) - ln(1)\nd\n\nlim\n=\nln(x)\n= 1\nh\nh\ndx\nx=1\n→\nIn all,\n\nk\nlim ln 1 +\n= 1.\nk→inf\nk\n\nk\nWe have just found that ak = ln[ 1 + k\n] → 1 as k →inf.\n\nk\nIf bk =\n1 + k\n, then bk = e ak → e 1 as k →inf. In other words, we have evaluated the limit we\nwanted:\n\nk\nlim\n1 +\n= e\nk→inf\nk\nRemark 1. We never figured out what the exact numerical value of e was. Now we can use this\nlimit formula; k = 10 gives a pretty good approximation to the actual value of e.\nRemark 2. Logs are used in all sciences and even in finance. Think about the stock market. If I\nsay the market fell 50 points today, you'd need to know whether the market average before the drop\nwas 300 points or 10, 000. In other words, you care about the percent change, or the ratio of the\nchange to the starting value:\nf 0(t)\nd\n=\nln(f(t))\nf(t)\ndt"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 7: Continuation and Exam Review",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/a30756fe9d577184f205b09bc6d6d005_lec7.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n18.01 Single Variable Calculus\nFall 2006\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nLecture 7\n18.01 Fall 2006\nLecture 7: Continuation and Exam Review\nHyperbolic Sine and Cosine\nHyperbolic sine (pronounced \"sinsh\"):\nsinh(x) = ex - e-x\nHyperbolic cosine (pronounced \"cosh\"):\nex + e-x\ncosh(x) =\nx\nx\nd sinh(x) = d\ne - e-x\n= e - (-e-x) = cosh(x)\ndx\ndx\nLikewise,\nd cosh(x) = sinh(x)\ndx\nd\n(Note that this is different from\ncos(x).)\ndx\nImportant identity:\ncosh2(x) - sinh2(x) = 1\nProof:\n\nx\ncosh2(x) - sinh2(x)\n=\nex +\ne-x\n-\ne -\ne-x\ncosh2(x) - sinh2(x)\n=\n4 e 2x + 2e x e-x + e-2x - 4 e 2x - 2 + e-2x = 4(2 + 2) = 1\nWhy are these functions called \"hyperbolic\"?\nLet u = cosh(x) and v = sinh(x), then\nu 2 - v 2 = 1\nwhich is the equation of a hyperbola.\nRegular trig functions are \"circular\" functions. If u = cos(x) and v = sin(x), then\nu 2 + v 2 = 1\nwhich is the equation of a circle.\n\nLecture 7\n18.01 Fall 2006\nExam 1 Review\nGeneral Differentiation Formulas\n(u + v)0\n=\nu0 + v0\n(cu)0\n=\ncu0\n(uv)0\n=\nu0v + uv0\n(product rule)\nu 0\n=\nu0v - uv0\n(quotient rule)\nv\nv2\nd f(u(x))\n=\nf 0(u(x)) u0(x) (chain rule)\ndx\n·\nYou can remember the quotient rule by rewriting\nu 0\n= (uv-1)0\nv\nand applying the product rule and chain rule.\nImplicit differentiation\nLet's say you want to find y0 from an equation like\ny 3 + 3xy 2 = 8\nd\nInstead of solving for y and then taking its derivative, just take\nof the whole thing. In this\ndx\nexample,\n3y 2 y0 + 6xyy0 + 3y 2\n=\n(3y 2 + 6xy)y0\n=\n-3y 2\ny0\n=\n-3y2\n3y2 + 6xy\nNote that this formula for y0 involves both x and y. Implicit differentiation can be very useful for\ntaking the derivatives of inverse functions.\nFor instance,\ny = sin-1 x\nsin y = x\n⇒\nImplicit differentiation yields\n(cos y)y0 = 1\nand\ny0 =\n=\ncos y\n√\n1 - x2\n\nLecture 7\n18.01 Fall 2006\nSpecific differentiation formulas\nYou will be responsible for knowing formulas for the derivatives and how to deduce these formulas\nn\nx\nfrom previous information: x , sin-1 x, tan-1 x, sin x, cos x, tan x, sec x, e , ln x .\nd\nFor example, let's calculate\nsec x:\ndx\nd\nd\n-(- sin x)\nsec x =\n=\n= tan x sec x\ndx\ndx cos x\ncos2 x\nd\nd\nYou may be asked to find\nsin x or\ncos x, using the following information:\ndx\ndx\nsin(h)\nlim\n=\nh\nh\n→\nlim cos(h) - 1\n=\nh\nh\n→\nRemember the definition of the derivative:\nd f(x) = lim f(x + Δx) - f(x)\ndx\nΔx\nΔx\n→\nTying up a loose end\nd\nHow to find\nx r, where r is a real (but not necessarily rational) number? All we have done so far\ndx\nis the case of rational numbers, using implicit differentiation. We can do this two ways:\n1st method: base e\nx\n=\ne ln x\nx r\n=\n\ne ln x r = e r ln x\nd\ndx x r\n=\nd\ndx e r ln x = e r ln x d\ndx (r ln x) = e r ln x r\nx\nd\ndx x r\n=\nx r r\nx\n\n= rx r-1\n2nd method: logarithmic differentiation\nf 0\n(ln f)0\n=\nf\nf\n=\nx r\nln f\n=\nr ln x\nr\n(ln f)0\n=\nx\nf 0 = f(ln f)0\n=\nx r\nr\n= rx r-1\nx\n\nLecture 7\n18.01 Fall 2006\nFinally, in the first lecture I promised you that you'd learn to differentiate anything-- even\nsomething as complicated as\nd\nx tan-1 x\ne\ndx\nSo let's do it!\nd\nd\ne uv\n=\ne uv\n(uv) = e uv (u0v + uv0)\ndx\ndx\nSubstituting,\nd e x tan-1 x\n=\ne x tan-1 x tan-1 x + x\ndx\n1 + x2"
        },
        {
          "category": "Resource",
          "title": "Approximation",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/ea98a7f8029b275f7cb9047f87bc4a29_a_approximations.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\nFall 2006\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nA. APPROXIMATIONS\nIn science and engineering, you often approaimate complicated functions by simpler ones\nwhich are easier to calculate with, and which show the relations between the variables more\nclearly. Of course, the approximation must be close enough to give you reasonable accuracy.\nFor this reason, approximation is a skill, one your other teachers will expect you to have.\nThis is a good place to start acquiring it.\nThroughout, we will use the symbol a to mean \"approximately equal to\"; this is a bit\nvague, but making approximations in engineering is more art than science.\n1. The linear approximation; linearizations.\nThe simplest way to approximate a function f(z) for values ofz near a jr\nis to use a linear funcion. The linear function we shall use is the one whose\ngraph is the tangent line to f(z) at x = a. This makes sense because the\ntangent line at (a, f(a)) gives a good approximation to the graph of f(z),\n_. i\nl.\nT\n.\n.\na\ns\nat\na\nto\ncose\ns\na\n(1)\nheight of the graph of f(z) s heightof the tangent at (a, f(a))\nTo turn (1) into calculus, we need the equation for the tangent line. Since the line goes\nthrough (a, f(a)) and has slope f'(a), its equation is\nS\nf(a) + f'(a)(x -\n),\nand therefore (1) can be expressed as\n(2)\nf(z)\nf() +f'(a)( - a),\nfor Tua.\nThis says that for x near a, the function f(z) can be approximated by the linear function\non the right of (2). This function - the one whose graph is the tangent line -\nis called the\nlinerization of f(z) at z = a.\nThe appraoimation (2) is often written in an equivalent form that you should become\nfamiliar with; it makes use of a dependent variable. Writing\n(3)\nv= f(z),\nA = z- a, Ay = f() -f(a),\nthe approximation (2) takes the form\n(2')\nAL\nf'(a)Az,\nfor Az\n.\nf(a)Ax\nIn this form, the quantity on the right represents the change in height of the\ntaent line, while the left measures the change in height of the eraph.\nHere are some examples of linear approximations. In all of them, we are taking a = 0,\nthis being the most important case All can be found by using (2)above and calculating\nthe derivative. You should verify each of them, and memorize the approximation.\n\nA. APPROXIMATIONS\nBasic Linear Approximations\n(4)\nr-a 1 + z,\nfor ax 0 ;\n(5)\n(1\n+ x)' s 1 + rz,\nfor z s 0;\nr is\nany real number\n(6)\n..\n. sinz\nz,\nfor zx 0.\nNote that (4) becomes a special case of (5) if we take r = -1 and replace x by -z;\nnonetheless, learn (4) separately since it is very common. As an example of verification, let\nus check (5):\nf(x) = (1+ x)'\nf'() = r(1 + x) -\n,\nfor any real r ;\nS f'(O)=r.\nTherefore, (2) becomes\nf(0) + f'(0)x\nM 1+r ,\nwhich is(5).\n(1 + z)r p f\n2. The algebraic viewpoint; examples\nThough the three basic approximations given above can be derived by using differen\ntiation, many people remember them better by relating them to high school algebra and\ngeometry. We show how.\nThe approximation (4) can be thought of as coming from the formula for the sumn of a\ngeometric series (memorize this too, if you have forgotten it):\n-= +xz+x2 +... +zX + ... ,\nIJ < 1.\n1-x\nIfx is small, then the terms z2, X3,... on the right are all negligible compared with the\nterm x, so they can be ignored, and we get (4).\nSimilarly, the approximation (5) can be thought of as coming from the binomial theorem,\nif r is a positive integer:\n(1+x)r = 1+rzx+ r(r - 1) 2+.. + r\nAs before, if xis small, we can neglect the terms in z2, s ,..., and we get the approximation\n(5). Even if r is not an integer, you will learn when you study infinite series that the binomial\ntheorem is still formally true. Though it gives an infinite sum on the right, instead of a\n-tini\nam the r-cffirienta arp till raleul.atMd hv the slmfe fnrmulas\n-\nAa\nA1,\nV A- .-6 U- '-\nJ\n-\nV\n'-'.V\n- -\n-\nv -\n..\nFinally,, the linear approximation (6) for sin should make sense if you\nthink of the trigonometric definition of sinz. Referring to the picture, it\nsays that a small arc 2z of the unit circle is approximately equal in length\ntn the chrd 2sinn~ it nsubtends.\nInte hr ysnxi ubed\n\nA. APPROXIMATIONS\nContinuing this algebraic viewpoint, many other linear approximation formulas can be\nderived from the basic ones above by using algebra, rather than by going back to (2) and\ncalculating derivatives. Here are some examples of this.\nExample 1. In each of the following, we want a linear approximation valid for z - 0.\nObserve in the first two how the variable is divided by a number (or \"scaled\", as one says,\nsince it amounts to a change of scale or change of units for the variable). The purpose is to\nput the expression in a form where one of the basic approximations can be used.\nzx\n(a)\n1f\n1/2/2\nI 1-\n,\nby scaling and using (4);\n2+(\n1+\n/2\ni\nI\n= %-\n,\ns 3(1+t/18) = 3+t/6,\nfor t- 0,by using (5).\n(b)\n/9 +t\n/\n= 3(1 + t/9) / 2\nby scaling;\nExample (b) above is just as easy to do by using (2), since dg +t\nI\nIn example (c) below, however, using (2) would definitely require more work.\n2+z\n2+z\n(c)\n2+x\n2+x2\n(2+ z)(1-\nz/2),\nusing (5), then (4);\n%1/T '1+z/2\nP 2,\nmultiplying out and neglecting terms in x2 .\nNotice that in this example, the linearization 2 + 0 turns out to be a constant function.\nApproximations for za\na, where a A0 .\nThough it is most common to work near a = 0, sometimes one wants another value of a.\nEither one can use the formula (2), or else one can make a change of variable: h, Az,e are\nall common choices, related to z by\n(7)\nz = a+ h,\nz= a+ Az,\nz= a+e.\nThe new variable is then close to 0 when zis close to a. Here is an example.\nExample 2. Approximate 3 + x4 for z m 1.\nSolution. Either use (2), or change variable; doing the latter, we put = 1 + h. Then\n3+z = 3+(1 + h)4,.\ns 3+(1+4h), h;0,\nusing (5);\nS4 + 4(x - I),\nfor z s I.\nApplications. Here are a few typical uses of the linearization.\nExample 3. In the theory of special relativity, the mass m of a body moving with speed\nv is given by\nm oc\nm\n-=\nm0 = mass at rest, c= velocity of light\nWhat speed produces a 1%increase in mass?\n\nA. APPROXIMATIONS\nSolution. We could crank out the answer, using the formula for m, but in practise a\nsimplifying approximation would be used. To begin with, scale m and v, i.e., divide them\nby suitable constants to make them dimensionless: m/tmo and v/c; this turns the above\nformula into (dividing top and bottom by c):\nm\nmo\n,/ 1 - v2/c2\n- (1- u2)1/2\nwhere we have set v/c = u;.when v is small compared with c, then u s~ 0. We get, using (5)\nwith r = 1/2,\n1.\nu*\n(1- U2)/ 2\n1- (1/2)U2 m 1+ + 21\nuM0,\nwhere the second approximation used (4), with x = u2/2.\nThis approximation shows that to make m/mo = 1.01 (this represents a 1%increase in\nthe mass), we want\n==2/2\ni.e.,\na\n/.\n- 1/7.\n.01,\n=\nThe corresponding velocity is (remember that u = v/c):\nv k c/7 s (186, 000/7 mi/sec ; 27, 000 mi/sec.\nExample 4.\nGive a useful approximate formula, valid for relatively small heights,\nshowing how the weight of a body decreases as it rises above the earth, and use your\nformula to determine how high it must rise to experience a 1%loss in weight.\nSolution. Let R be the radius of the earth. The force between two masses mi and M2\nwith centers of 'mass separated by a distance d is\nGmims\nF = Gmd7\nso if the earth weight is M and our body has weight m,\nGMm\nweight at surface =\nGMm\nweight at height h above surface = (R + h)2\nso that\nweight at height h\nR 2\nweight at surface\n(R + h)2\n(1 + h/R)2 '\nwhere in this last step we made the variable dimensionless by dividing numerator and\ndenominator by R2; this scaling also makes the expression simpler. We continue with\napproximations:\n-.\n(1 - h/R)2 ,\nusing (4);\nS1 - 2h/R,\n- using (5).\nThe approximation is valid if h/R e 0, i.e., if h is very small compared to R.\n\nA. APPROXIMATIONS\nUsing our approximation, we see that to make the ratio of the weights ; .99, we want\n2h\n.01R\n.01 .4, 000\nR S.01,\ni.e.,\nh = 2 =\n= 20 miles.\n4. Quadratic approximations.\nTo get greater accuracy, sometimes one wants to include higher-order terms in the ap\nproximating function. If we include second-order terms -\nthat is, terms in (x - a)2, we get\nwhat is called a quadratic approximation for x w a. It looks like\n(8)\nf ()\ns A + B(x - a) + C(x - a)2,\nz~ a.\nThere is a general formula for the coefficients A, B, C using calcihlus, but let's work alge\nbraically first, and consider the basic approximations.\nBasic Quadratic Approximations\n1-z\n(10)\n(1+x)r\n1 +rz+ r(r- 1);2 ,\nforxs0; risanyrealnumber\n(11).\nsinx M =,\nfor x\nO.\n(12)\ncos\n;U 1 - -\n,\nforzzX 0.\nDiscussion\nFormula (9) comes as before from the sum of the geometric series.\nFormula (10) is the beginning of the binomial theorem, if r is an integer.\nFormula (11) looks like our earlier linear approximation, but the assertion here is that it\nis also the best quadratic approximation -\nthat is, the term in x2 has 0 for its coefficient.\nThis is so because sin x is an odd function, so the approximating polynomial should be odd\nalso, which means it cannot have any x2 term.\nFormula (12) is derived from (11) and the identity sin x+\nz\ncos 2 x = 1; this is one of the\nexercises.\nUsing these basic quadratic approximations, we can by algebra get quadratic approxima\ntions to more involved expressions. Examples are given below. In studying the examples,\nSnotice that during the course of the calculation, all approximationsmust be quadratic. If\none of the approximations you use is only linear, then that contaminates the final result,\nwhich probably will not have the correct x2 term. This is the same principle you meet in\nadding numbers: if one of the numbers is only good to one decimal place, then no matter\nhow accurate all the other numbers are, the sum will only be good to one decimal place.\n\nA. APPROXIMATIONS\nExample 5.\nBy using the basic approximations, give quadratic approximations valid\nfor x.\n0 for each of the following:\ncosx\n(a) secx\n(b)\nV +3x\n(c)\n/1+x+x 2\n(d)\n1-x\nSolution.\n(a)\nsec x =\nz--\n1 + 2, by (12) and (4).\ncosax\n1 - 22/2'\n(b)\nV1\n= (1 + 3x)1/ 2 ;\n1 + I(3x) -\n(3x)2,\nby (10);\nSi+.2 x-x2 .\nS1+\n(X + X2)\n(\n)2 ;$ 1 +\n+ X,\n(X+\nby(10).\n(d)\n-x\n(1XX\n)\n+ +-.\nTo illustrate what happens if you don't keep enough terms during the calculation, observe\nthat if in (d).above we only used 1+ x in the right-hand factor, the answer would have been\n1+ x - x2/2, whose x2 term is incorrect.\n6. The quadratic approximation formula.\n(13)\nf(x) z f(a)- f'(a)(x - a)+ f\n- a)2,\nfor x a.\nExample 6. Check formulas (10) and (11) by using (13).\nSolution. Since the first two terms of (13) are the linearization, we can build on our earlier\nwork, and have only to calculate the quadratic coefficient f\"(0)/2. We get\n(a)\nsinx ;\n0 + x +02 2 ,\nsince sin\"(x) = -sin x\n=\nsin\"(0) = 0.\n(b)\nf\"(0)\nr(r-\n)\nr - '\n2 .\nas in(10).\nf (x)= (1+x)r\nf(x)=r(r -1)(1-\nx)\nThe usefulness of (13) is tempered by the fact that it requires calculation of second\nderivatives. This can get rather tedious -\nfunction (d) in Example 5 is a good illustration\n-\nso that using the algebraic techniques is often better.\n\nA. APPROXIMATIONS\nThe quadratic approximation formula (13) may be \"derived\" as follow. We are looking\nfor the right choice of coefficients in\n(14)\nf(s)\nA + B(z - a) + C(z - a)2,\nz a.\nLet us denote by Q(x) the polynomial on the right of (14). Then itmakes sense to choose\nthe coefficients A, B, C so that f(s) and Q(z) have the same 'alue and the. same first and\nsecond derivatives at a, i.e., so that\n(15)\nf(a) = Q(a),\nf'(a) = Q'(a),\nf\"(a) = Q\"(a).\nSince Q'(s) = B + 2C(r - a)and Q\"(z)= 2C, equations (15) say that\n(16)\nf(a) =A,\nf'(a) =B,\nf\"(a) = 2C;\nthese values for A, B, C turn (14) into (13), as promised. Note that the first two terms on\nthe right of (13) give the linearization at z = a; thus the quadratic approximation refines\nthe linear approximation by adding a quadratic term to it.\nExercises: Section 2A"
        },
        {
          "category": "Resource",
          "title": "Average Value",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/e13351c281c7b7d49d24aae95b0b18ff_av_average_value.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\nFall 2006\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nAV. AVERAGE VALUE\nWhat was the average temperature on July 4 in Boston?\nT\nT\nThe temperature is a continuous function f(z), whose graph over the\n24-hour period inight look as shown. How should we define the average\nvalue of such a function over the time interval [0, 24] -- measuring time\ns in hours, with z = 0 at 12:00AM ?\n18 24 x\nWe could observe the temperature in the middle of every hour, that is, at the times\nzX = .5,zs = 1.5,... ,z4 = 23.5, then average these 24 observations, getting\n=1\nTo get a more accurate answer, we could average measurements made more frequently, say\nevery ten minutes.\nFor a general interval [a, b] and function f(z), the analogous procedure would be to divide\nup the interval into n equal parts, each of length\nb-a\n(1)\nA. = -n\nand average the values of the function f(x) at a succession of points zi, where sx lies in the\ni-th interval. Then we ought to have\n(2)\naverage of f(z) over [a,b]b\nn\nz .\"(a\nWe can relate the sum on the right to a definite integral: using (1), (2) becomes\n(3)\naverage of f () over [a,b]\nf ()A\n.\nAsn-a\nAs n *- co, the sum on the right-hand side of (3) approaches the definite integral of f(z)\nover [a, b], and we thereforedefine the average value of the function f(z) on [a, b] by\n(4)\nA = averageof f() over [, ] =\n(x)dx.\nGeometrically, the average value A can be thought of as the height of that\nconstant function Awhich has the same area over [a, b] as f(z) does. This\nis so since (4a)shows that\nA.(-) =\nf()\n.\n.\n.\n-.-.\n\nAV. AVERAGE VALUE\nExample 1. In alternating current, voltage is represented by a sine\nwave with a frequency of 60 cycles/second, and a peak of 120 volts. What\nis the average voltage?\n21r\n27r\nSolution. The voltage function has frequency\n-\n= 120r,\n11/i2o\nperiod\n1/60\nand amplitude 120, so it is given by V(t) = 120 sin(120irt). Thus by (4).\n1/120\naverage V(t) = 120\nV(t) dt\n-\ncos(120rt\n= -.\n120.\nJO\no0\n(We integrate over [0,1/120] rather than [0,1/60] since we don't want zero\nas the average.)\nExample 2.\na) A point is chosen at random on the z-axis between -1 and 1; call\nit P.. What is the average length of the vertical chord to the unit circle\nDassing through P?\nb) Same question, but now the point P is chosen at random on the circumference.\nSolution. a) If P is at x, the chord has length 2 vTC, so we get\naverage of 2\nX over [-1, 1] =\n2V -x 2d = area of semicircle = 7r\n1.6.\no2e\n\nb) By symmetryi we can suppose P is on the upper semicircle. If P is at the angle 0,the\nchord has length 2 sin 0, so this time we get\naverage of 2sin0 over [0,] = 1 f\n2sin 0d = --2 cos] \" = -4\n1.3.\n(Intuitively, can you see why the average in part (b) should be less than the average in\npart (a) -\ncould you have predicted this would be so?)\nExercises: Section 4D"
        },
        {
          "category": "Resource",
          "title": "Continuity and Discontinuity",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/08a6c62564099e85ab310ca0db1e2182_c_cntnt_dscntnt.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\nFall 2006\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nC. CONTINUITY AND\nDISCONTINUITY\n1. One-sided limits\nWe begin by expanding the notion of limit to include what are called one-sided limits,\nwhere z approaches a only from one side -\nthe right or the left. The terminology and\nnotation is:.\nright-hand limit\nlim f(x)\n(z comes from the right, x > a)\nleft-hand limit\nlim f(z)\n(x comes from the left, z < a)\nX\nML_\nSince we use limits informally, a few examples will be enough to indicate the usefulness of\nthis idea.\n1/x 2\n.1\nEx. 1\nEx.2\nEx. 3\nEx.4\nExample 1.\nlim V/l-\n2 = 0\n=--1-\n=-*-1+\n(As the picture shows, at the two endpoints of the domain, we only have a one-sided limit.)\nz\nExample 2. Set f(z)=\n1 X 0.\nThen\n>\n,*---+\nlim f() = -1,\nX-00+\nlim f() =1.\nExample 3.\nli\n-\n= oo,\nlim -\n= -oo\n-_O0+ X\n2--0- 2\nExample 4.\nli n-\n=oo,\nlim\n-\noo\n.--40+\n2-o- z\nThe relationship between the one-sided limits and the usual (two-sided) limit.is given by\n(1)\n.\nIC-=+&\nlim f(x) = L\nlim f(x) = L and\nlim f() = L\n2-40+\nIn words, the (two-sided) limit exists if and only if both one-sided limits exist and are equal.\nThis shows for example that in Examples 2 and 3 above, lim f(z) does not exist.\nStudents often say carelessly that lim 1/x = oo, but this is not sloppy, it is simply\n2--+0\nwrong, as the picture for Example 3 shows. By contrast, lirn 1/z\n= oo is correct and\nacceptable terminology.\n\n2. Continuity\nTob understand continuity, it helps to see how a function can fail to be continuous. All\nof the important functions used in calculus and analysis are continuous except at isolated\npoints. Such points are called points of discontinuity. There are several types. Let's begin\nby first recalling the definition of continuity (cf. book, p. 75).\n(2)\nff()\nis continuous at a\ni\nlim f(x) = f(a).\nThus, if a is a point of discontinuity, 'something about the limit statement in (2) must fail\nto be true.\nTypes of Discontinuity\nX2 1\nsin (l I)\nsn(I/z)\nremovable\nremovable\njump\ninfinite\nessential\nIn a removable discontinuity, lim f(x) exists, but lim f(z) ff(a). This may be because\n--ia\ns--+a\nf(a) is undefined, or because f(a)has the \"wrong\" value. The discontinuity can be removed\nby changing the definition of f(x) at a so that its new value there is lir f(z). In the left-most\npicture,\n-\nis undefined when x = 1, but if the definition of the function is completed\nz-1\nby setting f(1) = 2, it becomes continuous -\nthe hole in its graph is \"filled in\".\nIn a jump discontinuity (Example 2), the right- and left-hand limits both exist, but\nare not equal. Thus, lim f(z) does not exist, according to (1). The size of the jump is\nthe difference between the right- and left-hand limits (it is 2 in Example 2, for instance).\nThough jump discontinuities are not common in functions given by simple formulas, they\noccur frequently in engineering -\nfor example, the square waves in electrical engineering,\nor the sudden discharge of a capacitor.\nIn an infinite discontinuity (Examples 3 and 4), the one-sided limits exist (perhaps as oo\nor -oo), and at least one of them is foo.\nAn essential discontinuity is one which isn't of the three previous types -\nat least one of\nthe one-sided limits doesn't exist (not even as ±oo). Though sin(1/z) is a standard simple\nexample of a function with an essential discontinuity at 0,in applications they arise rarely,\npresumably because Mother Nature has no use for them.\nWe say a function is continuous on an interval [a, b]if it is defined on that interval and\ncontinuous at every point of that interval. (At the endpoints, we only use the approrpiate\none-sided limit in applying the definition (2).)\n\nC.\nCONTINUITY AND DISCONTINUITY\nWe say a function is continuousif its domain is an interval,and it is continuous at every\npoint of that interval.\nA point of discontinuityis always understood to be isolated, i.e., it is the only bad point\nfor the function on some interval.\nWe illustrate the point of these definitions. (They are slightly different from the ones in\nyour book, but are more consistent with standard terminology in calculus.)\nI\nI\nI\nI\naI\nI\nt\n! II'\nIl 1I/II I 1:\nIrs\nX /I*I\nS\nI\ni/ ! I a/ a\nIi\nI\nI I\nI\nI\nI I\nII\nI\nExample 5\nExample 6\nExample 7\nExample 8\nExample 5. The function 1/z is continuous on (0, oo) and on (-oo, 0), i.e., for z > 0\nand for z < 0, in other words, at every point in its domain. However, it is not a continuous\nfunction since its domain is not an interval. It has a single point of discontinuity, namely\nz = 0, and it has an infinite discontinuity there.\nExample 6. The function tanz is not continuous, but is continuous on for example the\ninterval -ir/2 < x < ir/2. It has infinitely many points of-discontinuity, at +ir/2, 13r/2,\netc.; all are infinite discontinuities.\nExample 7; f () = Jz- is continuous, but f'(z) has a jump discontinuity at 0.\nExample 8.\nThe function in Example 2, f(z) =\n-1,\n1, X><0,0 does not have a\ncontinuous derivative f'(z) -\nstudents often think it does since it seems that f'(z) = 0\neverywhere. However this is not so: f'(0) does not exist, since by definition,\n'f(O\n+ Az) - f(0)\nAX--+O\nAX\nbut f(0) does not even exist. Even if f(0) is defined to be say 1, or 0, the derivative f'(0)\ndoes not exist.\nRemember the important little theorem (Simmons p. 75)\n(3)\nf(z) differentiable at a\nf(z) continuous at a\nor to put it contrapositively,\n\"f(x) discontinuous at a\n=-\nf(z) not differentiable at a\nThe function in Example 8 is discontinuous at 0, so it has no derivative at 0; the discontinuity\nof f'(x) at 0 is a removable discontinuity.\nExercises: Section 1D"
        },
        {
          "category": "Resource",
          "title": "Exponentials and Logarithms",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/bdc00eb95387b745a513f2ec3f877afd_xxpnentl_lgrthm.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\nFall 2006\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nX. EXPONENTIALS\nAND LOGARITHMS\n1. The Exponential and Logarithm Functions.\nWe have so far worked with the algebraic functions -\nthose involving polynomials and\nroot extractions - and with the trigonometric functions. We now have to add to our list the\nexponential and logarithm functions, since these are used in your science and engineering\ncourses from the beginning. Your book will handle the calculus of these functions; here we\nwant to review briefly their algebraic properties, and look at applications; one or two of\nthem might be new to you.\nWhere does one encounter exponentials and logarithms? In general, exponentials are\nused to express all sorts of simple growth and decay processes.\n1. The growth of a bacteria colony which doubles in size every day:\ny = yo2 t ,\nwhere t = time in days, y = population size, yo = the initial size, i.e., size at t = 0.\n2. Dollars in a bank account, at 5%.interest compounded annually:\nA = Ao(l.05)n ,\nwhere n = number of years, A = amount, Ao = initial amount (the \"principal\").\n3. Amount of radioactive substance, with a 1 year half-life:\nx = zo(1/2)n = zo2 n,\nwhere n = number of years,\nx = amount,\n0o= initial amount.\nThere are many other examples: the decay of electric charge on a capacitor, the way\na hot and cold body come to the same temperature when they are brought together, the\nfalling of a body through a resisting medium (a steel ball dropped into oil, for instance)\nall involve exponentials when you express them in mathematical terms.\nWe use logarithms when the base of the exponential is unimportant and we want to focus\nour attention on the exponents instead. Suppose the base is 10; writing simply \"log\" for\n\"log 0o\"in what follows, we have\ny = 10'\n€\nlogy = z.\n1. Star magnitude. The observed brightness B of a star is described by comparing it\nto a standard brightness Bo, using the equation\nB = BolO-\n5 .\nThe important thing is the number m called the magnitude; it is defined by the above\nequation, or equivalently, taking logs, by\nB\nm = -\nlog\n.\n2 Bo\nThus, the higher the magnitude, the fainter, the star. A star of brightness Bo, if one existed,\nwould have magnitude zero. Thus the constant Bo gives the magnitude scale a zero point.\nThis point was chosen so that the brightest stars (other than the sun) -\nSirius and Vega,\nfor example -\nwould have magnitude near 1, i.e., be stars of the first magnitude.\n\n2. Orders of magnitude. We speak of a kilometer as \"three orders of magnitude\"\ngreater than a meter. That is\nkm\n= 103,\n3 = log (.\nmeter\nmeter\nIn general, when we compare two quantities A and B;\nnumber of orders of magnitude by which A is greater than B = log(A/B) .\n3. Chemical pH. In chemistry, the acidity or alkalinity of a solution is measured by\nthe concentration [H+] of hydrogen ions in the solution, in units of moles/liter.\nIf for some solution the concentration [H+] = 10\", then we define\npH of solution = -n = - log [H+].\nThe negative sign is used to avoid always having negative pH's, since the concentration [H+]\nis very small.\nThe algebraic laws.\nWe will begin with a brief review of the algebra of exponentials.and logarithms. Please\nstudy the examples given of calculations, since many students seem to have trouble with\nthis.\nWe fix a number a > 0, called the base. The pair of equations\n(1)\ny = a=\nlog, y= z\nare equivalent; both express the same relation between z and y. If you substitute one\nequation into the other, you get two other equations which are often more useful in practice\nthan those in (1) (the examples will illustrate):\n(2)\ny= logYY\n.-log.(ac ) = x.\nIn the first, we take the log then exponentiate; in the second, we exponentiate, then take\nthe log; in both.cases, we end up where we started.\nThe two basic algebraic laws governing exponentials and logarithms are\n(3)\naL2+\n= ='2a 2'\nlog0. y + log. y2 = log01912.\n(4)\n(a=)k = ak..\nlog'y~ = k log. yl\nThe laws on either side follow from those on the other side by substituting y1 = a\"- or\nxj = log. y1, and then using (1) or (2).\n\nX. EXPONENTIALS AND LOGARITHMS\nGraphs of the exponential and log functions\nThe picture on the left below shows 'some graphs of the exponential function a\" for\ndifferent choices of bases. (We will discuss the base e ; 2.718 presently.)\nThe,graph rises if a > 1, falls if a < 1. Since\n= a-, the graph of (1/a)\" is\nobtained by reflecting the graph of a\" in the y-axis. (See Notes G, p. G.2.)\nThe pair of equivalent equations y = a' and x = log. y show that af and log, z are\ninversefunctions,according to Notes G, pp. G.8,9. Thus the graph of log, z is obtained by\nreflecting the graph of ae in the diagonal line y = z. The reasoning for this needs a lot of\nrepetition to sink in, so we repeat it briefly once more. Namely, consider in turn the three\nequations\ny = ax\nz = ay\ny = loga X\nSince the second equation arises from the first by switching z and y, the graph of the second\nequation is obtained by flipping the first graph around the diagonal line y = z; the third\nequation has the same graph as the second, since it's the same equation - we've only solved\nit for y. To sum up: the first and third equations have graphs which are reflections of each\nother about y = x.\nThe base e. Changing the base.\nThere are really only two or three bases in common use.\nFor areas in science and engineering where calculus does not enter, or where at least\nhistorically it was not used, the base 10 is the usual choice, because it associates naturally\nwith the decimal system, making rough estimations of the size of exponentials a fairly easy\nthing to do.\nOh the other hand, if you are going to have to differentiate or integrate, the base e.e 2.718\nis the usual choice. (The symbol e was introduced by Euler -\nthe first letter of both\n\"exponential\" and \"Euler\".) As you will see, it is the base which makes for the simplest-\nlooking differentiation and integration formulas for exponentials and logs.\n(To the above two, one should perhaps add the base 2,which is useful in computer science,\nwith its bits and bytes.)\nFor base e and base 10, the corresponding notations for the logarithm are\nlog, x = In x (the \"natural logarithm\");\nloglo X = log .\n(Warning: Many pure mathematics books use log z for the natural logarithm; science and\nengineering books generally use inz. Some computer science books use log z to mean log12 .)\n\nUsing science and engineering notation, the logarithm laws read, for the bases e and 10:\nln(ab) = Ina + nb\nlog(ab)' = log a + log b\nln(ab) = bIn a\nlog(ab) = bloga\nand\n(6)\nIn(e))= , eIn =\nlog(10) = z, 10 10og= X\nIne = 1, Inl=\n0; el =e, e\n= l\nlog 10 = 1, logl = 0; 10' = 10, 100= 1.\nOther bases. The reason why it is not necessary to use other bases is that all other\nexponentials and logarithms can be written in terms of e\" and Ins, or if one prefers, in\nterms of 10S and logz. Since in calculus we use the base e, let's use this base. Using (6),\nwe have for any base a > 0,\na\"= (e=n ). e.na.\nThus the functions ek' , for different-constants k, represent all of the exponential functions\na. no matter what the base a is.\nAs for logarithms, starting with the equations\na\" = y,\nz = log,\n,\ntaking the natural log of the equation on the left gives\n(In a) = Iny .\nSolving for x and substituting into the second equation, we get\nIny\nlog., =hav\nIna\nFinally, changing the variable from yto z,we have\nIns\nlog a =\n;7a\nin particular,\nlog\n= lin10\nStudy these transformations until you can reproduce them easily.\nIn calculations, students usually find it easy to use the laws of exponentials and logs.\nThe chief trouble is in passing from the log to the exponential and vice-versa. Study the\nfollowing examples, and don't go further until you have tried some of the exercises. Notice\nthat you can almost always get through using (2); whereas the equations (1) sometimes\naren't good enough, and even when they are, they require too much thinking. The general\nprinciple which governs the following calculations is:\n'(8)\nTo get rid of logs, exponentiate; to get rid of exponentials, take the logs.\n\nX. EXPONENTIALS AND LOGARITHMS\nExample 1. If one liquid has a pH which is 4 greater than the pH of a second liquid,\nwhat is the relation between the [H+] concentrations of the two liquids? (cf. p.X.1)\nFirst Solution. Using subscripts to distinguish the two liquids,\n(pH)i = .(pH)2 + 4\n-log[H+]i\n= -log[H +] +4\nMultipljing through by -1, then exponentiating both sides (base 10) and using (6),\n[H+] = [H+]2.-10- 4.\nSecond Solution.\nStarting from the second line, put both logs on the left side, and\ncombine them using (5) (remember that - log a = In(1/a)), then exponentiate both sides\n(or use (1)).\nExample 2 A radioactive substance decays according to the law y = yoekt . Two\nmeasurements are made: at time ti, there is an amount y7present, where i = 1,2. Express\nk in terms of the yj- and ti.\nSolution. We have y2 = yoe6kt and yl = yoekt. Dividing one equation by the other to\nget rid of 70,\n72/l1 = e2/ekt\n= ek(t-t);\nln(y/yl) = k(t -tl),\nk = ln(y/yl)-)Iny -lnyl\nt 2 - tl\nt 2 - tl\nExample 3. Solve for y the equation ln(y + 1) = Iny + x -2.\nSolution. Exponentiating both sides, y + 1 = y. ef- 2; this is of the form y + 1 = Ay,\nwhose solution for y is\n= 1- A\n1-\ne - 2'\nExample 4. Solve for y:\n(1 + z)S = 4- ' .\nSolution. Take the log of both sides, to get the y downstairs:\n31n(1+z) = -y2 n4\nS\nln(1 + z)\nFn-4\nI\n= y.\nExercises: Section 111"
        },
        {
          "category": "Resource",
          "title": "Graphing Functions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/de1c599e38662bfe32ce4082e767cad0_g_graphng_fnctns.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\nFall 2006\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nG. GRAPHING FUNCTIONS\nTo get a quick insight into how the graph of a function looks, it is very helpful to know\nhow certain simple operations on the graph are related to the way the function expression\nlooks. We consider these here.\n1. Right-left translation.\nLet c > 0. Start with the graph of some function f(x). Keep the x-axis and y-amis fixed,\nbut move the graph c units to the right, or c units to the left. (See the pictures below.) You\nget the graphs of two new functions:\n(1) Moving the f(x) graphc units to the\nright\ngives the graph of\n(x - c)\nS left\nf(X + c)\nIf f(x) is given by a formula in x, then f(x - c) is the function obtained by replacing x by\nx - c wherever it occurs in the formula. For instance,\nf()\n=(--)=\n(x-\n) 2+(X-)\n2--z, byalgebra.\nExample 1. Sketch the graph of f(s) = x2 - 2x + 1.\nSolution. By algebra, f(x) = (X - 1)2. Therefore by (1), its graph is\nthe one obtained by moving the graph of x2 one unit to the right, as shown.\nThe result is a parabola touching the x-axis at x = 1.\nTo see the reason for the rule (1), suppose the graph of f(z) is moved c units to the right:\nit becomes then the graph of a new function g(x), whose relation to f(s) is described by\n(see the picture):\nvalue of g() at zo\n=\nvalue off(z) at o - c\nf(zo-c).\nThis shows that g(z) = f(x - c).\nx\nj\nxY\nThe reasoning is similar if the\n(\ngraph is translated c units to the left. Try giving the argument\nyourself while referring to the picture.\nV-C\nThe effect of up-down translation of the graph is much simpler to see. If c > 0,\n(2) Moving the f(x)graph cunits\nf(T) +\ngives the graph of\n-c\ndown\nf (x)-- c\nsince for example moving the graph up by c units has the effect of adding c units to each\nfunction value, and therefore gives us the graph of the function f(s) + c\nExample 2. Sketch the graph of 1 +\nT/2-.\nSolution Combine rules (1) and (2). First sketch V- , then\nmove its graph 1 unit to the right to get the graph of Vf-Z,\nthen\n1 unit up to get the graph of 1 +-,- 1. as shown.\n1nio th o\nf1+-,fm -I\nx\n\nExample 3. Sketch the curve y = z2 + 4x +1.\nSolution We \"complete the square\":\n2+4z+\n= (z2 +4z+4)-3 = (z+2)2-3,\nso we move the graph of z2 to the left 2 units, then 3 units down,\nnettine\nthe granh shown.\n2. Changing scale: _stretching and shrinking.\nLet c > 1. To stretch the z-axis by the factor c means to move the point 1to the position\nformerly occupied by c,and in general, the point so to the position formerly occupied by c0o.\nSimilarly, to shrink the z-axis by the factor c means to move so to the position previously\ntaken by zo/c. What happens to the graph of f(z) when we stretch or shrink the z-axis?\nStretching\n{f(z/c)\n(3)\ntretci-ng\nthe -aisby c changes the graph of f(z) into that of\nf(c)\nShranking\nf(cm)\nThe picture explains this rule; it illustrates stretching by the factor\nfr\nXC)\nc > 1. The new function has the same value at so that f(z) has\nat zo/c, so that it is given by the rule zo -+ f(zo/c), which means\ni\nthat it is the function f(z/c).\n/e\n-o\nIf the y-axis is stretched by the factor c > 1, each y-value is multiplied by c, so the new\ngraph is that of the function cf(z):\n(4)\nrething the y-ais by c changes the graph of f(z) into that of\ncf(s)\nShrinking\nf(z)/c\nExample 4. Sketch the graph of\n.\nSolution. Start with the graph of 1/z,move it 1lunit to the\nright to get the graph of 1/(z - 1), then shrink the x-axis by the\nfactor 2 to get the graph of the given function. See the picture.\n3. Reflecting in the z- and V-axes: even and odd functions.\nTo reflect the graph of f(z) in the y-axis, just flip the plane over around\nthe y-axis. This carries the point (z, y) into the point (-z,y), and the graph\n.4A(x)\nof f(z) into the graph of f(-z). Namely, the new function has the same\ny-value at zo as f(s) has at -zo, so it is given by the rule so -+ f(-zo)\nand is the function f(-z).\n) an the\nSimilarly, reflecting the si-plane in the x-axis carries (z, y) to the point (Z,-y) and the\ngraph of f(z) gets carried into that of -f(s).\nFinally, relecting first in the y-axis and then in the x-axis carries the\npoint (z,y) into the point (-z,-y).\nThis is called a reflection through\nthe origin. The graph of f(z) gets carried into the graph of -f(-z), by\ncombining the above two results. Summarizing:\ny-azis\nf(-z)\n(5) Reflecting in the\nz-azis\nmove the graph of 1(z) into that of\n-f(z)\norigin\n-(-)\n\nG. GRAPHING FUNCTIONS\n{ y-axis\nf -X)\n(5) Reflecting inthe\ns-axis moves the graph off(s) into that of\n-f(s)\norigin .\n-f(-X).\nOf importance are those functions f(t) whose graphs, are symmetric with respect to the\ny-axis -\nthat is, reflection in the y-axis doesn't change the graph; such functions are called\neven. Funrctions whose graphs are symmetric with respect to the origin are called odd. In\nterms of their expression in s,\n(6)\nf(-X) =f(s)\ndefinition of even function\n(7)\nf(-X) = -f (s)\ndefinition of odd function\nExample 5. Show thit a polynomial with only even powers, like z4 - 2s2 + 7, is an\neven function, and a polynomial with only odd powers, like 35s - s + 2z, is an odd function\n-\nthis, by the way, explains the terminology \"even\" and uodd\" used for functions..\nSolution. We have to show (6) and (7) hold for polynomials with respectively only even\nor odd powers, but this follows immediately from the fact that for any non-negative integer\nn,.we have\ne\nifniseven,\n-e\",\nif n isodd.\nThe following easily proved rules predict the odd- or even-ness of the product or quotient\nof two odd or even functions:\n(8)\neven- even = even\nodd odd = even\nodd. even = odd\n(9)\neven/even = even\nodd/odd =even\nodd/even = odd\nExample 6.\nis of the form odd/even, therefore it is odd;\n(3 + x4)1/2(X - s)\nhas the form even odd, so it is odd.\n4. The trigonometric functions.\n... e trigonometric functions offer further illustrations of the ideas about translation,\nchange of scale, and symmetry that we have been discussing. Your book reviews the standard\nfacts about them in section 9.1, which you should refer to as needed.\nThe graphs of sinx and cos are crudely sketched below. (In calculus, the variable s is\nalways to be in radians; review radian measure in section 9.1 if you have forgotten it. Briefly,\nthere are 2r radians in a 3600 angle, so that for example a right angle is r/2 radians.)\nAs the graphs suggest and the unit circle picture shows,\n(10)\ncos(-s) = cos\n(even function)\nsin(-s) = -sin\n(odd function).\nFrom the standard triangle at the right, one sees that\ncos(7r/2 - x) \"= sinz,\nL\n\nand since cos x is an even function, this shows thi\n(11.)\ncos(x- r/2) = sinx.\nFrom (11), we see that moving the graph\nof cosax to the right by r/2 units turns it\ninto the graph of sin z. (See picture.)\nThe trigonometric function\nI\n(12)\ntan x= cos X\nis also important; its graph is sketched at the right. It is an odd\nfunction, by (9) and (10), since it has the form odd/even.\nPeriodicity\nAix-important property of the trigonometric functions is that they repeat their values:\n(13)\nsin(x + 27) = sinx,\ncos(r + 2w) = cos\n.\nThis is so because x + 2wr and x represent in radians the same angle.\nFrom the graphical point of view, equations (13) say that if we move the graph of sin\nor cosax to the left by 2wr units, it will coincide with itself.\nFrom the function viewpoint, equations (13) say that sin x and cos xare periodicfunctions,\nwith period 2r. In general, let c > 0; we say that f(x) is periodic, with period c, if\n(14)\nf(x + c) = f ()\nfor all x, and\n(14')\nc is the smallest positive number for which (14) is true.\nBy rule (1), the graph of a periodic function having period c coincides with itself when it is\ntranslated c units to the left. If we replace x by x- c in (14), we see that the graph will also\ncoincide with itself if it is moved to the right by c units. But beware: if a function is made\nby combining other periodic functions, you cannot always predict the period. For example,\nalthough it is true that\ntan(m +2r)= tanx\nand\ncos2(x + 2r)= cos 2x ,\nthe period of both tan x and cosa2 is actually r,as the above figure suggests for tan x.\nThe general sinusoidal wave.\nThe graph. of sin i is referred to as a \"pure wave\" or a \"sinusoidal oscillation\". We now\nconsider to what extent we can change how it looks by applying the geometric operations\nof translation and scale change discussed- earlier.\na) Start with sin i, which, has period 2r and oscillates between 11.\nb) Stretch the y axis by the factor A > 0; by.(4) this gives A sin x,which has period 2r\nand oscillates between ±A.\nc) Shrink the x-axis by the factor k > 0; by (3), this gives Asin ks, which has period\n2r/k,since\n2x\nAsink( + 2 ) = Asin(kx + 2r) A sin kx.\nAsmk( +%-) =\nd) Move the graph 0 units to the right; by (1), this gives\nI\n\nG. GRAPHING FUNCTIONS\n(15)\nAsink(z- ) ,\nA, ck> 0,@>0,\ngeneral sinusoidalwave\nwhich has\nA. ..--\nperiod2wi/k\n(the wave repeats itself every 2wr/k units);\nangularfrequency k\n(has kcomplete cycles as xgoes from 0 to 2wr);\nA\namplitude A\n(the wave oscillates between A and -A);\nphase angle 4\n(the midpoint of the wave is at x = 4).\n.\n----A\nNotice that the function (15) depends on three constants: k,A, and 4. We call such\nconstants parameters;their value determines the shape and position of the wave.\nBy using trigonometric identities, it is possible to write (15) in another form, which also\nhas three parameters:\n(16)\na sinkx + bcoskc\nThe relation between the parameters in the two forms is:\n(17)\na = A cos ko,\nb= -Asin ck;\nA== a2+b2,\ntantik\n-b a\nProof of the equivalence of (15) and (16).\n(15)\n= (16): from the identity sin(a + 6) = sin acosf + cosasinfl, we get\nAsin(k(x - ~)) = Asin(kx - kl)= AcosckIsinkC- Asin klcoskc\nwhich has the form of (16), with the values for a and b given in (17).\n(16)\n±= (15): square the two equations on the left of (17) and add them; this gives\na2 + b2 = A (cos2 k# + sinckt) = A2 , showing that A = /aa +b 2 .\nIf instead we take the ratio of the two equations on the left of (17), we get -b/a = tan k#,\nas promised.\nExample 7.\nFind the period, frequency, amplitude, and phase angle of the wave\nrepresented by the functions\na) 2sin(3z - r/6)\nb) -2 cos(2x - r/2)\nSolution.\na) Writing the function in the form (15), we get 2sin3(x - r/18), which shows it has\nperiod 2r/3, frequency 3, amplitude 2, and phase angle r/18 (or 100).\nb) We get rid of the - sign by using - cos\n= cos(z - 7r) -\ntranslating the cosine\ncurve wr units to the right is the same as reflecting it in the z-axis (this is the best way to\nremember such relations). We get then\n-2 cos(2z -\nr/2) = 2.cos(2z - r/2-\nr)\n= 2sin(2z- r),\nby (11);\n= 2sin2(x - 7r/2).\n\nExample 8. Sketch the curve sin2z + cos 2z.\nSolution Transforming it into the form (15), we can get A and # by using (19):\nA=v;\ntan 20=-l -\n2 =135= 3r/4, ~\n=3Ir/8.\nSo the function is also representable as V2sin2(z - 3r/8); it is a wave of amplitude V2,9\nperiod r, frequncy 2, and phase angle 37r/8, and can be sketched using this data.\n5. Reflection in the diagonal line; inverse functions.\nAs our final geometric operation on graphs, we consider the effect of\nin the diagonal line p = z.\nThis reflection can be carried out by flipping the plane over about t]\ndiagonal line. Each point of the diagonal stays fixed; the z-and y-axes a\ninterchanged. The points (a,b) and (b,a) are interchanged, as the picts\nshnws_ because the two rectangles are interchanged.\nTo see the effect of this on the function, let's consider first a simple example.\nExample 9.\nIf the graph of f() = a,\nz > 0 is reflected in the diagonal, what\nfunction corresponds to the reflected graph?\nSolution. The original curve is the graph of the equation: p=-z 2 ,\na > 0.\nReflection corresponds to interchanging the two axes; thus the reflected curve is the graph\nof the equation:\nz = y,\ny 2 0.\nTo find the corresponding function, we have to express p explicitly in terms of z, which\nwe do by solving the equation for y:\ny = V,\nz _20 ; the restriction on a follows\nbecause if z = y and y 2 0, then x2 0 also.\n-y y O= nodiag\nXxb\n_=zo'=\n,\nRemarks.\n1.\nWhen we flip the curve about the diagonal line, we do not interchange the labels\non the z- and p-axes. The coordinate axes remain the same -\nit is only the curve that\nis moved (imagine it drawn on an overhead-projector transparency, and the transparency\nfipped over). This is analogous to our discussion in section 1 oftranslation, where the curve\nwas moved to the right, but the coordinate axes themselves remained unchanged.\n2. It was necessary in the previous example to restrict the domain of z in the original\nfunction z2 ,so that after being fipped, its graph was still the graph of a function. Ifwe\nhadn't, the flipped curve would have been a parabola.lying on its side; this is not the graph\nof a function, since it has two y-values over each z-value.\nThe function having the refected graph, p=\n,\na 2 0 is called the inerse fno\ntion to the original function p = z2, z 2 0. The general procedure may be represented\nschematically by:\n\nG. GRAPHING FUNCTIONS\ny= J(s)\n-\n=fAy)\ny= 9(m)\noriginal graph\nswitch x and y\nreflected graph\nsolve for y\nreflected graph\nIn this scheme, the equations z = f (y) and y = g(z) have the same graph; all that has been\ndone is to transform the equation algebraically, so that y appears as an explicit function\nof z. This function g(z) is called the inverse function to f(z) over the given interval; in\ngeneral it will be necessary to restrict the domain of f(x) to an interval, so that the reflected\ngraph will be the graph of a function.\nTo summarize: f(z) and g(z) are inverse functions if\n(i) geometically,the graphs of f(z) and g(z) are reflections of each other in the diagonal\nline y =z;\n(ii) analytically, = f (y) and y = g(z) are equivalent equations, either arising from the\nother by solving exIlicitly for the relevant variable.\nExample 10. Find the inverse function to\n,\na>>1.1.\nz-1\nSolution. We introduce a dependent variable y, then interchange z and y, getting\ns=\n-\ny>1.\nWe solve this algebraically for y, getting\n(20)\ny= 1+-,1\n>O.\n(The domain is restricted because if y > 1, then equation (20) implies that\nx > 0.) The right side of (20) is the desired inverse function. The graphs\nenra\nsptc-heod\nIt often happens that in determining the inverse to f(z), the equation\n(21)\n= f(y)\ncannot be solved explicitly in terms of previously known functions. In that case, the corre\nsponding equation\n(22)\n,= 9()\nis viewed as deftning the inverse function to f(z), when taken with (21).. Once again, care\nmust be taken to restrict the domain of f(z) as necessary to ensure that the relected will\nindeed define a function g(z), i.e., will not be multiple-valued. A typical example is the\nfollowing.\nExample 11. Find the inverse function to sinx.\nSolution. Considering its graph, we see that for the reflected graph to define afunction,\nwe have to restrict the domain. The most natural choice is to consider the restricted function\n(23) \"\ny= sinz,\n-ir/2 < s < w/2.\n\nExample 11. Find the inverse function to sin x.\nSolution. Considering its graph, we see that for the reflected graph to define a function,\nwe have to restrict the domain. The most natural choice is to consider the restricted function\n(23)\ny = sinx,\n.- r/2 < z < 7r/2.\n-\nThe inverse function is then denoted sin\nx, or sometimes Arcsin x; it is defined by the\npair of equivalent equations\n(24)\n=siny,\n-ir/2 < x < r/2\n=\ny=sin-lz, -1\nz<l1.\nThe domain [-1, 1] of sin - 1 is evident from the picture -\nit is the same as the range of\nsinx over [--r/2, r/2].\nAs examples of its values, sin- ' = r/2, since sinir/2 = 1; similarly, sin- ' 1/2 = r/6.\nCare is needed in handling this function. For example, substituting the left equation in\n(24) into the right equation says that\n(25)\nsin-1(siny) = y,\n-r/2 < y 5 x/2 .\nIt is common to see the restriction on y carelessly omitted, since the equation by itself seems\n\"obvious\". But without the restriction, it is not even true; for example if y =i,\nsin - 1(sin 7r) = 0.\n.1\nI\nExercises: Section 1A"
        },
        {
          "category": "Resource",
          "title": "Heaviside's Cover-up Method",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/dda2e5c7522ac8cd77fba8ba9f8524ac_f_hscvr_up_methd.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\nFall 2006\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nF. HEAVISIDE'S COVER-UP METHOD\nThe eponymous method was introduced by Oliver Heaviside as a fast way to do a decom\nposition into partial fractions. In 18.01 we need the partial fractions decomposition in order\nto integrate'rational functions (i.e., quotients of polynomials). In 18.03, it will be needed as\nan essential step in using the Laplace transform to solve differential equations, and in fact\nthis yvas more or less Heaviside's original motivation.\nThe cover-up method can be used to make a partial fractions decomposition of a rational\nfunction p-) whenever the denominator can be factored into distinct linearfactors.\nWe illustrate with an example; though simple, it should convince you that the method is\nworth learning.\nx-7\nExample 1. Decompose (\n1)( +2) into partial fractions.\nSolution. We know the answer will have the form\nx-7\nA\nB\n(x-1)(x+2)-\nx-1 +x+2\nTo determine A by the cover-up method, on the left-hand side we mentally remove (or cover\nup with a finger) the factor x - 1 associated with A, and substitute x = 1 into what's left;\nthis gives A:\nx-7\n1-7\n(2)\n((+2)x,=1 = -\n= -2= A.\n1+2\nSimilarly, B is found by covering up the factor x + 2 on the left, and substituting x = -2\ninto what's left. This gives\nx-7\n-2-7\nx -\n= 3 = B.\n(x - 1)\nI=-2\n-2-1\nThus, our answer is\nx-7\n-2\n(3)\n(X-x\nWX + 2)\n+ 3\n(3)\n(x-1)(x+2)\nx-1\nX -\n+ x+2\n-\n+2\nWhy does the method work? The reason is simple. The \"right\" way to determine A from\nequation (1) would be to multiply both sides by (z - 1); this would give\n(4)\nx-\nB\n(4)\n(x + 2) = A +\n+ 2(-1).\nNow if we substitute x = 1, what we get is exactly equation (2), since the term on the right\ndisappears. The cover-up method therefore is just any easy way of doing the calculation\nwithout going to the fuss of writing (4) -\nit's unnecessary to write the term containing B\nsince it will become 0 .\n\nF. COVER-UP METHOD\nIn general, if the denominator of the rational function factots into the product of distinct\nlinear factors:\np(z)\nA1\nA,\n( -al)(z -a2)..-(-a)\n=\nA -\n+ ...+ -ar, 4r\na\na ,\nx-a\nthen Ai is found by covering up the factor x - ai pn the left, and setting x = a4 in. the rest\nof the expression.\nExample 2. Decompose\ninto partial fractions.\nSolution. Factoring, x33 _ =. x( 2 - 1) = z(x - 1)(z + 1). By the cover-up method,\n-1\n1/2\n1/2\nz(=-1)(z+11)\nX\nX-1\nz+ '\nTo be honest, the real difficulty in all of the partial fractions methods (the cover-up\nmethod being no exception) is in factoring the denominator. Even the programs which do\n*symbolic integration, like Macsyma, or Maple, can only factor polynomials whose factors\nhave integer coefficients, or \"easycoefficients\" like V2.and therefore they can only integrate\nrational functions with \"easily-factored\" denominators. .(Of course, these are the only kind\nyou'll see in 18.01 or 18.03.),\nHeaviside's cover-up method also can be used even when the denominator doesn't factor\ninto distinct linear factors. To be sure, it gives only partial results, but these can often be\na big help. We illustrate.\n5z\nExample 3. Decompose\n5= +6\n\n(x2+4)(x - 2)\nSolution. We write\n.5z+6\nAz+B\nC\n(5\n2)\nX2 +4 + x-2\n( 2 + 4)-x(z-\nWe first determine C by the cover-up method, getting C = 2. Then A and B can be.found\nby the method of undetermined coefficients; the work is greatly reduced since we need to\nsolve only two simultaneous equations to find A and B, not three.\nFollowing this plan, using C = 2, we combine terms.on the right of (5) so that both sides\nhave the same denominator. The numerators must then also be equal, which gives us\n(6)\n5x+ 6= (Ax + B)(z - 2) + 2(X2 + 4).\nComparing the coefficients say of z 2 and of the constant terms on both sides of (6) then\ngives respectively the two equations\n0=A+2\nand\n6= -2B + 8,\nfrom which A =-2 and .B= 1\nIn using (6), one could have instead compared the coefficients of x, getting 5 = -2A+ B;\nthis provides a valuable check on the correctness of our values for A and B.\n\nF. COVER-UP METHOD\nIn Example 3, an alternative to undetermined coefficients would be to substitute two\nnumerical values for x into the original equation (5), say z = 0 and z = 1 (any values\nother than x = 2 are usable). Again one gets two simultaneous equations for A and B.\nThis method requires addition of fractions, and is usually better when only one coefficient\nremains to be determined (as in Example 4 below).\nStill another method would be to factor the denominator completely into linear factors,\nusing complex coefficients, and then use the cover-up method, but with complex numbers. At\nthe end, conjugate complex terms have to be combined in pairs to produce real summands.\nThe calculations are sometimes longer, and require skill with complex numbers.\nThe cover-up method can also be used if a linear factor is repeated, but there too it gives\njust partial results. It applies only to the highest power of the linearfactor. Once again, we\nillustrate.\nExample 4. Decompose ((X- 1)2(X +2)\nSolution. We write\nA\nB\nC\n(7)\n(\n)\n+2\n-\n+\n+\n(Z- 1)2(X + 2)\n(z- 1)2\nz- 1\nx+2\nTo find A cover up ( - 1)2 and set x = 1; you get A = 1/3. To find C,cover up z+2,and\nset x = -2; you get C = 1/9.\nThis leaves B which cannot be found by the cover-up method. But since A and C are\nalready known in (7), B can be found by substituting any numerical value (other than 1 or\n-2) for z in equation (7). For instance, if we put z = 0 and remember that A = 1/3 and\nC = 1/9, we get\n1/3\nB\n1/9\ni = -+-l+-2\nfrom which we see that B = -1/9.\nB could also be found by applying the method of undetermined coefficients to the equation\n(7); note that since A and C are known, it is.enough to get a single linear equation in order\nto determine B -\nsimultaneous equations are no longer needed.\nThe fact that the cover-up method works for just the highest power of the repeated linear\nfactor can be seen just as before. In the above example for instance, the cover-up method\nfor finding A is just a short way of multiplying equation (5) through by (x - 1)2 and then\nsubstituting z = 1 into the resulting equation.\nExercises 5E"
        },
        {
          "category": "Resource",
          "title": "Improper Integrals",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/f8a4294ebb7f7bb04f6d80c404696934_int_imp_integrl.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\nFall 2006\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nINT.\nIMPROPER INTEGRALS\nIn deciding whether an improper integral converges or diverges, it is often awkward or\nimpossible to try to decide this by actually carrying out the integration, i.e., finding an\nantiderivative explicitly. For example both of these two improper integrals converge:\n0\"\ndaz\nz+3xs +22 + 1'\nand\nz\nbut there is no explicit antiderivative for the second integral, and finding one for the first\nwould be a hairy exercise in partial fractions, even ifone were able to factor the denominator.\nInstead of explicit integration, therefore, we show they converge by using estimation\ninstead, comparing them with simpler integrals which are known to converge. Thus, for the\nfirst,\ns6 + 3Zs + 2\n+1\n-\n~'\nX> 0,\nso that\nJ\nd0\n\"\ndsz + <\nz*=E.\nx\n\nz+3X3+2X2+1\nSince the right hand integral converges, so does the left, which is smaller (but still positive).\nIn a similar way, for the second integral, we estimate\n~P,'FF\nX> 0,\nso that\nI F\nv& d <\n<i\nF-/-\n= 2.\nIn the same way we can show the divergence say of F. Zd+\nt\nthis last being equivalent to 2zs/ 2>\n, i.e., to 8z Z2\n+ 1; thus we get\nJ idz\n.1J*\nx dz+\n>\n.FI2 0\nWe call the general principle we are using here the\n\nINT. IMPROPER INTEGRALS\nComparison Test for Improper Integrals. If 0 < f(z) < g(z) for a 5 z < oo,\n(1)\ng()\nconverges\n(z) d converges;\n(1')\n)f\ncf(x) diverges\ng(x) dz divevrs\nj(1\ndz\n*\n))da\ndiverges.\nIn other words, if the area under g(z) is finite and .f(x) lies below g(x)\n8x1\n(but still over the x-axis), then the area under f(z) will be finite also. Or\nequivalently, if the area under f(x) is infinite, so is the area under g(z).\nIn using the test, the lower limit of integration is of no importance, since if a < b,\nf() dT =\nf (x)dx +\nf (z) di,\nso that\n(3)\nf(z) d, converges\n*\nf (x) d, converges.\nAs another example of the use of the comparison test, we recall one used to illustrate the\nSecond Fundamental Theorem (cf. p. FT.3):\ne- Z < e-\nfor z > 1,and therefore\ne-2 dz converges, since\ne-\"dz converges.\n(Here we use (3) above to shift the lower limit from I to 0; also the comparison test (1).)\nIn using the comparison test, we have to have some standard integrals that we know\nconverge or diverge, to use for comparison purposes. The most useful are:\n(4)\njd/\" ds converges if p > 1,diverges ifp\n1;\n(5)\ne\"\nconverges ifa > 0.\nIt is important to notice that in using the test, you must make the inequality go in the\nright direction, which means you must guess in advance whether the integral will converge\nor diverge. For example, to test\nzIr-,\nwe see that as z -\n00o,\nX\nzX\nz3\n-\niF\nx'\ns\nso from (4) we guess it will converge. Unfortunately, s-\n>\n,a\n_ 2, so we can't\nuse j\nas the comparison integral; however, only a slight change is needed:\nz\n-2s2<2\nX>2,\nas we see by cross-multiplying: z8 < 2zs - 4; thus our integral converges, using (1). D\n\n·\nINT. IMPROPER INTEGRALS\nImproper integrals of the second kind\nThe comparison test also works for improper integrals of the form\nf(x) dz,\nwhere\nlim f(z) = oo,\nand\nf (x)d,\nwhere\nlim f () = oo.\nSometimes these are called improper integrals ofthe second kind\n- the first kind being the previous type of improper integral, where\none of the limits of integration is oo or -oo.\nb\nFor improper integrals of the second kind, useful standard comparison integrals are\n(6)b\nJ, I\n(b- x)P\nnd\nJ+ (x\ndx\n- a)P'\nwhich converge if p < 1, diverge if p _ 1 . Note that this is just the\nopposite of (4). However, it's easy to remember which is which if you\nthink of the picture at the right, which compares the graphs for p < 1\nand\n> 1.\n------ 4r-\n--\nm-\nC\nExample. Test J\nfor convergence.\nSolution. The. integrand becomes infinite when x = 0 and when z = 2, but 2 is of no\nimportance, since it's not in the interval over which we are integrating. In making our guess\nas to convergence or divergence, we note that\n.X\nszy O+ .\nThus, according to (6) above, we expect convergence, and make the comparison\n<\n(7)\nWhere is (7) valid (if at all)? Crossmultiplying, it claims that\n2,\nVi5/2 - x\nor squaring, x< 2x- z 2, or X2 <.\nThis last inequality is true if 0 < z < 1, so we're safe, since this is the region of integration.\nSo we see by the comparison test above that our integral converges because according to\nf 1 dx\n(6), the integral\n-\nconverges.\nRemark. It isn't customary to include the + and - symbols in a+ and b- when one\nwrites integrals of the second kind. We only did it here to point out where they are improper,\nand on which side. It doesn't appear in the solutions to the exercises on improper integrals.\nExercises: Section 6B"
        },
        {
          "category": "Resource",
          "title": "Mean-value Theorem",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/4dbb60ed1403fc2616e337fe9fb43224_mvt_mns_vluethrm.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\nFall 2006\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nMVT. MEAN-VALUE THEOREM\nThere are two forms in which the Mean-value Theorem can appear;1 you should get\nfamiliar with both of them. Assuming for simplicity that f(z) is differentiable on an interval\nwhose endpoints are a and b, or a and z, the theorem says\nf(b) - f(a) =\nfor some c between a and b;\nb-a\nf(x) = f(a) + f'(c)(z - a),\nfor some c between a and x\nThe first form (1) has an intuitive geometric interpretation in terms of the slope of a\nsecant being equal to the slope of the graph at some point c. and in this form, it's easy to\ngive an intuitive argument for the theorem.\nThe second form (2) looks less intuitive, but all that has been done is to multiply both\nsides of (1). by b-a, transpose a term, and change the name of btd z. Now it's not a theorem\nabout slopes; instead, it says that the value of f at some point z can be estimated, provided\nyou know the value of f at some fixed point a, and have information about the size of f' on\nthe interval [a,z]. In other words, from information about f',we can get information about\nf. (Such information can also be gotten by integration; one can think of the Mean-value\nTheorem as a poor-person's substitute for integration.)\nThe special case of (1)in which f(a) = f(b) = 0 is. usually called Rolle's theorem; it says\nthat if f is differentiable on [a, b],\nf(a) =f(b) = 0\nSf'(c)= 0\nfor some c, where a < c < b.\nExercises: Section 2G\nIsee Simmons, p. 76"
        },
        {
          "category": "Resource",
          "title": "Properties of Definite Integrals",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/1e8fe8c3547318a2648aebc582c55e54_pi_pr_dfntintgrl.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\nFall 2006\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nPI. PROPERTIES OF INTEGRALS\nFor ease in using the definite integral, it is important to know its properties. Your book\nlists the following' (on the right, we give a name to the property):\n(1)\nf () dx = -\nabf(f(s)da\nintegrating backwards\n(2)\nf(T)dz =0\n(3)\nf (x)dz =\nf(z)dx + jf(z)dz\ninterval addition\n(4)\n+g)d =\nf()d +\n9g(x)d\nlinearity\nl f(z)dz = c f ()dx\nlinearity\nf(z)d < jg(z)dl\ns\nif f()\ng() on [a, b\nestimation\nProperty (5)is useful in estimating definite integrals that cannot be calculated exactly.\nExample 1. Show that\nr-dz < 1.3.\nSolution. We estimate the integrand, and then use (6). We have\nz3 < X\non [0, 1];\ne VI w\n=\n(1+ )o\n=\n(2%/i-\n1) m1.22 < 1.3 .\nWe add two more properties to the above list.\n(Q) if(x)dxl :5\nf(x)j d .\nabsolute value property\nProperty (6) is used to estimate the size of an integral whose integrand is both positive and\nnegative (which often makes the direct use of (5) awkward). The idea behind (6) is that on\nthe left side, the intervals on which f(x) is negative give a negative value to the integral,\nand these \"negative\" areas lower the overall value of the integral; on the right the integrand\nhas been changed so that it is always positive, which makes the integral larger.\nExample 2. Estimatethesize of\ne-\nsin dz.\n1m Sinmoms pp. 214-215\n. ..\n\nr·\n·.\n~ ·\nPI. PROPEFTIES OF INTEGRALS\nSolution. A crude estimate would be\ne-' sinz dxj :_]\ne-x'sinzl-\nIo\n<\ne-\"dz,\nby (5), since Isinsxj 1;\n]00\n=--e -\n-e 10 +1 < 1.\nA final property tells one how to change the variable in a definite integral. The.formula is\nthe most important reason for including dz in the notation for the definite integral, that is,\nwriting\nf(z) dz for the integral, rather than simply\nf (z),as some authors do.\nId\n*\n,\n= u(\nd),\n(7)\nf(u)du =\nf(u())\ndx,\nc = U(a),\nchange of variables formula\nt = u(b).\nIn words, we can change the variable from u to z, provided we\n(i) express du in terms of dx;\n(ii) change the limits of integration.2\nThere are various possible hypotheses on u(x); the simplest is that it should be differen\ntiable, and either increasing or decreasing on the x-interval [a, b].\nExample 3. Evaluate\n(1+ue) / 2\nby substituting u= tanx.\nSolution.\nFor the limits, we have u = 0,1 corresponding to z = 0, /4; tan x is.\nincreasing.\nI\ndur\nl sec2 z\n+(1\n+ 2)s/2\nj=\nsec a\nw/4\n1/4\n-\ncos\ndxs = sin]\n= -N.\nProof of (7). We use the First Fundamental Theorem3 and the chain rule. Let F(u) be\nan antiderivative:\n(8)\nF(u) =\nf(u) du;\nd\ndF du\ndu\nF(U()) =\n-\n= f (uu)\n,\nby the chain rule. So\n(9)\n)) =\nf (\n))\nd.\nTherefore\nf(u) du = F(d) - F(c),\nby the First hdamental Theorem and (8);\n= F(u(b)) - F(u(a)) =\nf(u(x)) ds, '\nby the First -undamental Theorem and (9).\nD\nasee Simmons p.839 for a discussion and an example\n3aee the naxt page"
        },
        {
          "category": "Resource",
          "title": "Second Fundamental Theorem",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/5ee9af81a652866ad75ee70e4c4a07c7_ft_scn_fnd_thorm.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n\n18.01 Single Variable Calculus\nFall 2006\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nFT. SECOND FUNDAMENTAL THEOREM\n1. The Two Fundamental Theorems of Calculus\nThe Fundamental Theorem of Calculus really consists of two closely related theorems,\nusually called nowadays (not very imaginatively) the First and Second Fundamental Theo\nrems. Of the two, it is the First Fundamental Theorem that is the familiar one used all the\ntime. It is the theorem that tells you how to evaluate a definite integral without having to\ngo back to its definition as the limit of a sum of rectangles.\nFirst Fundamental Theorem Let f(x) be continuous on [a, b]. Suppose there is a\nfunction F(x) such that f(x) = F'(x) . Then\nb\nb\n(1)\nf f()dx\nF(b) F(a). =\n)\n=\n-\nF(x)\n(The last equality just gives another way of writing F(b) - F(a)that is in widespread use.)\nStill another way of writing the theorem is to observe that F(x) is an antiderivative for f(x),\nor as it-is sometimes called, an indefinite integral for f(x); using the standard notation for\nindefinite integral and the bracket notation given above, the theorem would be written\n(1')\nf (x) dx = f (x)d\n.\nWriting the theorem this way makes it look sort of catchy, and more importantly, it avoids\nhaving to introduce the new symbol F(x) for the antiderivative.\nIn contrast with the above theorem, which every calculus student knows, the Second\nFundamental Theorem is more obscure and seems less useful. The purpose of this chapter is\nto explain it, show its use and importance, and to show how the two theorems are related.\nTo start things off, here it is.\nSecond Fundamental Theorem. Let f(x) be continuous, andfix a.\n(2)\nSet F() =\nf (t)dt;\nthen F'(x) = f(x).\nWe begin by interpreting (2) geometrically. Start with the graph of f(t)\nin the ty-plane. Then F(x) represents the area under f(t) between a and\nx; it is a function' of x. Its derivative -\nthe rate of change of area with\nrespect to x -\nis the length of the dark vertical line. This is what (2) says\ne met.ricrnll\nISimmons calls this function A(x) on p. 207 (2nd edition); this section of Simmons is another presentation\nof much of the material given here.\n\nExample 1. Verify (2) if f(x) = 2xsin 2 and a= 0.\nSolution. Here we can integrate explicitly by finding an antiderivative (using the first\nfundamental theorem):\nF(W) =\n2tsintdt\n- cos t2\n-cosx\n+ 1;\ndifferentiating by the chain rule, we verify that indeed F'(x) = 2zsinx 2, as predicted by\n(2).\nO\n*Example2. Let F(z) =\nSsint dt.\nFind F'(7r/2).\nSolution.\nNeither integration techniques nor integral tables will produce an explicit\nantiderivative for the function in the integrand. So we cannot use (1). But we can use (2),\nwhich says that\nsin 7r/2\nF'(R/2) = i/2\n/2\nMany students feel the Second Fundamental Theorem is \"obvious\"; these students are\nconfusing it with the similar-looking\n(2')\nLet F(x) =\nf f(x) dx;\nthen F'(x) = f(x).\nIndeed, (2') is obvious. The \"integral\" in it is an indefinite integral, i.e., an antiderivative.\nSo what (2') says is: \"Let F(x) be an antiderivative for f (x); then F(x) is an antiderivative\nfor f(x) -\na true statement, but not a very exciting one (logicians call it a tautology.)\nThe Second Fundamental Theorem (2) looks almost the same as (2'), but it is actually\nentirely different, because F(x) is defined as a definite integral. The next section, which\ncontinues the discussion, should help show the difference.\n2. Do functions have antiderivatives?\nThe First Fundamental Theorem tells us how to calculate\nf(x) dx by finding an anti\nderivative for f(x). But the theorem isn't so useful if you can't find an antiderivative. Most\ncalculus students think for example that e'-\nhas no antiderivative -\n\"the integral isn't\nin the tables\", \"it can't be integrated\" are some of the ways they express this. Even for a\nsimple function like 11-\n, it is not obvious what the antiderivative is. Perhaps it doesn't\nhave any?\nThe Second Fundamental Theorem provides the answer; it says:\nEvery contiziuous function f(x) has an antoiderivative:\nf(t)dt.\nThe antiderivative may not be expressible in terms of elementary functions -\nthis is the\ndifficulty with e- \"2 -\nbut it always exists.\n\nFT. THE SECOND FUNDAMENTAL THEOREM\nExample 3. Find an antiderivative for\nSolution. This doesn't look so easy to do explicitly. But the Second Fundamental Theorem\nsays the following function is an antiderivative:\n(3) ..\nF(z).-\ndt\nDiscussion.\nYou may feel that this doesn't represent progress: the formula for the\nantiderivative is useless. But that's not so: the function F(z) can be calculated by numerical\nintegration. It can be programmed into a calculator so that when you press an z-value, the\nscreen will display the corresponding value of F(z) to 12 decimal digits. Pressing another\nbutton will draw the graph of F(z) over any interval on the z-axis that you specify.\nRepeating what we said earlier, the integral in (3) should be carefully distinguished from\ndz -\nthis \"integral\" is just another notation for the antiderivative, and is\ntherefore not a solution to the problem. The integral in (3) by contrast is a perfectly\ndefinite function, and it does solve the problem of finding an antiderivative..\nIn this case, it turns out that F(z) does have an expression in terms of elementary\nfunctions. It is\n(4)\nF(z) =\n(f+ 1)3/8-4(v +1)1'\n(You can prove this is correct by differentiating it; the 8/3 is put in to ensure that F(O) = 0,\nas definition (3)requires.)\nThe above way of writing F(z) is different from (3). Whether or not it is a better way\ndepends on what you want to know about F(.) and what use you want to make of it. For\ninstance,\nIs F(z) > 0 when z > 07\nThe answer is clearly \"yes\" if we look at the integral (3), since the integrand is positive; it\nis not at all clear what the answer is if instead we look at (4), because of the - sign. As\n.another example, what is F'(z) ? From (3) the answer is immediate, whereas from (4) you\nwould have to calculate for a while -\nas you will know if you took the trouble to check its\ncorrectnessl\n\n3. Defining new functions: In(x) and erf(x).\nOne important use of the Second Fundamental Theorem is to define new functions. Cal\nculus can then be used to study their properties.\nTo illustrate, we consider first an old function: Inz.Let's pretend we know nothing of\nlogarithms. We do know that\nSXd = n+l'\nno-1 .\nHowever, we know no explicit formula for an antiderivative of 1/x, i.e., when n = -1.\nWe therefore use the Second Fundamental Theorem to define an antiderivative of 1/z,\nnamely\n(5)\nL(x) =\nd\n(We use 1 as the lower limit of integration since the integrand is not defined at 0.) What\nare the properties of this function?\nProperties of L(x) =\ndt\nL-1. L(x) is defined for x > 0 (since 1/t is continuous for t > 0);\nL-2. L(1) = 0;\nL-3. L'(x) = 1/x, by the Second Fundamental Theorem;\nL-4. L\"(z) = -I/za, by differentiating 1/x;\nL-5. L(x) is increasing for all z > 0, since L'(x) > 0; its graph is concave (i.e., concave\ndown) since L\"(s) > 0;\nL-6. L(ab) =L(a) +L(b).\nOf course, it is this last which is the interesting property; the proof of it is elegant.\nProof of L-6. We break up the integral defining L(ab) into two parts, the first of which\nis L(a): to do this, we use the ihiterval addition rule (3) in Notes PI.1 .\n(6)\nL(ab)\na -d = =tIT +\nab dt\nComparing with Property L-6, we see we have to show the last integral on the right above\nhas the value L(b). To see this, make the change of variable t = au and apply the change\nof'variable rule (see (7), p. PI.2 in these notes). You get successively\ndt\nadu\ndo\nt= au,\ndt = adu,\nt\nau\ndu\nt\natu\n*u\nWe have to change the limits on the integral also: t = a and t = ab correspond respectively\nto u = 1,u = b. Thus the rule for changing variable in a definite integral gives\njabdt\ndu\n=\n\nFT. THE SECOND FUNDAMENTAL THEOREM\nwhich proves L-6.\nO\nOnce we have this, the other properties of the logarithm follow in a standard way.\nL-7. L(1/a) = -L(a), since L(1/a)+ L(a) = L(! - a) = L(1) = 0 .\nL-8.\nlim L(z) = oo; namely, L(z) is incrieasing and L(2\") increases without bound as\nn--+oo, since L(2\") = nL(2), by Property 6; note that L(2) > 0since f(z) is increasing. 0\nIn our definition of L(x), the number e appears as the unique number such that\nL(e) =\n=\n1.\nSuch a number exists by the Intermediate Value Theorem,2 since L(z) is increasing, contin\nuous (since it has a derivative), and gets bigger than 1.\nWe now turn to a second example of using the Second Fundamental Theorem to define\na function F(x) -\nthis time, the function will be genuinely new. It is closely related to\nan important function in probability and statistics, the error function erf x. (Statistics\noriented calculators have a button for it.) The two functions differ only by a change of scale\non the x- and y-axes. There is no simpler or more elementary expression for F(x).\nExample 5. Define a function F(z) by F(z) =\ne-' dt.\nSketch the graph of F(x), indicating relative maxima and minima', points of inflection,\nsaAmmeatrie.\nRatimQ+a\nP(1) ro 1 C&hlJ\ne-t\nSolution. The graph of f(t) = e- t is shown.\n/\nF(z) is the indicated shaded area under the graph of f(t).\nx\n2...\n-.\n-.\n.-\n-.\n..\n..\n..\n.\n-\nF'(x) = e\nby the Second Fundamental Theorem; since the exponential\npositive, this shows F(z) is increasing for all x, and therefore it has no relative ,\nminima.\nF\"(z) = -2xe- 2 ;since F\"(z) < 0 for z > 0, the graph of F(x) is\nconcave (down) when x > 0. Similarly, it is convex (concave up) for z < 0,\nand it h\na po\n=\nnint of inflection at x\n-\nF(z) is an odd function. To see this, we note that e 2 is an even function. As the\npicture shows, the.two shaded areas are equal; the one on the left however must be counted\nnegatively, since the integration is backwards: if z > 0, then\n'4y\n-=_r\nI\nF(., A.\n-\nI\nflt~ fIt\n'&\n-x\nI\nW-- =J\n-- Jo\n),I\n=j.--\nJof\n-+.... -WM.\n-X I\nThis shows F(x) is an odd function.\ne\nF(1) can be estimated as the area under f(t) between 0 and 1; it is\nroughly comparable to the area of the trapezoid shown, which about .7.\n2Simmons, p. 78\n\n4. Proof of the two Fundamental Theorems.\nWe will give an intuitive argument.for the Second Fundamental Theorem, and then\ndeduce the First Fundamental Theorem from the Second. Though the argument for the\nSecond Theorem is only suggestive, it has the right ideas in it, and can be easily made\nrigorous if you have available a precise definition of limit.3\n.-\nSecond Fundamental Theorem: Intuitive Argument\nWe wish to prove that if f (s) is continuous, then\nWe calculate F'(s)using the definition of derivative. Let a change by Az,\nandt let. AF be.the corresponding change in F(-s). From the picture.\nAF = F(x + Ax) - F(c) = j\nf(t) dt\n(9)\na f(s)Az,\nsince the area of the vertical strip under the curve is approximately the same as the area of\nthe rectangle. Dividing, we have\nAF\nwhere the error in the approximation is bounded by the height of the small curved triangle.\nSince .f(t)is continuous, the error is small compared with f(x), and disappears when we\npass to the limit as Ax --0; we get therefore\nF'(x) = lim\n= f(x).\nNote that if f(t) were discontinuous at the point x, the result would not be true; as the\npicture shows, the approximation in (9) would not be true.\nFirst Fundamental Theorem: Proof4\nWe want to show that if f(z) is continuous and f(x)'= F'(x), then\n(10)\nf(x) dz = F(b)- F(a).\nWe begin by defining\n(11)\n(z)=\nf (t) dt;\nthen\nG'(x) = f(x),\nby the Second Fundamental Theorem.\n3A somewhat fuller argument is given in Simmons, Step 1, p. 206-7.\n4this same classical reasoning is given in Simmons: Steps' 2 and 3, p. 207.\n\nFT. THE SECOND FUNDAMENTAL THEOREM\nSince G'(x) = f(x) = F'(x), we have (G(x) - F(s))' = 0, which implies G(x) - F(x) = C,\ni.e.,\n(12)\nG(x) = F(x) + C,\nfor some constant C.\nTo evaluate C, we put x = a in (12); since G(a) = 0, we get\nC = --F(a).\nFinally, put x = b in equation (12), and use the above value for C:\nG(b) = F(b)- F(a),\nwhich is exactly (10), in view of the definition of G(x).\nO\nRemark. Both fundamental theorems say that differentiation and definite integration\nare inverse operations: each undoes what the other does. In the First Fundamental Theorem\nwe differentiate, then integrate:\nF(x) --\nF'(x)\nF'(t)dt= F(x) -F(a);\nIn the Second Fundamental Theorem, we integrate, then differentiate:\nf(t) ---\nf)\n-W\ndA\nd\nf(t) dt = f(x) .\nIn both cases, the theorem says that we end up essentially where we started - only \"essen\ntially\" because of the additive constant in F(x).\n(Of course, differentiation and indefinite integration are also inverse operations, but this\nis trivial - it's just a restating of the definition of indefinite integration.)\nExercises: Section 3D"
        }
      ],
      "source_url": "https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/",
      "course_info": "18.01 | Undergraduate",
      "subject": "General"
    },
    {
      "course_name": "Linear Algebra",
      "course_description": "This course offers a rigorous treatment of linear algebra, including vector spaces, systems of linear equations, bases, linear independence, matrices, determinants, eigenvalues, inner products, quadratic forms, and canonical forms of matrices. Compared with 18.06 Linear Algebra, more emphasis is placed on theory and proofs.",
      "topics": [
        "Mathematics",
        "Linear Algebra",
        "Mathematics",
        "Linear Algebra"
      ],
      "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nPrerequisites\n\n18.02 Multivariable Calculus\n\nDescription\n\nThis course offers a rigorous treatment of linear algebra, including vector spaces, systems of linear equations, bases, linear independence, matrices, determinants, eigenvalues, inner products, quadratic forms, and canonical forms of matrices. Compared with\n18.06 Linear Algebra\n, more emphasis is placed on theory and proofs.\n\nTextbook\n\nAxler, Sheldon J.\nLinear Algebra Done Right\n. Springer, 2004. ISBN: 9780387982588. [Preview with\nGoogle Books\n]\n\nHomework\n\nThere will be nine graded homework assignments. Late homework will not be accepted.\n\nExams\n\nThere will be three eighty-minute exams during the lecture hour, and one three-hour final exam. All exams will be closed book.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem Sets\n\nEighty-minute Exam 1\n\nEighty-minute Exam 2\n\nEighty-minute Exam 3\n\nFinal Exam",
      "files": [
        {
          "category": "Assignment",
          "title": "18.700 Fall 2013 Problem Set 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-700-linear-algebra-fall-2013/2902800c50a29c090163aa39fb997b32_MIT18_700F13_ps1.pdf",
          "content": "18.700 Problem Set 1\nDue in class Tuesday September 17; late work will not be accepted. Your work on\ngraded problem sets should written entirely on your own, although you may consult\nothers before writing. I have written a few words about what kind of solution I am\nlooking for; of course you can get more hints about that (for future use) from the\nposted solutions.\n1. (3 points) Consider the set of complex numbers\nG = {a + bi | a, b ∈ Q}.\n(The G stands for Gauss; these numbers might be called Gaussian rational numbers,\nalthough I don't know if they actually are.) Is G a field (with the same addition\nand multiplication operations as in C)? For a question like this, you should either\nexplain why all the axioms for a field are satisfied (you can assume that they hold\nfor C), or else explain why one of the axioms fails. A few sentences could be enough\nto write.\n2. (3 points) Consider the set of complex numbers\nM = {re 2πiθ | r, θ ∈ Q}.\nIs M a field?\n3. (3 points) The vector space V = (F2)2 has exactly four vectors (0, 0), (0, 1),\n(1, 0), and (1, 1); so V has exactly 24 = 16 subsets. How many of these 16 subsets\nare linearly independent? How many bases does V have? For a question like this,\nyou might write some words explaining why some kinds of subset cannot possibly\nbe linearly independent (\"the vector (1, 1) is in the pay of Big Oil, and so cannot\nbe part of any linearly independent set\"). After this you might be left with just a\nfew cases; you could perhaps say a few words about why each of these is or is not\nlinearly independent.\n4. (3 points) Axler, page 19, exercise 5.\n5. (3 points) Axler, page 19, exercise 15.\n6. (3 points) Axler, page 35, exercise 1.\n7. (3 points) Axler, page 35, exercise 3.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.700 Linear Algebra\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.700 Fall 2013 Problem Set 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-700-linear-algebra-fall-2013/0c06d39b6060e86b711be1c3b1f0a224_MIT18_700F13_ps2.pdf",
          "content": "18.700 Problem Set 2\nDue in class Monday September 24; late work will not be accepted. Your work\non graded problem sets should written entirely on your own, although you may\nconsult others before writing.\n1. (3 points) Let V be the vector space of polynomials of degree at most five\nwith real coefficients. Define a linear map\nT : V → R3 ,\nT (p) = (p(1), p(2), p(3)).\nThat is, the coordinates of the vector T (p) are the values of p at 1, 2, and 3.\na) Find a basis of the null space of T .\nb) Find a basis of the range of T .\n2. (3 points) Let V be the vector space of polynomials of degree at most 999\nwith real coefficients. Define a linear map\n→ R100\nT : V\n,\nT (p) = (p(1), p(2), . . . , p(100)).\na) Find the dimension of the null space of T .\nb) Find the dimension of the range of T .\n3. (6 points) Let V be the vector space of polynomials of degree at most 99 with\nreal coefficients. Define a linear map\n→ R1000\nT : V\n,\nT (p) = (p(1), p(2), . . . , p(1000)).\na) Find the dimension of the null space of T .\nb) Find the dimension of the range of T .\nc) (This one is hard.) Is the vector (0, 1, 0, 1, 0, 1, . . . , 0, 1) in the range of T ? That\nis, is there a polynomial of degree at most 99 whose values at 1, 2, . . . , 1000\nalternate between 0 and 1?\n4. (2 points) Axler, page 36, exercise 12.\n5. (2 points) Axler, page 36, exercise 16.\n6. (2 points) Axler, page 36, exercise 17.\n7. (2 points) Axler, page 59, exercise 7.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.700 Linear Algebra\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.700 Fall 2013 Problem Set 3",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-700-linear-algebra-fall-2013/e83e35d60021462f72fa94d54a398fa9_MIT18_700F13_ps3.pdf",
          "content": "18.700 Problem Set 3\nDue in class Tuesday October 10; late work will not be accepted. Your work on\ngraded problem sets should written entirely on your own, although you may consult\nothers before writing.\n1. (3 points) Give an example of a 3×3 matrix A of real numbers whose reduced\nrow-echelon form is\n⎛\n⎞\n1/2\n⎝ 0\n1/3 ⎠\nand such that every entry of A is a nonzero integer.\n2. (3 points) The finite field F9 contains Z/3Z and an element I'll call x satisfying\nx2 + 1 = 0. Using this fact, write down the 9 × 9 multiplication table for F9.\n3. (6 points) Suppose p is any prime number. Imitating the complex numbers,\nyou can define a set of p elements with addition and multiplication:\nRp = {a + bi | a, b ∈ Z/pZ}\n(a + bi) + (c + di)\nc) + (b + d)i,\n=def (a +\n(a + bi)(c + di) =def (ac -bd) + (ad + bc)i\nThe associative, commutative, and distributive laws are all inherited by Rp from\nthe Gaussian integers m + ni (with m and n in Z) and so are the additive and\nmultiplicative identities and additive inverses. So Rp is a field if and only if it has\nmultiplicative inverses.\na) For the prime numbers p = 2, 3, 5, explain why Rp is or is not a field.\nb) Prove that R53 is not a field. (Hint: 53 = 72 + 22.)\nc) Explain your best guess about whether R251 is a field. (For example, you might\nsay, \"we found that Rp was not a field for the odd primes 3, 5, and 53, so\nprobably Rp is not a field for any odd prime p.\")\n4. (3 points) (Based on Axler, page 60, exercise 16). Suppose U is a finite-\ndimensional vector spaces, that S ∈L(V, W ), and that T ∈L(U, V ). Prove that\ndim null(ST ) = dim null(T ) + dim (range(T ) ∩null(S)) .\n5. (3 points) Give an example of problem 4 with U = V = W = R2, with null(S)\nand null(T ) both one-dimensional, but null(ST ) not 2-dimensional.\n6. (3 points) Suppose T ∈L(V, W ), and that V is finite-dimensional.\na) Prove that null(T ) = {0} if and only if for every linearly independent list\n(v1, v2, . . . , vp) in V , (Tv1, . . . , Tvp) is linearly independent in W .\nb) Prove that range(T ) = W if and only if for every spanning list (v1, v2, . . . , vq)\nin V , (Tv1, . . . , Tvq) is a spanning list in W .\nc) Prove T is invertible if and only if T takes each basis of V to a basis of W .\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.700 Linear Algebra\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.700 Fall 2013 Problem Set 4",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-700-linear-algebra-fall-2013/09ea022ecd1adcd741aefdb2f7ec7ae0_MIT18_700F13_ps4.pdf",
          "content": "18.700 Problem Set 4\nDue in class THURSDAY October 17; late work will not be accepted. Your work\non graded problem sets should be written entirely on your own, although you may\nconsult others before writing.\nFor the first problems, you may use the theorem I stated in class Tuesday October\n8: there is a one-to-one correspondence\nU ↔ Row(A)\nn\nbetween r-dimensional subspaces of F\nand r × n reduced row-echelon matrices\nhaving exactly one pivot in each row. (That is, no rows of A are zero.)\n1. (2 points) Suppose F = F9 is the field with nine elements. How many one-\ndimensional subspaces of F 3 are there?\n2. (3 points) Suppose F = Fq is the field with q elements. How many two-\ndimensional subspaces of F 4 are there? (The answer will be a formula depending\non q, something like e2q -sin(q) + 7.)\n3. (6 points) Still assume F = Fq.\na) Suppose r ≤ n. Explain why the number of r-element linear independent lists\nn\n(v1, . . . , vr) in F is equal to\nn\nn\nn\nn\n(q -1)(q -q)(q -q 2) · · · (q -q r-1).\nb) How many invertible n × n matrices with entries in F are there? (Hint: parts\n(a) and (b) have something to do with each other.)\nc) Suppose A is a very large random square matrix with entries in the field F2. Is\nthere a 30% chance that A is invertible?\n4. (3 points) Axler, page 94, exercise 5; but F is allowed to be any field, and\nyou need to see whether the answer depends on F .\n5. (3 points) Axler, page 94, exercise 7, with the same warning.\n6. (5 points) Axler, page 94, exercise 11.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.700 Linear Algebra\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.700 Fall 2013 Problem Set 5",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-700-linear-algebra-fall-2013/9033f2652287b7ca4ad2d35c619ae5c8_MIT18_700F13_ps5.pdf",
          "content": "18.700 Problem Set 5\nDue in class Tuesday October 22; late work will not be accepted. Your work\non graded problem sets should be written entirely on your own, although you may\nconsult others before writing.\n1. (6 points) Suppose we are given\nthree distinct elements x1, x2, and x3 in F ;\n(∗)\nand\nthree arbitrary elements a, b, and c in F ;\n(∗∗)\nThe problem is to find all polynomials\np(x) = u0 + u1x + u2x + u3x\n(∗∗∗)\nof degree less than or equal to three satisfying the conditions\np(x1) = a,\np(x2) = b,\np(x3) = c.\n(∗ ∗ ∗∗)\na) The conditions on p can be written as a system of three simultaneous linear\nequations in four unknowns.\nWrite the augmented matrix of this system of\nequations.\nb) Perform elementary row operations to bring this augmented matrix to reduced\nrow-echelon form. (This is a bit disconcerting, because some of the entries of the\nmatrix are not \"numbers\" like 7, but symbols for numbers, like x2. You know\nfrom algebra how to add, subtract, and multiply such symbols. What requires\ncare is dividing: before you divide by something like x2, you need to explain why\nit is not zero, or else worry separately about the case when it is zero. But you\nshould be able to manage.\nc) Write all the polynomials of degree less than or equal to three satisfying the\ncondition (∗ ∗ ∗∗).\n2. (4 points) In class I talked about the sequence of real numbers defined by\na0 = 0, a1 = 1, and\nan+1 = an + 2an-1\n(n ≥ 1).\nI explained that this sequence has something to do with the matrix\n(\n)\nA =\n.\na) Find all eigenvalues and eigenvectors of A.\n(\n)\nb) Write\nas a linear combination of eigenvectors of A.\n(\n)\nc) Write a formula for An\nnot using the matrix A. (A good answer is a vector\n(\n)\n2n + 1\nof formulas depending on n: something like\n.)\nn\n3. (3 points) Axler, page 94, exercise 19.\n4. (3 points) Axler, page 95, exercise 20.\n5. (3 points) Axler, page 96, exercise 21.\n6. (3 points) Axler, page 96, exercise 23.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.700 Linear Algebra\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.700 Fall 2013 Problem Set 6",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-700-linear-algebra-fall-2013/57e64332d6b04d29ffbe335b6cce0bc3_MIT18_700F13_ps6.pdf",
          "content": "Z\n18.700 Problem Set 6\nDue in class Tuesday November 5; late work will not be accepted. Your work\non graded problem sets should be written entirely on your own, although you may\nconsult others before writing.\n1. (8 points) Suppose that we are given three polynomials\np2(x) = ax + bx + c,\np1(x) = dx + e,\np0(x) = f\nwith real coefficients. This problem is about the differential operator\nd2\nd\nD = p2(x)\n+ p1(x)\n+ p0(x).\ndx2\ndx\na) Explain why the differential operator D preserves the m + 1-dimensional space\nPm(R) of real polynomials of degree less than or equal to m.\nb) Find all the diagonal entries of the matrix of D in the basis (1, x, x , . . . , xm).\n(Your answers will depend on the constants a, b, c, d, e, f .)\nc) Suppose that the constants satisfy 0 < a < d. Prove that D is diagonalizable on\nPm(R).\nd) Consider the special case\nd2\nd\n2 -\nD = (x\nx)\n+ (2x -1)\n.\ndx2\ndx\nFind a basis of eigenvectors of D on the four-dimensional space P3(R).\n2. (8 points) This problem is about the inner product space C[0, 1] of real-valued\ncontinuous functions on the interval [0, 1], with inner product\n(p, q) =\np(x)q(x)dx.\na) Let U = P3(R) be the four-dimensional subspace of C[0, 1] consisting of poly\nnomials of degree less than or equal to three. Apply the Gram-Schmidt process\ndescribed in the notes on orthogonal bases on the course web site to convert the\nbasis (1, x, x , x3) of U to an orthogonal basis having the same span. (I suggest\nthe notes because it's easier than the text.)\n√\nb) Define f (x) =\nx, regarded as a function in C[0, 1]. Calculate the orthogonal\nprojection pU (f ) of f on the subspace U.\n(Your answer should be a cubic\npolynomial in x with rational numbers as coefficients.)\nc) Make a little table displaying the values of f (x) and pU (f )(x) for at least the\nvalues x = 0, 1/4, 1/2, 3/4, 1, to three decimal places.\nd) Find some relationship between your answers to 1(d) and 2(a). Make a guess\nabout how to generalize that relationship to higher degree polynomials.\n3. (2 points) Axler, page 122, exercise 6.\n4. (2 points) Axler, page 123, exercise 12.\n5. (2 points) Axler, page 124, exercise 15.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.700 Linear Algebra\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.700 Fall 2013 Problem Set 7",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-700-linear-algebra-fall-2013/87fffc93e83d8a015e8764f1258fa740_MIT18_700F13_ps7.pdf",
          "content": "18.700 Problem Set 7\nDue in class Tuesday November 12; late work will not be accepted. Your work\non graded problem sets should be written entirely on your own, although you may\nconsult others before writing.\n1. (6 points) Suppose that V is a complex inner product space with orthogonal\nbasis (f1, . . . , fn), and T ∈L(V ).\na) Prove that any vector v ∈ V can be written\nn\nX hv, fii\nv =\nfi.\nhfi, fii\ni=1\nb) Find a formula involving inner products for the (i, j) entry aij of the matrix A\nof T in the basis (f1, . . . , fn).\nc) Find a formula involving inner products for the (i, j) entry bij of the matrix B\n∗\nof T in the basis (f1, . . . , fn).\nd) How do you pass from the matrix A to the matrix B?\n2. (4 points) Suppose V = R2 . The identity operator I is a positive selfadjoint\noperator, so according to Proposition 7.28 (which we'll do next Tuesday; but you\ndon't need it for this problem) I has a unique positive square root (namely I). Find\nall selfadjoint square roots of I.\n3. (4 points) Suppose V is a two-dimensional vector space over a field with q\nelements. How many square roots does the identity operator have? (Notice that\n\"selfadjoint\" doesn't appear in this problem.)\n4. (2 points) Axler, page 159, exercise 8.\n5. (4 points)\na) Find a real number a so that there exists a selfadjoint operator T ∈L(R3) with\nthe properties\nT (1, 2, 3) = (0, 0, 0),\nT (2, 5, a) = (2, 5, a).\nb) Find an eigenvector of T that is not in Span ((1, 2, 3), (2, 5, a)).\nHint: part (b) sounds like you're required to write down an operator T . You can\ndo that, but you can also find the eigenvector without finding T .\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.700 Linear Algebra\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.700 Fall 2013 Problem Set 8",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-700-linear-algebra-fall-2013/9ddeddfa0025d82976c1c82a0661f7a0_MIT18_700F13_ps8.pdf",
          "content": "18.700 Problem Set 8\nDue in class Monday November 26; late work will not be accepted. Your work\non graded problem sets should be written entirely on your own, although you may\nconsult others before writing.\n1. (12 points) Suppose that V is a (real or complex) inner product space, and\nthat (t1, . . . , tn) is a basis of V .\na) Show that there is just one n ×n matrix U = (uij ) with the following properties:\ni) U is upper triangular with strictly positive real diagonal entries:\nuij = 0\n(i > j),\nuii > 0;\nii) The list of vectors\nL\nfj =\nuij ti\ni≤j\nis an orthonormal basis of V .\n(Hint: think about the Gram-Schmidt process.)\nb) In part (a), suppose that V is Rn or Cn (thought of as column vectors), and that\nT is the n × n matrix with columns (t1, . . . , tn). Show that the matrix Q = TU\nis an isometry. (Hint: what are the columns of Q?)\nc) Prove that any invertible n × n real or complex matrix T can be written in\nexactly one way as a product T = QR, with Q an isometry and R an upper\ntriangular matrix with strictly positive real diagonal entries. (This is called the\nQR decomposition of T in the world of numerical linear algebra, and the Iwasawa\ndecomposition in the world of pure mathematics.)\nd) Calculate explicitly the matrices Q and R in case\n\nT =\n.\n2. (10 points) Part of the point of Problem 1 was to show that the QR decom\nposition is relatively easy to compute (in the sense of writing down formulas for\nit). The point of this problem is to show that the polar decomposition is relatively\ndifficult to compute (in the sense of writing down formulas).\na) Find the polar decomposition T = SP of the 2 × 2 real matrix\n\n-1\nT =\n.\n(Hint: This problem involves calculating some square roots. One useful formula\nthat might be difficult to find is\n\n!2\n√\n√\n3 +\n7 + 3\n=\n.)\nb) Suppose that v ∈ R2 is a vector of length 1. What is the largest possible length\nfor Tv? Find a vector v that achieves this maximum.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.700 Linear Algebra\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.700 Fall 2013 Problem Set 9",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-700-linear-algebra-fall-2013/mit18_700f13_ps9.pdf",
          "content": "18.700 Problem Set 9\nDue in class Wednesday December 4 (changed from syllabus); late work will\nnot be accepted. Your work on graded problem sets should be written entirely on\nyour own, although you may consult others before writing.\n1. (8 points) Suppose V is a real or complex inner product space. A linear map\nS ∈L(V ) is called skew-adjoint if S∗= -S. Suppose V is complex and finite-\ndimensional, and S is skew-adjoint. Show that the eigenvalues of S are all purely\nimaginary (that is, real multiples of i) and that there is an orthogonal direct sum\ndecomposition\nV =\nM\nλ∈R\nViλ.\n2. (16 points) Suppose V is an n-dimensional real inner product space, and S is\na skew-adjoint linear transformation of V .\na) Show that Sv is orthogonal to v for every v ∈V .\nb) Show that every eigenvalue of S2 is a real number less than or equal to zero.\nc) Suppose (still assuming S is skew-adjoint) that S2 = -I (the negative of the\nidentity operator on V ). Show that we can make V into a complex inner product\nspace, by defining scalar multiplication as\n(a + bi)v = av + bSv\nand the complex inner product as\n⟨v, w⟩C = ⟨v, w⟩-i⟨Sv, w⟩.\nWhat is the dimension of V as a complex vector space?\nd) Now drop the assumption that S2 = -I, but still assume S is skew-adjoint.\nShow that there is an orthonormal basis of V in which the matrix of S is\n\n-λ1\nλ1\n...\n-λr\nλr\n...\n\n,\nwith λ1 ≥· · · ≥λr > 0. That is, the matrix of S in this basis is block diagonal,\nwith r 2 × 2 blocks of the form\n\n-λ\nλ\n\nwith λ > 0, and n -2r 1 × 1 blocks (0). (Hint: first diagonalize S2.)\n3. (6 points) Give an example of a square complex matrix A with the property\nthat A has exactly three distinct eigenvalues, but A is not diagonalizable. (For full\ncredit, you should prove that your matrix has the two required properties.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.700 Linear Algebra\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        }
      ],
      "source_url": "https://ocw.mit.edu/courses/18-700-linear-algebra-fall-2013/",
      "course_info": "18.700 | Undergraduate",
      "subject": "General"
    },
    {
      "course_name": "Fourier Analysis",
      "course_description": "No description found.",
      "topics": [
        "Mathematics",
        "Mathematical Analysis",
        "Probability and Statistics",
        "Mathematics",
        "Mathematical Analysis",
        "Probability and Statistics"
      ],
      "syllabus_content": "Course Meeting Times\n\nLectures: 3 lectures / week, 1 hours / lecture\n\nPrerequisites\n\n18.100 Analysis I\n;\n18.06 Linear Algebra\n,\n18.700 Linear Algebra\n, or\n18.701 Algebra I\n\nTextbooks\n\nThe primary text book is Adams and Guillemin, but we will also refer to Stein and Shakarchi later in the semester.\n\nAdams, Malcolm Ritchie, and Victor Guillemin.\nMeasure Theory and Probability\n. Birkhause, 1996. ISBN: 9780817638849. [Preview with\nGoogle Books\n]\n\nStein, Elias M., and Rami Shakarchi.\nFourier Analysis: An Introduction\n. Princeton University Press, 2003. ISBN: 9780691113845.\n\nAnother useful book for reference is:\n\nKorner, T. W.\nFourier Analysis\n. Cambridge University Press, 1988. ISBN: 9780521251204.\n\nThis book is a series of vignettes that make entertaining reading in small doses. We will not be using it, but it gives an idea of the range of applications of Fourier analysis.\n\nCourse Description\n\nThis course continues the content covered in\n18.100 Analysis I\n. Roughly half of the subject is devoted to the theory of the Lebesgue integral with applications to probability, and the other half to Fourier series and Fourier integrals.\n\nThe task in the first half of the course is to introduce Lebesgue measure and establish properties of the Lebesgue integral. Our textbook (Adams and Guillemin) introduces Lebesgue measure using motivation and examples from probability theory. After we have developed probability theory on Bernoulli sequences, using a corresponding with Lebesgue measure on the unit interval, we will discuss the Lebesgue integral and some Fourier analysis. Then we will use some Fourier analysis to prove more theorems in probability. By the end of the semester we will have all the tools to discuss the continuum limit of a (suitably scaled) random walk, namely Brownian motion.\n\nOne of the main goals this course is to establish rules for the limiting behavior of functions so that we can deal with functions with as much confidence as we do real or complex numbers. An equally important motivation (that will only become clear in the second half) is that the systematic study of Fourier series requires the Lebesgue integral. The square mean convergence of Fourier series and Parseval's formula cannot be stated accurately in proper generality without the Lebesgue integral and Lebesgue integrable functions.\n\nAssignments and Exams\n\nThere will be 11 problem sets, due at the beginning of class on the due dates. Late homework will be accepted only if it is turned in within one week of the due date. There is no penalty for the first late homework assignment, but scores of all subsequent late papers will be multiplied by 1/2. Collaboration on problem sets is encouraged, but read each problem carefully and make an attempt to solve it by yourself. You must write up all homework problems by yourself.\n\nThere will be one in-class midterm test and a final exam.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem sets\n\n30%\n\nMidterm exam\n\n30%\n\nFinal exam\n\n40%\n\nCalendar\n\nLEC #\n\nTOPICS\n\nKEY DATES\n\nCoin Tossing, Law of Large Numbers, Rademacher Functions\n\nProblem Set 1 Due\n\nMeasure Theory, Random Models\n\nProblem Set 2 Due\n\nMeasurable Functions, Lebesgue Integral\n\nProblem Set 3 Due\n\nConvergence Theorems, Riemann Integrability\n\nProblem Set 4 Due\n\nFubini's Theorem, Independent Random Variables\n\nProblem Set 5 Due\n\nLebesgue Spaces, Inner Products\n\nProblem Set 6 Due\n\nHilbert Space, Midterm Review\n\nMidterm Test\n\nFourier Series and their Convergence\n\nProblem Set 7 Due\n\nApplications of Fourier Series\n\nProblem Set 8 Due\n\nFourier Integrals\n\nProblem Set 9 Due\n\nFourier Integrals of Measures, Central Limit Theorem\n\nProblem Set 10 Due\n\nBrownian Motion\n\nProblem Set 11 Due\n\nBrownian Motion Concluded, Review for Final Exam\n\nFinal Exam",
      "files": [
        {
          "category": "Assignment",
          "title": "18.103 Fourier Analysis, Problem Set 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/da2c68c10721f3dc7a4c74c84b1a14dc_MIT18_103F13_pset1.pdf",
          "content": "18.103 Fall 2013\nProblem Set 1\nNotation. AG §1.1 refers to Measure and Probability by Adams and Guillemin, Section 1.1.\nAG §1.1, pp 11-14: 1, 4, 7, 10, 12, 18, 19, 20*\n20* means do 20, but for non-strict inequalities, namely,\nμL({ω ∈I : f(ω) ≥α)}) ≤1\nα\nZ 1\n0 f dx\nAG §1.3, pp 39-42: 4, 5, 7.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.103 Fourier Analysis, Problem Set 10",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/c5c4ab50f927d82414ccabacf2d3cea3_MIT18_103F13_pset10.pdf",
          "content": "18.103 Fall 2013\nProblem Set 10\nDo AG §3.5/ 9, 11, 12.\n1. a) Let f ∈Lp(R), 1 < p < infand g ∈L1(R). Show that\n∥f ∗g∥p ≤∥f∥p∥g∥1\nHint: Let 1/p + 1/q = 1, and apply H older's inequality to\n|f(x -y)g(y)| = (|f(x -y)||g(y)|1/p)(|g(y)|1/q)\n(See Prop 17, AG Appendix B.) Note that this inequality is also true for p = 1, using\nFubini's theorem carried out in §3.5/9, and for p = inf, using more elementary properties of\nthe Lebesgue integral.\nb) Deduce that if f ∈Lp(R), 1 ≤p < infand K ∈L1(R) with\nZ\nR\nK(x) dx = 1;\nKε(x) = (1/ε)K(x/ε)\nthen\nlim\nε→0 ∥f ∗Kε -f∥p = 0\nc) Show that if f ∈Linf(R) and K ∈L1(R), then f ∗K ∈Cucb(R), where Cucb(R) is the\nclass of uniformly continuous functions bounded functions. (See also Fourier series notes 3,\nwhere the analogous statement on T is mentioned as an exercise, with a hint as to how it is\nproved.)\nd) Give a counterexample to the statement in part b) in the case p = inf.\n2. We will solve the equation\n∂\n∂tu = ∂2\n∂x2u + a ∂\n∂xu\n(1)\nfor a function u(x, t) with initial value\nu(x, 0) = f(x).\nThis is interpreted as a heat equation or diffusion equation with drift (the a(∂/∂x) term is\nthe drift).\na) Denote by ˆu(ξ, t) and ˆf(ξ) the Fourier transform in the x variable of u and f. For each\nfixed ξ find the ordinary differential equation for ˆu(ξ, t) formally (assuming the derivatives\nall make sense). Then solve the equation for ˆu in terms of ˆf.\n\nb) Take the inverse Fourier transform of your formula for ˆu(ξ, t) in part (a), and find a\nproposed formula for u in terms of f in the form\nu(x, t) = f ∗gt(x)\n(See §3.5/10 for the formula for gt in the case a = 0.)\nc) Show that the formula in part (b) solves the initial value problem. Namely, for f ∈L1, u\ngiven by the formula in (b) satisfies the differential equation (1) in t > 0, x ∈R, and satisfies\nthe initial condition in the sense that\nlim\nt→0\nZ inf\n-inf\n|u(x, t) -f(x)|dx = 0\n(t > 0).\n3. (See also §3.5/6; SS Chap 5/ Exercise 23, p. 168-169.) Define\nTf(y) =\n√\n2π\nZ\nR\nf(x)e-ixydx\na) Show that T 4 = I the identity mapping on S, the Schwartz class. (This extends by\ncontinuity to L2(R). Recall that we proved in lecture that T maps S to S. The Plancherel\nformula says that T is an isometry in the L2(R) norm. We also showed in lecture that, since\nS is dense in L2, one can extend T by continuity to the whole space L2(R), where it is again\nan isometry.)\nb) Suppose that h ∈S and Th = ch (an eigenvector for T with eigenvalue c). Find the short\nlist of possible values of c. (See SS, p. 163, 6.)\nc) Consider the so-called annihilation and creation operators A and B defined by\nA = d\ndx + x;\nB = -d\ndx + x\nand denote the L2(R) inner product by\n⟨f, g⟩=\nZ\nR\nf(x)g(x)dx\nShow that for all f and g in S, ⟨Af, g⟩= ⟨f, Bg⟩. This says that B = A∗, the adjoint of A,\nand A∗= B.\nd) Find numbers a and b such that\nTA = aAT;\nTB = bBT\ne) Let h0(x) = e-x2/2 and hk = Bkh0. Show that hk(x) = Hk(x)e-x2/2 with Hk a polynomial1\nof degree k and that\nThk = λkhk\n1Hk is known as a Hermite polynomial. Its generating function and other closely related formulas can be\nfound in SS p. 173.\n\nfor some λk. (Find λk explicitly.)\nf) Show that the hk/∥hk∥(with ∥· ∥the L2(R) norm) form a complete orthogonal system.\n(Hint: Consider ⟨Bkh0, Blh0⟩. Use part (c) and the commutator formula [A, Bn] = ncBn-1.\nIncidentally, the Hermite polynomials can also be obtained by applying the Gram-Schmidt\nprocess to the functions 1, x, x2, x3, etc, in L2(R, e-x2dx).)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.103 Fourier Analysis, Problem Set 11",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/2e63ed2682f2cf777b5910e7a985aa92_MIT18_103F13_pset11.pdf",
          "content": "18.103 Fall 2013\nProblem Set 11\n1. Consider f : R →R monotone increasing, i. e. x ≤y =⇒f(x) ≤f(y).\nFor -inf< a < b < inf, define\nνf((a, b)) = lim\nε→0+(f(b -ε) -f(a + ε))\nThe same proof as in the construction of Lebesgue measure shows that νf is a measure on\nthe ring of finite unions of intervals and extends to a measure on Borel sets. νf is sometimes\ndenoted df. (Thus, in the special case f(x) = x, we get Lebesgue measure νx = dx.)\na) Show that, conversely, for any measure μ on R that assigns a finite (nonnegative) number\nto each compact set there is a left continuous function f such that μ = νf. Left continuous\nmeans\nlim\nx→a-f(x) = f(a)\nand x →a-means x →a with x < a.\nb) Suppose that f is monotone increasing. Show that f is continuous at all but (at most)\ncountably many points xj. Find a continuous monotone increasing function g such that\nνf = νg +\nX\ncjδ(x -xj),\ncj = f(x+\nj ) -f(x-\nj )\nThe sum of delta functions is called the pure point or atomic part of the measure νf, and νg\nis the continuous part.\nc) Consider the Cantor function defined for each sequence ak ∈{0, 1} by\nC(x) =\nN-1\nX\nk=1\nak2-k + 2-N,\n3-N ≤x -2\nN-1\nX\nk=1\nak3-k ≤2 · 3-N\nShow that C can be extended uniquely to a continuous monotone increasing function on R\nsatisfying C(x) = 0 for all x ≤0 and C(x) = 1 for all x ≥1. Denote the corresponding\nCantor measure by dC = μC. Show that μC is supported on the Cantor set. In other words\nμC(E) = 0 for any E ⊂R such that E is disjoint from the standard middle third Cantor set.\nd) Let\nμ1 ∗μ2 ∗· · · ∗μn;\nwith μk = (1/2)[δ(x) + δ(x -2/3k)]\nFrom part (a) there is a monotone, left continuous function Cn such that Cn(x) = 0 for\nx ≤0 and dCn = μ1 ∗· · · ∗μn. Show that\nlim\nn→infCn(x) = C(x)\n\ne) Deduce that dCn tends weakly to μC and establish the Fourier transform formula\nˆμC(y) = eay\ninf\nY\nk=1\ncos(y/3k).\nIn the process identify the complex number a and show that the infinite product converges.\nMoreover, by considering the values at y = 2π3n, show that ˆμC(y) does not tend to zero as\ny →inf. In other words, we have constructed a continuous measure whose Fourier transform\ndoes not tend to zero at infinity.\n2. We say a function f : R →R has bounded variation or f is a BV function, if there is\nM < inffor which\nN\nX\nk=1\n|f(xk) -f(xk-1)| ≤M\nfor every sequence x0 < x1 < x2 < · · · < xN and every N.\na) Show that a function f of bounded variation on R is continuous except at countably many\npoints, and identify a left continuous function g that agrees with f at all but countably many\npoints. Give an example showing that g may have smaller total variation than f.\nb) Show that a left continuous function g of bounded variation can be written\ng(x) = h1(x) -h2(x)\nwith h1 and h2 left continuous, monotone increasing, and bounded. Hint: For x > 0, define\ng+(x) = sup{\nn\nX\nj=1\n[g(xj) -g(xj-1)]+ : 0 < x0 < x1 < · · · xn ≤x};\ng-(x) = sup{\nn\nX\nj=1\n[g(xj) -g(xj-1)]-: 0 < x0 < x1 < · · · xn ≤x}\nwhere a+ = max(a, 0) and a-= max(-a, 0).\nc) Consider g as in part (b). Show that for every φ ∈S,\n-\nZ\nR\nφ′(x)g(x) dx =\nZ\nR\nφ dh1 -\nZ\nR\nφ dh2\nThis says, by definition, that the generalized derivative of g is dh1 -dh2.\nd) The procedure in parts (b) and (c) works for all functions of bounded variation, not just\nones that are left continuous. Write down explicit monotone increasing functions h1 and h2\nsuch that f(x) = h1(x) -h2(x) for\nf(x) =\n\nx < 0\nx = 0\nx > 0\n\nand calculate the generalized derivative f ′. Alternatively, find g left continuous that agrees\nwith f except at one point and apply (b) and (c) to g. Compare the generalized derivative\ng′ with f ′. Are they the same?\n3. (Shannon sampling theorem; Stein-Shakarchi Problem 20) A function f is called band-\nlimited if its Fourier transform is supported on an interval of length L.\nWe show that\nband-limited functions whose frequencies come from an interval of length L can be recovered\nfrom values spaced by 2π/L. By dilation and translation we may assume that L = 2π and\ncenter the interval of frequencies around 0.\na) Let f ∈L2(R) be such that ˆf is supported on [-π, π]. Show that f is continuous. (More\nprecisely, f has a continuous representative which we will use from now on.)\nb) Show that f from part (a) satisfies\nZ inf\n-inf\n|f(x)|2 dx =\ninf\nX\n-inf\n|f(n)|2\nand\nˆf(ξ) = 1[-π,π](ξ)\ninf\nX\nn=-inf\nf(n)e-inξ\nin the sense of L2(R)-norm convergence.\nc) Show that\nf(x) =\ninf\nX\nn=-inf\nf(n)K1(x -n)\nfor K1 from optional Problem 5 below. In the course of the proof, explain why the series\nconverges for every x. (Warning: You may carry out the computation formally first. But\nwhen you justify the appropriate exchange of limits, remember that you only have norm\nconvergence in L2.)\nd) One can also recover f from more densely spaced samples. Show that\nf(x) =\ninf\nX\nn=-inf\nλf\nx\nλ\n\nKλ\n\nx -n\nλ\n\n,\nwith Kλ from Problem 5. Note that Kλ(y) = O(y-2) so that this series converges faster than\nthe one in (c).\n4. In this problem we deduce the Fourier series/inversion formula from its discrete analogue\non Z/NZ.\na) Consider f ∈C(R/2πZ) and xN(j) = 2πj/N. Define\ncN(n) = 1\nN\nN\nX\nj=1\nf(xN(j))e-ixN(j)n.\n\nShow that for any integer M,\nf(xN(j)) =\nN-M-1\nX\nn=-M\ncN(n)eixN(j)n.\n(1)\n(Later, we'll use the case M = N/2 if N is odd or M = (N + 1)/2 if N is even.)\nb) Show that for continuous f,\nlim\nN→infcN(n) = 1\n2π\nZ 2π\nf(x)e-inx dx\nc) Prove directly the following elementary version of the dominated convergence theorem.\nIf |aN(n)| ≤g(n),\ninf\nX\nn=-inf\ng(n) < inf, and\nlim\nN→infaN(n) = a(n),\nthen\nlim\nN→inf\ninf\nX\nn=-inf\naN(n) =\ninf\nX\nn=-inf\na(n)\nd) Suppose that f ∈C2(R/2πZ). Carry out the following steps to prove\n|cN(n)| ≤max |f ′′|/n2\n(2)\ni) Let g : Z →C satisfying g(j + N) = g(j). For n ∈Z, let ω = e2πin/N and define\nF(ω) = 1\nN\nN\nX\nj=1\ng(j)ω-j\nShow that\n(ω + ω-1 -2)F(ω) = 1\nN\nN\nX\nk=1\n(g(j + 1) + g(j -1) -2g(j))ω-j\nand deduce that for |n| ≤3N/4,\n|F(ω)| ≤N 2\nn2 max\nj\n|g(j + 1) + g(j -1) -2g(j)|\nii) Show that for f ∈C2(R/2πZ),\n|f(x + h) + f(x -h) -2f(x)| ≤h2 max |f ′′|\n\nusing the Taylor formula\nm(1) = m(0) + m′(0) +\nZ 1\nm′′(t)(1 -t) dt\napplied to m(t) = f(x + th) + f(x -th) -2f(x).\niii) Deduce (2) from (i) and (ii).\ne) Explain how (a)-(d) yield the Fourier series formula for every function f ∈C2(R/2πZ).\n5. This optional problem for no credit is here so that you can use the formulas from it to\ncarry out the Shannon sampling theorem above in Problem 3. We begin by showing that\nthe convolution of L2 functions is compatible with the Fourier transform, then compute the\nexplicit example used for Shannon's theorem.\na) Let f and g belong to L2(R). Denote\ngN(x) = 1\n2π\nZ N\n-N\nˆg(ξ)eixξ dξ\nShow that\nd\n(fg) = 1\n2π\nˆf ∗ˆg\nby evaluating\nlim\nN→inf\nZ inf\n-inf\nf(x)gN(x)e-ixξ dx\nin two ways.\nb) Let Ia(y) = 1[-a,a](y). For 0 < h ≤a, find Ja,h(x) such that\nˆJa,h(ξ) = Ia ∗Ih(ξ)\nc) Deduce from part (b) the formula for the function Kλ(x) of the form\nKλ(y) = C sin(Ay) sin(By)\ny2\n,\nλ > 1\nwhose Fourier transform has the trapezoidal shape\nˆKλ(ξ) =\n\n1,\n|ξ| ≤π\n(λπ -|ξ|)/π(λ -1),\nπ ≤|ξ| ≤πλ\n0,\nπλ ≤|ξ|\nIndeed, A = π(λ -1), B = π(λ + 1), C = 2/π2(λ -1). (See Stein-Shakarchi Problem 20, p.\n167-168. But be warned that our convention for the Fourier transform is different, and the\nformula there is written with cosines, not sines.)\n\nd) Evaluate Kλ(0) and show that\nK1(y) = lim\nλ→1 Kλ(y).\n6. This optional exercise shows how to construct a countable family of independent random\nvariables.\nIt's taken from the book Probability Theory, by D. W. Stroock.\nConsider a\ncountable sequence of probability spaces (Xn, Fn, μn). Define the ring of so-called cylinder\nsets, namely sets of the form B × Xn+1 × Xn+2 × · · · with B ∈F1 × F2 × · · · Fn.\nTheorem. There exists a unique measure μ on the cylinder ring satisfying\nμ(B × Xn+1 × Xn+2 × · · · ) = μ1 × μ2 × · · · × μn(B)\nFollow this outline to prove the theorem. Define X =\ninf\nY\nj=1\nXj, and define the projection\nπn : X →X1 × X2 × · · · × Xn,\nby\nπn(x) = (x1, x2, . . . , xn)\na) Show that the theorem follows if one shows that any nested sequence An ⊃An+1 in the\ncylinder ring, for which\nlim inf\nn→infμ(An) ≥ε > 0\nalso satisfies\ninf\n\\\nn=1\nAn = ∅\nb) Show that it suffices to consider the situation in which for some Bn ∈F1 × · · · × Fn, the\nnested sequence satisfies\nAn = π-1\nn (Bn);\nBn × Xn+1 ⊃Bn+1\nc) Define gm,m(x1, x2, . . . , xm) = 1Bm(x1, x2, . . . , xm). For all n > m, define\ngm,n : X1 × X2 × · · · × Xm →[0, 1] by\ngm,n(x1, x2, . . . , xm) =\nZ\nXm+1×···×Xn\n1Bn(x1, x2, . . . xn)d(μm+1 × · · · × μn)\nShow that the limit\ngm(x) := lim\nn→infgm,n(x),\nx = (x1, . . . , xm)\nexists and that\ngm(x1, . . . , xm) =\nZ\nXm+1\ngm+1(x1, . . . , xm+1)dμm+1\n\nd) Use induction to find x = (x1, x2, . . . ) ∈X such that for every m,\ngm(x1, x2, . . . , xm) ≥ε\nand deduce that\nx ∈\ninf\n\\\nm=1\nAm\nThis establishes (a) and concludes the construction of the infinite product measure.\ne) The monotone class theorem says that if M is a collection of subsets of a set Z that is\nclosed under nested countable union and nested countable intersection and contains a ring\nA, then M contains the sigma-ring generated by A. (The theorem does not say whether the\nwhole set Z is in M, but in our situation Z ∈A, so that the collection of subsets will be a\nsigma-field.) Show that this result implies that the measure constructed above is the unique\nmeasure on the sigma field generated by cylinder sets that agrees with the finite product\nmeasures. (You may take the monotone class theorem for granted or prove it. It's a bit\neasier than the π -λ theorem, but in the same spirit.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.103 Fourier Analysis, Problem Set 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/24e283fd9df3056e65e633543e922183_MIT18_103F13_pset2.pdf",
          "content": "18.103 Fall 2013\nProblem Set 2, updated version\n[If you did not hand in AG §1.1: 18 and 19 with Problem Set 1, then do so with Problem\nSet 2.]\nAG §1.1, pp. 11-14: 21.\nAG §1.3, pp 39-42: 6, 10, 14, 17.\nAG §1.4, pp. 49-52: 3, 5, 17.\nUpdate: Here's the extra exercise that was promised. Show that if\nlim\nn→infXn = 0\nwith probability 1, then for all ε > 0,\nlim\nn→infP({|Xn| > ε}) = 0\n(For example, if Xn = Sn/n, Sn = R1 + · · · + Rn, then this says that the conclusion of the\nstrong law of large numbers implies the conclusion of the weak law of large numbers. Hint:\nUse countable additivity.)\nHint for §1.3/10: Given a Cauchy sequence Ak, choose a subsequence Bj = Akj with a\ngeometric rate of convergence. Then let\nA = lim sup Bj ≡\ninf\n\\\nl=1\n[\nj≥l\nBj\nand show that Ak tends to A.\nRemark on §1.4/17: Use a base 4 expansion to make a probabilistic model representing\nthe random walk in the plane.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.103 Fourier Analysis, Problem Set 3",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/f926057033ae0d0f05aea967c552223a_MIT18_103F13_pset3.pdf",
          "content": "18.103 Fall 2013\nProblem Set 3\nAG §1.4, pp 49-52: 10, 19. (Problem 10 elaborates on the meaning of independence. Problem\n19 shows that there is no model of Bernoulli trials in a countable probability space.)\nAG §2.1, pp 58-60: 2, 3.\nAG §2.2, pp 69-72: 1, 3, 5, 9\n(Problem *) Existence of a set of real numbers that is not Lebesgue measurable\nNotations. Let R denote the real numbers and Q the rational numbers. As in §1.3/14,\nif E ⊂R, denote\nE + c = {x + c : x ∈E}\nLet I = [0, 1), the half-open interval.\na) Show that there exists a set E ⊂I such that for every x ∈R there exists a unique x′ ∈E\nsuch that x -x′ ∈Q. (This step uses the axiom of choice.)\nb) Show that if q1 and q2 are distinct rational numbers, then (E + q1) ∩(E + q2) = ∅.\nc) Show that\n[0, 1) ⊂\n[\nq∈Q,|q|≤1\nE + q ⊂[-1, 2),\nd) Deduce that E is not Lebesgue measurable.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.103 Fourier Analysis, Problem Set 4",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/a7947cfdbd8c037bac8dae719cd1f97b_MIT18_103F13_pset4.pdf",
          "content": "18.103 Fall 2013\nProblem Set 4\nAG §2.3, pp 80-82: 2, 3, 8, 10, 11, 15 (in 15b assume ci > 0).\nAG §2.4 pp 88-89: 2, 3, 6, 7.\nAG §1.3, pp 39-42: 19, 20.\nThe property (+) was introduced by Caratheodory and gives a slick approach to defining\nmeasurable sets. The idea of (+) is that if we formulate the correct notion of inner measure,\nthen measurable sets should be the ones for which the inner and outer measures of a set\ncoincide. The inner measure is expressed using outer measure of the complementary set.\nThe idea is analogous to upper and lower Riemann sums.\nThe subtlety is that the condition in (+) is imposed not just on A and Ac, but also on\neach A ∩E and Ac ∩E for every set E. Note that E is permitted to be any set, not just a\nmeasurable set. If (+) is to substitute as a definition of measurable set, it is incoherent to\ninsist that E be measurable -- we don't yet know what a measurable set is!\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.103 Fourier Analysis, Problem Set 5",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/edfb94debb0bc4111167c35c789fee2f_MIT18_103F13_pset5.pdf",
          "content": "18.103 Fall 2013\nProblem Set 5\nAG §2.5, pp 100-102: 11, 13 (With 13, say what hypothesis of Theorem 11, page 95, failed.)\nAG §2.6, pp 108-110: 2, 4, 5, 7, 8.\nFurther exercises\n1. Consider the probability space (I, M, μ) where I is the unit interval, M is the σ-field of\nLebesgue measurable sets, and μ is Lebesgue measure. Find three subsets, Ai, i = 1, 2, 3,\nthat are not independent, but which are pairwise independent.\n2. Another frequently used version of Fubini's Theorem\na) Correct the statement of the following theorem by adding the missing hypothesis on\nμ and ν (c.f. 2.5/13). Then deduce it from the other versions of Fubini's theorem.\nTheorem 0.1 (Fubini, almost correct version 4) Suppose that f(x, y) is a measurable func-\ntion on X × Y . Suppose further that\nZ\nY\nZ\nX\n|f(x, y)|dμ(x)\n\ndν(y) < inf\n(*)\nThen f is integrable (with respect to μ × ν on X × Y ) and\nZ\nY\nZ\nX\nf(x, y)dμ(x)\n\ndν(y) =\nZ\nX\nZ\nY\nf(x, y)dν(y)\n\ndμ(x) =\nZ\nX×Y\nf d(μ × ν)\n(**)\nb) Consider the function in 2.5/12\nf(x, y) =\nxy\n(x2 + y2)2\non the sets X = [-1, 1] and Y = [0, 1] with μ and ν Lebesgue measure. Show that in this\ncase hypothesis (*) above fails, the left-hand (iterated) integral in (**) exists while the other\ntwo do not. In particular, 2.5/12a is deceptive, the result of symmetry, not Fubini's theorem.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.103 Fourier Analysis, Problem Set 6",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/206051202c959e1dbd6c4fb5ced7494c_MIT18_103F13_pset6.pdf",
          "content": "18.103 Fall 2013\nProblem Set 6\nExercise 1. Let Mn = M(Rn), the measurable subsets of Rn. Show that M1 × M1 = M2\nby considering E × {0} with E ⊂[0, 1] such that E /∈M1.\nExercise 2. Show that if f : R2 →R is measurable with respect to M(R2), then there is a\nBorel function g such that f(x) = g(x) for almost every x ∈R2. (Hint: Start with the case\nf = 1E and use a similar approach to §2.2 Theorem 6, page 62.)\nExercise 1 gives an example of a function, namely, 1E×{0} that is M(R2) measurable but\nwhose slices need not be M(R) measurable. What Exercise 2 shows is that by modifying an\nM2 measurable function on a set of measure zero, we can turn it into a function to which\nFubini's theorem as stated in the text applies.\nAG §2.1, p 60: 11b (Read 11a to learn the terminology of pointwise convergence and conver-\ngence in measure. We already did 11a as the extra exercise on PS2. This is the statement\nthat the conclusion of the strong law implies the conclusion of the weak law of large numbers.)\nAG §3.1, pp 122-124: 1, 2, 3, 7, 8\nWarning: The sentence at the top of page 122 concerning a \"corollary to the Lebesgue\ndominated convergence theorem\" is misleading. The dominated convergence theorem is not\nneeded in the proof of the completeness of the Lebesgue spaces Lp. The part of the corollary\nthat is used does not employ the dominated convergence theorem. As explained in lecture,\nwe want to conclude that there is pointwise convergence, whereas the dominated convergence\ntheorem has pointwise convergence as a hypothesis.\nAG §3.2, pp 128-129: 1, 2, 4\nAG §3.3, pp 134-137: 6 (You may use the Pythagorean theorem, Theorem 2 from §3.3 or\nderive what you need from direct computation as we have already done a few times for\nRademacher functions. Otherwise, don't use the theorems in §3.3 for this; do it with the\ntheorems from §3.2. In other words, you don't need §3.3 to do this exercise.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.103 Fourier Analysis, Problem Set 7",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/0f64cc5afcab7d4b5afcffc50b8fb3a7_MIT18_103F13_pset7.pdf",
          "content": "18.103 Fall 2013\nProblem Set 7, revised\n0. Hand in your hour test with all the problems corrected, including more careful or efficient\npresentation of problems that you already did correctly. (You can improve your score up to\nhalf way to 100 from your present score.)\n1. Let f ∈L1(R/2πZ) and denote its Fourier coefficients by\nˆf(n) = 1\n2π\nZ π\n-π\nf(x)e-inx dx\nProve that\na) f is even if and only if ˆf(n) = ˆf(-n) for all n\nb) f is odd if and only if ˆf(n) = -ˆf(-n) for all n\nc) f is real-valued if and only if ˆf(n) = ˆf(-n) for all n.\n2. Compute the Fourier coefficients of the following functions. Note which symmetries of\nProblem 1 hold and express the series both in terms of complex exponentials and in terms\nof sine or cosine functions where appropriate. What does Parseval's formula tell us in each\ncase?\na) f(x) = x, -π < x < π and f(x + 2π) = f(x)\nb) g(x) = |x|, -π < x < π and g(x + 2π) = g(x)\nc) h(x) = f(x + π)\n3. A series\ninf\nX\nn=0\nan is called Cesaro summable if the Cesaro means\nσN = (s0 + · · · + sN-1)/N\nof the partial sums sN =\nN\nX\nan converge. Show that if sN converges then σN converges to\nthe same limit. Give an example showing that the converse is false.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "18.103 Fourier Analysis, Problem Set 8",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/9369dd190c3e352de81770e5b212e364_MIT18_103F13_pset8.pdf",
          "content": "18.103 Fall 2013\nProblem Set 8\n1. Let f ∈L1(R/2πZ), and let σN denote the Cesaro mean of its Fourier series. Prove that\nif f has a left and right limit at x, then\nσN(x) →(f(x+) + f(x-))/2 as N →inf\n(You may use the formula from lecture for FN such that σN(x) = f ∗FN(x).)\nHint: Formulate and prove a variant of the \"approximate identity\" lemma, with stronger\nhypotheses on KN in exchange for weaker properties of f, and confirm the stronger properties\nof FN that you need.\n2.\nConsider the Fourier series for f from 2a PS7 at x = 0 and x = π; g from 2b at x = 0;\nh from 2c at x = π/2. What are the consequences of the theorems in problems 3 PS7 and\nproblem 1 above at these points?\n3. Let RN denote the 2N dyadic intervals of [0, 1) of length 2-N, that is,\nRN = {I = [(k -1)/2N, k/2N) : k = 1, 2, . . . , 2N}\nConsider\nVN = span {1I : I ∈RN}\nLet PN : L2([0, 1]) →VN be the orthogonal projection onto VN, that is, the mapping such\nthat PNf = f for all f ∈VN and PNf ⊥(f -PNf) for all f ∈L2([0, 1]).\na) Find the formula for aI (in terms of I and f) such that\nPNf =\nX\nI∈RN\naI1I\nand show that PNf tends uniformly (on [0, 1)) to f for all f ∈C([0, 1]).\nb) Let 1 ≤p < inf. Show that PNf tends to f in Lp([0, 1]) for every f ∈Lp([0, 1]).\nc) For f ∈L1([0, 1]), find the formula for P0f and PN+1f -PNf in terms of ⟨f, Hn,k⟩and\nHn,k, the Haar functions defined in AG §3.3/11, pp. 136-137. Warning: identify the misprint\nin part (a) p. 137. Deduce that the Haar functions form a complete orthonormal system of\nL2([0, 1]).\n4. a) Do AG §3.3/9, p. 136 (Gram-Schmidt process).\nb) Use power series to show that every function einx can be uniformly approximated on\n[-π, π] by polynomials (ordinary polynomials in x).\n\nc) Deduce from (b) that polynomials are dense in L2([-π, π]).\nd) Denote by ψ0, ψ1, . . . , the functions obtained from the Gram-Schmidt process applied to\nthe polynomials f0(x) = 1, f1(x) = x, f2(x) = x2, . . . . Show that these form an orthonormal\nbasis of L2([-π, π]) and compute the first three. (The answers on [-1, 1] are listed in AG\n§3.3/10 p. 136.)\nShow further that the degree of ψn is n and that ψn is even if n is even and odd if n is odd.\ne) Show by integration by parts that\nRn(x) = dn\ndxn(x2 -1)n\nis orthogonal to 1, x, . . . , xn-1 in L2([-1, 1]) and Rn(1) = 2nn!. (Hint: x2-1 = (x-1)(x+1).)\nf) The Legendre polynomials are defined as the polynomials Pn(x) = Rn(x)/2nn!.\nIn other words, they are normalized1 so that Pn(1) = 1. Show how your formulas for ψn,\nn = 0, 1, 2 in (c) match this formula for Pn.\n5. Define the Laplace operator ∆= ∂2\n∂x2 + ∂2\n∂y2 on the (x, y)-plane.\na) Show that in polar coordinates (x = r cos θ, y = r sin θ),\n∆= ∂2\n∂r2 + 1\nr\n∂\n∂r + 1\nr2\n∂2\n∂θ2\nb) Let f ∈C(R/2πZ). Define u in polar coordinates by\nu(r, θ) =\ninf\nX\nn=-inf\nr|n| ˆf(n)einθ,\n0 ≤r < 1\nExpress u as a series in z = x + iy and z = x -iy. Confirm that u is infinitely differentiable\nin x2 + y2 < 1 and that ∆u = 0 for 0 ≤r < 1. Solutions to ∆u = 0 are known as harmonic\nfunctions.\n1The functions φn of AG §3.3/10 p. 136 indexed starting from n = 1 and with the normalization that the\nL2 norm on [-1, 1] is 1 differ from the customary notation for Legendre polynomials Pn. Further properties\n(not assigned) are as follows.\ninf\nX\nn=0\nPn(x)zn =\n√\n1 -2xz + z2\n(generating function)\nRecurrence formula and L2 norm:\n(n -1)Pn(x) = (2n -1)xPn-1(x) -nPn-2;\nZ 1\n-1\nPn(x)2dx = 2/(2n + 1).\n\nRemark.\nOne should think of f(θ) as a function on the unit circle {eiθ : θ ∈R/2πZ} in\nthe complex plane and u is a function of z = reiθ in the unit disk. Then u is the harmonic\nfunction with boundary values f, as we now prove.\nc) Compute the Poisson kernel Pr satisfying\nu(r, θ) = f ∗Pr(θ)\nProve that if f ∈C(R/2πZ), then\nmax\nθ\n|u(r, θ) -f(θ)| →0\nas r →1-\nIf f ∈L1(R/2πZ), then\nlim\nr→1-\nZ\n[-π,π]\n|f ∗Pr(θ) -f(θ)| dθ = 0\nd) (Extra credit) Prove that if f is continuous, then u extends to a continuous function on\nthe closed unit disk.2 In other words,\nu(rj, θj) →f(θ)\nwhenever rj →1-and θj →θ.\n2Given that u is continuous in the closed disk, one can prove that u is unique using what is known as the\nmaximum principle. The maximum principle (for the disk) says that if v(z) is real-valued and continuous in\n|z| ≤1 and harmonic in |z| < 1, then\nmax\n|z|≤1 v(z) ≤max\n|z|=1 v(z)\nLet v be ± the difference of any two real-valued harmonic functions with the same boundary values, then\nby the maximum principle, v = 0 and the two functions are the same. Using uniqueness for continuous\nboundary values, one can deduce uniqueness of u with boundary values in the L1 sense stated above.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Exam",
          "title": "18.103 Fourier Analysis, Final Exam Review",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/6790f08ac2dbd904526af294f8df9cca_MIT18_103F13_final-rev.pdf",
          "content": "18.103 Final Review 2013\nThe Fourier transform on R is defined for all f ∈L1(R) by ˆf(t) =\nZ\nR\nf(x)e-itxdx. Denoting\nG(x) =\n√\n2πe-x2/2, we have\nˆG(t) = e-t2/2\nMain Approximate Identity Lemma. Let K ∈L1(R) satisfy\nZ\nR\nK(x) dx = 1\nand denote Ka(x) = (1/a)K(x/a), a > 0. Then for every x ∈R and f ∈C0(R),\nlim\na→0+ f ∗Ka(x) = f(x)\nTheorem 1 (Fourier inversion on S). The mappings T1 and T2 defined for f and g in S(R)\nby the Riemann integrals\n(T1f)(t) =\nZ\nR\nf(x)e-itxdx;\n(T2g)(x) = 1\n2π\nZ\nR\ng(t)eitxdt\nsend the Schwartz class S to itself. Moreover, the compositions T2T1 and T1T2 are both the identity\nmapping on S.\nTheorem 2 (Plancherel).\na) For all f and g in S,\n∥T1f∥2 = 2π∥f∥2;\n2π∥T2g∥2 = ∥g∥2\nwhere\n∥f∥2 =\nZ\nR\n|f(x)|2dx\nb) T1 and T2 have unique extensions from S to continuous mappings from L2(R) to itself, T1T2\nand T2T1 are the identity mapping on L2(R) and the properties of part (a) are valid for all f and\ng in L2(R).\nTheorem 3 (Fourier inversion with truncation). Let f ∈L2(R), and denote\nsN(x) = 1\n2π\nZ N\n-N\nˆf(ξ)eixt dt\nThen\nlim\nN→inf\nZ\nR\n|sN(x) -f(x)|2dx = 0\nProposition. If f ∈L1(R) ∩L2(R), then\nT1f(t) =\nZ\nR\nf(x)e-ixt dx\nT2g(x) = 1\n2π\nZ\nR\ng(t)eixt dt\nTheorem 4. Let\nf ∗g(x) =\nZ\nR\nf(x -y)g(y)dy\nIf f ∈L1(R) and g ∈L1(R) or (and this requires more work) if f ∈L2(R) and g ∈L2(R), then\n\\\n(f ∗g)(t) = ˆf(t)ˆg(t)\n\nReview Problems\n1.\na) Find the Fourier series of the function\nf(x) =\n(\n1,\n0 < x < π;\n0,\n-π < x < 0.\nextended periodically with period 2π. Pay attention to three cases n = 0 and n = 0 odd and even,\nseparately.\nb) Express your series with real numbers, sines and cosines.\nc) At which points x does the series converge and to what value? Explain with statements of\ntheorems.\n2. Suppose that f ∈L2(R/2πZ) takes the form\nf(θ) =\ninf\nX\nn=1\naneinθ\nRecall that if z = reiθ = x + iy,\nF(z) =\ninf\nX\nn=1\nrnaneinθ\nis a harmonic (and even analytic) function in |z| < 1.\na) Why does the series for F(z) converge for |z| < 1?\nb) Let fr(θ) = F(reiθ), the values of F on the circle of radius r. Calculate ∥fr -f∥2 in terms r\nand an, and show that F takes on the boundary values in the sense that\nlim\nr→1-∥fr -f∥2 = 0\nc) Evaluate the integral\nZ Z\n|z|<1\n|(∂/∂r)F(z)|2(1 -|z|) dx dy\nin terms of the coefficients an. Explain at an appropriate point before, during or after the compu-\ntation, why the integral is finite.\n3.\nFourier inversion on the Schwartz class S(R). (Approximate identity Lemma and Theorem 1\nabove.)\na) Recall that C0(R) is defined as the class of continuous functions on R that tend to zero at\n±inf. Show that if K ∈L1(R) and\nZ\nR\nK(x)dx = 1;\nKa(x) = 1\naK(x/a),\na > 0\nthen\nlim\na→0 f ∗Ka(x) = f(x)\n\nfor every x ∈R and every f ∈C0(R). Make use in your proof of the quantities\nQ =\nZ\nR\n|K(x)|dx;\nM = max\nx∈R |f(x)|\nand the modulus of continuity of f,\nω(r) =\nmax\nx∈R; |y|≤r |f(x + y) -f(x)|\nb) Show that for every f ∈S,\nf(0) = 1\n2π\nZ\nR\nˆf(t)dt. You may assume without proof that for\nevery f and g in S, ˆf and ˆg belong to S and\nZ\nR\nf(y)ˆg(y)dy =\nZ\nR\nˆf(t)g(t)dt\nc) Deduce the Fourier inversion formula (formula for f(x) in terms of ˆf) for f ∈S.\n4. Fourier inversion formula on L2(R) (Proof of Theorem 3 and the proposition above.)\na) For f ∈L2(R) and denote\nsN(x) = 1\n2π\nZ N\n-N\nˆf(t)eixtdt\nExplain why the integral defining sN(x) converges and why sN is continuous.\nb) Prove that if f ∈L1(R) ∩L2(R), then\n(T1f)(t) =\nZ\nR\nf(x)e-itxdx\nfollowing the three steps with *'s below.\nYou may assume that for any f ∈L1 ∩L2, there is a sequence of functions fk ∈S such that\n∥f -fk∥L1 + ∥f -fk∥L2 →0 as k →inf. Define\nφk(t) =\nZ\nR\nfk(x)e-itxdx;\nφ(t) =\nZ\nR\nf(x)e-itxdx\n* Show that φk(t) tends to φ(t) for each t as k →inf.\n* Show that ∥φk -T1f∥L2 tends to 0 as k →inf.\n* Deduce that φ(t) = (T1f)(t) (This equality holds in what sense?) Hint: Fatou's lemma leads\nto the fastest proof, but you may use other methods.\nc) Deduce that\nlim\nN→inf\nZ\nR\n|f(x) -sN(x)|2 dx = 0\nusing the statement analogous to part (b) for T2 and the other theorems on the page of theorems\nas necessary.\n5. Poisson summation formula. Let φ ∈S(R). Show that\nX\nn∈Z\nφ(2πn) = 1\n2π\nX\nk∈Z\nˆφ(k)\n\nby calculating the Fourier series of\nF(x) =\nX\nn∈Z\nφ(x -2πn)\nin two ways.\n6. Recall that\nPy(x) = 1\n2π\nZ inf\n-inf\ne-y|ξ|eixξ dξ = 1\nπ\ny\nx2 + y2\nsatisfies for all x ∈R and all y > 0,\n∂2\n∂x2 + ∂2\n∂y2\n\nPy(x) = 0,\nIn other words, Py(x) is harmonic in the upper half-plane {(x, y) ∈R2 : y > 0} and for f ∈L1(R),\nu(x, y) = Py ∗f(x)\nis harmonic in the upper half plane y > 0.\nIf f ∈S(R), use the Fourier transform to calculate\nZ inf\nZ inf\n-inf\n|∇u(x, y)|2y dx dy\nin terms of f.\n(Either before during or after the calculation, justify all the exchanges of inte-\ngrals/differentiation/limits.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "18.103 Fourier Analysis, Midterm test description",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/fab13ed7a9917680589106ef9a7903c7_MIT18_103F13_midterm-des.pdf",
          "content": "Hour Test\nThe 50-minute test in class covers Sections 1.1 through 3.3 of the text by Adams and\nGuillemin, except Section 2.8. It also covers the notes that have beenposted. Here is a\nrough description of the types of problems, followed by last year's test. In all cases of\nproofs you should be ready to state carefully not only the theorem you areproving but\nalsotheingredientsintheproof.\n1. Establish the main step in the Lebesgue measure construction, that is, confirm that μ⇤\nequals the volume measure on the rectangle ring.\n2. Prove one of these convergence theorems. (I will pick one.)\na) The monotone convergence theorem\nb) Fatou's lemma\nc) The dominated convergence theorem\n3. Prove or establish a step or two in the proof of one of the following theorems.\na) Borel-Cantelli Lemma 1 or 2\nb) Bounded measurable functions are uniform limits of simple functions.\nc) Bounded Riemann integrable functions are Lebesgue integrable with the same value.\nd) L1(X, μ) is complete.\n4 + 5. Questions in which you have to decide what's true and why. You will be given state\nments related to limit theorems, Fubini's theorem, or issues of integrability or measurability.\nMost students find these to be the trickiest questions because of the uncertainty. They are\nnot designed to be devious, but you need to come to the test armed with examples related\nto where the hypotheses of the theorems do and don't work.\n6. The last question is up for grabs. It may include a computation or the evaluation of a\nlimit as an application of any of our theorems. For example,\na) A computation with Rademacher functions.\nb) Use of E(f1f2 · · · fn) = E(f1)E(f2) · · · E(fn) for independent random variables.\nc) Computation of the probability distribution μf given an explicit function f.\nd) Evaluation of a limit using a convergence theorem, Fubini's theorem, or density of\nsmooth functions in Lp functions, 1 p < 1.\ne) Evaluation of a probability using a Borel-Cantelli lemma\nLast year's test, on the next page, follows the rubric above.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "18.103 Fourier Analysis, Midterm test review",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/ec79a4c912faf80e8739032d6f9223d2_MIT18_103F13_midterm-rev.pdf",
          "content": "18.103 Fall 2013\nMidterm Review\nThe main topics so far have been\n1. The construction of Lebesgue measure\n2. The Lebesgue integral, convergence theorems and Fubini's theorem\n3. Applications\na) Probability theory\nb) Lebesgue Lp(X, μ) spaces are complete.\nc) Cinf\n0 (Rn) is dense in Lp(Rn), 1 ≤p < inf.\nd) Riemann-Lebesgue Lemma (follows from density).\ne) Fourier representation of 2π-periodic L2 functions\n(i. e., einx, n ∈Z is a complete orthonormal system).\nThe key to constructing Lebesgue measure is to confirm that volume is countably additive\non the rectangle ring. See Theorem 7, §1.3, and Exercise §1.1/21.\nNote that for measurable sets E and F,\n∥1E -1F∥1 =\nZ\nX\n|1E -1F| dμ = μ(S(E, F))\nwhere S(E, F) = (E -F) ∪(F -E) is the set-theoretical symmetric difference. Thus for\nmeasurable sets, the distance d(E, F) = μ∗(S(E, F)) is the same as the L1(X, μ) distance\nbetween the indicator functions 1E and 1F. After the basic key consistency property men-\ntioned in the last paragraph, the rest of the process we went through to construct Lebesgue\nmeasure is essentially the same as the process of completing any metric space. Thus an\nalternative definition of L1 is as the completion under the L1 distance of step functions (or\nRiemann integrable functions or continuous functions). In the abstract, the completion pro-\ncess yields equivalence classes of Cauchy sequences. The advantage of our present approach\nis that we can identify the limits as actual sets and functions.\nAlthough in the Lebesgue approach the limiting objects are actual functions, not just equiv-\nalence classes of Cauchy sequences, we still have to get used to some ambiguity. The mea-\nsurable sets and integrable functions we obtain as limits are certainly more satisfying than\nequivalence classes, but we still pay the price that they are only defined up to a set of measure\nzero. Since Lebesgue's time, we have figured out many ways to deal with that inconvenience\nand get back to \"classical\" statements involving continuous functions and functions with\ndefinite values at points of interest. We will learn some of these in the second half of the\ncourse.\n\nHour Test\nThe 50-minute test in class on Friday, October 25, covers Sections 1.1 through 3.3 of the\ntext by Adams and Guillemin, except Section 2.8. It also covers the notes that have been\nposted. Here is a rough description of the types of problems, followed by last year's test.\nIn all cases of proofs you should be ready to state carefully not only the theorem you are\nproving but also the ingredients in the proof.\n1. Establish the main step in the Lebesgue measure construction, that is, confirm that μ∗\nequals the volume measure on the rectangle ring.\n2. Prove one of these convergence theorems. (I will pick one.)\na) The monotone convergence theorem\nb) Fatou's lemma\nc) The dominated convergence theorem\n3. Prove or establish a step or two in the proof of one of the following theorems.\na) Borel-Cantelli Lemma 1 or 2\nb) Bounded measurable functions are uniform limits of simple functions.\nc) Bounded Riemann integrable functions are Lebesgue integrable with the same value.\nd) L1(X, μ) is complete.\n4 + 5. Questions in which you have to decide what's true and why. You will be given state-\nments related to limit theorems, Fubini's theorem, or issues of integrability or measurability.\nMost students find these to be the trickiest questions because of the uncertainty. They are\nnot designed to be devious, but you need to come to the test armed with examples related\nto where the hypotheses of the theorems do and don't work.\n6. The last question is up for grabs. It may include a computation or the evaluation of a\nlimit as an application of any of our theorems. For example,\na) A computation with Rademacher functions.\nb) Use of E(f1f2 · · · fn) = E(f1)E(f2) · · · E(fn) for independent random variables.\nc) Computation of the probability distribution μf given an explicit function f.\nd) Evaluation of a limit using a convergence theorem, Fubini's theorem, or density of\nsmooth functions in Lp functions, 1 ≤p < inf.\ne) Evaluation of a probability using a Borel-Cantelli lemma\nLast year's test, on the next page, follows the rubric above.\n\nPRACTICE HOUR TEST (Throughout, μ denotes Lebesgue measure on R.)\n1. (20 pts) If J is a finite union of bounded intervals, define l(J) as the length of J.\na) Give a definition of Lebesgue outer measure μ∗on R in terms of l.\nb) Prove directly from this definition that μ∗([0, 1]) = 1. Use without proof that lis finitely\nsubadditive and finitely additive on finite unions of intervals. You may not, however, use\nthe fact that lis countably subadditive or countably additive.\n2. (20 pts) Deduce the dominated convergence theorem from Fatou's lemma. (Your answer\nmust include a careful statement of both the theorem and the lemma.)\n3.\na) (10 pts) Show that every Cauchy sequence in L1(I, μ) has a subsequence that converges\npointwise almost everywhere.\nb) (6 pts) Find a Cauchy sequence as in part (a) that does not converge pointwise almost\neverywhere.\n4. (16 pts) Decide if the following statements true or false and give a reason if true and\na counterexample if false. (4 points for each correct answer; 4 points for the reason or\ncounterexample.) As in all parts of the test, μ denotes Lebesgue measure on R.\na) (T/F) If Ak are measurable subsets of R, then lim\nN→infμ\nN\n\\\nk=1\nAk\n!\n= μ\ninf\n\\\nk=1\nAk\n!\nb) (T/F) If f(x, y) ≥0 is measurable on R×R, and\nZ\nR\nZ\nR\nf(x, y)dμ(x)\n\ndμ(y) < inf, then\nxyf(x, y)\nx2 + y2\nis integrable on R × R.\n5. (16 pts) Let fn be a sequence of measurable functions on [0, 1] such that 0 ≤fn(x) ≤1.\nFind the relationship between\nlim sup\nn→inf\nZ 1\nfn(x) dμ(x)\nand\nZ 1\nlim sup\nn→inffn(x) dμ(x),\nIn other words, decide if they are equal or if one is necessarily less than or equal to the other.\nProve your answer and give an example if they can be unequal.\n6. (12 pts) Show that for all f ∈L1(R, μ),\nlim\nt→0\nZ\nR\n|f(x) -f(x + t)|dμ(x) = 0\nOne way to prove this (not the fastest) is to start with 1E.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "18.103 Fourier Analysis, Practice midterm test",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/14dc6503325ff96b225c147408682ed7_MIT18_103F13_prac-mid.pdf",
          "content": "PRACTICE HOUR TEST (Throughout, μ denotes Lebesgue measure on R.)\n1. (20 pts) If J is a finite union of bounded intervals, define `(J) as the length of J.\na) Give a definition of Lebesgue outer measure μ⇤ on R in terms of `.\nb) Prove directly from this definition that μ⇤([0, 1]) = 1. Use without proof that ` is finitely\nsubadditive and finitely additive on finite unions of intervals. You may not, however, use\nthe fact that ` is countably subadditive or countably additive.\n2. (20 pts) Deduce the dominated convergence theorem from Fatou's lemma. (Your answer\nmust include a careful statement of both the theorem and the lemma.)\n3.\na) (10 pts) Show that every Cauchy sequence in L1(I, μ) has a subsequence that converges\npointwise almost everywhere.\nb) (6 pts) Find a Cauchy sequence as in part (a) that does not converge pointwise almost\neverywhere.\n4. (16 pts) Decide if the following statements true or false and give a reason if true and\na counterexample if false. (4 points for each correct answer; 4 points for the reason or\ncounterexample.) As in all parts of the test, μ denotes Lebesgue measure on R.\nN\n!\n!\na) (T/F) If Ak are measurable subsets of R, then lim μ\n\\\nAk\n= μ\n\\\nAk\nN!1\nk=1\nk=1\nZ\n✓Z\n◆\nb) (T/F) If f(x, y) ≥ 0 is measurable on R⇥R, and\nf(x, y)dμ(x) dμ(y) < 1, then\nR\nR\nxyf(x, y) is integrable on R ⇥ R.\nx2 + y2\n5. (16 pts) Let fn be a sequence of measurable functions on [0, 1] such that 0 fn(x) 1.\nFind the relationship between\nZ\nZ\nlim sup\nfn(x) dμ(x) and\nlim sup fn(x) dμ(x),\nn!1\nn!1\nIn other words, decide if they are equal or if one is necessarily less than or equal to the other.\nProve your answer and give an example if they can be unequal.\n6. (12 pts) Show that for all f 2 L1(R, μ),\nZ\nlim\n|f(x) - f(x + t)|dμ(x) = 0\nt!0\nR\nOne way to prove this (not the fastest) is to start with 1E .\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "18.103 Fourier Analysis, Practice midterm test solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/70314e9e62e2d42145492dfcb88df6ca_MIT18_103F13_prac-mid-sol.pdf",
          "content": "18.103 Fall 2013\nOld Hour Test Solutions.\n4. a) (T/F) If Ak are measurable subsets of R, then lim\nN→infμ\nN\n\\\nk=1\nAk\n!\n= μ\ninf\n\\\nk=1\nAk\n!\nFalse. (Only works when one of the measures is finite.) Let Ak = [k, inf), then the limit is\ninfinity, whereas\n∅=\ninf\n\\\nk=1\nAk,\nso that the right side is zero.\nb) (T/F) If f(x, y) ≥0 is measurable on R × R, and\nZ\nR\nZ\nR\nf(x, y)dμ(x)\n\ndμ(y) < inf,\nthen xyf(x, y)\nx2 + y2\nis integrable on R × R.\nTrue. Note that xyf(x, y)\nx2 + y2\nis measurable. By the version of Fubini's theorem on a problem\nset, f is integrable on R2 with respect to μ×μ. Finally, because x2-2xy+y2 = (x-y)2 ≥0,\n\nxy\nx2 + y2\n≤1\nTherefore,\nZ\nR×R\n\nxyf(x, y)\nx2 + y2\nd(μ × μ) ≤1\nZ\nR×R\nf(x, y)d(μ × μ) < inf\nThus the function is integrable.\n5. If fn is a sequence of measurable functions on [0, 1] such that 0 ≤fn(x) ≤1. Then\nlim sup\nn→inf\nZ 1\nfn(x) dμ(x) ≤\nZ 1\nlim sup\nn→inffn(x) dμ(x),\nThis is proved by applying Fatou's lemma to the functions gn(x) = 1-fn(x). The inequality\nmay be strict as in this example with LHS = 1/2; RHS = 1.\nf2n(x) =\n(\n0 ≤x ≤1/2\n1/2 < x ≤1 ;\nf2n+1(x) =\n(\n0 ≤x ≤1/2\n1/2 < x ≤1 .\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.103 Fourier Analysis\nFall 2013\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        }
      ],
      "source_url": "https://ocw.mit.edu/courses/18-103-fourier-analysis-fall-2013/",
      "course_info": "18.103 | Undergraduate",
      "subject": "General"
    },
    {
      "course_name": "Mathematical Methods for Engineers II",
      "course_description": "No description found.",
      "topics": [
        "Engineering",
        "Systems Engineering",
        "Computational Science and Engineering",
        "Mathematics",
        "Applied Mathematics",
        "Differential Equations",
        "Linear Algebra",
        "Engineering",
        "Systems Engineering",
        "Computational Science and Engineering",
        "Mathematics",
        "Applied Mathematics",
        "Differential Equations",
        "Linear Algebra"
      ],
      "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nPrerequisites\n\nCalculus (18.02), Differential Equations (\n18.03\n) or Honors Differential Equations (\n18.034\n).\n\nTextbooks\n\nThis course as taught during the Spring 2006 term on the MIT campus used the following text:\n\nStrang, Gilbert.\nIntroduction to Applied Mathematics.\nWellesley, MA:\nWellesley-Cambridge Press\n, 1986. ISBN: 9780961408800. (\nTable of Contents\n)\n\nSince that time, Professor Strang has published a new textbook that is being used for this course as it is currently taught on the MIT campus, as well as for Mathematical Methods for Engineers I (18.085). Information about the new book can be found at the\nWellesley-Cambridge Press\nWeb site, along with a link to Prof. Strang's new \"Computational Science and Engineering\" Web page developed as a resource for everyone learning and doing Computational Science and Engineering.\n\nStrang, Gilbert.\nComputational Science and Engineering\n. Wellesley, MA:\nWellesley-Cambridge Press\n, 2007. ISBN: 9780961408817.\n\nDescription\n\nThis course has two major topics:\n\nInitial Value Problems\n\nLinear: Wave Equation, Heat Equation, Convection Equation\n\nNonlinear: Conservation Laws, Navier-Stokes Equation\n\nFinite Difference Methods: Accuracy and Stability\n\nLax Equivalence Theorem: CFL and Von Neumann Conditions\n\nFourier Analysis: Diffusion, Dissipation, Dispersion\n\nSeparation of Variables and Spectral Methods\n\nSolution of Large Linear Systems\n\nFinite Differences, Finite Elements, Optimization\n\nDirect Methods: Reordering by Minimum Degree\n\nIterative Methods and Preconditioning\n\nSimple Iteration (Jacobi, Gauss-Seidel, Incomplete LU)\n\nKrylov Methods: Arnoldi Orthogonalization\n\nConjugate Gradients and GMRES\n\nMultigrid Methods\n\nInverse Problems and Regularization\n\nRequirements\n\nThere are no exams in 18.086. Two computational projects take their place, one on each of the major topics in the course. The projects are chosen by each student and they include a brief report.",
      "files": [
        {
          "category": "Resource",
          "title": "josephkovacpro.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/12c176f3c3db7c3d8f72c9dcad3e9390_josephkovacpro.pdf",
          "content": "Joseph Kovac\n18.086 Final Project\nSpring 2005\nProf. Gilbert Strang\n\nThe Fundamentals and Advantages of Multi-grid Techniques\n\nIntroduction\n\nThe finite difference method represents a highly straightforward and logical\napproach to approximating continuous problems using discrete methods. At its heart is a\nsimple idea: substitute finite, discrete differences for derivatives in some way appropriate\nfor a given problem, make the time and space steps the right size, run the difference\nmethod, and get an approximation of the answer.\n\nMany of these finite difference methods can ultimately be written in a matrix\nform, with a finite difference matrix multiplying a vector of unknowns to equal a known\nquantity or source term. In this paper, we will be examining the problem Au=f, where A\nrepresents a finite difference matrix operating on u, a vector of unknowns, and f\nrepresents a time-independent vector of source terms. While this is a general problem,\nwe will specifically examine the case where A is the finite difference approximation to\nthe centered second derivative. We will examine solutions arising when f is zero\n(Laplace's equation) and when it is nonzero (Poisson's equation).\n\nThe discussion would be quite straightforward if we wanted it to be; to find u, we\nwould simply need to multiply both sides of the equation by A-1, explicitly finding\nu= A-1f. While straightforward, this method becomes highly impractical as the mesh\nbecomes fine and A becomes large, requiring inversion of an impractically large matrix.\nThis is especially true for the 2D and 3D finite difference matrices, whose dimensions\ngrow as the square and cube of the length of one edge of the square grid.\n\nIt is for this reason that relaxation methods became both popular and necessary.\nMany times in engineering applications, getting the exact answer is not necessary; getting\nthe answer right to within a certain percentage of the actual answer is often good enough.\nTo this end, relaxation methods allow us to take steps toward the right answer. The\nadvantage here is that we can take a few iterations toward the answer, see if the answer is\ngood enough, and if it is not, iterate until it is. Oftentimes, using such an approach,\ngetting an answer \"good enough\" could be done with orders of magnitude less time and\ncomputational energy than with an exact method.\n\nHowever, relaxation methods are not without their tradeoffs. As will be shown,\nthe error between the actual answer and the last iteration's answer ultimately will decay\nto zero. However, not all frequency components of the error will get to zero at the same\nrate. Some error modes will get there faster than others. What we seek is to make all the\nerror components get to zero as fast as possible by compensating for this difference in\ndecay rates. This is the essence of multi-grid; multi-grid seeks to allow the error modes\nof the solution to decay as quickly as possible by changing the resolution of the grid to\nlet the error decay properties of the grid be an advantage rather than a liability.\n\nBasic Theory of the Jacobi Relaxation Method\n\nBefore going into the theory of the method, I first want to state that much of the\nfollowing closely comes from an explanation in A Multi-grid Tutorial by William Briggs\net al. This text explained the material as clearly and concisely as one could hope for. To\na large extent, much of the \"theory section\" following will be reiteration of their\nexplanation, but with emphasis on concepts which will be validated in the numerical\nexperiments later. In no way do I claim these derivations as my own. The following is a\nderivation of the Jacobi method in matrix form, which is the relaxation method which\nwill be used for the rest of the paper.\n\nWe can first express the matrix A as a sum of its diagonal component D and lower\nand upper triangular components L and U:\n\nU\nL\nD\n+\n+\n=\nA\n(1)\n\nso\n\nf\nu\nU\nL\nD\n=\n+\n+\n)\n(\n(2)\n\nWe can move the upper and lower triangular parts to the right side:\n\nf\nu\nU\nL\nDu\n+\n+\n-\n=\n)\n(\n(3)\n\nWe can then multiply both sides by D-1:\n\n)\n)\n(\n(\nf\nu\nU\nL\nD\nu\n+\n+\n-\n=\n-\n(4)\n\nWe can define\n\n)\n(\nU\nL\nD\nRJ\n+\n-\n=\n-\n(5)\n\nTherefore, we have defined the iteration in matrix form, and can write, in the notation of\nBriggs's chapter in Multi-grid Methods:\n\nf\nD\nu\nR\nu\nJ\n)\n(\n)\n(\n-\n+\n=\n(6)\n\nWeighed Jacobi takes a fraction of the previous iteration and adds it to a fraction of the\nprevious iteration with the Jacobi iteration applied:\n\nf\nD\nu\nR\nI\nu\nJ\n)\n(\n)\n1(\n]\n)\n[(\n-\n+\n+\n-\n=\nω\nω\nω\n(7)\n\nWe can rewrite the above as\n\nf\nD\nu\nR\nu\n)\n(\n)\n1(\n-\n+\n=\nω\nω\n(8)\n\nwhere\n]\n)\n[(\nJ\nR\nI\nR\nω\nω\nω\n+\n-\n=\n(9)\n\nThis iteration and its characteristics on the grid is the focus of this paper. Before\nattempting to implement this method, it is first good to predict the behavior we expect to\nsee theoretically. One way to do this is to look at the eigenvalues of the matrix Rω. The\nfollowing again stems from Briggs, but some of the following was not explicitly\nexplained and left as an \"exercise\" in the text.\n\nWe first note that, by the properties of eigenvalues and by eq. 9,\n\nRJ\nR\nωλ\nω\nλ ω\n+\n-\n=\n)\n1(\n(10)\n\nTherefore, we first need to find λRJ. We observe that:\n\nI\nA\nU\nL\n-\n=\n+\n(11)\n\nTherefore,\n\n-\n=\n+\nA\nU\nL\nλ\nλ\n(12)\n\nNoting that, for the 1D case,\n\nI\nD\n1 =\n-\n(13)\n\nSo, using eq. 5 and properties of eigenvalues,\n\n)\n(\n+\n-\n=\n-\n-\n=\nA\nA\nRJ\nλ\nλ\nλ\n(14)\n\nTherefore, remembering eq. 10,\n\nA\nR\nωλ\nλ ω\n-\n=\n(15)\n\nThe kth eigenvalue of the matrix A is:\n\n),\n(\nsin\n)\n(\n-\n≤\n≤\n=\nn\nk\nn\nk\nA\nk\nπ\nλ\n(16)\n\nSo, by eq. 15, the eigenvalues λω are:\n\n1,\nsin\n)\n(\n-\n≤\n≤\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n-\n=\nn\nk\nn\nk\nR\nk\nπ\nω\nλ\nω\n(17)\n\nAnd the jth component of the kth eigenvector is:\n\nn\nj\nn\nk\nn\njk\nj\nk\n≤\n≤\n-\n≤\n≤\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n=\n0,1\n1,\nsin\n,\nπ\nω\n(18)\n\nWe can make two quick observations here. First, for ω between 0 and 1, the\neigenvalues will always lie between -1 and 1, implying stability to the iteration. Second,\nwe remember that all vectors in the space of the matrix A can be represented as a\nweighed sum of the eigenvectors:\n\n∑\n-\n=\n=\n)\n(\nn\nk\nk\nkc\nu\nω (19)\n\nIn this case, since the eigenvectors are Fourier modes, there is an additional useful\ninterpretation of the weighting coefficients ck of the linear combination of eigenvectors;\nthese are analogous to the Fourier series coefficients in a periodic replication of the\nvector u. The other key point to see here is that varying the value of ω allows us to adjust\nhow the eigenvalues of A vary with the frequency of the Fourier modes. Plotted below is\nthe eigenvalue magnitude versus k, for n=32. We can easily see that varying ω\nsignificantly changes the relative eigenvalue magnitude at various frequencies.\n\nFigure 1: Distribution of eigenvalue magnitude as ω is varied\n\nThe implications of the graph above manifest themselves when we think of the\nhomogenous case of Au=0. If we were to use the Jacobi iteration in this case, and started\nfrom a vector as our \"guess\" at the final answer:\n\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n=\nn\njk\nu\nπ\nsin\n)\n(\n(20)\n\nwhere k is between 1 and n-1, we would have an error made of only one mode, i.e. the\nerror would lie perfectly along a single eigenvector of the matrix A and ck would be zero\nfor all k except the k which the vector u lay along. A priori, in this simple case, we know\nthat the solution should converge to 0 with enough steps, as there are no source or sink\nterms.\n\nIn practical situations, we won't know the correct u, and the error will not be\nsimply along one eigenvector. Now, the importance of Figure 1 becomes clear: adjusting\nω allows us to decide which error frequencies on the grid we want to decay quickly\nrelative to others. Picking the correct ω is somewhat application specific. In Briggs'\nexample, he picks to have the eigenvalue magnitudes of the middle frequency and highest\nfrequency match, so that the grid we work on will be decidedly favored towards either\ndecay of high frequency modes or low frequency modes. The motive for this choice will\nbecome apparent later. For this condition, ω=2/3. This value of ω will be used\nthroughout the numerical experiments.\n\nBasic Theory of Multi-grid\n\nThere is an obvious disadvantage to the relaxation method so far: while high\nfrequency error components can quickly decay, the eigenvalues of lower frequency\ncomponents approach 1, meaning that these lower frequency error components take many\nmore iterations to be damped out than higher frequencies. The essence of multi-grid is to\nuse this feature to our advantage rather than to our detriment. What if we were to\nsomehow take a few iterations to first smooth out the high-frequency components on the\nfine grid, then downsample the problem onto a coarser grid where the lower frequency\ncomponents would decay faster, then somehow go back up to the fine grid?\n\nFirst, consider what happens to the kth mode when downsampled onto a grid half\nas fine as the original vector (i.e. downsampling by a factor of 2). The kth mode on the\nfine grid becomes the kth mode on the coarse grid. This also implies that the \"more\noscillatory\" modes on the fine grid become aliased on the coarse grid. A rigorous\nargument complete with Fourier series plots could be made here, but that is not the point.\nThe implication is that now the error that refused to decay quickly on the fine grid has\nbeen frequency-shifted so that it has become high-frequency error on the coarse grid and\nwill decay quickly.\n\nAll that is left to do is to define what it means to move from a fine grid to a coarse\ngrid and eventually come back again, and how to correctly state the problem so that the\nanswer achieved is accurate. First, a few basic relationships need to be established.\nAgain, this is not original thought, and closely follows the Briggs text. First, the\nalgebraic error of the current iteration is defined as\n\n)\n(\n)\n(\nn\nn\nu\nu\ne\n-\n=\n(21)\n\nwhere u is the correct value of u, and u(n) is the resulting approximation after n steps.\n\nThe residual is defined as the amount by which the current guess at u(n) fails to satisfy\nAu=f:\n\n)\n(\n)\n(\nn\nn\nu\nf\nr\nA\n-\n=\n(22)\n\nGiven these relationships, we can also state that\n\n)\n(\n)\n(\nn\nn\nr\ne\n=\nA\n(23)\n\nThis fact lets us make the following statement about relaxation, as quoted from Briggs:\n\n\"Relaxation on the original equation Au=f with an arbitrary initial guess v is equivalent to\nrelaxing on the residual equation Ae=r with the specific initial guess e=0.\"\n\nThis makes intuitive sense by eqs. 21-23: We don't know the error, but we know\nthat the error will be zero when the residual is zero. Therefore, we can either iterate to\nsolve Au=f or we can ask, what would the error vector have to be to yield the current\nresidual? If we know the error, we know the answer by simple rearrangement of eq. 21.\nIn more mathematical terms, what the above statements are saying is the\nfollowing: if we take a few iterations to get the current value of r, we could then\nreformulate the problem by taking that value of r, then solving the new problem Ae=r\nusing Jacobi iteration, and read off the value of e after a few iterations. This will give us\na guess at what the error was before the problem was restated. Rearrangement of eq. 21\nwould then imply that if we just added the calculated value of e to the u(n) we had before\nrestating the problem, we would get a refined guess at the true vector u.\n\nPutting this fact together with the idea of moving from grid to grid, we can\ncombine the overall idea into the following:\n\n1) Relax the problem for a few steps on the fine grid with Au=f\n2) Calculate the residual r=f-Au(n)\n3) Downsample the residual onto a coarser grid\n4) Relax on Ae=r for a number of steps, starting with a guess of e=0\n5) Upsample and interpolate the resulting e onto the fine grid\n6) Refine our guess at u by adding e on the fine grid to the original value of u(n)\n\nThe above method is the central theory of multi-grid and variations of it will show\nthat there are significant gains to be made by changing the grid.\n\nImplementing a Multi-grid Solver - 1D\n\nUp to this point, the paper has mostly been a reiteration and thorough explanation\nof the Briggs text, specifically highlighting points which will be of importance later. At\n\nthis point, however, the subtleties and difficulties of actually implementing a multi-grid\nsolver arise, and while a few of the upcoming points were explained in the Briggs text,\nmuch of the actual implementation required original struggling on my part. It was quite\ndifficult despite the clarity of the theoretical basis of multi-grid. I also consulted with\ntwo students in Prof. Jacob White's group to help me think about aspects of boundary\nconditions more clearly.\n\nIn the 1-D case, I sought to implement as simple of a solver as possible; I was far\nmore interested in developing a more feature-rich 2D solver. Therefore, in the 1-D case,\nI developed a solver which would solve Laplace's equation only, and with zero boundary\nconditions. In other words, I wanted to solve only the homogenous case to demonstrate\nthat the error decays faster on the fine grid for high frequencies versus low frequencies,\nand that an inter-grid transfer would make error decay faster.\n\nSince I only needed to deal with zero boundary conditions in this case, I was able\nto use the standard, second finite difference matrix with zero boundary conditions from\nclass. To demonstrate the multi-grid method, I designed one solver and its associated\nfinite difference matrix for a 16 point grid problem, and another which would operate on\nan 8 point grid. The finite difference method was the standard one from class.\n\nThe inter-grid transfers between the fine and coarse grids were the trickier parts.\nBriggs implements downsampling from the fine grid to the coarse grid by the following\n\"full weighting\" definition:\n\n(\n)\n-\n≤\n≤\n+\n+\n=\n+\n-\nn\nj\nv\nv\nv\nv\nh\nj\nh\nj\nh\nj\nh\nj\n(24)\n\nFor a vector 7 components long, this operation can be implemented by a\nmultiplication by the following matrix:\n\n1 2 1 0 0 0 0\n\n1⁄4 *\n0 0 1 2 1 0 0 (25)\n\n0 0 0 0 1 2 1\n\nSuch a matrix would move the vector from a grid of seven points to a grid of three\npoints. This takes care of the coarsening operation; a scaled transpose of this matrix\nperforms linear interpolation, and allows us to transfer data from the coarse grid to the\nfine grid. That fact is the primary motivation for using the full weighting method rather\nthan simply downsampling by taking every other point from the fine grid.\nUnfortunately, practical, non-ideal interpolators will also introduce error through\nthe interpolation; this error will need to be smoothed out by relaxing again on the fine\ngrid as it will likely have some higher-frequency components in the interpolation error\nbest smoothed by the finer grid. If one transposes the above matrix and scales it by 2, the\nlinear interpolation scheme would be realized.\nAs stated before, I only sought to confirm the idea that the higher frequency error\nwill decay faster on the fine grid than the low frequency error. In the graph below, I\ndefined the initial \"guess\" as the sum of a low frequency (discrete frequency π/8) and a\nhigher frequency (discrete frequency 15π/16). It is obvious that the high frequency\ncomponent decays much faster than the low frequency component.\n\nFigure 2: High-frequency component of error decays faster than low frequency component\n\nThe only other thing left to confirm in the 1D case was that a multi-grid approach\nshowed some promise of benefit. To demonstrate this, I used the same initial function\nand compared a relaxation of thirty steps on the fine grid with a relaxation of ten steps on\nthe fine grid, ten on the coarser grid, and ten more to smooth out interpolation error at the\nend on the fine grid, giving both approaches the same total number of steps. The results\nfor the single grid approach versus the multi-grid approach are shown below.\n\nFigure 3: The advantage of the grid transfer quickly becomes apparent\nI must qualify the above plot with the following information. There was a bit of a\ndiscrepancy with the definition of h in the finite difference method (i.e. the 1/h2 term in\nfront of the matrix K). Intuitively, as the grid coarsens, h should change. This change\nwas necessary and gave the best results in the 2D case. However, in the 1D case I had to\ntweak this factor a bit; I had to multiply the proper K on the coarse grid by 4 to get the\n\nexpected advantage working with the grid transfer. I couldn't find the source of the\ndiscrepancy, and it might be a subtlety that I missed somewhere. Nonetheless, even with\nthis mysterious \"gain factor,\" the above experiment proves that faster convergence to the\nzero error state can happen with a grid transfer rather than simply staying on the fine grid\nfor all steps.\n\nImplementing a Multi-grid Solver - 2D\n\nThe 2D case shares a number of similarities with the 1D case, but it carries a\nnumber of subtleties with it that make implementation of the method significantly more\ndifficult than the 1D case. The most difficult aspect to attack was getting the boundary\nconditions right. I decided that I would stick to Dirichlet boundary conditions for this\nproject, as their implementation was significant work, let alone think about Neumann\nconditions.\n\nThe 1D case was implemented minimally, only thoroughly enough to demonstrate\nthe relative rates at which the different modes of the error in the homogenous case\ndecayed and that grid transfers showed a hint of promise. In the 2D case, I wanted to\nimplement a more useful and practical solver. Specifically, I wanted to be able to specify\nDirichlet boundaries, source terms in the grid, and boundaries within the grid. In the\nelectrostatics case, this would be like saying that I wanted to be able to specify the\nboundary voltages of my simulation grid, any charge source in the medium, and the\nvoltages of any electrodes existing internal to the grid.\n\nSpecifying charge sources is very easy: just specify them in f. However,\nspecifying boundary conditions is more difficult. I decided to incorporate the boundary\nvalues by altering both the matrix A and the right-hand side f. As we learned, the 2D\nfinite difference matrix generally has the following form:\n\nFigure 4:The K matrices from Prof. Strang's new text\n\nIn order to properly implement the boundary condition, we must remember the\nequations underlying the K2D matrix: we are simply solving an N2 by N2 system of linear\nequations. Therefore, if we fix u(p)=b for some value p and constant b, this means that in\nour system of linear equations, whenever the value u(p) shows up in one of the\nsimultaneous equations, its value must be b. The way to accomplish this is simple; we\nmust alter Au=f to reflect this fact. If we simply set the pth row of A to zero, and then set\nthe pth column of that row to be 1 (i.e. set the pth diagonal entry to 1), the value at u(p)\nwill be forced to f(p). Therefore, assuming f was originally the zero vector, we must now\nsatisfy that the pth entry of f now be equal to u(p), so now f(p)=b. This has forced u(p)=b.\nOne might wonder if we should also set the pth column to zero. We should not, as\nthe columns allow the forced value of u(p) to propagate its information into other parts of\n\nthe grid. Physically, at least in electrostatics, there is no intuition of having a source at a\npoint where there is a boundary condition, because the boundary manifests itself as a\nsource in this implementation. Therefore, if there is a boundary at u(p), f(p) will be zero\nat that point before we put the constraint on the point.\n\nThe above method works excellently for interior boundary points. The actual grid\nboundaries, where Dirichlet conditions were sought, are not as straightforward. Some\nfinite difference methods deal with these points implicitly by never explicitly defining\ngrid points at the edges. Instead, I decided to explicitly define these points and alter the\nA matrix, creating a matrix format which deviated from that in the figure above.\n\nThe difficulties in the above implementation arise when the difference matrix of\n\"K2D\" \"looks\" outside the grid implicitly when it touches the edges of the grid. This is\neasier to see in the 1D K matrix. The first and last rows of K are missing -1's in that\nmatrix. Implicitly, this means that the finite difference operator looked \"off the grid\" and\nfound zero, unless a nonzero value shows up to make a non-zero entry in f. I decided to\nexplicitly define the boundary values instead of trying to keep up with these issues.\n\nFirst, the ordering definition of u and A must be defined. For my implementation,\nu(0) was the upper-left corner of the grid and u(N) was the lower-left corner. u(N+1) was\nthe point right of u(0), and u(2N) was the point to the right of u(N). u(N2-N+1) was the\nupper-right corner, and u(N2) was the lower-right corner.\n\nTherefore, to define explicit boundaries, I needed to set the first N values of u to\nthe Dirichlet conditions. Therefore, when constructing A, by the reasoning from the\ninterior boundary points described above, the upper-left corner of A was a block identity\nmatrix of size N, and f(1...N) was set to the boundary value. This construction dealt with\nthe left edge easily. I constructed the rest of A by using the traditional 2D stencil from\nclass. In order to account for the top and bottom edges, I made sure to set those\ncorresponding rows in A to zero, except with a 1 on the diagonal and the corresponding\nvalue of f to the boundary condition. When I reached the lower-right corner, I augmented\nA with another block identity matrix of size N, and set f to the boundary condition at\nthose points. A matrix density plot is shown below to illustrate this construction.\nApproaching the boundaries explicitly made them easier to track, but an algorithm to\nconstruct a general A for a given mesh size was quite difficult; that is the tradeoff.\n\nFigure 5: Sparsity pattern of my altered finite difference matrix which allows for explicit boundary\ndefinition. Notice the periodic gaps along the diagonal representing the top and bottom edges of the\ngrid.\n\nWith boundary conditions properly incorporated, the last topic to address was that\nof inter-grid transfers: what matrix downsamples the original data to a coarser grid?\nWhich matrix transfers from the coarse grid to the fine grid? The proper way to phrase\nthe question is this: what is the proper way to transfer data from one grid to another?\n\nIn going from a coarse grid to a fine grid, the central problem is interpolation.\nThe central ideas of interpolation and downsampling were discussed in the 1-D section.\nThe 2D implementation is highly similar, but with a little more complexity than the 1D\ncase due to slightly trickier boundaries on the edges. I decided that I would again seek to\ndo downsampling as a weighted averaging of neighboring points rather than by injection.\nAgain, the reason for this approach was so that simply transposing the downsampling\nmatrix would yield the linear interpolation matrix for upsampling and linear interpolation.\n\nSuch a downsampling matrix was rather straightforward to implement for the\ninterior points of the grid. Incorporating the edges would have been somewhat trickier,\nand the averaging scheme used, if simply allowed to include the edges, would have\nchanged the boundary values themselves, which is to be avoided at all costs. Therefore, I\ntook the following approach.\n\nDownsampling\n\n1) Calculate the residual\n2) Remove the edges from the fine grid residual data\n3) Design a downsampling matrix to transform the inner grid residual data from the\nfine grid to the twice-as-coarse grid\n4) Apply the downsampling matrix to the interior residual data\n\n5) Append zeros around downsampled residual data grid to represent the fact that the\nresiduals are by definition zero at the edges where we have defined the exterior\nboundaries.\n\nUpsampling\n\n1) Remove the zeros from the coarse grid's edges (this is after we have relaxed the\nresidual problem on the coarser grid)\n2) Apply the scaled, transposed downsampling matrix to the interior points to get the\ninterpolated guess at the error on the fine grid\n3) Pad the resulting upsampled interior points with zeros since there is no refinement\nin the error at the known boundaries\n4) Add the upsampled guess at the error to the original guess at u\n\nThe downsampling operator was defined explicitly in Briggs, though in an index form\nrather than matrix form. I implemented the operation as a matrix in order to speed up\ncomputation in MatLab. Briggs defines the downsampling operation as follows in 2D\n(v2h is the vector represented on the coarse grid, vh is the grid on the fine grid):\n\n(\n)\n,\n1,\n,\n,1\n,1\n,\n,\n,1\n,1\n,1\n,1\n-\n≤\n≤\n⎥\n⎥\n⎥\n⎥\n⎦\n⎤\n⎢\n⎢\n⎢\n⎢\n⎣\n⎡\n+\n+\n+\n+\n+\n+\n+\n+\n=\n+\n-\n+\n-\n+\n+\n-\n+\n+\n-\n-\n-\nn\nj\ni\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nij\n(26)\n\nI implemented it instead as a matrix operator. In order to implement the above\noperation, the following stencil was used:\n\n))\n)(\n)\n((\n)\n)(\n)3\n)((\n((\nzeros\nM\nzeros\nM\n-\n-\n\n(27)\n\nM was the number of interior points in one edge of the fine grid from which the\nmapping was supposed to take place. As a final point, the stencil was not simply\nreplicated along the diagonal, rather to implement the correct operation it was\nimplemented in a staggered pattern (similar to 1D) as shown in the sparsity pattern\nbelow.\n\nFigure 6: Sparsity pattern for the downsampling matrix; stencil is replicated in the matrix in a\nstaggered fashion\n\nBriggs describes the indexed form of the 2-D linear interpolation operator, and it is\nsimply implemented by transposing the above matrix and scaling by 4.\n\nNumerical Experiments in 2-D\n\nFinally, with all the tools in place for 2-D, numerical experiments could be\nundertaken. A convincing example that the system was working properly would be\nsolution of a problem with a known solution. To this end, I decided to compare the\nmulti-grid solver's solution to the actual solution to Laplace's equation with the\nfollowing boundary conditions:\n\nFigure 7: Boundary conditions for the known solution to Laplace's equation\n\nThe solution to Laplace's equation in this case can be expressed as a sum of sines\nand hyperbolic sines. I will not go through the derivation here for that answer, but I\nwrote a loop in MatLab to compute a partial sum of the relevant terms to produce the\ncorrect answer so that it could be compared to the multi-grid solver's output. The two\nsolutions produced are very similar. The Gibbs phenomenon is apparent in the partial\nsum of sines. They are plotted below.\n\nFigure 8: Fourier expansion of actual solution. Right edge converges to 1; perspective of plot is\nmisleading.\n\nFigure 9: My solver's output after 1000 steps on the fine grid\n\nThe next obvious experiment is to see how quickly the relaxation error decays to\nzero. The decay of pure modes was examined for the 1-D case. Now however, the solver\nwas considerably more powerful, so examining more sophisticated problems would be\ninteresting. In general, we don't know the final solution; we only know the residual after\na step. So, from now on, instead of discussing error, we will examine how the norm of\nthe residual decays.\n\nAn interesting case to examine would be a unit spatial impulse. The Fourier\nexpansion of an impulse has equal weights on all frequencies, so examining how an\ninitial guess of an \"impulse\" decays to zero everywhere in homogenous conditions would\nbe insightful. The following plots show a unit impulse located at 67,67 on a 133 by 133\ngrid after various numbers of steps.\nFigure 10: Decay of a unit impulse. Notice that after 30 steps, the solution is much \"smoother\" than\nafter 10. This is because the higher-frequency modes have been filtered out by the fine grid.\n\nFigure 11: Stalling of residual decay on the fine grid\n\nWe can see that the residual decays very quickly initially, but the decay rate then\nstalls. This is because the error that is left is lower-frequency error which does not decay\nquickly on the fine grid. This is seen in the figure, as after twenty and thirty iterations,\nthe solution looks very smooth.\n\nThe question to ask now is, how much better could the answer be after a number\nof steps if we employ a multi-grid approach? In the following experiment, three grid\ncoarseness levels were available. Grid 1 was the fine grid. Grid 2 was the \"medium\"\ngrid, and was twice as coarse as Grid 1. Grid 3 was the \"coarse\" grid, and was twice as\ncoarse as grid 2.\n\nAn experiment similar to the one in Figure 10 was attempted with the unit\nimpulse. Three relaxations were performed, starting with homogenous conditions and a\nunit impulse initial condition.\n\nTrial 1: Relax with 2500 steps on Grid 1\n\nTrial 2:\na) Relax with 534 steps on Grid 1\nb) Move to Grid 2 and relax for 534 steps\nc) Move back to Grid 1, incorporate the refinement from (b), and relax\nfor 1432 steps for a total of 2500 steps\n\nTrial 3:\n\na) Relax with 300 steps on Grid 1\n\nb) Relax with 300 steps on Grid 2\n\nc) Relax with 300 steps on Grid 3\n\nd) Move back to Grid 2, incorporate refinement from (c) and relax for 800\n\nsteps\n\ne) Move back to Grid 1, incorporate refinement from (d) and relax for 800\n\nsteps for a total of 2500 steps\n\nThis scheme was chosen because it gave all methods the total number of steps.\nAdditionally, for trial 2 and trial 3, the ratio of forward relaxations (i.e. relaxation after\nmoving from fine to coarse) to backwards relaxation was constant at 3/8. The detail after\n2500 steps is shown below for all three cases.\n\nFigure 12: The three-grid scheme outperforms both the single and dual grid schemes.\n\nIt is clearly visible that given the same number of steps, the three-grid scheme\noutperforms the single grid scheme and the dual grid scheme. However, it is not a given\nthat this result will always be the case. If the error, for example, was known to be almost\npurely high-frequency, the advantage of the grid transfers might be outweighed by the\ncomputation power necessary to keep making the transfers and interpolations.\nThe case shown above for the unit impulse is a case where the frequencies are\nequally weighted in the initial conditions. As a second trial, I examined how the\nresiduals decayed for an initial condition with more low-frequency content. This case\nwas again homogenous with boundary conditions of zero, but the initial \"guess\" was 1\neverywhere except at the boundaries. I repeated the experiment with these initial\nconditions, and the results are shown below.\n\nFigure 13: Decay of residuals for the three schemes. The only fair comparison across methods is\nafter all three trials have made it back to the fine grid (i.e. after the last spike in residual on the green\nline which comes from the interpolation error). The three-grid method is the most accurate after\n2500 steps.\n\nFigure 14: Detail of the final residual values for the three methods. The three-grid method clearly\nwins out over the others. This is a more drastic out-performance than before since the initial\ncondition contained more low frequency error, which was better managed on the coarser grids.\nOnce again, the three-grid scheme wins. It is important to note that in the first\nfigure, the \"norm\" during the steps while the problem's residual resides in the coarser\ngrid is not comparable to the norm of vectors in other grids, as the norm is vector-size\ndependent. Therefore, the only truly fair points on the graph to compare the methods are\nwhen all methods are on the same grids, namely the very beginning and very end (shown\nin detail in Figure 14).\n\nThere are two ways to interpret the results. We can get a better answer with the\nsame number of steps by using the three-grid cycle. Alternatively, we could stop earlier\nwith the three-grid cycle and settle for the value that the other methods would have\nproduced with more steps. The tradeoff is completely problem dependent.\nI was suspicious as to how much difference the above residuals made in the\nappearance of the answer, especially given the much higher initial values of the residuals.\nThe difference after trials 1, 2 and 3 is stark and is shown below. Remember, with an\ninfinite number of steps, all three methods would converge to a value of zero everywhere.\n\nThe results are obviously different. Trial 3 yielded an answer visually very close\nto the correct answer of 0. It is clear that going beyond simply one coarsening operation\nyielded great benefits. The natural next step would be to try a fourth, coarser grid, and\ncontinue coarsening. One could coarsen the grid all the way to a single point. Also,\n\ntrying a multitude of different step distribution schemes in order to maximize efficiency\nof steps at each grid could be tried too.\nOne could easily write a book about these concerns, but going far down either of\nthese paths would step outside the scope of this introduction to multi-grid and its benefits.\nInstead, it would be more appropriate to confirm this limited-multi-grid system on other\nproblems.\nAs stated earlier, a key goal of my 2D implementation was the ability to impose\nboundary conditions within the grid. I designed my Jacobi relaxer, as described earlier,\nto support internal boundaries as well. I implemented the system so that I could simply\nuse Windows Paint (r) to draw whatever regions I wanted to specify as at a particular\n\"voltage.\" As an appropriate example, I decided to determine the potential distribution\nresulting from having an electrode in the shape of the letters \"MIT\" in the grid, with 0\nvolt boundary conditions on the edge of the grid. The bitmap used to create the boundary\nconditions is shown below. The black letters are defined to be 1 volt, the white area zero\nvolts.\n\nShown below is a plot of the relaxation solution (still staying all the time on the\nfine grid) of the solution to the problem.\n\nFigure 15: \"Electrodes\" in shape \"MIT\" relaxed on fine grid\n\nFinally, just to prove that the ability to add charge into the simulation was added,\nI added a line of charge the under the \"electrodes\" used to write \"MIT\" to underline the\nword.\n\nFigure 16: MIT electrodes with a line of charge underlining them\n\nPlacement of arbitrary charge within the medium with arbitrary boundary\nconditions was supported as well. The figure below shows the gains made with a 930\nstep double-grid method vs. a single grid method; the point is to show that the charge\nplacement was supported across multiple grids.\n\nFigure 17: Multi-grid support included for arbitrary charge distributions as well\n\nAs for multi-grid support of internal boundary conditions (i.e. if we wanted to\nrelax the MIT electrode problem with multi-grid), I did not quite get around to that. I\nthought I had it done, but I discovered too late that I had forgotten a subtle aspect. When\nrelaxing on Ae=r, I forgot to pin internal values of e at the boundaries to zero, as by\ndefinition there would never be error at one of the internal boundaries. Without doing\nthis, the compensated error approximation from the coarse grid will attempt to force the\nvalue at the boundary to deviate from the correct internal boundary condition.\nThis could be fixed by changing the matrix A by making the rows corresponding\nto these points zero, except for a 1 on the diagonal. Additionally, r at that point would\nneed to be 0, but I had already thought of and taken care of that and had implemented that\naspect. As simple as the fix sounds, I had an elaborate setup in the algorithm for the\ncurrent system, and making the change would have meant tearing the whole system down\nand building it back up, which was unrealistic as late as I found the problem. However, I\ndid determine the source of the problem and its likely fix.\n\nConclusion\n\nThe most convincing figure of the paper is replicated below.\n\nThis figure truly sums up the power of multi-grid. In the same number of steps,\nthe approach with the largest utilization of coarsening got closest to the right answer.\nOne can be more quantitative about the true efficiency of the method: what is the\ncomputational tradeoff between doing a few more iterations on the fine grid and moving\nto a coarse grid? Do the benefits of moving outweigh the computational costs of\ndownsampling and interpolation? What is the best way to design a multi-grid cycle\nscheme? How long should one spend on a coarse grid versus a fine one? These are all\nexcellent questions of multi-grid, and there is no definitive right answer.\n\nAs for the tradeoff between interpolation and downsampling versus spending time\non a fine grid, making an absolutely definitive answer is difficult. However, multiplying\nby the Jacobi matrix for an iteration and multiplying by an upsampling or downsampling\nmatrix consist of matrix multiplications of relatively the same size and density, making\nthe intergrid transfers relatively cheap and insignificant compared to large numbers of\n\nsteps of relaxation computation. It is more likely that the tradeoffs will come from\ndetermining the proper amount of time to spend at each grid. A possible way to do this\nwould be to look at the FFT of the residual, try to predict the spectral content of the error,\nand adaptively decide which grid to move to based on that result. Other ways would be\nto look for patterns in the particular class of data being examined. Such design issues\nwould make excellent projects in and of themselves.\n\nWhat is definite, however, is that multi-grid can yield astonishing results in the\nright circumstances and can give excellent answers in a fraction of the time that a single-\ngrid relaxation would need. If an exact solution is not necessary, and the grid is huge,\nmulti-grid is an excellent way to go.\n\nReferences\n\nBriggs, William, et al. A Multigrid Tutorial, Second Edition. (c) 2000 by Society for\nIndustrial and Applied Mathematics.\n\nJaydeep Bardhan and David Willis, two great advisors from Prof. Jacob White's group.\n\nProf. Gilbert Strang, for his draft of his new textbook."
        },
        {
          "category": "Resource",
          "title": "overview.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/4cd6d25812214df9a2883e6a96ab3229_overview.pdf",
          "content": "Overview of Multi-grid Project\n\nThis project example on this OCW site is a slight refinement of my final\nproject in Prof. Strang's 18.086 class. The project implements a 2D multi-grid solver\nfor Laplace's and Poisson's equation. Multi-grid support is included for Dirichlet\nboundary conditions, as well as for sources within the grid. Internally pinned\nDirichlet boundaries, while possible to specify, do not function properly if multi-grid\nschemes are used in my implementation. I wanted to add this feature, but ran out of\ntime at the end of the project.\nI have attempted to give an explanation of how to use the solver by including\nthe file \"ProjectOCW.m,\" a simple numerical experiment conducted with the solver.\nOnce all project files are placed in the same directory, one should simply be able to\nopen the mentioned file and run it in MatLab. The file will plot the error and the\nguess at the solution at various steps in a V-cycle approach to multi-grid.\nAdditionally, I have extensively commented this file to give the user enough\nknowledge to modify the file to conduct experiments of one's own. The solver tracks\nthe norm of the residual as the solver progresses, which is a key statistic to keep an\neye on. Upon altering the MatLab script, the user can decide parameters like how\nmany steps to stay on a grid, what Jacobi damping factor to use, or a number of other\nparameters.\nThe user can also import images to use as a two-dimensional source term in\nthe problem, or import other images to define the boundary conditions or initial guess.\nImages can be imported into MatLab. In order to be incorporated into the solver,\nhowever, the images must become appropriately sized matrices with values\ncorresponding to some interpretation in Laplace's equation. The simplest example is\nimporting a monochrome bitmap for source terms, where black pixels could map to\nzero entries in the source terms, and white to some determined value.\nI attempted to thoroughly explain \"ProjectOCW.m,\" but left explanation of the\nhelper functions more vague. This project was not intended originally to teach multi-\ngrid, but rather act as an example of implementation of a multi-grid solver.\nFurthermore, there are definitely more efficient and elegant ways to implement a\nnumber of the operations of the solver in MatLab. For these reasons, I found a\nthorough explanation of the inner workings of the solver to be outside the scope of\nthis posting, which is meant to show multi-grid in action, rather than thoroughly\nexplain every detail of design decision in implementing the system.\nI hope you enjoy running the pre-designed experiment and will alter the\nexample file to run experiments of your own. The system here is capable of a good\nnumber of experiments. I learned a great deal about the method by simply changing\nparameters easily alterable in this example.\nFinally, I'd like to thank Prof. Strang for all of his help and advice, Dave\nWillis and Jaydeep Bardhan for their help, and would like to acknowledge the\nimmense help of A Multigrid Tutorial by Briggs et al., a must-read for anyone\ninterested in learning multi-grid.\n\nJoseph Kovac, S.B. MIT '05\n\n9/12/05\n\nGraduate Student, MIT EECS"
        },
        {
          "category": "Resource",
          "title": "project1domnguez.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/4bfb14a306e4cc0a7f89e0d719754fdb_project1domnguez.pdf",
          "content": "MASSACHUSETTS INSTITUTE OF\nTECHNOLOGY\nDEPARTMENT OF MECHANICAL ENGINEERING\nCOURSE: 18.086\nMathematical Methods for Engineers II\nProject 1\nExperimental Analysis of the Two Dimensional Laplacian\nMatrix (K2D): Applications and Reordering Algorithms\n\nProject 1\nCourse: 18.086\nExperimental Analysis of the Two Dimensional Laplacian\nMatrix (K2D): Applications and Reordering Algorithms\nSection 1: Introduction\nThis project focuses on the analysis and experimentation of the two dimensional (2D)\nLaplacian matrix (K2D). Special emphasis is put on solving large systems, particularly\nfor problems related to digital image processing and optical imaging. We discuss a\ntypical application of the K2D matrix in finding the edges of an image. In the forward\n2 ⋅\nproblem, F = K D U , the input is the image we want to process by a Laplacian based\nedge detection algorithm. As will be discussed in the next section, the input image is\npreprocessed by a raster scanning algorithm to form the vector U. The output vector F is\nalso processed by the inverse raster scanning algorithm to form the 2D Laplacian of the\nimage. This new matrix is used in the edge detection algorithm to find all the edges.\n-1 ⋅\n=\nIn the inverse problem, U\nK D\nF\n\n, the Laplacian of the image is the input and we\ntry to recover the original object. This is the case for certain optical systems as discussed\nin Section 2. In order to solve this problem efficiently, we discuss the properties of K2D\nand experiment several ways to speed up the elimination and also reduce the storage\nduring computations.\nIn Section 3, we analyze three popular reordering algorithms: minimum degree, red-black\nand graph separator orderings. We developed several experiments to explore their\nbehavior under conditions such as variable matrix sizes. In addition, experiments are\ndeveloped to estimate their required computational time and efficiency under LU and\nCholesky decompositions.\n\nSection 2: Application of the Laplacian Matrix in Digital\nImage Processing\nIn this section, the implementation of the two dimensional (2D) Laplacian Matrix (K2D)\non a digital image processing problem is discussed. In this problem, the second derivative\nof the image (i.e. the Laplacian) is used to find discontinuities by implementing an edge\ndetection algorithm. Edge detection algorithms are widely used in applications such as\nobject detection and recognition, shape measurement and profilometry, image analysis,\ndigital holographic imaging and robotic vision.\nEdges in images appear as regions with strong intensity variations. In the case of images\nobtained with a conventional camera, edges typically represent the contour and/or\nmorphology of the object(s) contained in the field of view (FOV) of the imaging system.\nFrom an optics perspective, edges represent the high spatial frequency information of the\nscattered wave coming from an illuminated object. If an object contains a sharp\ndiscontinuity, the information for this region will be mapped in a region of high\nfrequency in the Fourier plane. Edge detecting an image is very important as it\nsignificantly reduces the amount of data and filters out useless information, while\npreserving structural properties of the image [1].\nThere are two main categories of edge detection algorithms: gradient and Laplacian based\nalgorithms. In a typical gradient based algorithm, the first derivative of the image is\ncomputed in both dimensions (row and column wise). In this new image, the edges\nbecome apparent and the maximum and minimum gradients of the image are compared to\na threshold. If there is a maximum with a value larger than the threshold, the spatial\ncoordinate of that maximum is considered an edge. An example of gradient based\nalgorithms is the Sobel edge detection algorithm. Figure 2.1 shows a 1D discontinuity\ncentered at x=0. Figure 2.2 is a plot of the first derivative of the intensity function of\nFigure 2.1. It is important to note that the maximum is also centered at x=0. If we set the\nthreshold of the edge detection algorithm equal to 0.2, an edge would be detected at x=0.\nFigure 2.1: Example of 1D edge in an image\n\nFigure 2.2: First derivative of the 1D edge of Figure 2.1.\nFor a Laplacian based algorithm, the second derivative of the original image is computed.\nThe zero crossings of the resulted matrix are used to find the edges. Figure 2.3 shows the\nsecond derivative computed for the 1D example discussed above. The Matlab code used\nto generate Figures 2.1-2.3 is included in Appendix A.1.\nFigure 2.3: Second derivative of the 1D edge of Figure 2.1.\nIn the 2D case, the Laplacian is computed in both dimensions (row and column wise).\nThis can be achieved by using the matrix K2D in the forward problem:\n⋅\nK D U = F ,\nwhere U is a vector formed after raster scanning the original image. The measurement\nvector F is the raster scanned version of the 2D Laplacian of the image. The matrix K2D\nis an N × N matrix formed by the addition of the Kronecker tensor products\n2 =\n(\n,\n( ,\nK D kron K I ) + kron I K ).\nHere, I is the N\nN identity matrix and K is also N\nN and tridiagonal of the form:\n×\n×\n\n-1\n\"0\nK =\n-1\n-1 \"0\n\n.\n#\n%\n%\n%\n\n\" -1\nTo exemplify the 2D forward problem, consider the 500 ×500 pixels image of Figure 2.4.\nTo form the vector U, we need to implement a raster scanning algorithm. In the raster\nscanning algorithm, the N\nN matrix (i.e. the image) is decomposed into a vector of size\n×\nN 2 . This is achieved by extracting each column of the original matrix and placing it at\nthe bottom of the vector. For example, a 3 3\npixels image would produce\n×\nu1\n\nu4\nu7\n\nu3\nu1\nu2\nraster\nu2\nl\n\nU = u4\nu5\nu6\n→ scanning → U = u .\n\nu7\n\nu8\nu9\nu8\n\nu3\nu6\nu9\n\nFigure 2.4: Photograph of Prof. Gilbert Strang (500 × 500 pixels ).\nThe forward problem is solved by multiplying the K2D matrix, which in this case is\n250000 × 250000 , with the vector U. Since both the vectors and the matrix are large, it is\nnecessary to implement the 'sparse' function available in Matlab. The 'sparse' function\nreduces the required storage by only considering the coordinates of the nonzero elements\nin the sparse matrix. The resultant matrix is obtained by the implementation of the\ninverse-raster scanning algorithm. Figure 2.5 shows the 2D Laplacian obtained after\n\nsolving the forward problem for the image of Figure 2.4. The code used to generate\nFigure 2.5 is included in Appendix A.2.\nFigure 2.5: 2D Laplacian of the image of Figure 2.4.\nFor a Laplacian based algorithm, the information contained in Figure 2.5 is used to find\nthe edges of the image. The result of this algorithm is a binary image such as the one\nshown in Figure 2.6. For more information about edge detection algorithms refer to [2].\nFigure 2.6: Result of applying the Laplacian edge detection algorithm on Figure 2.4.\nNow we discuss the inverse problem. In the inverse problem, we are given the Laplacian\nof the image and our task is to find the original image\nU\nK2D 1\n.\n=\n- F\n\nThis is a common problem in communications where we want to transmit as little\ninformation as possible. In an extreme case, we would like to transmit just the edges of\nthe image (a sparse matrix) and from this recover the original image. Another example is\nan optical imaging system that has a high-pass Fourier filter in the Fourier plane, such as\nthe one shown in Figure 2.7. In the optical configuration of Figure 2.7, an object is\nilluminated by a mutually-coherent monochromatic wave and the forward scatter light\nenters a 4f system. The 4f system is a combination of two positive lenses that are\n= 1+\nseparated by a total distance of d\nf\nf\n\n2 , where f1and f 2 are the focal lengths of\nboth lenses. The first lens functions like an optical Fourier Transform operator. A high-\npass Fourier filter is positioned in between both lenses (at the focal plane). This high-pass\nfilter is designed to block the low spatial frequency content of the optical field and only\nlet the high frequency information pass. The filtered spectrum is inverse Fourier\ntransformed by the second lens and forms an image on a CCD detector. If the cut-off\nfrequency is set relatively high (i.e. cutting a great part of the low frequency\ninformation), the edges of the object start being accentuated at the image plane.\nFigure 2.7: Optical realization of edge extraction\nIn the inverse problem, the input is an image similar to the one shown in Figure 2.5 and\nthe output is another image like the one shown in Figure 2.4. The algorithm to compute\nthis includes an elimination step (for the code included in Appendix A.3, the elimination\nis done using Matlab's '\\' function) and a reordering. Different types of reordering such\nas minimum degree, red-black and nested dissection orderings will be analyzed and\ncompared in Section 3. Choosing a suitable reordering algorithm is very important\nespecially for large systems (for images with a high space-bandwidth product), as it\nreduces the number of fill-ins during elimination.\n2.1 The K2D Matrix\nThe K2D matrix can be though of as the discrete Laplacian operator applied to vector U.\nAs mentioned before, K2D can be formed from the addition of the Kronecker products of\nK and I. K2D has 4's in the main diagonal and -1's for the off-diagonal terms. For\nexample, the 9 9 K2D matrix is given by\n×\n\n-1\n-1\n-1\n-1\n-1\n\n-1\n-1\n\n-1\n-1\n-1\n\nK D = 0\n-1\n-1\n-1\n-1\n\n-1\n-1\n-1\n-1\n-1\n\n-1\n-1\n-1\n-1\n-1\n\nThe main characteristics of K2D is that it is symmetric and positive definite. From\n2 =\nsymmetry we know that K D K DT and that all its eigenvalues are real and its\neigenvectors are orthogonal. From positive defines we know that all the eigenvalues of\nK2D are positive and larger than zero; therefore, its determinant is larger than zero. The\ndeterminant for the 9 9 matrix shown above is det\n×\nK D = 100352 . The Gershgorin\ncircle theorem applied to the matrix K2D indicates that all its eigenvalues should be\ncontained inside a circle centered at 4 and with a radius of 4. Figure 2.8 shows the\neigenvalues of the 9 9 K2D matrix in the complex plane. As predicted by the\n×\nGershgorin circle theorem, all the eigenvalues are between 0 and 8. The code to generate\nthe plot of Figure 2.8 is included in Appendix A.4. Figure 2.9 shows the eigenvalues for a\nmuch larger K2D matrix ( 4900 × 4900 ). These 4900 eigenvalues also remain between 0\nand 8.\nFigure 2.8: Eigenvalues of the 9 9K2D matrix\n×\n\nFigure 2.9: Eigenvalues of a 4900 ×4900 K2D matrix\nAnother interesting measure that describes the behavior of K2D is the condition number\nor in the context of information theory, the Rayleigh metric. The condition number is\ndefined as\nλmax\nc =\n,\nλmin\nand gives information about how close the matrix is to ill-posedness. If c = 1, the matrix\nis said to be well-posed, but if the condition number is large ( c →inf), the matrix gets\nclose to becoming singular. In other words, we cannot completely trust the results\nobtained from an ill-posed matrix. Figure 2.10 shows the condition number for K2D of\ndifferent sizes. The code used to generate this figure is included in Appendix A.5.\nFigure 2.10: Condition number of K2D as a function of size\n\nSection 3: Reordering Algorithms\nIn the previous section we described some of the properties of the K2D matrix along with\nrelated problems (forward and inverse problems) that are important for digital image\nprocessing. The main difficulty in solving such systems arises when the system is large.\nA large system would be required if we intend to process an image with a large number\nof pixels. If we do the elimination directly on K2D, several fill-ins would appear and the\ncomputational costs (memory and computational time) would increase. For this reason, in\nthis section we discuss different reordering algorithms that help reduce the number of fill-\nins during elimination.\n3.1: Minimum Degree Algorithm\nn\n(\n)\nThe minimum degree algorithms reorder K2D to form a new K D = P K\n2D PT by\neliminating the pivots that have the minimum degree. If we draw the graph corresponding\nto K2D, the minimum degree algorithm eliminates the edges (off-diagonal terms) from\nthe nodes that have the minimum degree (i.e. the least number of edges connected to it).\nTo describe the steps involved in this algorithm, we carry the reordering algorithm on the\n9 9K2D matrix described above. Figure 3.1 shows its corresponding graph. The steps of\n×\nthe algorithm are as follows:\nFigure 3.1: Graph corresponding to a 9 × 9 K2D matrix\n1. We start by choosing the first element in the matrix as the pivot (in reality, you\ncan choose any of the corner nodes of the graph, as all of them are degree 2).\nUsing this pivot, carry out elimination as usual. This is equivalent to eliminating\nthe edges connected to the pivot node. Two new fill-ins are generated in this step,\nand this is shown in the graph as a new edge as shown in Figure 3.2.\n2. Update the permutation vector: P = [1]. The permutation vector keeps track of the\norder of the nodes we choose for elimination.\n3. Go back to step one and choose one of the three remaining nodes that are degree\n2. For example, if the next node we choose is node 3, after elimination, the\npermutation vector becomes: P = [1 3].\nFigure 3.3 shows the graph sequence for the remaining steps of the minimum degree\nalgorithm. The final permutation vector is P = [1\n4]. Figure 3.4\n\nshows a comparison between the structures of the original and the reordered K2D\nmatrices. Figure 3.5 shows the lower triangular matrices produced after LU decomposing\nthe original and the reordered K2D matrices. As can be seen from this figure, reordering\nthe matrix produces fewer numbers of nonzeros during the LU decomposition. The\nMatlab code used to generate Figures 3.1-3.5 was obtained at [3].\nFigure 3.2: Graph of the 9 ×9 K2D matrix after step one of the minimum degree\nalgorithm\nFigure 3.3: Sequence of graphs for the remaining steps in the minimum degree algorithm\n\nFigure 3.4: Structural comparison between the original and the reordered K2D matrices\nFigure 3.5: Lower triangular matrices produced after LU decomposition of the original\n(#nonzeros = 29) and the reordered (#nonzeros = 26) K2D matrices\nSeveral minimum degree reordering algorithms are available. Each of these algorithms\nproduces a different permutation vector. We now compare five of such algorithms:\n1. Matlab's \"symamd\" algorithm: Symmetric approximate minimum degree\npermutation.\n2. Matlab's \"symmmd\" algorithm: Symmetric minimum degree permutation.\n3. Matlab's \"colamd\" algorithm: Column approximate minimum degree\npermutation.\n4. Matlab's \"colmmd\" algorithm: Column minimum degree permutation.\n5. Function \"realmmd\" algorithm [3]: Real minimum degree algorithm for\nsymmetric matrices.\nFigure 3.6 shows the structure of a 81× 81K2D matrix. Figure 3.7 shows a structural\ncomparison of the reordered matrices using the five minimum degree algorithms\nmentioned above. Figure 3.8 shows their respective lower triangular matrix produced\nafter LU decomposing the reordered K2D matrix. From this figure, it is evident that the\nreal minimum degree algorithm produces the least number of nonzero elements (469) in\n\nthe decomposed lower triangular matrix. The code used to generate Figures 3.6-3.8 is\nincluded in Appendix B.1.\nFigure 3.6: Structure of a 81 × 81K2D matrix\nFigure 3.7: Structural comparison of reordered matrices using the five algorithms\ndescribed above\n\nFigure 3.8: Structural comparison of the lower triangular matrices decomposed from the\nreordered matrices using the five algorithms.\nNow we compare how the five minimum degree algorithms behave as a function of\nmatrix size. In particular, we are interested to know the number of nonzeros in the lower\ntriangular matrix, L, after LU decomposing the reordered K2D matrices for different\nmatrix sizes. Figure 3.9 shows this comparison for matrices with sizes ranging from 4 4\n×\nto 3844 ×3844 entries. Figure 3.10 zooms into Figure 3.9 to show an interesting behavior\nof the 'realmmd' and the 'symamd' algorithms with relative small sized matrices. For\nsmall matrices, the 'realmmd' algorithm produces the least number of nonzeros in L;\nhowever, if the matrix size increases (especially for large matrices), the 'symamd'\nalgorithm is the winner. The code used to generate Figure 3.9-3.10 is included in\nAppendix B.2.\nIn our next experiment, we compare four of these minimum degree methods for different\nmatrix sizes after performing a Cholesky decomposition on the reordered matrix. The\n×\nsize of the matrices ranges from 4 4 to 120409 ×120409 in steps of 25. The results of\nthis experiment are plotted in Figure 3.11. As in the previous experiment, we can see that\nthe methods 'symamd' and 'symmmd' produce the least number of nonzeros entries in\nthe factorized matrix. However, the question now is: are these methods (symamd and\nsymmmd) fast to compute?\n\nFigure 3.9: Comparison of the nonzeros generated by the five algorithms as a function of\nmatrix size\nFigure 3.10: Zoomed in from Figure 3.9.\nFigure 3.11: Nonzeros generated by the Cholesky factorized matrix, reordered with four\ndifferent minimum degree algorithms\n\nTo answer the question stated above, we now turn to compare the required computational\ntime for reordering K2D by these four minimum degree algorithms. The results are\nshown in Figure 3.12. From this figure we can see that although it is slightly faster to\nreorder a large matrix with 'colamd' rather than using 'symamd', the number of nonzeros\ngenerated in the decomposed matrix is significantly less for a matrix reordered by the\n'sysmamd' algorithm. In conclusion, 'sysmamd' is our big winner.\nFigure 3.12: Computational time for reordering K2D of different sizes\nThe reason we didn't include the 'realmmd' algorithm in the comparison of Figure 3.13,\nis because this algorithm requires a long computational time (even for relatively small\nmatrices) as shown in Figure 3.13.\nFigure 3.13: Computational time required by 'realmmd' as a function of matrix size\nIf we use Matlab's functions 'chol' and 'lu' to produce the Cholesky and LU\ndecompositions respectively, it is fair to ask: what is the computational time required by\nboth functions? Is the computational time dependent on the input matrix (i.e. if the matrix\nwas generated by 'symamd', 'symmd', 'colamd' or 'colmmd')? How does the\ncomputational time behave as a function of the matrix size? To answer all of these\nquestions, we generated the plots shown in Figures 3.14 and 3.15. The Matlab code used\nto generate Figures 3.11-3.15 is included in Appendix B.3.\n\nFigure 3.14: Computational time required by Matlab's Cholesky decomposition function\n'chol'\nFigure 3.15: Computational time required by Matlab's LU decomposition function 'lu'\n3.2: Red-Black Ordering Algorithm\nThe red-black ordering algorithm is an alternative reordering technique in which it is\ndivided in a way similar to a checkerboard (red and black or odd and even squares). The\npermutation vector is simply generated by first selecting for elimination the odd nodes\nand then all the even nodes on the grid. The red-black permutation is given by\nP K )\nBT\n(\n2D PT =\n4Ired\n4I\nB\nred\n,\nwhere B is a matrix composed by -1s from the off-diagonal or black elements of K2D.\n\nFor example, Figure 3.16 shows a structural comparison between the original 9 9K2D\n×\nmatrix and the its equivalent reordered using the red-black ordering algorithm. The\npermutation vector in this example is: P = [1\n8] . The reordered\nK2D matrix is given by\n-1 -1\n-1\n-1\n\n-1 -1 -1 -1\n\n-1\n-1\n(\n)\nP K D PT = 0\n-1 -1 .\n\n-1 -1 -1\n-1\n-1 -1\n\n-1 -1\n-1\n-1 -1 -1\n\nOriginal 9x9 K2D\nK2D reordered by red-black algorithm\nnz = 33\nnz = 33\nFigure 3.16: Structural comparison between the original K2D (to the left) and the\nreordered matrix using the red-black algorithm (to the right)\nFigure 3.17 shows the sequence of eliminations that occur on the graph representation of\nthe matrix described above. From this figure, we can see that the algorithm starts by\neliminating all the edges of the odd/red nodes (remember that the graph is numbered row\nby row starting from the bottom left corner). After finishing with the odd nodes, the\nalgorithm continues to eliminate the even nodes until all the edges have been eliminated.\nA comparison between the lower triangular matrices produced after elimination of the\noriginal and reorder matrices is shown in Figure 3.18. The reordered matrix produced less\nnumber of nonzero entries in L (27 nonzeros instead of 29). The Matlab code used to\ngenerate Figures 3.16-3.18 is included in Appendices B.4 and B.5. This code generates a\nmovie of the sequence followed during elimination in the graph.\n\nFigure 3.17: Sequence of eliminations for the red-black ordering algorithm\nFigure 3.18: Comparison of lower triangular matrices produced after LU decomposing\nthe original and the reordered K2D matrices\n\nWe now try to study the behavior of the red-black ordering algorithm under matrices of\ndifferent sizes. Figure 3.19 shows a comparison between the original and the reordered\nmatrices for the number of nonzeros generated in the lower triangular matrix during LU\ndecomposition. Especially for large matrices, the red-black algorithm shows an important\nimprovement due to the significant reduction of the number of nonzeros in L. From this\nfigure we can see that the red-black algorithm performs worse than the minimum degree\nalgorithms discussed earlier. However, the main advantage of the red-black algorithm\nresides in its simplicity and easy implementation.\nFigure 3.19: Number of nonzeros generated in L after LU decomposing the original and\nthe reordered K2D matrices\nFigure 3.20 shows the computational time required by the red-black algorithm as a\nfunction of matrix size.\nFigure 3.20: Computational time required by the red-black ordering algorithm as a\nfunction of matrix size\n\nFinally, we want to compare the computational time required for the LU decomposition\nof the original and the reordered matrices as a function of matrix size. This is shown in\nFigure 3.21. As expected, the reordered matrix requires less time to produce the factors L\nand U. The Matlab code used to generate Figures 3.19-3.21 is included in Appendix B.6.\nFigure 3.21: Computational time required by the LU decomposition of the original and\nthe reordered matrices\n3.3: Graph Separators Algorithm and Nested Dissection\nIn this subsection we discuss an alternative reordering algorithm called graph separators.\nAgain, it is easier to understand the logic behind this algorithm by looking directly at the\ngraph. The key idea is that a separator, S, is introduced to divide the graph in two parts P\nand Q (subsequent divisions of the graph would lead to nested dissection algorithms).\nThe separator S is formed by a collection of nodes and it has typically a smaller or equal\nsize than P and Q. To illustrate this algorithm, we go back to our 9 9matrix. The graph\n×\nof this matrix is shown in Figure 3.22. As shown in this figure, the graph is divided in\ntwo parts by S. In this case, P, Q and S have the same size. The graph was initially\nnumbered row wise as before. In the graph separators algorithm, the graph is reordered\nstarting from all nodes in P, then all nodes in Q and finally all nodes in S as shown in the\nsame\nfigure.\nFor\nthis\nexample,\nthe\npermutation\nvector\nbecomes:\nP = [1\n8] . The elimination sequence is performed following\nthis new order.\nFigure 3.23 shows the structure of the reordered matrix and its corresponding lower\ntriangular factor. The graph separator permutation is given by\nKP\nKPS\nP K )\n\n(\n2D PT = 0\nKQ\nKQS\n\n.\n\nK\n\nSP\nKSQ\nKS\n\nFigure 3.22: Reordering occurred in the graph separators algorithm\nFigure 3.23: Structure of the reordered and the lower triangular factor of the 9 × 9K2D\nmatrices\nAs we mentioned before, the introduction of additional separators will produce a nested\ndissection algorithm. For the example described above, we can introduce a maximum of\ntwo additional separators as shown in Figure 3.24. With this ordering, the permutation\nvector becomes: P = [1\n8] . Figure 3.25 shows the new structure\nof the reordered matrix as well as the structure for L. The total number of nonzeros got\nreduced from 28 to 26 (remember that with out any reordering L has 29 nonzeros).\n\nFigure 3.24: Nested dissection algorithm\nFigure 3.25: Structure of the reordered matrix and its lower triangular factor\nWe now turn to experiment with more sophisticated nested dissection algorithms\navailable in Matlab Mesh Partitioning and Graph Separator Toolbox [4]. In\nparticular, we will compare two algorithms:\n1. \"specnd\": Spectral nested dissection ordering. This algorithm makes use of\ncertain properties of the Laplacian matrix to compute the proper separators. For\nmore information regarding spectral nested dissection algorithms refer to [7].\n2. \"gsnd\": Geometric spectral nested dissection ordering.\n\nFigure 3.26 compares the number of nonzero entries in L after LU decomposing the\nmatrices reordered by both algorithms and the original K2D. From this figure we can see\nthat the results produced by the spectral-based nested dissection algorithms get closer to\nthose obtained by the minimum degree algorithms; however, some of the minimum\ndegree algorithms such as the \"symamd\" algorithm perform much better.\nFigure 3.26: Number of nonzeros generated in L after LU decomposing the original and\nthe reordered K2D matrices (by \"gsnd\" and \"specnd\")\nA comparison of the computational time required by both algorithms is plotted in Figure\n3.27. Again, we confirm that Matlab's minimum degree algorithms perform much faster\non large matrices than the nested dissection algorithms.\nFigure 3.27: Computational time required by \"gsnd\" and \"specnd\" as a function of matrix\nsize\n\nFinally, we compare the computational time required by the LU factorization algorithm\nwhen the input matrix was reordered by both algorithms. The results are plotted in Figure\n3.28. The Matlab codes used to generate Figures 3.26-3.28 is included in Appendix B.6.\nFigure 3.28: Computational time required by the LU decomposition of the original and\nthe reordered matrices\nSection 4: References\n[1]: Internet access: http://www.pages.drexel.edu/~weg22/edge.html. Date accessed:\n04/02/2006.\n[2]: J. S. Lim, Two-Dimensional Signal and Image Processing, Prentice Hall, 1990.\n[3]: Internet access: http://www.cerfacs.fr/algor/Softs/MESHPART/, Matlab mesh\npartitioning and graph separator toolbox. Date accessed: 04/04/06.\n[4]: G. Strang, Introduction to applied mathematics, Wellesley, Cambridge Press.\n[5]: A. Pothen, H.D. Simon, L. Wang, Spectral Nested Dissection, 1992.\nSection 5: Appendices\nA.1: This Appendix contains the Matlab code used to generate Figures 2.1-2.3.\n%Computation of the first and second derivatives of an intensity function\n%I(x) to be used to detect discontinuities using and edge detection\n%algorithm\n%General Parameters\nm = 0:0.01:1;\nK = 5;\nx = -100:100;\n%1D discontinuity\nF1 = exp(-K*m);\nF2 = -F1+2;\nF = fliplr(F1);\n\nF(length(F2)+1:length(F2)*2-1)=F2(2:end);\n%Plot 1\nfigure;\nplot(x,F,zeros(length(x),1)',[0:0.01:2],'--r',x,zeros(length(x),1)','--r')\ntitle('Intensity variation at the discontinuity')\nxlabel('x')\nylabel('Intensity')\n%Gradient\ndelF = gradient(F);\n%Plot 2\nfigure;\nplot(x,delF,zeros(length(x),1)',[0: 2.4279e-004:0.0488],'--r',x,zeros(length(x),1)','--r')\ntitle('First derivative')\nxlabel('x')\nylabel('Intensity')\n%Laplacian\ndel2F = gradient(gradient(F));\n%Plot 2\nfigure;\nplot(x,del2F,zeros(length(x),1)',[-0.0023:2.3e-005:0.0023],'--r',x,zeros(length(x),1)','--\nr')\ntitle('Second derivative')\nxlabel('x')\nylabel('Intensity')\nA.2: Matlab code used to generate Figure 2.4.\n%Forward problem: given an image, find the Laplacian for edge detection\n%General parameters\nU = imread('g1','bmp');\nU = double(U);\nN = size(U,1);\nh = 0.5;\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\n%Raster Scanning the image\ncount = 1;\nfor m = 1:N\nUr(count:count+N-1,1) = U(:,m);\ncount = count+N;\nend\n%Forward problem\nF = K2D*Ur;\n%Inverse-raster scanning for F:\ncount = 1;\nfor m = 1:N\nFnew(:,m) = F(count:count+N-1,1);\ncount = count+N;\nend\nFnew = Fnew(2:499,2:499);\nFnew = -Fnew*h^2;\nfigure; imagesc(Fnew);\ncolorbar\ncolormap gray\naxis equal\n%Note: alternative solution is obtained using Matlab's del2 function:\n% Fnew = del2(U);\nA.3: Inverse Problem code\n%Inverse Problem: Given the Laplacian of an image, find original image\n%General parameters\n\nF = imread('g1','bmp');\nF = double(F);\nN = size(F,1);\nh = 0.5;\n%Computing the 2D Laplacian\nF = del2(F);\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\n%Raster Scanning Measurement:\ncount = 1;\nfor m = 1:N\nFr(count:count+N-1,1) = F(:,m);\ncount = count+N;\nend\n%Elimination\nU = K2D\\Fr;\n%Undo raster scanning for U:\ncount = 1;\nfor m = 1:N\nUnew(:,m) = U(count:count+N-1,1);\ncount = count+N;\nend\nUnew = -Unew/h^2;\nfigure; imagesc(Unew);\ncolorbar\ncolormap gray\naxis equal\nA.4: Plots the eigenvalues of a 9 9 K2D matrix\n×\n%Code to plot the eigenvalues of a 9X9 K2D matrix inside Gershgorin circle\n%General Parameters\nb=-1:0.0125:9;\nc = -4:0.01:4;\nN=3;\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\n%Eigenvalues of K2D\nE = eig(K2D);\n%Gershgorin Circle\nx = 0:0.01:8;\ny = sqrt(4^2-(x-4).^2);\n%Plot\nfigure;\nhold on\nplot(x,y,'--r',x,-y,'--r',b,zeros(801,1),'--r',zeros(801,1),c,'--r')\nplot(E,zeros(length(E),1),'X')\nhold off\naxis equal\nxlabel('Real')\nylabel('Imaginary')\ntitle('Eigenvalues of K2D')\nA.5: Plots the condition number as a function of size for K2D\n%Code to plot condition number as a function of size for matrix K2D\nT=2:70;\nfor m = 1:length(T)\nN = T(m);\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\n\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear K; clear I; clear N;\n%Compute Eigenvalues and condition number\nE = eig(K2D);\ncondi(m) = max(E)/min(E);\nclear E;\nend\n%Plot\nplot(T.^2,condi)\nxlabel('size: N^2')\nylabel('Condition Number')\ntitle('Condition Number vs size of K2D')\nB.1: Generates plots that compare different minimum degree algorithms\n%Comparison of different minimum degree algorithms\nN = 9;\n%1D difference matrix\nK = sparse(diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1));\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear I K;\nfor m = 1:5\nif m ==1\np=symamd(K2D);\ns = 'symamd';\nend\nif m ==2\np=symmmd(K2D);\ns = 'symmmd';\nend\nif m ==3\np=colamd(K2D);\ns = 'colamd';\nend\nif m ==4\np=colmmd(K2D);\ns = 'colmmd';\nend\nif m ==5\np=realmmd(K2D);\ns = 'realmmd'\nend\nK2Dmod=K2D(p,p);\nfigure;\nspy(K2Dmod)\ntitle(['Matrix reordered using: ',s]);\n[L,U]=lu(K2Dmod);\nfigure;\nspy(L)\ntitle(['Lower triangular matrix from K2D reordered with: ',s]);\nend\nB.2: Comparison of different minimum degree algorithms as a function of matrix size\n%Comparison of different minimum degree algorithms\nT = 2:5:62;\nfor k = 1:length(T)\nN = T(k);\n%1D difference matrix\nK = sparse(diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1));\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear I K;\nfor m = 1:5\nif m ==1\np=symamd(K2D);\nend\nif m ==2\np=symmmd(K2D);\nend\nif m ==3\np=colamd(K2D);\n\nend\nif m ==4\np=colmmd(K2D);\nend\nif m ==5\np=realmmd(K2D);\nend\nK2Dmod=K2D(p,p);\n[L,U]=lu(K2Dmod);\nNoN(m,k) = nnz(L);\nclear L K2Dmod;\nend\nend\n%Plot\nfigure;\nplot(T.^2,NoN(1,:),T.^2,NoN(2,:),T.^2,NoN(3,:),T.^2,NoN(4,:),T.^2,NoN(5,:))\nlegend('symamd','symmmd','colamd','colmmd','realmmd')\ntitle('Comparison of minimum degree algorithms vs matrix size')\nylabel('Number of nonzeros in L');\nxlabel('Size: N^2')\nfigure;\nplot(T(4:6).^2,NoN(1,4:6),T(4:6).^2,NoN(2,4:6),T(4:6).^2,NoN(5,4:6))\nlegend('symamd','symmmd','realmmd')\ntitle('Comparison of minimum degree algorithms vs matrix size')\nylabel('Number of nonzeros in L');\nxlabel('Size: N^2')\nB.3: Comparison: nonzeros in Cholesky factor, computational time for LU and Cholesky\ndecompositions, computational time in reordering\n%Comparison of different minimum degree algorithms: Computational time,\n%Cholesky and LU decomposition and number of nonzero elements\nT = 2:5:347;\nfor k = 1:length(T)\nN = T(k);\n%1D difference matrix\nK = sparse(diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1));\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear I K;\nfor m = 1:4\nif m ==1\ntic\np=symamd(K2D);\ntime = toc;\nK2Dmod=K2D(p,p);\nend\nif m ==2\ntic\np=symmmd(K2D);\ntime = toc;\nK2Dmod=K2D(p,p);\nend\nif m ==3\ntic\np=colamd(K2D);\ntime =toc;\nK2Dmod=K2D(p,p);\nend\nif m ==4\ntic\np=colmmd(K2D);\ntime=toc;\nK2Dmod=K2D(p,p);\nend\nif m ==5\ntic\np=realmmd(K2D);\ntime=toc;\nK2Dmod=K2D(p,p);\nend\ntic\n[L]=chol(K2Dmod);\ntime2 = toc;\ntic\n[L2,U2]=lu(K2Dmod);\ntime4 = toc;\nNoN1(m,k) = nnz(L);\n\nNoN2(m,k) = nnz(L2);\nTiMe(m,k)=time;\nTiMeChol(m,k)=time2;\nTiMeLU(m,k)=time4;\nclear L K2Dmod time conn time2 time3 time4 time5;\nend\nend\n%Plot\nfigure;\nplot(T.^2,NoN1(1,:),T.^2,NoN1(2,:),T.^2,NoN1(3,:),T.^2,NoN1(4,:))\nlegend('symamd','symmmd','colamd','colmmd')\ntitle('Comparison of minimum degree algorithms vs matrix size')\nylabel('Number of nonzeros in U');\nxlabel('Size: N^2')\nfigure;\nplot(T.^2,TiMeChol(1,:),T.^2,TiMeChol(2,:),T.^2,TiMeChol(3,:),T.^2,TiMeChol(4,:))\nlegend('Cholesky <= symamd','Cholesky <= symmmd','Cholesky <= colamd','Cholesky <=\ncolmmd')\ntitle('Computational time for Cholesky decomposing reordered matrices vs size')\nylabel('Time (sec)');\nxlabel('Size: N^2')\nfigure;\nplot(T.^2,TiMeLU(1,:),T.^2,TiMeLU(2,:),T.^2,TiMeLU(3,:),T.^2,TiMeLU(4,:))\nlegend('LU <= symamd','LU <= symmmd','LU <= colamd','LU <= colmmd')\ntitle('Computational time for LU decomposing reordered matrices vs size')\nylabel('Time (sec)');\nxlabel('Size: N^2')\nfigure;\nplot(T.^2,TiMe(1,:),T.^2,TiMe(2,:),T.^2,TiMe(3,:),T.^2,TiMe(4,:))\nlegend('symamd','symmmd','colamd','colmmd')\ntitle('Reordering computational time')\nylabel('Time (sec)');\nxlabel('Size: N^2')\nB.4: Movie for red-black ordering algorithm (based on code obtain at [3])\n%Movie for red-black ordering algorithm\nN=3;\nA=sparse(toeplitz([2,-1,zeros(1,N-2)]));\nI=speye(N,N);\nB=kron(A,I)+kron(I,A);\n[x,y]=ndgrid(1:N,1:N);\np=redblack(B);\nB=B(p,p);\nR=chol(B(p,p));\nxy=[x(p);y(p)]';\nn=size(B,1);\nL=zeros(n);\nlpars={'marker','.','linestyle','none','markersize',64};\ntpars={'fontname','helvetica','fontsize',16,'horiz','center','col','g'};\nfor i=1:n\nclf\ngplot(B,xy);\nline(xy(i:N^2,1),xy(i:N^2,2),'color','k',lpars{:});\nfor j=i:n\ndegree=length(find(B(:,j)))-1;\ntext(xy(j,1),xy(j,2),int2str(degree),tpars{:});\nend\naxis equal,axis off,axis([1,N,1,N])\npause(0.3);\nline(xy(i,1),xy(i,2),'color','r',lpars{:});\npause(0.3);\nL(i,i)=sqrt(B(i,i));\nL(i+1:n,i)=B(i+1:n,i)/L(i,i);\nB(i+1:n,i+1:n)=B(i+1:n,i+1:n)-L(i+1:n,i)*L(i+1:n,i)';\nB(i, i:n)=0;\nB(i:n, i)=0;\nend\nspy(L,32)\nB.5: Red-black ordering algorithm\nfunction p=redblack(A)\n%REDBLACK: Computes the permutation vector implementing the red-black\n%ordering algorithm\n\nn=size(A,1);\ntemp = 1:n;\ncount = 0;\nodd = 2;\nflag =1;\nfor m = 1:n\nif flag\np(m)=temp(m+count);\ncount = count+1;\nif p(m)==n|p(m)+2>n\nflag = 0;\nend\nelse\np(m)=temp(odd);\nodd = odd+2;\nend\nend\nB.5: Red-black algorithm: comparison code\n%Comparison code: red-black algorithm\nT=3:2:233;\nfor m = 1:length(T)\nN = T(m);\nA=sparse(toeplitz([2,-1,zeros(1,N-2)]));\nI=speye(N,N);\nB=kron(A,I)+kron(I,A);\nclear N I A;\ntic\np=redblack(B);\ntime = toc;\nC=B(p,p);\ntic\n[L,U]= lu(B);\ntime2 = toc;\ntic\n[L2,U2]= lu(C);\ntime3 = toc;\nNoN1(m)=nnz(L);\nNoN2(m)=nnz(L2);\nTiMe(m)=time;\nTiMeLUor(m)=time2;\nTiMeLUrb(m)=time3;\nclear L L2 U U2 time time2 time3 p B C m;\nend\nfigure;\nplot(T.^2,NoN1,T.^2,NoN2)\ntitle('Number of nonzeros in L as a function of matrix size')\nlegend('L<=Original K2D','L<=Reordered K2D');\nxlabel('Size: N^2')\nylabel('Number of nonzeros in L')\nfigure;\nplot(T.^2,TiMe)\ntitle('Computational time for red-black ordering algorithm')\nxlabel('Size: N^2')\nylabel('Time (sec)')\nfigure;\nplot(T.^2,TiMeLUor,T.^2,TiMeLUrb)\ntitle('Computational time of the LU decomposition algorithm')\nlegend('L<=Original K2D','L<=Reordered K2D');\nxlabel('Size: N^2')\nylabel('Time (sec)')\nB.6: Nested dissection ordering algorithm: comparison code\n%Comparison code: red-black algorithm\nT=3:4:100;\nfor m = 1:length(T)\nN = T(m);\nA=sparse(toeplitz([2,-1,zeros(1,N-2)]));\nI=speye(N,N);\nB=kron(A,I)+kron(I,A);\nclear N I A;\ntic\np = gsnd(B);\ntime = toc;\ntic\nd = specnd(B);\n\ntime4 = toc;\nC=B(p,p);\nD = B(d,d);\ntic\n[L,U]= lu(B);\ntime2 = toc;\ntic\n[L2,U2]= lu(C);\ntime3 = toc;\ntic\n[L3,U3]= lu(D);\ntime6 = toc;\nNoN1(m)=nnz(L);\nNoN2(m)=nnz(L2);\nNoN3(m)=nnz(L3);\nTiMe(m)=time;\nTiMe2(m)=time4;\nTiMeLUor(m)=time2;\nTiMeLUnd1(m)=time3;\nTiMeLUnd2(m)=time6;\nclear L L2 U U2 time time2 time3 p B C m time4 time6 D d;\nend\nfigure;\nplot(T.^2,NoN1,T.^2,NoN2,T.^2,NoN3)\ntitle('Number of nonzeros in L as a function of matrix size')\nlegend('L<=Original K2D','L<=Reordered by \"gsnd\"','L<=Reordered by \"specnd\"');\nxlabel('Size: N^2')\nylabel('Number of nonzeros in L')\nfigure;\nplot(T.^2,TiMe,T.^2,TiMe2)\ntitle('Computational times for nested dissection algorithms: \"gsnd\" and \"specnd\"')\nlegend('\"gsnd\"','\"specnd\"')\nxlabel('Size: N^2')\nylabel('Time (sec)')\nfigure;\nplot(T.^2,TiMeLUor,T.^2,TiMeLUnd1,T.^2,TiMeLUnd2)\ntitle('Computational time of the LU decomposition algorithm')\nlegend('L<=Original K2D','L<=Reordered by \"gsnd\"','L<=Reordered by \"specnd\"');\nxlabel('Size: N^2')\nylabel('Time (sec)')"
        },
        {
          "category": "Resource",
          "title": "am35.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/5f469576670a2c7a874999dbafc8c3cb_am35.pdf",
          "content": "c2006 Gilbert Strang\n3.5\nFinite Differences and Fast Poisson Solvers\nK\nIt is extremely unusual to use eigenvectors to solve a linear system KU = F. You\nneed to know all the eigenvectors of K, and (much more than that) the eigenvector\nmatrix S must be especially fast to work with. Both S and S-1 are required, because\n-1 = S-1S-1 . The eigenvalue matrices and -1 are diagonal and quick. When\nthe derivatives in Poisson's equation -uxx\nyy = f(x, y) are replaced by second\n- u\ndifferences, we do know the eigenvectors (discrete sines in the columns of S). Then\nthe Fourier transform quickly inverts and multiplies by S.\nOn a square mesh those differences have -1, 2, -1 in the x-direction and -1, 2, -1\nin the y-direction (divided by h2, where h = meshwidth). Figure 3.20 shows how\nthe second differences combine into a \"5-point molecule\" for the discrete Laplacian.\nBoundary values u = u0(x, y) are assumed to be given along the sides of a unit square.\nThe square mesh has N interior points in each direction (N = 5 in the figure).\nIn this case there are n = N 2 = 25 unknown mesh values Uij . When the molecule is\ncentered at position (i, j), the discrete Poisson equation gives a row of K2D U = F:\nK2D U = F\n4uij - ui, j-1 - ui-1, j - ui+1, j - ui, j+1 = h2f(ih, jh) = Fij .(1)\nThe inside rows of K2D have five nonzero entries 4, -1, -1, -1, -1. When\n(i, j) is next to a boundary point of the square, the known value of u0 at that neigh\nboring boundary point moves to the right side of equation (1). It becomes part of\nthe vector F, and a -1 drops out of the corresponding row of K. So K2D has five\nnonzeros on inside rows and fewer nonzeros on next-to-boundary rows.\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nN\nN\nFigure 3.20: 5-point molecules at inside points (fewer -1's next to boundary).\nThis matrix K2D is sparse. Using blocks of size N, we can create the 2D matrix\nfrom the familiar N by N second difference matrix K. Number the nodes of the\nsquare a row at a time (this \"natural numbering\" is not necessarily best). Then the\n-1's for the neighbor above and the neighbor below are N positions away from the\n\n3.5. FINITE DIFFERENCES AND FAST POISSON SOLVERS c2006 Gilbert Strang\nmain diagonal of K2D. The 2D matrix is block tridiagonal with tridiagonal blocks:\n⎡\n⎡\n2 -1\nK + 2I\n-I\n⎢\n2 -1\n⎢\nK + 2I -I\nK = 6 -1\n⎢\nK2D = 6\n-I\n⎢\n(2)\n⎣\n⎣\n·\n·\n·\n·\n·\n·\n-I\nK + 2I\n-1\nN 2\nSize N\nSize n =\nBandwidth w = N\nTime N\nSpace nw = N 3\nTime nw2 = N 4\nThe matrix K2D has 4's down the main diagonal in equation (1). Its bandwidth\nw = N is the distance from the main diagonal to the nonzeros in -I. Many of the\nspaces in between are filled during elimination ! This is discussed in Section 6.1.\nKronecker product\nOne good way to create K2D from K and I is by the kron\ncommand. When A and B have size N by N, the matrix kron(A, B) is N 2 by N 2 .\nEach number aij is replaced by the block aij B.\nTo take second differences in all rows at the same time, kron(I, K) produces a\nblock diagonal matrix of K's. In the y-direction, kron(K, I) multiplies -1 and 2 and\n-1 by I (dealing with a column of meshpoints at a time). Add x and y differences:\n⎡\n⎡\nK\n2I\n-I ·\nK2D = kron(I, K) + kron(K, I) = 4\nK\n⎣ + 4 -I\n2I\n· ⎣\n(3)\n·\n·\n·\n·\nThis sum agrees with the 5-point matrix in (2). The computational question is how\nto work with K2D. We will propose three methods:\n1.\nElimination in a good order (not using the special structure of K2D)\n2.\nFast Poisson Solver (applying the FFT = Fast Fourier Transform)\n3.\nOdd-Even Reduction (since K2D is block tridiagonal).\nThe novelty is in the Fast Poisson Solver, which uses the known eigenvalues and\neigenvectors of K and K2D. It is strange to solve linear equations KU = F by\nexpanding F and U in eigenvectors, but here it is extremely successful.\nElimination and Fill-in\nFor most two-dimensional problems, elimination is the way to go. The matrix from\na partial differential equation is sparse (like K2D). It is banded but the bandwidth\nis not so small. (Meshpoints cannot be numbered so that all five neighbors in the\nmolecule receive nearby numbers.) Figure 3.21 has points 1, . . . , N along the first\nrow, then a row at a time going up the square. The neighbors above and below point\nj have numbers j - N and j + N.\n\nc\n2006 Gilbert Strang\nOrdering by rows produces the -1's in K2D that are N places away from the\ndiagonal. The matrix K2D has bandwidth N. The key point is that elimination\nfills in the zeros inside the band. We add row 1 (times 4 ) to row 2, to eliminate\nthe -1 in position (2, 1). But the last -1 in row 1 produces a new -1 in row 2. A\nzero inside the band has disappeared. As elimination continues from A to U, virtually\nthe whole band is filled in.\nIn the end, U has about 5 times 25 nonzeros (this is N 3, the space needed to store\nU). There will be about N nonzeros next to the pivot when we reach a typical row,\nand N nonzeros below the pivot. Row operations to remove those nonzeros will require\nup to N 2 multiplications, and there are N 2 pivots. So the count of multiplications is\nabout 25 times 25 (this is N 4, for elimination in 2D).\nFigure 3.21: Typical rows of K2D have 5 nonzeros. Elimination fills in the band.\nSection 6.1 will propose a different numbering of the meshpoints, to reduce the\nfill-in that we see in U. This reorders the rows of K2D by a permutation matrix\nP, and the columns by P T . The new matrix P(K2D)P T is still symmetric, but\nelimination (with fill-in) proceeds in a completely different order. The MATLAB\ncommand symamd(K) produces a nearly optimal choice of P.\nElimination is fast in two dimensions (but a Fast Poisson Solver is faster !). In\nthree dimensions the matrix size is N 3 and the bandwidth is N 2 . By numbering the\nnodes a plane at a time, vertical neighbors are N 2 nodes apart. The operation count\nfor elimination becomes N 7, which can be seriously large. Chapter 6 on Solving Large\nSystems will introduce badly needed alternatives to elimination in 3D.\nSolvers Using Eigenvalues\nOur matrices K and K2D are extremely special. We know the eigenvalues and eigen\nvectors of the second-difference matrix K. The eigenvalues have the special form\n= 2 -2 cos ∂, and the eigenvectors are discrete sines. There will be a similar pattern\nfor K2D, which is formed in a neat way from K (by Kronecker product). The Poisson\nSolver uses those eigenvalues and eigenvectors to solve (K2D)(U2D) = (F2D). On a\nsquare mesh it is much faster than elimination.\nHere is the idea, first in one dimension. The matrix K has eigenvalues 1, . . . , N\nand eigenvectors y1, . . . , yN . There are three steps to the solution of KU = F:\n1.\nExpand F\nF = a1y1 +\n+ aN yN\n2.\nak\nk\n3.\nU\na1/1) y1 +\naN /N ) yN .\nas a combination\n· · ·\nof the eigenvectors\nDivide each\nby\nRecombine eigenvectors into\n= (\n· · · + (\n\n3.5. FINITE DIFFERENCES AND FAST POISSON SOLVERS c2006 Gilbert Strang\nThe success of the method depends on the speed of steps 1 and 3. Step 2 is fast.\nTo see that U in step 3 is correct, multiply it by the matrix K. Every eigenvector\ngives Ky = y. That cancels the in each denominator. The result is that KU\nagrees with the vector F in step 1.\nNow look at the calculation required in each step, using matrices. Suppose S is\nthe eigenvector matrix, with the eigenvectors y1, . . . , yN of K in its columns. Then\nthe coefficients a1, . . . , aN come by solving Sa = F:\n⎡\n⎡\na1\n6 . ⎢\nStep 1\nSolve Sa = F\n4 y1 · · · yN ⎣4 .. ⎣ = a1y1 +\n+ aN yN = F . (4)\n· · ·\naN\nThus a = S-1F. Then step 2 divides the a's by the 's to find -1a = -1S-1F.\n(The eigenvalue matrix is just the diagonal matrix of 's.) Step 3 uses those\ncoefficients ak/k in recombining the eigenvectors into the solution vector U:\n⎡\n⎡\na1/1\n.\n⎢\nStep 3\nU = 4 y1\nyN ⎣4\n.\n⎣ = S-1 a = S-1S-1F .\n(5)\n.\n· · ·\naN /N\nYou see the matrix K-1 = S-1S-1 appearing in that last formula. We have K-1F\nbecause K itself is SS-1--the usual diagonalization of a matrix. The eigenvalue\nmethod is using this SS-1 factorization instead of K = LU from elimination.\nThe speed of steps 1 and 3 depends on multiplying quickly by S-1 and S. Those\nare full matrices, not sparse like K. Normally they both need N 2 operations in one\ndimension (where the matrix size is N). But the \"sine eigenvectors\" in S give the\nDiscrete Sine Transform, and the Fast Fourier Transform executes S and S-1 in\nN log2 N steps. In one dimension this is not as fast as cN from elimination, but in\ntwo dimensions N 2 log(N 2) easily wins.\nColumn k of the matrix S contains the eigenvector yk. The number Sjk = sin jk\nN+1\nis the jth component of that eigenvector. For our example with N = 5 and N +1 = 6,\nyou could list the sines of every multiple of /6. Here are those numbers:\np\np\n3 1\nSines ,\n, 1,\n,\n, 0, (repeat with minus signs) (repeat 12 numbers forever) .\nThe kth column of S (kth eigenvector yk) takes every kth number from that list:\n⎡\n⎡\n⎡\n⎡\n⎡\n1/2\np\n3/2\np\n3/2\n1/2\n6 p\n3/2⎢\n6 p\n3/2⎢\n⎢\n6 p\n3/2⎢\n6 p\n3/2⎢\n6 0\n⎢\n⎢\n⎢\n6 -\n⎢\n6 -\n⎢\n0 ⎢\n⎢\n0 ⎢\ny5 = 6\n1 ⎢\ny1 = 6\n1 ⎢\ny2 = 6\n-\np\n3/2\n⎢\ny3 = 6-1 ⎢\ny4 =\n⎢\n⎢\n⎢\n4 p\n3/2⎣\n⎣\n4 0⎣\n4 p\n3/2⎣\n⎣\n1/2\n-\np\n3/2\n-\np\n3/2\n-\np\n3/2\n1/2\nThose eigenvectors are orthogonal ! This is guaranteed by the symmetry of K. All\nthese eigenvectors have length 3 = (N + 1)/2. Dividing each column by\np\n3, we have\northonormal eigenvectors. S/\np\n3 is an orthogonal matrix Q, with QTQ = I.\n\nc2006 Gilbert Strang\nIn this special case, S and Q are also symmetric. So Q-1 = QT = Q.\nNotice that yk has k - 1 changes of sign. It comes from k loops of the sine curve.\nThe eigenvalues are increasing: = 2 -\np\n3, 2 - 1, 2 - 0, 2 + 1, 2 +\np\n3. Those\neigenvalues add to 10, which is the sum down the diagonal (the trace) of K5. The\nproduct of the 5 eigenvalues (easiest by pairs) confirms that det(K5) = 6.\nFast Poisson Solvers\nTo extend this eigenvalue method to two dimensions, we need the eigenvalues and\neigenvectors of K2D. The key point is that the N 2 eigenvectors of K2D are separable.\nEach eigenvector ykl separates into a product of sines:\ny\ny\nis sin N\nsin N+1 .\n(6)\nEigenvectors\nkl\nThe (i, j) component of\nkl\nik\n+1\njl\nWhen you multiply that eigenvector by K2D, you have second differences in the x-\ndirection and y-direction. The second differences of the first sine (x-direction) produce\na factor k = 2 - 2 cos k . This is the eigenvalue of K in 1D. The second differences\nN+1\nof the other sine (y-direction) produce a factor l = 2 - 2 cos l . So the eigenvalue\nN+1\nin two dimensions is the sum k + l of one-dimensional eigenvalues:\n(K\ny\n= y\n\n-\nN+1\n-\nN+1\n(7)\n2D)\nkl\nkl\nkl\nkl = (2\n2 cos k ) + (2\n2 cos l ) .\nNow the solution of K2D U = F comes by a two-dimensional sine transform:\n⎤⎤ akl\nik\njl\nFi,j = ⎤⎤ akl sin ik sin jl\nUi,j =\nsin N +1 sin\n(8)\nN +1\nN+1\nkl\nN+1\nAgain we find the a's, divide by the 's, and build U from the eigenvectors in S:\na = S-1F\n-1a\n-1S-1F\nU = S-1S-1F\nStep 1\nStep 2\n=\nStep 3\nSwartztrauber [SIAM Review 19 (1977) 490] gives the operation count 2N 2 log2 N.\nThis uses the Fast Sine Transform (based on the FFT) to multiply by S-1 and S.\nThe Fast Fourier Transform is explained in Section 4.3.\nS\nNote\nWe take this chance to notice the good properties of a Kronecker product\nC = kron(A, B). Suppose A and B have their eigenvectors in the columns of SA and\nB . The eigenvalues are in A and B . Then we know SC and C :\nThe eigenvectors of kron(A, B) are in kron(SA, SB ).\n(9)\nThe eigenvalues are in kron(A, B ).\nThe diagonal blocks in kron(A, B) are entries k(A) times the diagonal matrix B.\nSo the eigenvalues kl of C are just the products k(A)l(B).\n\n3.5. FINITE DIFFERENCES AND FAST POISSON SOLVERS c2006 Gilbert Strang\nIn our case A and B were I and K. The matrix K2D added the two products\nkron(I, K) and kron(K, I). Normally we cannot know the eigenvectors and eigen\nvalues of a matrix sum--except when the matrices commute. Since all our matrices\nare formed from K, these Kronecker products do commute. This gives the separable\neigenvectors and eigenvalues in (6) and (7).\nCyclic Odd-Even Reduction\nThere is an entirely different (and very simple) approach to KU = F. I will start in\none dimension, by writing down three rows of the usual second difference equation:\nRow i - 1\n-Ui-2 + 2Ui-1 - Ui\n= Fi-1\nRow i\n-Ui-1 + 2Ui - Ui+1\n= Fi\n(10)\nRow i + 1\n-Ui + 2Ui+1 - Ui+2 = Fi+1\nMultiply the middle equation by 2, and add. This eliminates Ui-1 and Ui+1:\n-Ui-2\nUi - Ui\n= Fi-1\nFi + Fi\n.\n(11)\nOdd-even reduction in 1D\n+ 2\n+2\n+ 2\n+1\nNow we have a half-size system, involving only half of the U's (with even indices).\nThe new system (11) has the same tridiagonal form as before. When we repeat, cyclic\nreduction produces a quarter-size system. Eventually we can reduce KU = F to a\nvery small problem, and then cyclic back-substitution produces the whole solution.\nHow does this look in two dimensions ? The big matrix K2D is block triangular:\n⎡\nA\n-I\n⎢\nA\n-I\nK2D = 6 -I\n⎢\nwith A = K + 2I\nfrom equation (4) .\n(12)\n⎣\n·\n·\n·\nA\n-I\nThe three equations in (10) become block equations for whole rows of N mesh values.\nWe are taking the unknowns Ui = (Ui1, . . . , UiN ) a row at a time. If we write three\nrows of (10), the block A replaces the number 2 in the scalar equation. The block\n-I replaces the number -1. To reduce (K2D)(U2D) = (F2D) to a half-size system,\nmultiply the middle equation (with i even) by A and add the three block equations:\nReduction in 2D\n-IUi-2 + (A2 - 2I)Ui - IUi+2 = Fi-1 + AFi + Fi+1 . (13)\nThe new half-size matrix is still block tridiagonal. The diagonal blocks that were\npreviously A in (12) are now A2 - 2I, with the same eigenvectors. The unknowns are\nthe 1 N 2 values Ui,j at meshpoints with even indices i.\nThe bad point is that each new diagonal block A2 - 2I has five diagonals, where\nthe original block A = K+2I had three diagonals. This bad point gets worse as cyclic\nreduction continues. At step r, the diagonal blocks become Ar = A2\nr-1 - 2I and their\nbandwidth doubles. We could find tridiagonal factors (A-\np\n2I)(A+\np\n2I) = A2 - 2I,\n\nc2006 Gilbert Strang\nbut the number of factors grows quickly. Storage and computation and roundoff error\nare increasing too rapidly with more reduction steps.\nStable variants of cyclic reduction were developed by Buneman and Hockney. The\nclear explanation by Buzbee, Golub, and Nielson [SIAM Journal of Numerical Anal\nysis 7 (1970) 627] allows other boundary conditions and other separable equations,\ncoming from polar coordinates or convection terms like C@u/@x + D@u/@y. After m\nsteps of cyclic reduction, Hockney went back to a Fast Poisson Solver.\nThis combination FACR(m) is widely used, and the optimal number of cyclic\nreduction steps (before the FFT takes over) is small. For N = 128 a frequent choice\nis m = 2. Asymptotically mopt grows like log log N and the operation count for\nFACR(mopt) is 3N 2 log log N. In practical scientific computing with N 2 unknowns\n(or with N 3 unknowns in three dimensions), a Fast Poisson Solver is a winner.\n****** Add code for Fast Poisson *******\nProblem Set 3.5\nProblems 1-\nare for readers who get enthusiastic about kron.\nWhy is the transpose of C = kron(A, B) equal to kron(AT, BT) ? Why is the\ninverse equal to C-1 = kron(A-1, B-1) ? You have to transpose each block aij B\nof the Kronecker product C, and then patiently multiply CC-1 by blocks.\nC is symmetric (or orthogonal) when A and B are symmetric (or orthogonal).\nWhy is the matrix C = kron(A, B) times the matrix D = kron(S, T ) equal to\nCD = kron(AS, BT ) ? This needs even more patience with block multiplica\ntion. The inverse CC-1 = kron(I, I) = I is a special case.\nNote\nSuppose S and T are eigenvector matrices for A and B. From AS =\nSA and BT = T B we have CD = kron(AS, BT ) = kron(SA, T B ). Then\nCD = D kron(A, B ) = DC . So D = kron(S, T ) is the eigenvector matrix\nfor C."
        },
        {
          "category": "Resource",
          "title": "am36.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/cb76484c6f1637c4ae205b66064c28c8_am36.pdf",
          "content": "CHAPTER 3. BOUNDARY VALUE PROBLEMS\n3.6\nSolving Large Linear Systems\nFinite elements and finite differences produce large linear systems KU = F. The\nmatrices K are extremely sparse. They have only a small number of nonzero entries in\na typical row. In \"physical space\" those nonzeros are clustered tightly together--they\ncome from neighboring nodes and meshpoints. But we cannot number N2 nodes in a\nplane in any way that keeps neighbors close together! So in 2-dimensional problems,\nand even more in 3-dimensional problems, we meet three questions right away:\n1. How best to number the nodes\n2. How to use the sparseness of K (when nonzeros can be widely separated)\n3. Whether to choose direct elimination or an iterative method.\nThat last point will split this section into two parts--elimination methods in 2D\n(where node order is important) and iterative methods in 3D (where preconditioning\nis crucial).\nTo fix ideas, we will create the n equations KU = F from Laplace's difference\nequation in an interval, a square, and a cube. With N unknowns in each direction,\nK has order n = N or N2 or N3 . There are 3 or 5 or 7 nonzeros in a typical row of\nthe matrix. Second differences in 1D, 2D, and 3D are shown in Figure 3.17.\n-1\n-1\nBlock\n-1\nTridiagonal K\nTridiagonal\nK\n-1\n-1\n-1\n-1\n-1\n-1\nN2 by N2\nN by N\n-1\nN3 by N3\n-1 -1\nFigure 3.17: 3, 5, 7 point difference molecules for -uxx, -uxx - uyy , -uxx - uyy - uzz .\nAlong a typical row of the matrix, the entries add to zero. In two dimensions this\nis 4 - 1 - 1 - 1 - 1 = 0. This \"zero sum\" remains true for finite elements (the element\nshapes decide the exact numerical entries). It reflects the fact that u = 1 solves\nLaplace's equation and Ui = 1 has differences equal to zero. The constant vector\nsolves KU = 0 except near the boundaries. When a neighbor is a boundary point\nwhere Ui is known, its value moves onto the right side of KU = F. Then that row of\nK is not zero sum. Otherwise K would be singular, if K ∗ ones(n, 1) = zeros(n, 1).\nUsing block matrix notation, we can create the 2D matrix K = K2D from the\nfamiliar N by N second difference matrix K. We number the nodes of the square a\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nrow at a time (this \"natural numbering\" is not necessarily best). Then the -1's for\nthe neighbor above and the neighbor below are N positions away from the main\ndiagonal of K2D. The 2D matrix is block tridiagonal with tridiagonal blocks:\n⎡\n⎤\n⎤\n⎡\n2 -1\nK + 2I\n-I\n⎢⎢⎣\n-1\n2 -1\n·\n·\n·\n⎥⎥⎦\nK2D =\n⎢⎢⎣\n-I\nK + 2I -I\n·\n·\n·\n⎥⎥⎦\nK =\n(1)\n-1\n\n-I\nK\n\n+ 2I\nSize N\nElimination in this order: K2D has size n = N2\nTime N\nBandwidth w = N, Space nw = N3 , Time nw2 = N4\nThe matrix K2D has 4's down the main diagonal. Its bandwidth w = N is the\ndistance from the diagonal to the nonzeros in -I. Many of the spaces in between are\nfilled during elimination! Then the storage space required for the factors in K = LU\nis of order nw = N3 . The time is proportional to nw2 = N4, when n rows each\ncontain w nonzeros, and w nonzeros below the pivot require elimination.\nThose counts are not impossibly large in many practical 2D problems (and we\nshow how they can be reduced). The horrifying large counts come for K3D in three\ndimensions. Suppose the 3D grid is numbered by square cross-sections in the natural\norder 1, . . . , N. Then K3D has blocks of order N2 from those squares. Each square\nis numbered as above to produce blocks coming from K2D and I = I2D:\n⎤\n⎡ K2D + 2I\n-I\nSize n = N3\nK3D =\n⎢⎢⎣\n-I\nK2D + 2I -I\n·\n·\n·\n⎥⎥⎦\nBandwidth w = N2\nElimination space nw = N5\n-I\nK2D + 2I\nElimination time ≈ nw2 = N7\nNow the main diagonal contains 6's, and \"inside rows\" have six -1's. Next to a point\nor edge or corner of the boundary cube, we lose one or two or three of those -1's.\nThe good way to create K2D from K and I (N by N) is to use the kron(A, B)\ncommand. This Kronecker product replaces each entry aij by the block aij B. To take\nsecond differences in all rows at the same time, and then all columns, use kron:\nK2D = kron(K, I) + kron(I, K) .\n(2)\nThe identity matrix in two dimensions is I2D = kron(I, I). This adjusts to allow\nrectangles, with I's of different sizes, and in three dimensions to allow boxes. For a\ncube we take second differences inside all planes and also in the z-direction:\nK3D = kron(K2D, I) + kron(I2D, K) .\nHaving set up these special matrices K2D and K3D, we have to say that there are\nspecial ways to work with them. The x, y, z directions are separable. The geometry\n(a box) is also separable. See Section 7.2 on Fast Poisson Solvers. Here the matrices\nK and K2D and K3D are serving as models of the type of matrices that we meet.\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nMinimum Degree Algorithm\nWe now describe (a little roughly) a useful reordering of the nodes and the equations\nin K2DU = F. The ordering achieves minimum degree at each step--the number\nof nonzeros below the pivot row is minimized. This is essentially the algorithm\nused in MATLAB's command U = K\\F , when K has been defined as a sparse matrix.\nWe list some of the functions from the sparfun directory:\nspeye (sparse identity I)\nnnz (number of nonzero entries)\nfind\n(find indices of nonzeros)\nspy\n(visualize sparsity pattern)\ncolamd and symamd\n(approximate minimum degree permutation of K)\nYou can test and use the minimum degree algorithms without a careful analysis. The\napproximations are faster than the exact minimum degree permutations colmmd and\nsymmmd. The speed (in two dimensions) and the roundoff errors are quite reasonable.\nIn the Laplace examples, the minimum degree ordering of nodes is irregular com\npared to \"a row at a time.\" The final bandwidth is probably not decreased. But the\nnonzero entries are postponed as long as possible! That is the key.\nThe difference is shown in the arrow matrix of Figure 3.18. On the left, minimum\ndegree (one nonzero off the diagonal) leads to large bandwidth. But there is no fill-in.\nElimination will only change its last row and column. The triangular factors L and\nU have all the same zeros as A. The space for storage stays at 3n, and elimination\nneeds only n divisions and multiplications and subtractions.\n⎤\n⎡\n⎤\n⎡ ∗\n∗\n∗\n∗\n⎢⎢⎢⎢⎢⎢⎢⎢⎣\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n⎥⎥⎥⎥⎥⎥⎥⎥⎦\nBandwidth\n6 and 3\nFill-in\n0 and 6\n⎢⎢⎢⎢⎢⎢⎢⎢⎣\n∗\n∗\n∗\n∗\n\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n∗\n∗\nF\nF\n\n∗\nF ∗ F\n⎥⎥⎥⎥⎥⎥⎥⎥⎦\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n∗\nF\nF\n∗\nFigure 3.18: Arrow matrix: Minimum degree (no F) against minimum bandwidth.\nThe second ordering reduces the bandwidth from 6 to 3. But when row 4 is\nreached as the pivot row, the entries indicated by F are filled in. That full lower\nquarter of A gives 1\n8n 2 nonzeros to both factors L and U . You see that the whole\n\"profile\" of the matrix decides the fill-in, not just the bandwidth.\nThe minimum degree algorithm chooses the (k + 1)st pivot column, after k\ncolumns have been eliminated as usual below the diagonal, by the following rule:\nIn the remaining matrix of size n -k, select the column with the fewest nonzeros.\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nThe component of U corresponding to that column is renumbered k + 1. So is the\nnode in the finite difference grid. Of course elimination in that column will normally\nproduce new nonzeros in the remaining columns! Some fill-in is unavoidable. So\nthe algorithm must keep track of the new positions of nonzeros, and also the actual\nentries. It is the positions that decide the ordering of unknowns. Then the entries\ndecide the numbers in L and U .\nExample\nFigure 3.19 shows a small example of the minimal degree ordering, for\nLaplace's 5-point scheme. The node connections produce nonzero entries (indicated by\n∗) in K. The problem has six unknowns. K has two 3 by 3 tridiagonal blocks from\nhorizontal links, and two 3 by 3 blocks with -I from vertical links.\nThe degree of a node is the number of connections to other nodes. This is the\nnumber of nonzeros in that column of K. The corner nodes 1, 3, 4, 6 all have degree\n2. Nodes 2 and 5 have degree 3. A larger region has inside nodes of degree 4, which\nwill not be eliminated first. The degrees change as elimination proceeds, because of\nfill-in.\nThe first elimination step chooses row 1 as pivot row, because node 1 has minimum\ndegree 2. (We had to break a tie! Any degree 2 node could come first, leading to different\nelimination orders.) The pivot is P, the other nonzeros in that row are boxed. When\nrow 1 operates on rows 2 and 4, it changes six entries below it. In particular, the two\nfill-in entries marked by F change to nonzeros. This fill-in of the (2, 4) and (4, 2) entries\ncorresponds to the dashed line connecting nodes 2 and 4 in the graph.\nF\nF\nF\n∗\n∗\nP\n∗\nF\n∗\nF\n∗\nP\nF\n∗\nF\n∗\n∗\n\n∗\n∗\n∗\n\n∗\n∗\nF\n∗\n∗\n∗\n∗\n\nP\nFill-in F\nfrom\nPivots\nZeros\nelimination\n∗\nF\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n∗\n∗\n\n∗\n∗\n\nFigure 3.19: Minimum degree nodes 1 and 3. The pivots P are in rows 1 and 3; new\nedges 2-4 and 2-6 in the graph match the matrix entries F filled in by elimination.\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nNodes that were connected to the eliminated node are now connected to each other.\nElimination continues on the 5 by 5 matrix (and the graph with 5 nodes). Node 2 still has\ndegree 3, so it is not eliminated next. If we break the tie by choosing node 3, elimination\nusing the new pivot P will fill in the (2, 6) and (6, 2) positions. Node 2 becomes linked\nto node 6 because they were both linked to the eliminated node 3.\nThe problem is reduced to 4 by 4, for the unknown U 's at the remaining nodes\n2, 4, 5, 6. Problem\nasks you to take the next step--choose a minimum degree node\nand reduce the system to 3 by 3.\nStoring the Nonzero Structure = Sparsity Pattern\nA large system KU = F needs a fast and economical storage of the node connections\n(which match the positions of nonzeros in K). The connections and nonzeros change\nas elimination proceeds. The list of edges and nonzero positions corresponds to the\n\"adjacency matrix \" of the graph of nodes. The adjacency matrix has 1 or 0 to indicate\nnonzero or zero in K.\nFor each node i, we have a list adj(i) of the nodes connected to i. How to combine\nthese into one master list NZ for the whole graph and the whole matrix K? A simple\nway is to store the lists adj(i) sequentially in NZ (the nonzeros for i = 1 up to i = n).\nAn index array IND of pointers tells the starting position of the sublist adj(i) within\nthe master list NZ. It is useful to give IND an (n + 1)st entry to point to the final\nentry in NZ (or to the blank that follows, in Figure 3.20). MATLAB will store one\nmore array (the same length nnz(K) as NZ) to give the actual nonzero entries.\nNZ\n| 2\n\n↑\n| 1\n\n↑\n5 | 2\n\n↑\n| 1\n\n↑\n|\n↑\n4 6 |\n↑\n5 |\n↑\nIND\n\nNode\nFigure 3.20: Master list NZ of nonzeros (neighbors in Figure 3.19). Positions in\nIND.\nThe indices i are the \"original numbering\" of the nodes. If there is renumbering,\nthe new ordering can be stored as a permutation PERM. Then PERM(i) = k when\nthe new number i is assigned to the node with original number k. The text [GL] by\nGeorge and Liu is the classic reference for this entire section on ordering of the nodes.\nGraph Separators\nHere is another good ordering, different from minimum degree. Graphs or meshes are\noften separated into disjoint pieces by a cut. The cut goes through a small number\nof nodes or meshpoints (a separator). It is a good idea to number the nodes in the\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nseparator last. Elimination is relatively fast for the disjoint pieces P and Q. It only\nslows down at the end, for the (smaller) separator S.\nThe three groups P, Q, S of meshpoints have no direct connections between P and\nQ (they are both connected to the separator S). Numbered in that order, the \"block\narrow\" stiffness matrix and its K = LU factorization look like this:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nK\nKP\nKP S\nLP\nUP\nA\nK = ⎣\nKQ\nKQS ⎦\nL = ⎣ 0\nLQ\n⎦\nU = ⎣\nUQ B ⎦\nSP KSQ\nKS\nX\nY\nZ\nC\n(3)\nThe zero blocks in K give zero blocks in L and U. The submatrix KP comes first\nin elimination, to produce LP and UP . Then come the factors LQUQ of KQ, followed\nby the connections through the separator. The major cost is often that last step, the\nsolution of a fairly dense system of the size of the separator.\nS\nQ\nP\nP\nS\nQ\nArrow matrix\nSeparator comes last\nBlocks P, Q\n(Figure 3.18)\n(Figure 3.19)\nSeparator S\nFigure 3.21: A graph separator numbered last produces a block arrow matrix K.\nFigure 3.21 shows three examples, each with separators. The graph for a perfect\narrow matrix has a one-point separator (very unusual). The 6-node rectangle has a\ntwo-node separator in the middle. Every N by N grid can be cut by an N-point\nseparator (and N is much smaller than N2). If the meshpoints form a rectangle, the\nbest cut is down the middle in the shorter direction.\nYou could say that the numbering of P then Q then S is block minimum degree.\nBut one cut with one separator will not come close to an optimal numbering. It\nis natural to extend the idea to a nested sequence of cuts. P and Q have their\nown separators at the next level. This nested dissection continues until it is not\nproductive to cut further. It is a strategy of \"divide and conquer.\"\nFigure 3.22 illustrates three levels of nested dissection on a 7 by 7 grid. The first\ncut is down the middle. Then two cuts go across and four cuts go down. Numbering\nthe separators last within each stage, the matrix K of size 49 has arrows inside arrows\ninside arrows. The spy command will display the pattern of nonzeros.\nSeparators and nested dissection show how numbering strategies are based on the\ngraph of nodes and edges in the mesh. Those edges correspond to nonzeros in the\nmatrix K. The nonzeros created by elimination (filled entries in L and U) correspond\nto paths in the graph. In practice, there has to be a balance between simplicity and\noptimality in the numbering--in scientific computing simplicity is a very good thing!\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nzero\nzero\n3 × 18\n3 × 18\nzero\nzero\nzero\nzero\n∗0 ∗\n\n0 ∗∗ 5\n∗∗∗\n9×9\n1 to 9\n22 to 30\n19 to 21\n40 to 42\nK =\n10 to 18\n31 to 39\n7 × 42\n7×7\nFigure 3.22: Three levels of separators. Still\nnonzeros in K, only\nin L.\nA very reasonable compromise is the backslash command U = K\\F that uses a\nnearly minimum degree ordering in Sparse MATLAB.\nOperation Counts (page K)\nHere are the complexity estimates for the 5-point Laplacian with N 2 or N 3 nodes:\nMinimum Degree\nSpace (nonzeros from fill-in)\nTime (flops for elimination)\nn = N 2 in 2D\nn = N 3 in 3D\nX\nX\nX\nX\nNested Dissection\nSpace (nonzeros from fill-in)\nX\nX\nTime (flops for elimination)\nX\nX\nIn the last century, nested dissection lost out--it was slower on almost all applica\ntions. Now larger problems are appearing and the asymptotics eventually give nested\ndissection an edge. Algorithms for cutting graphs can produce short cuts into nearly\nequal pieces. Of course a new idea for ordering could still win.\nIterative versus Direct Methods\nThis section is a guide to solution methods for problems Ax = b that are too large\nand expensive for ordinary elimination. We are thinking of sparse matrices A, when\na multiplication Ax is relatively cheap. If A has at most p nonzeros in every row,\nthen Ax needs at most pn multiplications. Typical applications are to large finite\ndifference equations or finite element problems on unstructured grids. In the special\ncase of a square grid for Laplace's equation, a Fast Poisson Solver (Section 7.2) is\navailable.\nWe turn away from elimination to iterative methods and Krylov subspaces.\nPure iterative methods are easier to analyze, but the Krylov subspace methods are\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nmore powerful. So the older iterations of Jacobi and Gauss-Seidel and overrelaxation\nare less favored in scientific computing, compared to conjugate gradients and GM\nRES. When the growing Krylov subspaces reach the whole space Rn, these methods\n(in exact arithmetic) give the exact solution A-1b. But in reality we stop much ear\nlier, long before n steps are complete. The conjugate gradient method (for positive\ndefinite A, and with a good preconditioner ) has become truly important.\nThe next ten pages will introduce you to numerical linear algebra. This has\nbecome a central part of scientific computing, with a clear goal: Find a fast stable\nalgorithm that uses the special properties of the matrices. We meet matrices that\nare sparse or symmetric or triangular or orthogonal or tridiagonal or Hessenberg or\nGivens or Householder. Those matrices are at the core of so many computational\nproblems. The algorithm doesn't need details of the entries (which come from the\nspecific application). By using only their structure, numerical linear algebra offers\nmajor help.\nOverall, elimination with good numbering is the first choice until storage and CPU\ntime become excessive. This high cost often arises first in three dimensions. At that\npoint we turn to iterative methods, which require more expertise. You must choose\nthe method and the preconditioner. The next pages aim to help the reader at this\nfrontier of scientific computing.\nPure Iterations\nWe begin with old-style pure iteration (not obsolete). The letter K will be reserved\nfor \"Krylov\" so we leave behind the notation KU = F. The linear system becomes\nAx = b with a large sparse matrix A, not necessarily symmetric or positive definite:\nLinear system Ax = b\nResidual rk = b - Axk\nPreconditioner P ≈ A\nThe preconditioner P attempts to be \"close to A\" and at the same time much easier\nto work with. A diagonal P is one extreme (not very close). P = A is the other\nextreme (too close). Splitting the matrix A gives an equivalent form of Ax = b:\nSplitting\nPx = (P - A)x + b .\n(4)\nThis suggests an iteration, in which every vector xk leads to the next xk+1:\nIteration\nPxk+1 = (P - A)xk + b .\n(5)\nStarting from any x0, the first step finds x1 from Px1 = (P - A)x0 + b. The iteration\ncontinues to x2 with the same matrix P, so it often helps to know its triangular factors\nL and U. Sometimes P itself is triangular, or its factors L and U are approximations\nto the triangular factors of A. Two conditions on P make the iteration successful:\n1. The new xk+1 must be quickly computable. Equation (5) must be fast to solve.\n2. The errors ek = x - xk must converge quickly to zero.\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nSubtract equation (5) from (4) to find the error equation. It connects ek to ek+1:\nError\nPek+1 = (P - A)ek which means ek+1 = (I - P -1A)ek = Mek .\n(6)\nThe right side b disappears in this error equation. Each step multiplies the error\nvector by M = I - P -1A. The speed of convergence of xk to x (and of ek to zero)\ndepends entirely on M . The test for convergence is given by the eigenvalues of M :\nConvergence test\nEvery eigenvalue of M must have |λ(M )| < 1.\nThe largest eigenvalue (in absolute value) is the spectral radius ρ(M ) = max |λ(M )|.\nConvergence requires ρ(M ) < 1. The convergence rate is set by the largest eigen\nvalue. For a large problem, we are happy with ρ(M ) = .9 and even ρ(M ) = .99.\nSuppose that the initial error e0 happens to be an eigenvector of M . Then the\nnext error is e1 = Me0 = λe0. At every step the error is multiplied by λ, so we must\nhave |λ| < 1. Normally e0 is a combination of all the eigenvectors. When the iteration\nmultiplies by M , each eigenvector is multiplied by its own eigenvalue. After k steps\nthose multipliers are λk . We have convergence if all |λ| < 1.\nFor preconditioner we first propose two simple choices:\nJacobi iteration\nP = diagonal part of A\nGauss-Seidel iteration\nP = lower triangular part of A\nTypical examples have spectral radius ρ(M ) = 1 - cN -1 . This comes closer and\ncloser to 1 as the mesh is refined and the matrix grows. An improved preconditioner\nP can give ρ(M ) = 1 - cN -1/2. Then ρ is smaller and convergence is faster, as in\n\"overrelaxation.\" But a different approach has given more flexibility in constructing\na good P , from a quick incomplete LU factorization of the true matrix A:\nI ncomplete LU\nP = (approximation to L)(approximation to U ) .\nThe exact A = LU has fill-in, so zero entries in A become nonzero in L and U . The ap\nproximate L and U could ignore this fill-in (fairly dangerous). Or P = LapproxUapprox\ncan keep only the fill-in entries F above a fixed threshold. The variety of options,\nand the fact that the computer can decide automatically which entries to keep, has\nmade the ILU idea (incomplete LU ) a very popular starting point.\nExample\nThe -1, 2, -1 matrix A = K provides an excellent example. We choose\nthe preconditioner P = T , the same matrix with T11 = 1 instead of K11 = 2. The LU\nfactors of T are perfect first differences, with diagonals of +1 and -1. (Remember that\nall pivots of T equal 1, while the pivots of K are 2/1, 3/2, 4/3, . . .) We can compute the\nright side of T -1Kx = T -1b with only 2N additions and no multiplications (just back\nsubstitution using L and U ). Idea: This L and U are approximately correct for K.\nThe matrix P -1A = T -1K on the left side is triangular. More than that, T is a rank 1\nchange from K (the 1, 1 entry changes from 2 to 1). It follows that T -1K and K-1T\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nare rank 1 changes from the identity matrix I. A calculation in Problem\nshows that\nonly the first column of I is changed, by the \"linear vector\" l = (N, N - 1, . . . , 1):\nP -1A = T -1K = I + leT\nand\nK-1T = I - (leT\n1 )/(N + 1) .\n(7)\nT\nHere e1 =\n. . . 0\nso leT has first column l. This example finds x = K-1b by\na quick exact formula (K-1T)T -1b, needing only 2N additions for T -1 and N additions\nand multiplications for K-1T. In practice we wouldn't precondition this K (just solve).\nThe usual purpose of preconditioning is to speed up convergence for iterative methods,\nand that depends on the eigenvalues of P -1A. Here the eigenvalues of T -1K are its\ndiagonal entries N+1, 1, . . ., 1. This example will illustrate a special property of conjugate\ngradients, that with only two different eigenvalues it reaches the true solution x in two\nsteps.\nThe iteration Pxk+1 = (P - A)xk + b is too simple! It is choosing one particular\nvector in a \"Krylov subspace.\" With relatively little work we can make a much better\nchoice of xk . Krylov projections are the state of the art in today's iterative methods.\nKrylov Subspaces\nOur original equation is Ax = b. The preconditioned equation is P -1Ax = P -1b.\nWhen we write P -1, we never intend that an inverse would be explicitly computed\n(except in our example). The ordinary iteration is a correction to xk by the vector\nP -1rk:\nPxk+1 = (P - A)xk + b\nor\nPxk+1 = Pxk + rk\nor\nxk+1 = xk + P -1 rk . (8)\nHere rk = b - Axk is the residual. It is the error in Ax = b, not the error ek\nin x. The symbol P -1rk represents the change from xk to xk+1, but that step is\nnot computed by multiplying P -1 times rk . We might use incomplete LU, or a few\nsteps of a \"multigrid\" iteration, or \"domain decomposition.\" Or an entirely new\npreconditioner.\nIn describing Krylov subspaces, I should work with P -1A. For simplicity I will\nonly write A. I am assuming that P has been chosen and used, and the precondi\ntioned equation P -1Ax = P -1b is given the notation Ax = b. The preconditioner is\nnow P = I. Our new matrix A is probably better than the original matrix with that\nname.\nThe Krylov subspace Kk(A, b) contains all combinations of b, Ab, . . . , Ak-1b.\nThese are the vectors that we can compute quickly, multiplying by a sparse A. We\nlook in this space Kk for the approximation xk to the true solution of Ax = b. Notice\nthat the pure iteration xk = (I - A)xk-1 + b does produce a vector in Kk when xk-1\nis in Kk-1 . The Krylov subspace methods make other choices of xk . Here are four\ndifferent approaches to choosing a good xk in Kk --this is the important decision:\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\n1. The residual rk = b -Axk is orthogonal to Kk (Conjugate Gradients, . . . )\n2. The residual rk has minimum norm for xk in Kk (GMRES, MINRES, . . . )\n3. rk is orthogonal to a different space like Kk(AT) (BiConjugate Gradients, . . . )\n4. ek has minimum norm (SYMMLQ; for BiCGStab xk is in ATKk(AT); . . . )\nIn every case we hope to compute the new xk quickly and stably from the earlier x's.\nIf that recursion only involves xk-1 and xk-2 (short recurrence) it is especially fast.\nWe will see this happen for conjugate gradients and symmetric positive definite A.\nThe BiCG method in 3 is a natural extension of short recurrences to unsymmetric\nA--but stability and other questions open the door to the whole range of methods.\nTo compute xk we need a basis for Kk . The best basis q1, . . . , qk is orthonormal.\nEach new qk comes from orthogonalizing t = Aqk-1 to the basis vectors q1, . . . , qk-1\nthat are already chosen. This is the Gram-Schmidt idea (called modified Gram-\nSchmidt when we subtract projections of t onto the q's one at a time, for numerical\nstability). The iteration to compute the orthonormal q's is known as Arnoldi's\nmethod:\n1 q1 = b/∥b∥2;\n% Normalize to ∥q1∥ = 1\nfor j = 1, . . ., k -1\nt = Aqj ;\n% t is in the Krylov space Kj+1(A, b)\nfor i = 1, . . . , j\nhij = qi\nTt;\n% hij qi = projection of t onto qi\nt = t -hij qi; % Subtract component of t along qi\nq\nh\nend;\nj+1,j = ∥t∥2;\n% t is now orthogonal to q1, . . ., qj\nj+1 = t/hj+1,j ;\n% Normalize t to ∥qj+1∥ = 1\nend\n% q1, . . . , qk are orthonormal in Kk\nPut the column vectors q1, . . . , qk into an n by k matrix Qk . Multiplying rows of\nT\nQT by columns of Qk produces all the inner products qi qj , which are the 0's and 1's\nk\nin the identity matrix. The orthonormal property means that QT\nk Qk = Ik .\nArnoldi constructs each qj+1 from Aqj by subtracting projections hij qi.\nIf\nwe\n\nexpress the steps up to j = k-1 in matrix notation, they become AQk-1 = Qk Hk,k-1:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nArnoldi\nh11\nh12\n· h1,k-1\nAQk-1 =\n⎢⎢⎣ Aq1\n· · · Aqk-1\n⎥⎥⎦ =\n⎢⎢⎣ q1\n· · · qk\n⎢⎢⎣\n⎥⎥⎦\nh21\nh22\n· h2,k-1\nh23\n·\n·\n· hk,k-1\n⎥⎥⎦ .\n(9)\nn by k -1\nn by k\nk by k -1\nThat matrix Hk,k-1 is \"upper Hessenberg\" because it has only one nonzero diagonal\nbelow the main diagonal. We check that the first column of this matrix equation\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\n(multiplying by columns!) produces q2:\nAq1 - h11q1\nh\nAq1 = h11q1 + h21q2\nor\nq2 =\n.\n(10)\nThat subtraction is Step 4 in Arnoldi's algorithm. Division by h21 is Step 6.\nUnless more of the hij are zero, the cost is increasing at every iteration. We have\nk dot products to compute at step 3 and 5, and k vector updates in steps 4 and 6. A\nshort recurrence means that most of these hij are zero. That happens when A = AT .\nThe matrix H is tridiagonal when A is symmetric. This fact is the founda\ntion of conjugate gradients. For a matrix proof, multiply equation (9) by QT\n. The\nk-1\nright side becomes H without its last row, because (QT\nk-1Qk )Hk,k-1 = [ I 0 ] Hk,k-1.\nThe left side QT\nk-1AQk-1 is always symmetric when A is symmetric. So that H matrix\nhas to be symmetric, which makes it tridiagonal. There are only three nonzeros in\nthe rows and columns of H, and Gram-Schmidt to find qk+1 only involves qk and qk-1:\nArnoldi when A = AT\nAqk = hk+1,k qk+1 + hk,k qk + hk-1,k qk-1 .\n(11)\nThis is the Lanczos iteration. Each new qk+1 = (Aqk - hk,kqk - hk-1,kqk-1)/hk+1,k\ninvolves one multiplication Aqk , two dot products for new h's, and two vector updates.\nThe QR Method for Eigenvalues\nAllow me an important comment on the eigenvalue problem Ax = λx. We have seen\nthat Hk-1 = QT\nk-1AQk-1 is tridiagonal if A = AT. When k - 1 reaches n and Qn is\nsquare, the matrix H = QTAQn = Q-1AQn has the same eigenvalues as A:\nn\nn\nSame λ\nHy = Q-1AQny = λy\ngives\nAx = λx with x = Qny .\n(12)\nn\nIt is much easier to find the eigenvalues λ for a tridiagonal H than the for original A.\nThe famous \"QR method\" for the eigenvalue problem starts with T1 = H, factors\nit into T1 = Q1R1 (this is Gram-Schmidt on the short columns of T1), and reverses\norder to produce T2 = R1Q1. The matrix T2 is again tridiagonal, and its off-diagonal\nentries are normally smaller than for T1. The next step is Gram-Schmidt on T2,\northogonalizing its columns in Q2 by the combinations in the upper triangular R2:\nQR Method Factor T2 into Q2R2 . Reverse order to T3 = R2Q2 = Q-1T2Q2 (13)\n.\nBy the reasoning in (12), any Q-1TQ has the same eigenvalues as T . So the matrices\nT2, T3, . . . all have the same eigenvalues as T1 = H and A. (These square Qk from\nGram-Schmidt are entirely different from the rectangular Qk in Arnoldi.) We can\neven shift T before Gram-Schmidt, and we should, provided we remember to shift\nback:\nShifted QR\nFactor Tk - skI = Qk Rk . Reverse to Tk+1 = Rk Qk + bk I .\n(14)\nT\nWhen the shift sk is chosen to be the n, n entry of Tk , the last off-diagonal entry of\nk+1 becomes very small. The n, n entry of Tk+1 moves close to an eigenvalue. Shifted\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nQR is one of the great algorithms of numerical linear algebra. It solves moderate-size\neigenvalue problems with great efficiency. This is the core of MATLAB's eig(A).\nFor a large symmetric matrix, we often stop the Arnoldi-Lanczos iteration at a\ntridiagonal Hk with k < n. The full n-step process to reach Hn is too expensive, and\noften we don't need all n eigenvalues. So we compute (by the same QR method) the k\neigenvalues of Hk instead of the n eigenvalues of Hn. These computed λ1k, λ2k, . . . , λkk\ncan provide good approximations to the first k eigenvalues of A. And we have an\nexcellent start on the eigenvalue problem for Hk+1, if we decide to take a further step.\nThis Lanczos method will find, approximately and iteratively and quickly, the\nleading eigenvalues of a large symmetric matrix.\nThe Conjugate Gradient Method\nWe return to iterative methods for Ax = b. The Arnoldi algorithm produced or\nthonormal basis vectors q1, q2, . . . for the growing Krylov subspaces K1 , K2 , . . .. Now\nwe select vectors x1, x2, . . . in those subspaces that approach the exact solution to\nAx = b. We concentrate on the conjugate gradient method for symmetric positive\ndefinite A.\nThe rule for xk in conjugate gradients is that the residual rk = b - Axk should\nbe orthogonal to all vectors in Kk. Since rk will be in Kk+1, it must be a multiple\nof Arnoldi's next vector qk+1! Each residual is therefore orthogonal to all previous\nresiduals (which are multiples of the previous q's):\nOrthogonal residuals\nri\nT rk = 0\nfor i < k .\n(15)\nThe difference between rk and qk+1 is that the q's are normalized, as in q1 = b/∥b∥.\nK\nSince rk-1 is a multiple of qk, the difference rk -rk-1 is orthogonal to each subspace\ni with i < k. Certainly xi - xi-1 lies in that Ki. So ∆r is orthogonal to earlier\n∆x's:\n(xi - xi-1)T(rk - rk-1) = 0\nfor i < k .\n(16)\nThese differences ∆x and ∆r are directly connected, because the b's cancel in ∆r:\nrk - rk-1 = (b - Axk) - (b - Axk-1) = -A(xk - xk-1) .\n(17)\nSubstituting (17) into (16), the updates in the x's are \"A-orthogonal\" or conjugate:\nConjugate updates ∆x (xi - xi-1)TA(xk - xk-1) = 0\nfor i < k .\n(18)\nNow we have all the requirements. Each conjugate gradient step will find a new\n\"search direction\" dk for the update xk - xk-1. From xk-1 it will move the right\ndistance αkdk to xk. Using (17) it will compute the new rk. The constants βk in\nthe search direction and αk in the update will be determined by (15) and (16) for\ni = k - 1. For symmetric A the orthogonality in (15) and (16) will be automatic for\ni < k - 1, as in Arnoldi. We have a \"short recurrence\" for the new xk and rk.\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nHere is one cycle of the algorithm, starting from x0 = 0 and r0 = b and β1 = 0. It\ninvolves only two new dot products and one matrix-vector multiplication Ad:\nConjugate 1 βk = rk-1rk-1/rT\nT\nk-2rk-2\n% Improvement this step\nGradient\n2 dk = rk-1 + βk dk-1\n% Next search direction\nT\nMethod\n3 αk = rk-1rk-1/dTAdk\n% Step length to next xk\nk\n4 xk = xk-1 + αk dk\n% Approximate solution\n5 rk = rk-1 - αk Adk\n% New residual from (17)\nThe formulas 1 and 3 for βk and αk are explained briefly below--and fully by Trefethen-\nBau ( ) and Shewchuk ( ) and many other good references.\nDifferent Viewpoints on Conjugate Gradients\nI want to describe the (same!) conjugate gradient method in two different ways:\n1. It solves a tridiagonal system Hy = f recursively\n2. It minimizes the energy 1 xTAx - xTb recursively.\nHow does Ax = b change to the tridiagonal Hy = f ? That uses Arnoldi's or\nthonormal columns q1, . . . , qn in Q, with QTQ = I and QTAQ = H:\nAx = b is (QTAQ)(QT x) = QTb which is Hy = f = (∥b∥, 0, . . . , 0) .\n(19)\nT\nSince q1 is b/∥b∥, the first component of f = QTb is q1 b = ∥b∥ and the other com\nponents are qTb = 0. The conjugate gradient method is implicitly computing this\ni\nsymmetric tridiagonal H and updating the solution y at each step. Here is the third\nstep:\n⎡\n⎤ ⎡\n⎤\n⎡\n⎤\nh11\nh12\n∥b∥\nH3y3 = ⎣ h21\nh22\nh23 ⎦ ⎣ y3 ⎦ = ⎣ 0 ⎦ .\n(20)\nh32\nh33\nThis is the equation Ax = b projected by Q3 onto the third Krylov subspace K3 .\nThese h's never appear in conjugate gradients. We don't want to do Arnoldi too!\nIt is the LDLT factors of H that CG is somehow computing--two new numbers at\neach step. Those give a fast update from yj-1 to yj . The corresponding xj = Qj yj\nfrom conjugate gradients approaches the exact solution xn = Qnyn which is x = A-1b.\nIf we can see conjugate gradients also as an energy minimizing algorithm, we can\nextend it to nonlinear problems and use it in optimization. For our linear equation\nAx = b, the energy is E(x) = 1 xTAx - xTb. Minimizing E(x) is the same as solving\nAx = b, when A is positive definite (the main point of Section 1.\n). The CG\niteration minimizes E(x) on the growing Krylov subspaces. On the first\nsubspace K1, the line where x is αb = αd1, this minimization produces the right\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nvalue for α1:\nbTb\nE(αb) = α2bTAb - αbTb\nis minimized at\nα1 =\n.\n(21)\nbTAb\nThat α1 is the constant chosen in step 3 of the first conjugate gradient cycle.\nThe gradient of E(x) = 2\n1 xTAx - xTb is exactly Ax - b. The steepest descent\ndirection at x1 is along the negative gradient, which is r1! This sounds like the perfect\ndirection d2 for the next move. But the great difficulty with steepest descent is that\nthis r1 can be too close to the first direction. Little progress that way. So we add\nthe right multiple β2d1, in order to make d2 = r1 + β2d1 A-orthogonal to the first\ndirection d1.\nThen we move in this conjugate direction d2 to x2 = x1 + α2d2. This explains the\nname conjugate gradients, rather than the pure gradients of steepest descent. Every\ncycle of CG chooses αj to minimize E(x) in the new search direction x = xj-1 + αdj .\nThe last cycle (if we go that far) gives the overall minimizer xn = x = A-1b.\nExample\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n⎣ 1\n⎦⎣ -1 ⎦ = ⎣\nAx = b\nis\n0 ⎦ .\n\n-1\nFrom x0 = 0 and β1 = 0 and r0 = d1 = b the first cycle gives α1 = 2 and x1 = 1b =\n(2, 0, 0). The new residual is r1 = b - Ax1 = (0, -2, -2). Then the second cycle yields\n⎡\n⎤\n⎡\n⎤\nβ2 =\n,\nd2 = ⎣ -2 ⎦ ,\nα2 =\n,\nx2 = ⎣ -1 ⎦ = A-1b !\n-2\n-1\nA\nThe correct solution is reached in two steps, where normally it will take n = 3 steps. The\nreason is that this particular A has only two distinct eigenvalues 4 and 1. In that case\n-1b is a combination of b and Ab, and this best combination x2 is found at cycle 2. The\nresidual r2 is zero and the cycles stop early--very unusual.\nEnergy minimization leads in [ ] to an estimate of the convergence rate for the\n√\nerror e = x - xj in conjugate gradients, using the A-norm ∥e∥A =\neTAe:\nλ\n√\n√\n\nj\nmax -\nλmin\nλ\nError estimate\n∥x - xj ∥A ≤ 2 √\n√\n∥x - x0∥A .\n(22)\nmax +\nλmin\nλ\nThis is the best-known error estimate, although it doesn't account for any clustering of\nthe eigenvalues of A. It involves only the condition number λmax/λmin. Problem\ngives the \"optimal\" error estimate but it is not so easy to compute. That optimal\nestimate needs all the eigenvalues of A, while (22) uses only the extreme eigenvalues\nmax(A) and λmin(A)--which in practice we can bound above and below.\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nMinimum Residual Methods\nWhen A is not symmetric positive definite, conjugate gradient is not guaranteed to\nsolve Ax = b. Most likely it won't. We will follow van der Vorst [ ] in briefly\ndescribing the minimum norm residual approach, leading to MINRES and GMRES.\nThese methods choose xj in the Krylov subspace Kj so that ∥b- Axj ∥ is minimal.\nFirst we compute the orthonormal Arnoldi vectors q1, . . . , qj . They go in the columns\nof Qj , so QT\nj Qj = I. As in (19) we set xj = Qj y, to express the solution as a\ncombination of those q's. Then the norm of the residual rj using (9) is\n∥b - Axj ∥ = ∥b - AQj y∥ = ∥b - Qj+1Hj+1,j y∥ .\n(23)\nT\nT\nThese vectors are all in the Krylov space Kj+1, where rj (Qj+1Qj\nT\n+1rj ) = rj rj . This\nsays that the norm is not changed when we multiply by Qj\nT\n+1. Our problem becomes:\nChoose y to minimize ∥rj ∥ = ∥Qj\nT\n+1b - Hj+1,j y∥ = ∥f - Hy∥ .\n(24)\nThis is an ordinary least squares problem for the equation Hy = f with only j + 1\nequations and j unknowns. The right side f = QT\nj+1b is (∥r0∥, 0, . . ., 0) as in (19).\nThe matrix H = Hj+1,j is Hessenberg as in (9), with one nonzero diagonal below the\nmain diagonal. We face a completely typical problem of numerical linear algebra:\nUse the special properties of H and f to find a fast algorithm that computes y. The\ntwo favorite algorithms for this least squares problem are closely related:\nMINRES A is symmetric (probably indefinite, or we use CG) and H is tridiagonal\nGMRES\nA is not symmetric and the upper triangular part of H can be full\nIn both cases we want to clear out that nonzero diagonal below the main diagonal of\nH. The natural way to do that, one nonzero entry at a time, is by \"Givens rotations.\"\nThese plane rotations are so useful and simple (the essential part is only 2 by 2) that\nwe complete this section by explaining them.\nGivens Rotations\nThe direct approach to the least squares solution of Hy = f constructs the normal\nequations HTHy = HTf. That was the central idea in Chapter 1, but you see what\nwe lose. If H is Hessenberg, with many good zeros, HTH is full. Those zeros in H\nshould simplify and shorten the computations, so we don't want the normal equations.\nThe other approach to least squares is by Gram-Schmidt. We factor H into\northogonal times upper triangular. Since the letter Q is already used, the or\nthogonal matrix will be called G (after Givens). The upper triangular matrix is G-1H.\nThe 3 by 2 case shows how a plane rotation G-1 can clear out the subdiagonal entry\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nh21:\n⎡\n⎤ ⎡\n⎤\n⎡\n⎤\ncos θ sin θ 0\nh11\nh12\n∗\n∗\n\nG-1\n21 H = ⎣-sin θ cos θ 0 ⎦ ⎣ h21\nh22 ⎦ = ⎣ 0 ∗⎦ .\n(25)\nh32\n0 ∗\nG\nThat bold zero entry requires h11 sin θ = h21 cos θ, which determines θ. A second\n-1\nrotation G-1\n32 , in the 2-3 plane, will zero out the 3, 2 entry. Then G-1\nH is a\nsquare upper triangular matrix U above a row of zeros!\nThe Givens orthogonal matrix is G = G21G32 but there is no reason to do this mul\ntiplication. We use each Gij as it is constructed, to simplify the least squares problem.\nRotations (and all orthogonal matrices) leave the lengths of vectors unchanged:\n21 Hy -G-1G-1\nU\nF\n∥Hy -f ∥= ∥G-1G-1\nf ∥= ∥\ny -\n∥.\n(26)\ne\nThis length is what MINRES and GMRES minimize. The row of zeros below U\nmeans that the last entry e is the error--we can't reduce it. But we get all the other\nentries exactly right by solving the j by j system Uy = F (here j = 2). This gives\nthe best least squares solution y. Going back to the original problem of minimizing\n∥r∥= ∥b -Axj ∥, the best xj in the Krylov space Kj is Qj y.\nFor non-symmetric A (GMRES rather than MINRES) we don't have a short\nrecurrence. The upper triangle in H can be full, and step j becomes expensive and\npossibly inaccurate as j increases. So we may change \"full GMRES\" to GMRES(m),\nwhich restarts the algorithm every m steps. It is not so easy to choose a good m.\nProblem Set 3.6\nCreate K2D for a 4 by 4 square grid with N 2 = 32 interior mesh points (so\nn = 9). Print out its factors K = LU (or its Cholesky factor C = chol(K) for\nthe symmetrized form K = CTC). How many zeros in these triangular factors?\nAlso print out inv(K) to see that it is full.\nAs N increases, what parts of the LU factors of K2D are filled in?\nCan you answer the same question for K3D? In each case we really want an\nestimate cN p of the number of nonzeros (the most important number is p).\nUse the tic; ...; toc clocking command to compare the solution time for K2Dx =\nrandom f in ordinary MATLAB and sparse MATLAB (where K2D is defined as\na sparse matrix). Above what value of N does the sparse routine K\\f win?\nCompare ordinary vs. sparse solution times in the three-dimensional K3Dx =\nrandom f . At which N does the sparse K\\f begin to win?\nIncomplete LU\nConjugate gradients\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nDraw the next step after Figure 3.19 when the matrix has become 4 by 4 and\nthe graph has nodes 2-4-5-6. Which have minimum degree? Is there more\nfill-in?\nRedraw the right side of Figure 3.19 if row number 2 is chosen as the second\npivot row. Node 2 does not have minimum degree. Indicate new edges in the\n5-node graph and new nonzeros F in the matrix.\nT\nTo show that T -1K = I + leT\n1 in (7), with e1 = [ 1 0 . . . 0 ], we can start from\nT\nT\nK = T + e1e1 . Then T -1K = I + (T -1e1)e1 and we verify that e1 = Tl:\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n1 -1\nN\nTl =\n⎢⎢⎣\n-1\n2 -1\n·\n·\n·\n⎢⎢⎣\n⎥⎥⎦\nN -1\n·\n⎥⎥⎦ =\n⎢⎢⎣\n·\n⎥⎥⎦ = e1 .\n-1\nSecond differences of a linear vector l are zero. Now multiply T -1K = I + leT\ntimes I -(leT\n1 )/(N + 1) to establish the inverse matrix K-1T in (7).\nArnoldi expresses each Aqk as hk+1,kqk+1 + hk,kqk + · · · + h1,k q1. Multiply by qT\ni\nT\nto find hi,k = qi Aqk . If A is symmetric you can write this as (Aqi)Tqk. Explain\nwhy (Aqi)Tqk = 0 for i < k -1 by expanding Aqi into hi+1,iqi+1 + · · · + h1,iq1.\nWe have a short recurrence if A = AT (only hk+1,k and hk,k and hk-1,k are\nnonzero)."
        },
        {
          "category": "Resource",
          "title": "am37.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/d9ad8a20efdcd46c4cccd981bc4306e5_am37.pdf",
          "content": "Partial Differential Equations\n3.7\nFour Model Examples\nThe differential equations in Chapter 1 were very ordinary. There were time deriva\ntives d/dt or space derivatives d/dx but not both:\ndu\nd2u\nd\ndu\n= -Ku or M\n+ Ku = F (t)\nor\n-\nc(x)\n= f (x) .\n(1)\ndt\ndt2\ndx\ndx\nA partial differential equation contains two or more derivatives (they have to be partial\nderivatives like ∂/∂x and ∂/∂y and ∂/∂t so we can tell them apart). The solution\nu(x, y) or u(x, t) or even u(x, y, t) is a function of those \"independent variables\" x\nand y and t.\nIt is important to distinguish different types of equations, above all the distinction\nbetween \"boundary value problems\" and initial value problems\". The time variable t\nindicates an initial value problem. The first equation in (1) starts from an initial value\nu(0). The solution u(ε) evolves?? for t > 0 by obeying the equation du/dt = Au.\nThe second equation needs also an initial value du/dt(0) for the velocity, because\nthe leading term involves d2u/dt2 . Boundary values were given at endpoints x = 0\nand x = 1. Inside the boundary (in the interior) u(x) solved the equation, with just\nenough freedom (two arbitrary constants) to satisfy the two boundary conditions. All\ngood. The third equation in (1) described a steady state u(x).\nFor partial differential equations, start with initial value problems. We will focus\non three examples. They involve first or second order derivatives in t and in x and\nu(x, t) is a scalar. The names of the equations are important too:\n∂u\n∂u\nOne way wave equation\n∂t = ∂x\n(2)\n∂u\n∂2u\nHeat equation, diffusion equation\n=\n∂t\n∂x2\n(3)\n∂2u\nWave equation (with velocity c)\n∂t2 = c 2 ∂2u\n(4)\n∂x2\nThe first two equations involve ∂/∂t (first order) so initial values u(x, 0) will\nbe given (at t = 0). We know where the solution starts, and in Figure 3.1 those\ninitial values are delta functions. Notice the difference at t = 1! In the one way\nwave equation, the delta function moved to the left. In the heat equation, the delta\nfunction diffused into a Gaussian. And it spreads out even further by the time t = 2.\n\n′\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nSince the initial value is symmetric around the centerpoint x = 0, so is the solution\nu(x, t). The heat equation doesn't notice if you change x to -x, but it sure notices if\nyou switch t to -t. The \"backward heat equation\" -∂u/∂t = ∂2u/∂x2 is impossible\nto solve. Physically, hot air can spread into a room, but time doesn't reverse and the\ndiffused heat doesn't return back to the starting point.\nThe full wave equation involves ∂2u/∂t2, so we need an initial velocity ∂u/∂t(x, 0)\nin addition to u(x, 0). In Figure 3.2a that initial velocity is zero. We see waves going in\nboth directions (symmetrically). In Figure 3.2b the initial velocity is ∂u/∂t(x, 0) = 1.\nThe wave to the left is different from the wave to the right. You might note that the\nsame word \"velocity\" was used for the number c (velocity in x - t space) and for\n∂u/∂t (velocity in u - t space).\nHow could those examples be extended? The one way wave equation could become\n∂u\n∂u\n∂u\n∂u\n∂u\n∂u\n???\n= c\nor\n= c(x)\nor even\n= c(u)\n.\n(5)\n∂t\n∂x\n∂t\n∂x\n∂t\n∂x\nThe last of those is nonlinear! It is highly important, one good application is to traffic\nflow. At a point x on the highway, the car density is u(x, t) at time t. If the density\nup ahead (one way drivers!) is greater, then cars slow down and get denser. The\nrelation depends on u itself, it is not linear. This produces the waves of stop and go\ndriving that a helicopter sees in a traffic jam.\nThe heat equation should have a \"diffusivity constant\" c, with the dimension of\n(distance)2/time. In fact this fits our framework exactly, there is a perfect analogy\nwith K = ATCA and u = -Ku:\n∂u\n∂\n∂u\n=\nc(x)\n.\n(6)\n∂t\n∂x\n∂x\nWhen c(x) is a positive constant, we can rescale time to make c = 1. That is the case\nwe can solve. (When c is a negative constant, nobody can solve the backward heat\nequation. We never allowed c < 0 in Chapters 1 and 2 either.) When c depends on\nu or ∂u/∂x, the equation becomes nonlinear and we don't expect an exact formula\n(but we can compute!).\n??? The wave equation would also look better in its symmetric form using ATCA.\nNotice also that it can be rewritten as\n\n∂u\n∂u\n∂\n∂t\n0 c\n∂\n=\n∂t\n∂t\nc ∂u\nc 0\n∂x\nc ∂u\n.\n(7)\n∂x\n∂x\nIn a sense (Problem A) this is a pair of one way wave equations!\nThose time-dependent wave and heat equations will come after we study the\nall-important equation of equilibrium: Laplace's equation. This describes a steady\nstate. The variables are x, y, z (in three space dimensions) or x and y (in two dimensions--\nwe will concentrate on this very remarkable model).\n\n3.7. FOUR MODEL EXAMPLES\nLaplace's equation has pure second derivatives ∂2u/∂x2 = uxx and ∂2u/∂y2 = uyy :\n∂2u\nd2u\nLaplace:\n+ ∂y2 = uxx + uyy = 0 in a plane region R\n(8)\n∂x2\nThis describes the steady state temperature u(x, y) over the region R, when there is\nno heat source inside (the right side of the equation is zero). The problem doesn't\nhave initial conditions, it has boundary conditions! The boundary of R is a closed\ncurve C. At every point of C we may prescribe either a fixed temperature u0 or a\nheat flux F0:\n∂u\nBoundary conditions:\nu = u0 or\n= F0\nat each point of C .\n(9)\n∂n\n∂u\nThat \"normal derivative\" ∂n is the rate of change of u in the direction perpendicular\nto the boundary. At a point where the boundary is insulated (meaning that no heat\ncan flow through) the flux is ∂u/∂n = 0.\nThis is the problem of Section 3.2: Laplace's equation (8) with boundary condi\ntions (9). It is the two-dimensional analogue, a partial differential equation, of the\nmost basic two-point value problem:\nd2u = 0 with [u(0) or u ′(0)] and [u(1) or u ′(1)] given at the endpoints . (10)\ndx2\nThis describes the displacement (or it could be the temperature) in a rod. The\nsolution to equation (10) is just u(x) = A + Bx. For Laplace's equation we will list\nan infinite family of solutions (which we need because there are infinitely many more\nboundary points!).\nEquation (10) was our simple model, with no applied force f and with a constant\ncoefficient c = 1. The more general form in Chapter 2 was\nd\ndu\ndu\n-\nc(x)\n= f(x), with boundary conditions on u or w = c\n.\n(11)\ndx\ndx\ndx\nThose possibilities for f and c are also seen in two dimensions. When there is a source\nterm f(x, y) we have Poisson's equation (pronounced Pwa-son):\n∂2u\n∂2u\nPoisson:\n+ ∂y2 = uxx + uyy = f(x, y) in the region R .\n(12)\n∂x2\nWhen the material in R is not homogeneous, the constant coefficient c = 1 becomes\na variable coefficient c(x, y):\n∂\n∂u\n∂\n∂u\nNonhomogeneous:\nc\n+\nc\n= f(x, y) in the region R\n(13)\n∂x\n∂x\n∂y\n∂y\nMaybe you can see that we are closing in on our favorite framework ATCAu = f!\nSection 3.2 sets this framework, by identifying A and AT . Those are the key\noperators of vector calculus, the gradient and the divergence. Laplace's equation,\nwith c = 1 and f = 0, is seen as div grad u = 0. Then we concentrate on solving this\nexceptional equation, by analysis or by scientific computing:\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nExact solution (formula and series): Section 3.2\nNumerical solution (finite differences and finite elements): Section 3.3."
        },
        {
          "category": "Resource",
          "title": "am51.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/e548f1cb5f6c6bc9a5c90ee5180a3b91_am51.pdf",
          "content": "Chapter 5\nInitial Value Problems\n5.1\nFinite Difference Methods\nWe don't plan to study highly complicated nonlinear differential equations. Our first\ngoal is to see why a difference method is successful (or not). The crucial questions\nof stability and accuracy can be clearly understood for linear equations. Then we\ncan construct difference approximations to a great variety of practical problems.\nAnother property we need is computational speed. That depends partly on\nthe complexity of the equation u0 = f (u, t). Often we measure speed by the number\nof times that f (u, t) has to be computed in each time step (that number could be\none). When we turn to implicit methods and predictor-corrector methods, to improve\nstability, the cost per step goes up but we gain speed with a larger step t.\nThis chapter begins with basic methods (forward Euler, backward Euler) and then\nimproves. Each time, we test stability on u0 = a u. When a is negative, t is often\nlimited by -a t ↑ C. This has an immediate effect: the equation with a = -99\nrequires a much smaller t than a = -1. Let me organize the equations as scalar\nand vector, nonlinear in general or linear with constant coefficients a and A:\nN equations\nu 0 = f (\n)\nu 0\ni = fi(\n)\nu 0 = au\nu 0 = Au\na\na 0\nAij\ni\nj\nRe (A) 0\n1 equation\nu, t\nu, t\n@f/@u\n@f /@u\nFor an ordinary differential equation u0 = f (u, t), good codes will increase the\naccuracy (and keep stability) far beyond the O(t) error in Euler's methods. You\ncan rely on freely available software like ode45 to make the two crucial decisions:\n1.\nto choose an accurate difference method (and change the formula adaptively)\n2.\nto choose a stable time step (and change t adaptively).\nWe will introduce accurate methods, and find the stability limits on t.\nc2006 Gilbert Strang\n\n\"\n\"\n\nc2006 Gilbert Strang\nCHAPTER 5. INITIAL VALUE PROBLEMS\nStiff Differential Equations\nFirst we call attention to one extra question: Is the equation stiff ? Let me begin\nwith a made-up example to introduce stiffness and its effects:\nv(t) = e-t + e-99t\ncontrols decay\ncontrols t\nThe step t is 99 times\nsmaller because of e -99t\nthat disappears anyway\nThose decay rates -1 and -99 could be the eigenvalues of A, as in Example 1.\nExample 1\nd\nv\n-50\nv\nv(0)\n=\nwith\n=\n.\n(1)\ndt w\n49 -50\nw\nw(0)\n-99t\nThe solution has v(t) = e-t + e-99t and w(t) = e-t - e\n. The time scales are different\nby a factor of 99 (the condition number of A). The solution will decay at the slow\ntime scale of e-t, but computing e-99t may require a very small t for stability.\nIt is frustrating to have t controlled by the component that is decaying so fast.\nAny explicit method will have a requirement 99t ↑ C. We will see how this happens\nand how an implicit method (like ode15s and od23t in MATLAB) can avoid it.\nTrefethen [ ] points to these applications where stiffness comes with the problem:\n1.\nChemical kinetics (reactions go at very different speeds)\n2.\nControl theory (probably the largest application area for MATLAB)\n3.\nCircuit simulation (components respond at widely different time scales)\n4.\nMethod of Lines (large systems come from partial differential equations).\nExample 2 The N by N second difference matrix K produces a large stiff system:\ndu\n-Ku\ndui\nui+1 - 2ui + ui-1\nMethod of Lines\n=\nhas\n=\n(x)2\n.\n(2)\ndt\n(x)2\ndt\nThis comes from the heat equation @u/@t = @2 u/@x2, by discretizing only the space\nderivative. Example 1 had eigenvalues -1 and -99, while Example 2 has N eigenvalues--\nbut the difficulty is essentially the same ! The most negative eigenvalue here is about\na = -4/(x)2 . So a small x (for accuracy) will require a very small t (for stability).\nThis \"semidiscrete\" method of lines is an important idea. Discretizing the\nspace variables first produces a large system that can be given to an ODE solver. (We\nhave ordinary differential equations in time.) If it wants to, that solver can vary the time\nstep t and even the discretization formula as u(t) speeds up or slows down.\n\nreplacements\nc\n5.1. FINITE DIFFERENCE METHODS\n2006 Gilbert Strang\nThis method splits the approximation of a PDE into two parts. Finite differences/finite\nelements in earlier chapters produce the first part (discrete in space). The upcoming\nstability-accuracy analysis applies to the second part (discrete in time). This idea is very\nsimple and useful, even if it misses the good methods later in this chapter that take full\nadvantage of space-time. For the heat equation ut = uxx, the useful fact utt = uxxxx\nallows us to cancel space errors with time errors--which we won't notice when they are\nseparated in the semidiscrete method of lines.\nForward Euler and Backward Euler\nThe equation u 0 = f(u, t) starts from an initial value u(0). The key point is that the\nrate of change u0 is determined by the current state u at any moment t. This model\nof reality, where all the history is contained in the current state u(t), is a tremendous\nsuccess throughout science and engineering. (It makes calculus almost as important\nas linear algebra.) But for a computer, continuous time has to change to discrete\ntime. One differential equation allows many difference equations!\nThe simplest method (Euler is pronounced \"Oiler\") uses a forward difference:\nUn+1 - Un\nForward Euler\n= f(Un, tn)\nis\nUn+1 = Un + t fn .\n(3)\nt\nOver each t interval, the slope of U doesn't change. Figure 5.1 shows how the correct\nsolution to u0 = au follows a smooth curve, while U(t) is only piecewise linear. A\nbetter method (higher accuracy) will stay much closer to the curve by using more\ninformation than the one slope fn = f(Un, tn) at the start of the step.\nu\nt\nu = et\nU\nt\n(0) = 1\n= 1 +\n(forward)\nt\nt\nt\nu = et\nU =\n1 - t\n(backward)\nFigure 5.1: Forward Euler and Backward Euler for u0 = u. One-step errors 1\n2 (t)2 .\nBackward Euler comes from using fn+1 at the end of the step, when t = tn+1:\nUn+1 - Un\nBackward Euler\n= f(Un+1, tn+1) is Un+1 - t fn+1 = Un . (4)\nt\nThis is an implicit method. To find Un+1, we have to solve equation (4). When\nf is linear in u, we are solving a linear system at each step (and the cost is low if\n\nc2006 Gilbert Strang\nCHAPTER 5. INITIAL VALUE PROBLEMS\nthe matrix is tridiagonal, like I - t K in Example 2). We will comment later on\niterations like Newton's method or predictor-corrector in the nonlinear case.\nThe first example to study is the linear scalar equation u 0 = au. Compare forward\nand backward Euler, for one step and for n steps:\nForward\nUn+1 = (1 + a t)Un leads to Un = (1 + a t)nU0 .\n(5)\nBackward\n(1 - a t)Un+1 = Un leads to Un = (1 - a t)-nU0 .\n(6)\nForward Euler is like compound interest, at the rate a. Each step starts with Un and\nadds the interest a t Un. As the stepsize t goes to zero and we need T/t steps to\nreach time T, this discrete compounding approaches continuous compounding. The\ndiscrete solution Un approaches the continuous solution of u0 = au:\n(1 + a t)T /t approaches e aT\nas t ! 0 .\nThis is the convergence of U to u that we will prove below, more generally. It holds\nfor backward Euler too, because (1 - a t)-1 = 1 + at + higher order terms.\nThe stability question arises for very negative a. The true solution e-atu(0)\nis extremely stable, approaching zero. (If this is your own money, you should change\nbanks and get a > 0.) Backward Euler will be stable because it divides by 1 - a t\n(which is greater than 1 when a is negative). Forward Euler will explode if 1 + a t\nis smaller than -1, because its powers grow exponentially :\nInstability\n1 + a t < -1\nwhen a t < -2\n(7)\n1+at\n-1\nThat is a pretty sharp borderline between stability and instability, at -a t = 2. For\nu 0 = -20u which has a = -20, the borderline is t = 20 =\n. Compare the results\nat time T = 2 from t = 1 (22 steps) and t = 9 (18 steps):\n\nStable t =\n(1 + a t)22 = -\n.012\n\nUnstable t =\n(1 + a t)18 = -\n37.043\nI would describe backward Euler as absolutely stable (A-stable) because it is stable\nwhenever Re a < 0. Only an implicit method can be A-stable. Forward Euler is a\nstable method(!) because it succeeds as t ! 0. For small enough t, it is on the\nstable side of the borderline.\nIn this example a good quality approximation requires more than stability (even\nt = 1 is too big). Those powers of - 9 alternate in sign, while e-20t stays positive.\nAccuracy and Convergence\nSince forward and backward differences are first order accurate, it is no surprise that\nthe errors from forward and backward Euler are O(t). This error e = u - U is\n\nreplacements\nc\n5.1. FINITE DIFFERENCE METHODS\n2006 Gilbert Strang\nt\nt\nt is small\nU = 1 - t < -1\nu = e-t\nU\n- t\nu = e-t\nt\n= 1\ntoo big\nFigure 5.2: Forward Euler (stable and unstable, with -at going above 2).\nmeasured at a fixed time T. As t decreases, so does the new error added at each\nstep. But the number of steps to reach that time increases, keeping n t = T.\nTo see why the error u(T) - U(T) is O(t), the key is stability. We need to know\nthat earlier errors don't increase as they are carried forward to time T. Forward Euler\nis the simplest difference equation, so it is the perfect example to follow through first.\n(The next sections will apply the same idea to partial differential equations.) Euler's\ndifference equation for u0 = f(u, t) = au is\nUn+1 = Un + t f(Un, tn) = Un + a t Un .\n(8)\nThe true solution at time n t satisfies (8) except for a discretization error DE:\nun+1 = un + t u n\n0 + DE = un + a t un + DEn+1 .\n(9)\nThat error DE is of order (t)2 because the second-order term is missing (it\n2 (t)2\ne\ndifference equation for the error\nshould be\nun, but Euler didn't keep it). Subtracting (8) from (9) gives a\n= u - U, propagating forward in time:\nError equation\nen+1 = en + a t en + DEn+1 .\n(10)\nYou could think of this one-step error DEn+1 as a deposit like (t)2 into your account.\nOnce the deposit is made, it will grow or decay according to the error equation. To\nreach time T = N t, each error DEk at step k has N - k more steps to go:\neN = (1 + a t)N -1 DE1 + · · · +\na t)N -k DEk\n(1 +\n+ · · · + DEN .\n(11)\nNow stability plays its part. If a is negative, those powers are all less than 1 (in\nabsolute value)--provided 1 + a t does not go below -1. If a is positive, those\na t)N\naT\npowers of 1 + a t are all less than (e\n= e\n. Then the error eN has N terms\nin (11), and every term is less than c(t)2 for a fixed constant c:\nError bound\n|eN | = |uN -UN | N c (t)2 = c T t .\n(12)\nThe errors along the way, of size (t)2 , combined after N = T/t steps into an\noverall t error. This depended on stability--the local errors didn't explode.\n\nc2006 Gilbert Strang\nCHAPTER 5. INITIAL VALUE PROBLEMS\nNotice that the error growth follows the difference equation in (10), not the dif\nferential equation. The stiff example with a t = (-20)( 1\n9 ) gave a large 1 + a t,\neven when ea t was small. We still call forward Euler a stable method, because\nas soon as t is small enough the danger has passed. Backward Euler also gives\n|eN | = O(t). The problem with these first-order methods is their low accuracy.\nThe local discretization error DE tells us the accuracy. For Euler that error\nDE 1\n2 (t)2u 00 is the first term that Euler misses in the Taylor series for u(t + t).\nBetter methods will capture that term exactly, and miss on a higher-order term. The\ndiscretization error (we find it by comparing u(t+t) with Un+1, assuming u(t) agrees\nwith Un) begins with a power (t)p+1:\nDE c(t)p+1 dp+1u\ndtp+1 .\n(13)\nLocal discretization error\nThe method decides c and p + 1. With stability, T/t steps give a global error of\norder (t)p. The derivative of u shows whether the problem is hard or easy.\nError estimates like (13) appear everywhere in numerical analysis. The 1, -2, 1\nsecond difference has error 12 (x)4u 0000 . Partial differential equations (Laplace, wave,\nheat) produce similar terms. In one line we find c = -1 for backward Euler :\n(t)2\n(t)2\n(un+1 -un) -t u n0\n+1 t u 0 +\nu 00 -t(u 0 + t u n ) -\nun . (14)\nn\nn\nn\nThe global estimate |u -U| ↑ C t max |u 00| shows no error when u is linear and\nu 00 is zero (Euler's approximation of constant slope becomes true). For nonlinear\nequations, the key is in the subtraction of (8) from (9). A \"Lipschitz\" bound L on\n@f/@u replaces the number a in the error equation:\n|f(u, t) -f(U, t)| ↑L|u -U|\ngives\nen+1 ↑(1 + Lt) en + DEn+1 .\n(15)\nSecond-Order Methods\nHere is a first way to increase the accuracy. We could center the equation at the\nmidpoint (n + 2 )t of the step, by averaging f(Un, tn) and f(Un+1, tn+1). The result\nis an implicit second-order method use in MATLAB's ode23t:\nUn+1 - Un\nTrapezoidal rule/Crank-Nicolson\n=\n(fn+1 + fn) .\n(16)\nt\nSo Un+1 -1 t fn+1 = Un + 2 t fn. For our model u0 = f(u) = au, this is\n1 + 1 a t\n(1 - a t) Un+1 = (1 + a t) Un which gives Un+1 =\nUn . (17)\n1 -1 a t\nThe true solution has un+1 = eat un. Problem 1 will find DE (t)3 . The equation\nis stable. So N = T/t steps produce |eN | = |uN -UN | ↑cT(t)2 .\n\nc\n5.1. FINITE DIFFERENCE METHODS\n2006 Gilbert Strang\nHow to improve forward Euler and still keep it explicit ? We don't want Un+1 on\nthe right side of the equation, but we are happy with Un-1 (from the previous step !).\nHere is a combination that gives second-order accuracy in a two-step method:\nUn+1 - Un\n\"Adams-Bashforth\"\n=\nf(Un, tn) -\nf(Un-1, tn-1) .\n(18)\nt\nRemember that f(Un-1, tn-1) is already computed in the previous step, going from\nn-1 to n. So this explicit multistep method requires no extra work, and improves\nthe accuracy. To see how (t)2 comes out correctly, write u for f:\nu 0 -\nun-1 2 un\n0 - (u 0 - t u n) = un + 2 t u n .\nn\nn\nMultiplied by t in (18), that new term 1\n2 (t)2u00 is exactly what Euler missed. Each\nn\nextra term in the difference equation can increase the accuracy by 1.\nA third possibility uses the already computed value Un-1 (instead of the slope\n2 , - 4\nfn-1). With 3\n2 , chosen for second-order accuracy, we get an implicit backward\ndifference method:\n3Un+1 - 4Un + Un-1\nBackward differences\n= f(Un+1, tn+1) .\n(19)\n2t\nWhat about stability ? The implicit method (17) is stable even in the stiff case,\nwhen a is very negative. 1 - 1 a t (left side) will be larger than 1 + 1 a t (right side).\n(19) is also stable. The explicit method (18) will be stable if t is small enough, but\nthere is always a limit on t for stiff systems.\nHere is a quick way to find the stability limit in (18) when a is real. The limit\noccurs when the growth factor is exactly G = -1. Set Un+1 = -1 and Un = 1\nand Un-1 = -1 in (18). Solve for a when f(u, t) = au:\n-2\nStability limit in (18)\n=\na +\na\ngives a t = -1 .\nSo C = 1 . (20)\nt\nThose second-order methods (17)-(18)-(19) are definitely useful ! The reader might\nsuggest including both Un-1 and fn-1 to increase the accuracy to p = 3. Sadly, this\nmethod is violently unstable (Problem 4). We may use older values of U in backward\ndifferences or f(U) in Adams formulas, but including both U and f(U) for even higher\naccuracy produces instability for all t.\nMultistep Methods: Explicit and Implicit\nBy using p older values of either U or f(U) (already computed at previous steps !),\nthe accuracy can be increased to order p. Each ∗U is U(t) - U(t - t) and p = 2\n\nc2006 Gilbert Strang\nCHAPTER 5. INITIAL VALUE PROBLEMS\nis (19):\n\n∗ + 1\n2 ∗2 + · · · + 1\np ∗p\nUn+1\n(Un+1\nn+1) .\nBackward differences\n= t f\n, t\n(21)\nU\nMATLAB's stiff code ode15s varies from p = 1 to p = 5 depending on the local error.\nUsing older values of the right side f(U) gives an Adams-Bashforth method:\nn+1 - Un = t(b1 fn + · · · + bp fn-p+1) with fn = f(Un, tn) .\n(22)\nThe table shows the numbers b up to p = 4, starting with Euler for p = 1.\norder of\nb1\nb2\nb3\nb4\naccuracy\np = 1\np = 2\n3/2\n-1/2\np = 3\n23/12 -16/12\n5/12\np = 4\n55/24 -59/24 37/24 -9/24\nlimit on at\nfor stability\n-2\n-1\n-6/11\n-3/10\nconstant c\nin error DE\n1/2\n5/12\n3/8\n251/720\nThe fourth-order method is often a good choice, although astronomers go above p = 8.\nAt the stability limit G = -1 as in (20). The local error DE c(t)p+1u(p+1) is a\nproblem of step control. Whether it is amplified by later steps is a problem of stability\ncontrol.\nImplicit methods have an extra term c0fn+1 at the new time level. Properly\nchosen, that adds one extra order of accuracy--as it did for the trapezoidal rule,\nwhich is the second method in the new table. Backward Euler is the first:\norder of\naccuracy\np = 1\np = 2\np = 3\np = 4\nc0\nc1\nc2\nc3\n1/2\n1/2\n5/12\n8/12\n-1/12\n9/24 19/24 -5/24 1/24\nlimit on at\nfor stability\n-→\n-→\n-6\n-3\nconstant c\nin error DE\n-1/2\n-1/12\n-1/24\n-19/720\nEvery row of both tables adds to 1, so u 0 = 1 is solved exactly.\nYou see that the error constants and stability are all in favor of implicit methods.\nSo is the user, except when solving for Un+1 becomes expensive. Then there is a\nsimple and successful predictor-corrector method, or else Newton's method.\nP:\nUse the explicit formula to predict a new U≈\nPredictor-\n≈\nn+1\nE:\nUse un+1 to evaluate the right side f≈\nCorrector\nn+1\nC: Use fn≈\n+1 in the implicit formula to correct to a new Un+1.\n\nc\n5.1. FINITE DIFFERENCE METHODS\n2006 Gilbert Strang\nu\nThe stability is much improved if another E step evaluates fn+1 with the corrected\nn+1. In principle we should continue the corrector to convergence. Frequently 1-2\ncorrections are enough, and the extra calculation can be put to use. By comparing the\npredicted U≈\nand corrected Un+1 the code can estimate the local error and change\nn+1\nt:\nc\nlocal error DE c≈-c (Un+1 -Un≈\n+1),\n(23)\nwhere c≈ and c are the error constants in the tables for the predictor and corrector.\nU\nImplicit methods often have a Newton loop inside each time step, to compute\nn+1. The kth iteration in the Newton loop is a linear system, with the Jacobian\nmatrix A(k) = @fi/@uj evaluated at the latest approximation U (k) with t = tn+1.\nij\nn+1\nHere is the kth i to solve Un+1 -c0f(Un+1, tn+1) = old values:\nNewton iteration (I -c0A(k))(U (k+1)\nn+1) = c0f(U (k)\n-U (k)\nn+1, tn+1) + old values .(24)\nn+1\nWhen f(u) = Au is linear, you only need one iteration (k = 0 starts with U (0) = Un).\nn+1\nFor nonlinear f(u), Newton's rapid convergence squares the error at each step (when\nit gets close). The price is a new evaluation of f(u) and its matrix A(k) of derivatives.\nRunge-Kutta Methods\nA-stable methods have no stability limit. Multistep methods achieve high accuracy\nwith one or two evaluations of f. If more evaluations are not too expensive, Runge-\nKutta methods are definitely competitive (and these are self-starting). They are\ncompound one-step methods, using Euler's Un + t fn inside f :\n\nUn+1 -Un\nSimplified Runge-Kutta\n=\nfn + f(Un + t fn, tn+1) .\n(25)\nt\nYou see the compounding of f. For the model equation u 0 = au the result is\n\nun+1 = un +\nt aun + a(un + t aun) = 1 + at + 1 a 2t2\nun = G un .\n(26)\nThis confirms the second-order accuracy; the growth factor G agrees with eat through\n(t)2 . There will be a stability threshold at = C, where |G| = 1:\n\nStability limit\n|G| = 1 + C +\nC2 = 1 (for complex C = a + ib)\nThe complete stability limit is not a point but a closed curve in the complex plane.\nFigure 5.\nshows all the complex numbers C at which |G| = 1.\n\nc2006 Gilbert Strang\nCHAPTER 5. INITIAL VALUE PROBLEMS\nThe famous version of Runge-Kutta is compounded four times and achieves p = 4:\nUn+1 - Un\nRunge-Kutta\n= (k1 + 2k2 + 2k3 + k4)\n(27)\nt\nk1 =\nf(Un, tn)\nk3 =\nf(Un + t k2, tn+1/2)\nk2 =\nf(Un + t k1, tn+1/2)\nk4 =\nf(Un + 2t k3, tn+1)\nFor this one-step method, no special starting instructions are necessary. It is simple\nto change t as you go. The growth factor reproduces eat through 24 a4t4 . The\nerror constant is the next coefficient\n1 . Among highly accurate methods, Runge\nKutta is especially easy to code and run--probably the easiest there is. MATLAB's\nworkhorse is ode45.\nTo prove that the stability threshold at = -2.78 is genuine, we reproduce the\nsolution of u 0 = -100u + 100 sin t. With u(0) = 0, Runge-Kutta gives\nU120\n.\nu(3)\nt = 3\n120 and at = -2.5\nU100\n,000,000,000\nt = 3\n100 and at = -3\n(28)\n= 0 151 =\nwith\n= 670\nwith\nDifferential-Algebraic Equations\nOur system of equations might not come in the form u0 = f(u, t). The more general\nform F(u0, u, t) = 0 appears in many applications. It may not be easy (or possible)\nto solve this system explicitly for u0 . For large electrical networks (VLSI on chips),\ntransistors produce a highly nonlinear dependence on u0 and u.\nHere is a simple model in which solving for the vector u0 is impossible:\nMu 0 = f(u, t)\nwith a singular matrix M (rank r < N) .\n(29)\nSuppose we change variables from u to v, so as to diagonalize M. In the new variables\nwe have r true differential equations and N - r algebraic equations (no derivatives).\nThe system becomes a differential-algebraic equation (DAE):\ndvi\n= fi(v1, . . . , vN , t) i = 1, . . . , r\nDAE\ndt\n(30)\n= fi(v1, . . . , vN , t) i = r + 1, . . . , N .\nThe N - r algebraic equations restrict v to a surface in N-dimensional space. The r\ndifferential equations move the vector v(t) along that surface.\nPetzold [46] developed the DASSL package for solving DAE's in the general form\nF(u0, u, t) = 0, replacing u by a backward difference of u. The best recommendation\nwe can make is to experiment with this software !\n\nc\n5.1. FINITE DIFFERENCE METHODS\n2006 Gilbert Strang\nProblem Set 5.1\nThe error in the trapezoidal (Crank-Nicolson) method (\n) comes from the\ndifference\n\ne at - 1 + (at/2)\n\nat\nat\nat\n= e at -\n1 +\n1 +\n+\n+ · · ·\n1 - (at/2)\nThis involves the two series that everyone should learn: the exponential series\nfor eat and the geometric series 1 + x + x2 + · · · for 1-x .\nMultiply inside the brackets to produce the correct 2 (at)2 . Show that the\n(at)3 term is wrong by c = 12 . Then the error is DE 12 (t)3 u .\nTry Runge-Kutta on u 0 = -100u + 100 sin t with t = -.0275 and -.028.\nThose are close to the stability limit -.0278.\nFor the backward difference error in (19), expand 1\n2 (3eat - 4 + e-at ) - at eat\ninto a series in at. Show that the leading error is - 1\n3 (at)3 so that c = - 1 .\nStability in (19)."
        },
        {
          "category": "Resource",
          "title": "am52.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/307c9b34baca5b281add586a56de5756_am52.pdf",
          "content": "c2006 Gilbert Strang\n5.2\nAccuracy and Stability for ut = c ux\nThis section begins a major topic in scientific computing: Initial-value problems\nfor partial differential equations. Naturally we start with linear equations that\ninvolve only one space dimension x (and time t). The exact solution is u(x, t) and its\ndiscrete approximation on a space-time grid has the form Uj,n = U(jx, nt). We\nwant to know if U is near u--how close they are and how stable U is.\nBegin with the simplest wave equation (first-order, linear, constant coefficient):\n= c\n.\n(1)\nOne-way wave equation\n@u\n@t\n@u\n@x\nWe are given u(x, 0) at time t = 0. We want to find u(x, t) for all t > 0. For simplicity,\nthese functions are defined on the whole line -∗ < x < ∗. There are no difficulties\nwith boundaries (where waves could change direction and bounce back).\nThe solution u(x, t) will have the typical feature of hyperbolic equations: signals\ntravel at finite speed. Unlike the second-order wave equation utt = c2uxx, this first-\norder equation ut = c ux sends signals in one direction only.\nSolution for u(x, 0) = eikx\nThroughout this chapter I will solve for a pure exponential u(x, 0) = eikx . At every\ntime t, the solution remains a multiple Geikx . The growth factor G will depend\non the frequency k and the time t, but different frequencies do not mix. Substituting\nu = G(k, t) eikx into ut = c ux yields a simple ordinary differential equation for G,\nbecause we can cancel eikx . The derivative of eikx produces the factor ik:\ndG\nut = c ux\nis\ndGe ikx = ikc Geikx\nor\n= ikcG .\n(2)\ndt\ndt\nThe growth factor is G(k, t) = eikc t . The initial value is G = 1.\nAn exponential solution to @u = c @u is u(x, t) = e ikc t e ikx = e ik(x+ct) .\n(3)\n@t\n@x\nImmediately we see two important features of this solution:\n1. The growth factor G = eikc t has absolute value |G = 1.\n|\nik(x+ct)\n2. The initial function eikx moves to the left with fixed velocity c, to e\n.\nThe initial value at the origin is u(0, 0) = eik0 = 1. This value u = 1 appears\nat all points on the line x + ct = 0. The initial data propagates along the\ncharacteristic lines x + ct = constant, in Figure 5.3. Right now we know this\nfact for the special solutions eik(x+ct). Soon we will know it for all solutions.\n\nc\n5.2. ACCURACY AND STABILITY FOR UT = C UX\n2006 Gilbert Strang\n\nx\nt\nu(P\nu(0, 0)\nu(Q\nu(\n0)\nP\nx + ct\nQ\nx + ct = X\n) =\n) =\nX,\ncharacteristic line to\n= 0\ncharacteristic line to\nu(0, 0)\nu(X, 0)\nFigure 5.3: The solution u(x, t) moves left with speed c, along characteristic lines.\nFigure 5.3 shows the travel path of the solution in the x-t plane (we are introducing\nthe characteristic lines). Figure 5.4 will graph the solution itself at times 0 and t.\nThat step function combines exponentials eik(x+ct) for different frequencies k. By\nlinearity we can add those solutions.\nSolution for Every u(x, 0)\nIn almost all partial differential equations, the solution changes shape as it travels.\nHere the shape stays the same. All pure exponentials travel at the same velocity c,\nso every initial function moves with that velocity. We can write down the solution:\n@u\n@u\nGeneral solution\n= c\nis solved by u(x, t) = u(x + ct, 0) .\n(4)\n@t\n@x\nThe solution is a function only of x + ct. That makes it constant along characteristic\nlines, where x + ct is constant. This dependence on x + ct also makes it satisfy the\nequation ut = c ux, by the chain rule. If we take u = (x + ct)n as an example, the\nextra factor c appears in @u/@t:\n@u\n@u\n@u\n= n (x + ct)n-1\nand\n= cn (x + ct)n-1\nwhich is c\n.\n@x\n@t\n@x\nA Taylor series person would combine those powers (different n) to produce a large\nfamily of solutions. A Fourier series person combines exponentials (different k) to\nproduce an even larger family. In fact all solutions are functions of x + ct alone.\nHere are two important initial functions--a light flashes or a dam breaks.\nExample 1 u(x, 0) = delta function ∂(x) = flash of light at x = 0, t = 0\nBy our formula (4), the solution is u(x, t) = ∂(x + ct). The light flash reaches the point\nx = -c at the time t = 1. It reaches x = -2c at the time t = 2. The impulse is traveling\nto the left at speed dx/dt = c. In this example all frequencies k are present in equal\n|\n|\namounts, because the Fourier transform of a delta function is a constant.\n\nc2006 Gilbert Strang\nNotice that a point goes dark again as soon as the flash passes through. This is the\nHuygens principle in 1 and 3 dimensions. If we lived in two or four dimensions, the wave\nwould not pass all at once and we wouldn't see clearly.\nExample 2 u(x, 0) = step function S(x) = wall of water at x = 0, t = 0\nThe solution S(x + ct) is the moving step function in Figure 5.4. The wall of water travels\nto the left (one-way wave). At time t, the \"tsunami\" reaches the point x = -ct. The\nflash of light will get there first, because its speed c is greater than the tsunami speed.\nThat is why a warning is possible for an approaching tsunami.\nx\nx\n-ct\nu(x, 0)\nu(\n)\nS(x\nt\nS(x + ct\nt\nx, t\ninitial profile\n) at = 0\nlater profile\n) at time\nFigure 5.4: The wall travels left with velocity c (all waves eikx do too).\nAn actual tsunami is described by the nonlinear \"shallow water equations\" that come\nlater. The feature of finite speed still holds.\nFinite Difference Methods for ut = c ux\nThe one-way wave equation is a perfect example for creating and testing finite dif\nference approximations. We can replace @u/@t by a forward difference with step t.\nHere are four choices for the discrete form of @u/@x at meshpoint ix:\nUi+1 - Ui\n1. Forward =\n= upwind: Low accuracy, conditionally stable for c > 0.\nx\nUi+1 - Ui-1\n2. Centered =\n: Unstable after a few steps as we will prove !\n2x\n3. Lax-Friedrichs: (20) has low accuracy, conditionally stable also for c < 0.\n4. Lax-Wendroff : (14) has extra accuracy, conditionally stable also for c < 0.\nThe list doesn't end there. We have reached a central problem of scientific computing,\nto construct approximations that are stable and accurate and fast. That topic can't\nbe developed on one page, especially when we move to nonlinear equations.\nConditionally stable means that the time step t is restricted. The need for this\nrestriction was noticed by Courant, Friedrichs, and Lewy. When the space difference\n\nc\n5.2. ACCURACY AND STABILITY FOR UT = C UX\n2006 Gilbert Strang\nreaches no further than x + x, there is an automatic stability restriction:\nt\nCFL requirement for stability\nr = c\n1 .\n(5)\nx\nThat number c t/x is often called the Courant number. (It was really Lewy who\nrecognized that r √ 1 is necessary for stability and convergence.) The reasoning is\nstraightforward, based on using the initial value that controls u(x, t):\nThe true solution at (x, t) equals the initial value u(x + ct, 0). Taking n\ndiscrete steps to reach t = n t uses information on the initial values as far\nout as x + n x. If x + ct is further than x + n x, the method can't work:\nt\nCFL condition x + c t √ x + n x\nor c n t √ n x\nor r = c x √ 1 . (6)\nIf the difference equation uses U(x + 2x, t), then CFL relaxes to r √ 2.\nA particular finite difference equation might require a tighter restriction on t for\nstability. It might even be unstable for all ratios r (we hope not). The only route to\nunconditional stability for all t is an implicit method, which computes x-differences\nat the new time t + t. This will be useful later for diffusion terms like uxx. For\nadvection terms (first derivatives), explicit methods with a CFL limitation are usually\naccepted because a much larger t would lose accuracy as well as stability.\nTo repeat, if r > 1 then the finite difference solution at x, t does not use initial\nvalue information near the correct point x← = x + ct. Hopeless.\nAccuracy of the Upwind Difference Equation\nLinear problems with constant coefficients are the ones to understand first. Exactly\nas for differential equations, we can follow each pure exponential eikx . After a single\ntime step, there will be a growth factor in U(x, t) = Geikx . That growth factor\nG(k, t, x) may have magnitude G < 1 or G > 1. This will control stability or\n| |\n| |\ninstability. The order of accuracy (if we compute in the k-! domain) comes from\ncomparing G with the true factor eikct from the differential equation.\nWe now determine that the order of accuracy is p = 1 for the upwind method.\nU(x, t + t) -U(x, t)\nU(x + x, t) -U(x, t)\nForward differences\n= c\n.\n(7)\nt\nx\nWe will test the accuracy in the x-t domain and then the k-! domain. Either way we\nuse Taylor series to check the leading terms. Substituting the true solution u(x, t) in\nplace of U(x, t), its forward differences are\nTime\n[u(x, t + t) -u(x, t)] = ut + t utt +\n(8)\nt\n· · ·\nc\nSpace\n[u(x + x, t) -u(x, t)] = c ux + 2 c x uxx +\n(9)\nx\n· · ·\nOn the right side, ut = c ux is good. One more derivative gives utt = c uxt = c uxx.\nNotice c2 . Then t utt matches c x uxx only in the special case ct = x:\n\nc2006 Gilbert Strang\nct\nt c2 uxx\nequals\nc x uxx\nonly if\nr =\n= 1.\nx\nFor any ratio r = 1, the difference between (8) and (9) has a first-order error. Let\n≥\nme show this also in the k-! Fourier picture and then improve to second-order.\nFix the ratio r = ct/x as x ! 0 and t ! 0. In the difference equation (7),\nwrite each new value at time t + t as a combination of two old values of U:\nU(\nt\n1 - r) U(\nr U(x\n) .\n(10)\nDifference equation\nx, t + ) = (\nx, t) +\n+ x, t\nStarting from U(x, 0) = eikx we quickly find the growth factor G at time t:\nikx\nAfter 1 step (1 -r)e ikx + r e ik(x+x) =\n\n1 - r + r eikx e\n= G e ikx .\n(11)\nTo test the accuracy, compare this G = Gapprox to the exact growth factor eickt .\nUse the power series 1 + x + x2/2! + · · · for any ex:\nAccuracyGapprox = 1 -r + r e ikx = (1 -r) + r + r(ikx) + r (ikx)2 +\n· · ·\nirkx\nGexact = e ickt = e\n= 1 + irkx +\n(irkx)2 +\n(12)\n· · ·\nThe first terms agree as expected. Forward differences replaced derivatives, and the\nmethod is consistent. We saw ut = c ux in comparing (8) with (9). The next terms\ndo not agree unless r = r2:\nCompare r(ikx)2 with 1 r 2(ikx)2 . Single-step error of order (kt)2 . (13)\nAfter 1/t steps, those errors of order k2(t)2 give a final error O(k2t). Forward\ndifferences are only first order accurate, and so is the whole method.\nThe special case r = 1 means ct = x. The difference equation is exactly\ncorrect. The true and approximate solutions at (x, t) are both u(x + x, 0). We\nare on the characteristic line in Figure 5.3. This is an interesting special case (the\ngolden t, but hard to repeat in scientific computing when c varies).\nConclusion\nExcept when r = r2 , the upwind method is first-order accurate.\nHigher Accuracy for Lax-Wendroff\nTo upgrade the accuracy, we need to match the 1 t utt error term in the forward time\ndifference by an additional space difference that gives 2 t c2uxx. This is achieved by\nthe Lax-Wendroff method:\nU(x, t + t) -U(x, t)\nU(x + x, t) -U(x -x, t)\n= c\nt\n2x\nt 2\nU(x + x, t) -2U(x, t) + U(x -x, t)\n(14)\n+\nc\n.\n(x)2\nSubstituting the true solution, that second difference produces 1 c2t uxx plus higher\norder terms. This cancels the 1 t utt error term in the time difference, computed\n\nc\n5.2. ACCURACY AND STABILITY FOR UT = C UX\n2006 Gilbert Strang\nin equation (8). (Remember utt = cuxt = c2uxx. The centered difference has no x\nterm.) Thus Lax-Wendroff has second-order accuracy.\nTo see this in the k-! frequency domain, rewrite the LW difference equation (14):\nU (x, t + t) = (1 - r 2)U (x, t) + (r 2 + r)U (x + x, t) + (r 2 - r)U (x - x, t) . (15)\nSubstitute U (x, t) = eikx to find the one-step growth factor G at time t + t:\nikx\n-ikx\nGrowth factor for LW\nG = (1 - r 2) + (r 2 + r)e\n+\n(r 2 - r)e\n.\nExpanding eikx and e-ikx in powers of ik x, this becomes\n1 2\nG = 1 + r(ikx) + r (ikx)2 + O(kx)3 .\n(16)\nComparing with Gexact = eirkx in equation (12), three terms agree. So the one-step\nerror is of order (kx)3 . After 1/t steps the second-order accuracy of Lax-Wendroff\nis confirmed.\nFigure 5.5 shows by actual computation the improvement in accuracy. For a first-\norder method, the \"wall of water\" is smeared out. High frequencies have growth\nfactors G(k) much smaller than 1. There is too much dissipation. For the first-order\n|\n|\nLax-Friedrichs method, the dissipation is even worse (Problem 2). The second-order\nLax-Wendroff method stays much closer to the discontinuity. But it's not perfect--\nthose oscillations are not good.\nFor an ideal difference equation, we want to add enough dissipation very close to\nthe shock, to avoid that oscillation (the Gibbs phenomenon). A lot of thought has\ngone into high resolution methods, to capture shock waves cleanly.\nGreater accuracy is achievable by including more terms in the difference equation.\nIf we go from the three terms in Lax-Wendroff to five terms, we can reach fourth-\norder accuracy. If we use all values U (jx, nt) at every time step, which requires\nmore work, we can achieve spectral accuracy. Then the error decreases faster than\nany power of x, provided u(x, t) is smooth enough to allow derivatives of all orders.\nSection\ngives a separate discussion of this spectral method.\nStability of the Four Finite Difference Methods\nNow we turn from accuracy to stability. Accuracy requires G to stay close to the\ntrue eickt . Stability requires G to stay inside the unit circle. We need G √ 1 for all\n| |\nfrequencies k or the finite difference approximation Gneikx will blow up.\nWe now check whether or not G √ 1, in the four methods.\n| |\n1. Forward differences in space and time: U/t = c U/x (upwind).\n\nc2006 Gilbert Strang\nRecall from equation (11) that G = 1 - r + reikx . If the Courant number r is\nbetween 0 and 1, the triangle inequality gives G √ 1:\n| |\nStability for 0 r 1\n1 - r + re\n= 1 - r + r = 1 .\n(17)\n|G| √|\n|\n|\nikx |\nThis sufficient condition 0\nc t/x √ 1 is exactly the same as the Courant-\n√\nFriedrichs-Lewy necessary condition ! They reasoned that U(x, nt) depends on the\ninitial values between x and x + nx. That domain of dependence must include\nthe point x + c nt. (Otherwise, changing the initial value at the point x + c nt\nwould change the true solution u but not the approximation U.) Then c nt must\nlie between 0 and nx, which means that 0 √ r √ 1.\nFigure 5.5 shows G in the stable case r = 3 and the unstable case r = 4 (when t\nis too large). As k varies, and eikx goes around a unit circle, the complex number\nG = 1 - r + reikx goes in a circle of radius r. The center is 1 - r. Always G = 1 at\nzero frequency (constant solution, no growth).\n1-r\nStable\n|G|<1\nr\nunit circle\nG = 1 - r + reikx\nG = 1-2r\nG = 1\n1-r\nr\nk = 0\nkx =\nUnstable\n|G| > 1\nFigure 5.5: Stable (upwind) and unstable (centered): CFL numbers r = 3 and r = 4 .\n2. Forward difference in time, centered difference in space.\nThis combination is never stable ! The shorthand Uj,n will stand for U(jx, nt):\nUj,n+1 - Uj,n\nUj+1,n - Uj-1,n\nr\n= c\nor\nUj,n+1 = Uj,n +\n(Uj+1,n - Uj-1,n) . (18)\nt\n2x\nThose coefficients 1 and r/2 and -r/2 go into the growth factor G, when the solution\nis a pure exponential and eikx is factored out:\nr\nr -ikx\ne\nUnstable: |G| > 1\nG = 1 +\nikx\ne\n= 1 + ir sin kx .\n(19)\n- 2\nThe real part is 1. The magnitude is G → 1. Its graph is on the left side of Figure 5.6.\n| |\n\nc\n5.2. ACCURACY AND STABILITY FOR UT = C UX\n2006 Gilbert Strang\n3. Lax-Friedrichs Method (upwind-downwind).\nWe can recover\n+ Uj-1,n) of its neighbors:\nReplace Uj,n\n2 (Uj+1,n\nstability for centered differences by changing the time difference.\nby the average\nUj,n+1 - 1\n2 (Uj+1,n + Uj-1,n)\nUj+1,n -Uj-1,n\nLax-Friedrichs\n= c\n.\n(20)\nt\n2x\nTwo old values Uj+1,n and Uj-1,n produce each new value Uj,n+1. Moving terms to\nthe right-hand side, the coefficients are 2\nr\n2 (1 -r\n(1 + ) and\n). The growth factor is\n1 + r\n-ikx\nG =\ne ikx + 1 -r e\n= cos kx + ir sin kx .\n(21)\nThe absolute value is G 2 = (cos kx)2 + r2(sin kx)2 . In Figure 5.6, G √ 1 when\n| |\n| |\nr √ 1. This stability condition agrees again with the CFL condition.\nG = 1 + ir sin kx\nLax-Friedrichs\nStable for r2 1\n|G| > 1\nUnstable\n|G| < 1\nG\nkx + ir sin kx !\nr\nForward in time-centered in space\n= cos\nFigure 5.6: Equation (18) is unstable for all r. Equation (20) is stable for r2 √ 1.\nNotice that c and r can be negative. The wave can go either way! This will\nbe useful for the two-way wave equation, but the accuracy is still first-order. The\nLax-Friedrichs G matches the next term in the exact growth factor only if r2 = 1:\nG = cos kx + ir sin kx = 1 + irkx - (kx)2 +\n(22)\nG\n· · ·\nexact = e ikrx = 1 + irk x + 1 i2 r 2(k x)2 +\n· · ·\nIn the exceptional cases r = 1 and r = -1, G agrees with Gexact . Staying exactly\non the characteristic line, Uj,n+1 matches the true u(jx, t + t). For r2 < 1, Lax-\nFriedrichs has an important advantage and disadvantage:\nGood Each new Uj,n+1 is a positive combination of old values.\nNot good The accuracy is only first-order.\nProblem 6 will show that second-order is impossible with positive coefficients.\n\nc2006 Gilbert Strang\n4. Lax-Wendroff Method (second-order accurate).\nThe LW difference equation (14) combines Uj,n and Uj-1,n and Uj+1,n to compute the\nnew value Uj,n+1. The coefficients of these three old values go into G:\nLax-Wendroff\nG = (1 - r 2) + 1(r 2 + r)e ikx + 1(r 2 - r)e -ikx .\n(23)\nThis is G = 1 - r2 + r2 cos kx + ir sin kx. At the dangerous frequency kx = ,\nthe growth factor is 1 - 2r2 . That stays above -1 if r2 √ 1.\nProblem 5 shows that G √ 1 for every kx. Lax-Wendroff is stable when\n| |\never the CFL condition r2 1 is satisfied. Again the wave can go either way\n(or both ways) since c and r can be negative. This is the most accurate of the five\nmethods in Figure 5.7.\nupwind\nwrong way\ncentered\nLax-Friedrichs\nLax-Wendroff\nstable\nunstable\nunstable\nstable\nstable\nif r √ 1\nall t\nall t\nif |r| √ 1\nif |r| √ 1\nFigure 5.7: Difference methods for the one-way wave equation ut = cux.\nEquivalence of Stability and Convergence\nDoes the discrete solution U approach the true solution u as t ! 0 ? The ex\npected answer is yes. But there are two requirements for convergence, and one of\nthem--stability --is by no means automatic. The other requirement is consistency --\nthe discrete problem must approximate the correct continuous problem. The fact that\nthese two properties are sufficient for convergence, and also necessary for convergence,\nis the fundamental theorem of numerical analysis:\nLax equivalence theorem\nStability is equivalent to convergence, for a consistent\napproximation to a well-posed linear problem.\nLax proved the equivalence theorem for initial-value problems. The rate of conver\ngence is given in (26). The theorem is equally true for boundary-value problems, and\nfor the approximation of functions, and for the approximation of integrals. It applies\nto every discretization, when the given problem Lu = f is replaced by LhUh = fh.\nAssuming the inputs f and fh are close, we will prove that u and Uh are close--\nprovided Lh is stable. The key points of the proof take only a few lines when the\nequation is linear, and you will see the essence of this fundamental theorem.\nSuppose f is changed to fh and L is replaced by Lh. The requirements are\n\nc\n5.2. ACCURACY AND STABILITY FOR UT = C UX\n2006 Gilbert Strang\nConsistency: fh ! f and Lhu ! Lu for smooth solutions u.\nWell-posed:\nThe inverse of L is bounded: ≤u≤=\n.\n≤L-1f ≤√C≤f ≤\nStability:\nThe inverses L-1 remain uniformly bounded: ≤L-1\n.\nh\nh fh≤√C≤fh≤\nUnder those conditions, the approximation Uh = L-1fh will approach u as h goes to\nh\nzero. We subtract and add L-1Lu = L-1f when u is smooth:\nh\nh\nConvergence\nu -Uh = L-1(Lhu -Lu) + L-1(f -fh) ! 0 .\n(24)\nh\nh\nConsistency controls the quantities in parentheses (they go to zero). Stability controls\nthe operators L-1 that act on them. Well-posedness controls the approximation of\nh\nall solutions by smooth solutions. Then always Uh converges to u.\nIf stability fails, there will be an input for which the approximations Uh = L-1f\nh\nare not bounded. The uniform boundedness theorem produces this bad f , from the\ninputs fh on which instability gives ≤L-1\n. Convergence fails for this f .\nh fh≤! ∗\nA perfect equivalence theorem goes a little further, after careful definitions:\nConsistency + Stability () Well-posedness + Convergence .\nOur effort will now concentrate on initial-value problems, to estimate the error (the\nconvergence rate) in u -Uh. The parameter h becomes t. We take n steps.\nThe Rate of Convergence\nConsistency means that the error at each time step goes to zero as the mesh is refined.\nOur Taylor series estimates have done more: The order of accuracy gives the rate\nthat this one-step error goes to zero. The problem is to extend this local rate to a\nglobal rate of convergence, accumulating the errors over n time steps.\nLet me write S for a single finite difference step, so U(t + t) = S U(t). The\ncorresponding step for the differential equation will be u(t + t) = R u(t). Then\nconsistency means that Su is close to Ru, and the order of accuracy p tells how close:\nAccuracy of discretization ≤Su -Ru≤√C1(t)p+1 for smooth solutions u.\nWell-posed problem\n≤Rn\nu≤for n t √T .\nu≤√C2≤\nStable approximations\n≤SnU ≤√C3≤U ≤for n t √T .\nThe difference between U = Snu(0) and the true u = Rnu( ) is (Sn -Rn)u( ).\nThe key idea is a \"telescoping identity\" that involves n single-step differences S -R:\nSn -Rn = Sn-1(S -R) + Sn-2(S -R)R + · · · + (S -R)Rn-1 .\n(25)\nEach of those n terms has a clear meaning. First, a power Rk carries u(0) to the true\nsolution u(k t). Then (S -R)u(k t) gives the error at step k of order (t)p+1 .\nThen powers of S carry that one-step error forward to time nt. By stability, this\n\nP\nP\nP\nP\nP\nP\nP\nP\nc2006 Gilbert Strang\namplifies the error by no more than C3. There are n √ T/t steps. The final rate\nof convergence for smooth solutions is (t)p:\nT\n≤U(n t) - u(n t)≤ = ≤(Sn - Rn)u(0)≤ √ C1C2C3\n(t)p+1 = C1C2C3 T (t)p .\nt\nu\n(26)\nNotice how smoothness was needed in the Taylor series (8) and (9), when t and\nx multiplied utt and uxx. That first-order accuracy would not apply if u or ut or\nx had a jump. Still the order of accuracy p gives a practical estimate of the overall\napproximation error u - U. The problem of scientific computing is to get beyond\np = 1 while maintaining stability and speed.\nProblem Set 5.2\nIntegrate ut = c ux from -∗ to ∗ to prove that mass is conserved: dM/dt = 0.\nMultiply by u and integrate uut = c uux to prove that energy is also conserved:\nZ 1\nZ 1\nM(t) =\nu(x, t) dx and E(t) = 1\n(u(x, t))2 dx stay constant in time.\n-1\n-1\nSubstitute the true u(x, t) into the Lax-Friedrichs method (21) and use ut = cux\nand utt = c2uxx to find the coefficient of the numerical dissipation uxx.\nThe difference equation Uj,n+1 =\namUj+m,n has growth factor G =\nameimkx .\nShow consistency with eickt (first-order accuracy at least) when\nam = 1 and\nmam = ct/x = r.\nThe condition for second-order accuracy is\nm am = r2, from the Taylor se\nries. Check this for Lax-Wendroff with a0 = 1-r2, a1 = 2 (r2+r), a-1 = 2 (r -r).\nWith nonnegative coefficients am → 0, the Schwarz inequality (\nmpampam)2 √\n(\nm2am)(\nam) becomes an equality r2 = r . This equality only happens if\nmpam = (constant)pam. Second-order is impossible with am → 0, unless the\ndifference equation has only one term Uj,n+1 = Uj+m,n.\nThe Lax-Wendroff method has G = 1 - r2 + r2 cos kx + ir sin kx. Square the\nreal and imaginary parts to get (eventually!) G 2 = 1 - (r2\n4)(1 - cos kx)2 .\n- r\nProve stability, that G 2 √ 1 if r √ 1.\n| |\n| |\nSuppose the coefficients in a linear differential equation change as t changes.\nThe one-step solution operators become Sk and Rk, for the step from k t to\n(k + 1)t. After n steps, products replace the powers Sn and Rn in U and u:\nU(n t) = Sn-1Sn-2 . . . S1S0 u(0) and u(n t) = Rn-1Rn-2 . . . R1R0 u(0) .\nChange the telescoping formula (25) to produce this U - u. Which parts are\ncontrolled by stability ? Which parts by well-posedness (= stability of the dif\nferential equation) ? Consistency still controls Sk - Rk .\n\n5.2. ACCURACY AND STABILITY FOR UT = C UX\nc2006 Gilbert Strang\nEven an unstable method will converge to the true solution u = eickteikx for\neach separate frequency k. Consistency assures that the single-step growth\nfactor G is 1 + ick t + O(t)2 . Then for t = n t,\nGn =\n\n1 + ickt\nn + O( 1\nn2 )\nn\n-! e ickt which is convergence.\nHow can we have convergence for each u(0) = eikx and still prove divergence for\na combination of frequencies u(0) = P1\n-1 ck eikx ?\nThe upwind method with r > 1 is unstable because the CFL condition fails.\nBy Problem 3, it does converge to eik(x+ct) based on values of u(x, 0) = eikx\nthat do not reach as far as x + ct. The method must be finding a \"correct\"\nextrapolation of eikx . So propose an initial u(x, 0) for which convergence fails."
        },
        {
          "category": "Resource",
          "title": "am53.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/8eaa23367474c809cc0816f24fdacc7f_am53.pdf",
          "content": "c\n2006 Gilbert Strang\n5.3\nThe Wave Equation and Staggered Leapfrog\nThis section focuses on the second-order wave equation utt = c uxx. We find\nthe exact solution u(x, t). Accuracy and stability are confirmed for the leapfrog\nmethod (centered second differences in t and x). This two-step method requires that\nwe rethink the growth factor G, which was clear for a single step. The result will be\np = 2 for the order of accuracy, and ct/x 1 for stability.\nv\nIt is useful to rewrite the wave equation as a first-order system. The components\n1 and v2 of the vector unknown can be @u/@t and c @u/@x. Then we are back to a\nsingle-step growth factor, but G is now a 2 by 2 matrix.\nSecond-order accuracy extends to this system vt = Avx if we use a staggered\nmesh. The mesh for v2 lies in between the mesh for v1. This has become the\nstandard method in computational electromagnetics (solving Maxwell's equations).\nThe physical laws relating the electric field E and the magnetic field H are beautifully\ncopied by the difference equations on a staggered mesh. The mesh becomes especially\nimportant in more space dimensions (x-y and x-y-z), and in finite volume methods.\nThis section goes beyond the one-way wave equation in at least five ways:\n1.\nTwo characteristic lines x+ ct = Cleft and x- ct = Cright go through each (x, t).\n2.\nThe leapfrog method involves three time levels t + t, t, and t - t.\n3.\nFirst-order systems have vector unknowns v(x, t) and growth matrices G.\n4.\nStaggered grids give the much-used FDTD method for Maxwell's equations.\n5.\nMore space dimensions lead to new CFL and vN stability conditions on t.\nWith -≈ < x < ≈, we don't yet have boundary conditions in space. And we are\nnot facing real problems like utt = c2(x) uxx + F eikx , with a high frequency forcing\nterms (k >> 1) and a varying speed c(x).\nSolution of the Wave Equation\nExactly as for the one-way equation ut = cux, we solve the two-way wave equation\nutt = c2uxx for each pure exponential. That allows us to separate the variables.\nikx:\nThe space variable is in eikx , and we look for solutions u(x, t) = G(t)e\n@2u\n@2u\nEach k\n= c\nbecomes d2G e ikx = i2 c 2k2G e ikx .\n(1)\n@t2\n@x2\ndt2\nThus Gtt = i2c2k2G. This second-order equation has two solutions, Gleft = eickt and\nGright = e-ickt . So there are two waves with speed c:\nuright(x, t) = eik(x-ct)\nPure waves\nuleft(x, t) = eik(x+ct)\nand\n.\n(2)\n\n5.3. THE WAVE EQUATION AND STAGGERED LEAPFROG c\n2006 Gilbert Strang\nCombinations of left-going waves eik(x+ct) will give a general function F1(x + ct).\nCombinations of eik(x-ct) give F2(x - ct). The complete solution includes both:\nu(x, t) = uleft(x, t) + uright(x, t) = F1(x + ct) + F2(x - ct) .\n(3)\nWe need those two functions to match an initial shape u(x, 0) and velocity ut(x, 0):\nAt t = 0\nu(x, 0) = F1(x) + F2(x) and ut(x, 0) = c F 1\n0(x) - c F 2\n0(x) .\n(4)\nSolving for F1 and F2 gives the unique solution that matches u(x, 0) and ut(x, 0):\nu(x, t) = u(x + ct, 0) + u(x - ct, 0)\n1 Z x+ct\nSolution\n+\nut(x, 0) dx .\n(5)\n2c\nx-ct\nThe \"domain of dependence\" of u(x, t) includes the initial values from x - ct to x + ct.\nThat domain is on the left side of Figure 5.9, bounded by the characteristic lines.\nExample 1 Starting with zero velocity, ut(x, 0) = 0, the integrated term in formula (5)\nis zero. A step function S(x) (wall of water ) will travel left and right along characteristic\nlines, as in Figure 5.8. It reaches the points x = 1 and x = -1 at time t = 1/c:\nTwo walls\nu(x, t) =\nS(x + ct) +\nS(x - ct) = 0 or\nor 1 .\nBy time t, the initial jump at x = 0 affects the solution between x = -ct and x = ct.\nThat is the \"domain of influence\" of the point x = 0, t = 0.\nTime 0\nu(x, 0) = 1\nTime t u(x, t) = 1\nu(x, t) = 1\ncharacteristic\nx\nx\nu(x, 0) = 0\n-ct\nct\nFigure 5.8: Two-wall solution to the wave equation starting from a step function.\nThe Semidiscrete Wave Equation\nU\nU\nLet me start by discretizing only the space derivative uxx. The second difference\nj+1 - 2Uj + Uj-1 is the natural choice, divided by (x)2 . For the approximations\nj (t) at the meshpoints x = jx, we have a family of ODEs in the time direction\n(method of lines):\nSemidiscrete utt = c2uxx\nU 00\nj =\nc2\n(x)2 (Uj+1 - 2Uj + Uj-1 .\n(6)\n\nc\n2006 Gilbert Strang\nAgain we follow every exponential, looking for Uj (t) = G(t)eikjx . Substitute into (6)\nand cancel the common factor eikjx . Instead of Gtt = -c2k2G we have\nc\nc\n-ikx\nGrowth equation Gtt = (x)2 (e ikx - 2 + e\n)G =\n(x)2 (2 - 2 cos kx)G .\n(7)\n-\nThe correct right side -c2k2G is multiplied by a factor F 2 . This F 2 turns up so often\nthat we need to recognize it! Use 2 - 2 cos α = 4 sin2(α/2):\nSinc squared F 2 = 2 - 2 cos kx = 4 sin2(kx/2)\nsin(kx/2) 2\n=\n.\n(8)\nk2(x)2\nk2(x)2\nkx/2\nThe \"sinc function\" is defined as sin α divided by α. When α = kx is small, this is\n1 + 0(α2). Then F 2 near 1 and equation (7) is close to the correct Gtt = -c2k2G.\nFor every kx, the growth equation (7) has two exponential solutions:\nSemidiscrete growth\nGtt = -c 2F 2k2G gives G(t) = e ±icF kt .\n(9)\nThe wave speed c is multiplied by F to give the numerical \"phase velocity\" cF. Notice\nthat F depends on k. Different frequencies eikx are traveling at different speeds cF(k).\nThis is dispersion and we will see it again in Section 5.\n.\nI will mention that the \"group velocity\"--the derivative of cFk with respect to\nk--is a more important quantity than the phase velocity cF.\nThe semidiscrete form suggests a good algorithm for the wave equation, if we have\nboundary conditions (say u = 0 along the lines t = 0 and t = ω). If h = x =\n\nn+1 ,\nthis interval has interior meshpoints. The n by n second difference matrix is the\nspecial K from earlier chapters (but now we have -K):\n2c\nSemidiscrete with boundaries\nU 00(t) =\nKU .\n(10)\n(x)2\nThis is just the equation MU 00 + KU = 0 of oscillating springs in Section 2.2.\nThe n eigenvalues of K are positive numbers 2 - 2 cos jx. The only change\nfrom the equation on an infinite line is that j takes only the values 1, 2, . . . , n. The\noscillations go on forever as in (8), the energy is conserved, and now the waves bounce\nback from the boundaries instead of continuing out to x = ±≈.\nLeapfrog from Centered Differences\nA fully discrete method also approximates utt by a centered differences. This time\ndifference \"leaps over \" the space difference at t = nt:\nLeapfrog method\nUj,n+1 - 2Uj,n + Uj,n-1\n2 Uj+1,n - 2Uj,n + Uj-1,n\n= c\n.\n(11)\n(t)2\n(x)2\nThis has two key differences from the 5-point molecule for uxx + uyy = 0 (Laplace).\nFirst, utt - c2uxx has a minus sign. Second, we have two conditions at t = 0 and no\n\n5.3. THE WAVE EQUATION AND STAGGERED LEAPFROG c\n2006 Gilbert Strang\nconditions at a later time. We are marching forward in time (marching with Laplace's\nequation would be totally unstable). A separate calculation for the first time step\ncomputes Uj,1 from the initial shape u(x, 0) and the velocity ut(x, 0).\nThe accuracy of leapfrog is normally second-order. Substitute the true u(x, t)\ninto (11), and use the Taylor series for second differences (Section 1.2). The first\nterms in the local error give consistency.\nSecond-order\nutt + 1 (t)2 utttt +\n= c 2(uxx +\n(x)2 uxxxx +\n(12)\n· · ·\n· · ·).\nIn this case utttt = c2uxxtt = c uttxx = c uxxxx. The two sides of (12) differ by\nLocal discretization error\n1 [(t)2 c 4 -(x)2 c ]uxxxx +\n(13)\n· · ·\nAgain ct = x is the golden time step that follows the characteristic exactly. The\ntwo triangles in Figure 5.9 become exactly the same in this borderline case r = 1.\nThe CFL reasoning shows instability for r > 1. We now show that r 1 is stable.\nlines\n\nt\nx\nu(\n)\nc and - 1\nc\n\nU (\nt)\nt\nx\n- t\nx\nx, t\ncharacteristic\nslopes\nx, n\nslope\nslope\nx -ct\nx + ct\nx -nx\nx + nx\nFigure 5.9: Domains of dependence: u from wave equation and U from leapfrog.\nStability of the Leapfrog Method\nA difference equation must use the initial conditions in this whole interval, to have\na chance of converging to u(x, t). The domain of dependence for U must include the\ndomain of dependence for u. The slopes must have t/x 1/c. Since convergence\nrequires stability, we have a Courant-Friedrichs-Lewy condition on t:\nr = c t/x 1.\nCFL condition The leapfrog method will require\nFor a double-step difference equation, we still look for pure solutions U (x, nt) =\nGneikx , separating time from space. In the leapfrog equation (11) this gives\nGn+1 -2Gn + Gn-1\nikx -2 + e-ikx\nikx\nikx\ne\n= c 2Gn e\ne\n.\n(t)2\n(x)2\nG\nSet r = ct/x and cancel Gn-1eikx . This leaves a quadratic equation for G:\n2 -2G + 1 = r 2 G (2 cos kx -2) .\n(14)\nx\n\nc\n2006 Gilbert Strang\nThe two-step leapfrog equation allows two G's (of course !). For stability, both must\nsatisfy G 1 for all frequencies k. Rewrite equation (14) for G:\n| |\nG2 -2\n\n1 -r 2\nGrowth factor equation\n+ r cos kx G + 1 = 0 .\n(15)\nThe roots of G2 -2aG + 1 = 0 are G = a ±\np\na -1. Everything depends on that\nsquare root giving an imaginary number, when a2 = [1 -r2 + r2 cos kx]2 1:\nIf a 2 1 then G = a ± i\np\n1 -a2 has\n= a + (1 -a 2) = 1 .\n|G| 2\nThe CFL condition r 1 does produce a2 1 and the leapfrog method is stable:\n1 -r 2\nStability\nIf r 1 then a =\n+ r cos kx (1 -r ) + r 2 = 1 .\n(16)\n| |\n|\n|\nAn unstable r > 1 would produce a = 1 -2r2 > 1 at the dangerous kx = ω.\n| |\n|\n|\nThen both G's are real, and their product is 1, and one of them has G > 1.\n| |\nNote 1\nSuppose r is exactly 1, so that ct = x. At this \"golden ratio\" we\nexpect perfect accuracy. The middle terms -2Uj,n and -2r2Uj,n cancel in the leapfrog\nequation (11), leaving a complete leap over the center points at (j, n) when r = 1:\nExact leapfrog\nUj,n+1 + Uj,n-1 = Uj+1,n + Uj-1,n .\n(17)\nThe difference equation is satisfied by u(x, t), because it is satisfied by every wave\nU (x + ct) and U (x -ct). Take Uj,n = U (jx + cnt) and use ct = x:\nU\nUj,n+1 and Uj+1,n\nare both equal to\nU (jx + cnt + x)\nj,n-1 and Uj-1,n\nare both equal to\nU (jx + cnt -x)\nSo (17) is satisfied by all traveling waves U (x + ct), and similarly by U (x -ct).\nNote 2\nYou could also apply leapfrog to the one-way equation ut = c ux:\nUj,n+1 -Uj,n-1 = ct\nx (Uj+1,n -Uj-1,n).\n(18)\nOne-way leapfrog\nNow the growth factor equation is G2 -2(ir sin kx)G -1 = 0. Problem\nconfirms that the stability condition is again r 1. In that stable case, one growth\nfactor is sensible and the other is strange:\nG1 = e ir sin kx\nickt\nand G2 =\ne -ir sin kx\n(19)\ne\nG\n-\n-1 .\n1 and G2 are exactly on the unit circle. With G = 1 there is no room to move.\n| |\nNumerical diffusion ∂(Uj+1,n -2Uj,n + Uj-1,n) usually adds extra stability, but not\nhere. So leapfrog for first-order equations can be dangerous.\nSection 5.4 will study the convection-diffusion equation ut = c ux + d uxx.\n\nZ Z Z\n5.3. THE WAVE EQUATION AND STAGGERED LEAPFROG c\n2006 Gilbert Strang\nWave Equation in Higher Dimensions\nThe wave equation extends to three-dimensional space (with speed set at c = 1):\n3D Wave equation\nutt = uxx + uyy + uzz .\n(20)\nWaves go in all directions, and the solution is a superposition of pure harmonics.\nThese plane waves now have three wave numbers k, π, m, and frequency w:\nExponential solutions\nu(x, y, z, t) = e i(!t + kx + `y + mz) .\nSubstituting into the wave equation gives !2 = k2 + π2 + m . So there are two\nfrequencies ± !. These exponential solutions combine to match the initial wave height\nu(x, y, z, 0) and its velocity ut(x, y, z, 0).\nSuppose the initial velocity is a three-dimensional delta function (x, y, z):\nλ(x, y, z) = λ(x)λ(y)λ(z) gives\nf(x, y, z) λ(x, y, z) dV = f(0, 0, 0) .\n(21)\nThe resulting u(x, y, z, t) will be the fundamental solution of the wave equation. It is\nthe response to the delta function, which gives equal weight to all harmonics. Rather\nthan computing that superposition we find it from the wave equation itself. Spherical\nsymmetry greatly simplifies uxx + uyy + uzz , when u depends only on r and t:\n@2 u\n@2 u\n2 @u\nSymmetry produces u(r, t)\n=\n+\n.\n(22)\n@t2\n@r2\nr @r\nMultiplying by r, this is a one-dimensional equation (ru)tt = (ru)rr ! Its\nsolutions ru will be functions of r - t and r + t. Starting from a delta function is like\nsound going out from a bell, or light from a point source. The solution is nonzero\nonly on the sphere r = t. So every point hears the bell only once, as the sound wave\npasses by. An impulse in 3D produces a sharp response (this is Huygen's principle).\nIn 2D, the solution does not return to zero for t > r. We couldn't hear or see\nclearly in Flatland. You might imagine a point source in two dimensions as a line\nsource in the z-direction in three dimensions. The solution is independent of z, so it\nsatisfies utt = uxx + uyy . But in three dimensions, spheres starting from sources along\nthe line continue to hit the listener. They come from further and further away, so the\nsolution decays--but it is not zero. The wave front passes, but waves keep coming.\nEXERCISE ON EQ.(26)\nLeapfrog Method in Higher Dimensions\nIn one dimension, two characteristics go out from each point (x, 0). In 2D and 3D,\na characteristic cone goes out from (x, y, 0) and (x, y, z, 0). It is essential to see how\nthe stability condition changes from r = ct/x 1.\n\n\"\n#\n\nc\n2006 Gilbert Strang\nThe leapfrog method replaces uxx and uyy by centered differences at time nt:\nUn+1 - 2Un + Un-1\n2 Un\ny Un\nLeapfrog for utt = uxx + uyy\n=\nx\n+\n.\n(t)2\n(x)2\n(y)2\nU0 and U1 come from the given initial conditions u(x, y, 0) and ut(x, y, 0). We look\nfor a solution Un = Gneikxei`y with separation of variables. Substituting into the\nleapfrog equation and canceling Gn-1eikxei`y produces the 2D equation for two G's:\nG2 - 2G + 1\n(2 cos k x - 2)\n(2 cos π y - 2)\nGrowth factor\n= G\n+ G\n. (23)\n(t)2\n(x)2\n(y)2\nAgain this has the form G2 - 2aG + 1 = 0. You can see a in brackets:\n\nt\nt\nG2 - 2 1 -\n(1 - cos k x) -\ny\n(1 - cos π y) G + 1 = 0 .\n(24)\nx\nBoth roots must have G = 1 for stability. This still requires -1 a 1. When the\n| |\ncosines are -1 (the dangerous value) we find the stability condition for leapfrog:\n\nt\nt\nt 2\nt 2\nStability\n-1 1 - 2\nneeds\n+\n1.\n(25)\nx\n- 2\ny\nx\ny\nFor x = y on a square grid, this is t x/\np\n2. In three dimensions it would be\nt x/\np\n3. Those also come from the CFL condition, that the characteristic cone\nmust lie inside the pyramid that gives the leapfrog domain of dependence. Figure 5.10\nshows the cone and pyramid just touching, when t = x/\np\n2.\n(0, x, 0)\nCone has circular base for utt = uxx + uyy\nPyramid has diamond base for leapfrog\nt\n(x, 0, 0)\nCone and pyramid go up to (0, 0, t)\nFigure 5.10: The pyramid contains and touches the cone when (t)2 = (x)2/2.\nAn Equivalent First-order System\nI can display a system of two equations vt = Avx that is equivalent to utt = c2uxx:\n@\nut\n0 c\n@\nut\nFirst-order system\n=\n.\n(26)\n@t cux\nc 0 @x cux\nThe first equation recovers utt = c2uxx. The second is the identity cuxt = cutx. Notice\nthat the 2 by 2 matrix is symmetric and its eigenvalues are the wave velocities ±c.\n\nR\n\n@\n@\n@\n@\n5.3. THE WAVE EQUATION AND STAGGERED LEAPFROG c\n2006 Gilbert Strang\nThis \"symmetric hyperbolic\" form vt = Avx is useful in theory and practice. The\nenergy E(t) =\n2 √v(x, t)√2 dx is automatically constant in time ! Here is the proof\nfor any equation vt = Avx with a symmetric matrix A:\nX @vi\n@\n@\nT\n@t\n2 √v√\n=\nvi @t = v vt = v TAvx =\nv TAv .\n(27)\n@x\nWhen you integrate over all x, the left side is @E/@t. The right side is 2 vTAv at\nthe limits x =\n. Those limits give zero (no signal has reached that far). So the\n±≈\nderivative of E(t) is zero, and E(t) stays constant.\nThe Euler equations of compressible flow are also a first-order system, but not\nlinear. In physics and engineering, a linear equation deals with a small disturbance.\nSomething from outside acts to change the equilibrium, but not by much:\nin acoustics it is a slowly moving body\nin aerodynamics it is a slender wing\nin elasticity it is a small load\nin electromagnetism it is a small source.\nBelow some level, the cause-effect relation is very close to linear. In acoustics, the\nsound speed is steady when pressure is nearly constant. In elasticity, Hooke's law\nholds until the geometry changes or the material begins to break down. In electro\nmagnetism, nonlinearity comes with relativistic and quantum effects.\nThe case to understand has A = constant matrix, with n real eigenvalues and\neigenvectors w. The vector equation vt = Avx will split into n scalar one-way wave\nequations Ut = Ux. When Aw = w we look for v(x, t) = U(x, t)w:\n@U\n@U\n@U\n@U\n@U\nvt = Avx becomes\nw = A\nw =\nw so\n=\n.\n(28)\n@t\n@x\n@x\n@t\n@x\nThe complete solution vector is v(x, t) = U1(x + 1t)w1 +\n+ Un(x + nt)wn. The\n· · ·\nproblem vt = Avx has n signal speeds i and it sends out n waves.\nThere are n characteristic lines x + it = constant. The wave equation has\n0 c\nc 0\n2 , -2 ) and ( 1\nand two eigenvalues = c and = -c. The eigenvectors w are ( 1\n2 , 2 ).\nThen the two scalar equations Ut = Ux produce left and right waves:\n1 =\nc\n(ut + c ux) =\nc\n(ut + c ux)\n@t\n@x\n(29)\n2 = -c\n(ut -c ux) = -c\n(ut -c ux) .\nu\n@t\n@x\nEach equation agrees with utt = c2uxx. The one-way waves are U(x, t) = U(x+ t, 0).\nThe vector solution v(x, t) is recovered from U1w1 + U2w2:\n\nt\nv =\n= (ut + c ux) -1 + (ut -c ux)\n1 .\n(30)\nc ux\nA stable difference method for vt = Avx comes from a stable method for ut = ±cux.\nJust replace c by A in Lax-Friedrichs and Lax-Wendroff, or go to leapfrog.\n\nc\n2006 Gilbert Strang\nLeapfrog on a Staggered Grid\nThe discrete case should copy the continuous case. The two-step leapfrog difference\nequation should reduce to a pair of one-step equations. But if we don't keep the\nindividual equations centered, they will lose second-order accuracy. The way to\ncenter both first-order equations is to use a staggered grid (Figure 5.11).\nPlease allow me to name the two components v1 = E and v2 = H. Then the\nstaggered grid for the wave equation matches Yee's method for Maxwell's equations.\nYee's idea transformed the whole subject of computational electromagnetics (it is\nnow called the FDTD method: finite differences in the time domain). Previously\nthe moment method, which is Galerkin's method, had been dominant--but staggered\ngrids are so natural for E and H. We stay with the wave equation here, copying (26):\nMaxwell in 1D\n@E/@t = c @H/@x\ntE/t=cxH/x\nbecomes\n(31)\n(normalized)\n@H/@t = c @E/@x\ntH/t=cxE/x .\nThose first derivatives of E and H are replaced by first differences. I will put E\non the standard grid and H on the staggered (half-integer) grid. Notice how all the\ndifferences are centered in Figure 5.11a. This gives second-order accuracy.\nThe identities Etx = Ext and Htx = Hxt lead to wave equations for E and H:\nEt = cHx\nEtt\n= cHxt = cHtx = c2Exx\nbecomes\nHt = cEx\nHtt = cExt\n= cEtx\n= c2Hxx\n(32)\nIn the discrete case, the identity is x(t) = t(x). Differences copy derivatives.\nWhen we eliminate H, we get the two-step leapfrog equation for E.\nAnd eliminating E gives the leapfrog equation for H. This all comes from the finite\ndifference analogue of the cross-derivative identity utx = uxt:\n@\n@u\n@\n@u\nx(tU)\nt(xU)\n=\ncorresponds to\n=\n.\n(33)\n@x\n@t\n@t\n@x\n(x)(t)\n(t)(x)\nWith equal denominators, we only need to check the numerators. On any grid, the\nsame 1's and -1's appear both ways in xt and tx !\n-1\nx(tU )\n=\n(Un+1,j+1 - Un,j+1)\n(Un+1,j - Un,j )\n-\n-1\nt(xU )\n=\n(Un+1,j+1 - Un+1,j )\n(Un,j+1 - Un,j )\n-\nYou could compare ( ) with the Cauchy-Riemann equations ux = sy and uy = -sx\nfor the potential u(x, y) and stream function s(x, y). (Those solve Laplace's equation\nand not the wave equation.) It would be natural to discretize the Cauchy-Riemann\nequations on a staggered grid.\nMay I emphasize that these grids are useful for many other equations too. We will\nsee the \"half-point\" grid values in Section 5.\nfor the flux F in the conservation\nlaw ut + F (u)x = 0, which is a nonlinear extension of the one-way wave equation.\nHalf-point values are centrally important throughout the finite volume method.\nMaxwell's equations in integral form lead to the finite integration technique [ ].\n\n5.3. THE WAVE EQUATION AND STAGGERED LEAPFROG c\n2006 Gilbert Strang\nMaxwell's Equations\nFor electrodynamics, the number c in the wave equation is the speed of light. It is the\nsame large number that appears in Einstein's e = mc2 . The CFL stability condition\nc2(t)2 (x)2 + (y)2 + (z)2 for the leapfrog method might require very small\ntime steps (on the scale of ordinary life). But we all know that the wavelength for light\nis nothing like a meter or a centimeter. The leapfrog method is entirely appropriate,\nand we write Maxwell's equations without source terms:\n@E\n@H\nMaxwell's equations in free space\n=\ncurl H and\n= - μ curl E .\n(34)\n@t\nδ\n@t\nAn important application is the reflection of a radar signal by an airplane. The region\nof interest is exterior to the plane. In principle that region extends infinitely far\nin all directions. In practice we compute inside a large box, and choose boundary\nconditions that don't reflect waves back into the box from its artificial boundary\n(which is a computational region and not physical).\nThose absorbing boundary conditions [ ] are crucial to a good discretization.\nChapter 7 of [ ] describes how a \"perfectly matched layer \" can select coefficients\nso that waves go through the boundary with very little reflection. Applications of\nMaxwell's equations range all the way from the Earth's electromagnetic environment\nto cell phones (safety of the user) to micron-scale lasers and photonics.\nThe first of Maxwell's six equations in ( ) involves the electric field component\nEx:\n@\n1 @\n@\nEx =\nHy .\n(35)\n@t\nδ @y Hz - @z\nYee's difference equation computes Ex at the new time (n + 1)t from Ex at time\nnt and the space differences of Hz and Hy at time (n + 2 )t. Figure 5.11\nshows how those components of the magnetic field H are on a grid that is staggered\nwith respect to the grid for E. We have six differential equations like (35) and six\ndifference equations, to produce Ex, Ey , Ez at time (n + 1)t and then Hx, Hy , Hz at\ntime (n + 1.5)t.\nThe stability condition c2(t)2 (x)2 +(y)2 +(z)2 is acceptable. Perhaps the\ngreatest drawback is the rectangular grid (finite elements are always more adaptable).\nBut the FDTD method has been used with 109 meshpoints, which we cannot afford\non an unstructured mesh. Finite differences also have numerical dispersion--the\ndiscrete wave speeds depend on the wave number k = (kx, ky , kz ). Those speeds\ndon't exactly match c. We will have phase factors like F in equation ( ), extended\nto include y and z. When the dispersion creates significant errors, we can upgrade\nthe spatial differences to fourth-order accuracy (using more mesh values). But those\nwider difference methods can go across material interfaces and external boundaries.\nThis produces the ever-present give and take of numerical analysis: higher accuracy\nbrings greater complexity. We can take larger steps t but every step is slower (and\nharder to code).\n\nc\n2006 Gilbert Strang\nFIGURE TO COME...\nE\nH\nH\nE\nFigure 5.11:\n= c\nand\n= c\nare staggered but centered.\nt\nx\nt\nx\nProblem Set 5.3\nWrite the equation utt = uxx + uyy as a first-order system vt = Avx + Bvy\n(u\nwith the vector unknown v = (ut, ux, uy ). The matrices A and B should be\nsymmetric. Then the energy E(t) = 1 R\n+ u2 + uy ) dx is constant.\nt\nx\nHow was the symmetry of A used in the final step vTAvx = ( 1 vTAv)x in equa-\nP P\ntion (27) ? You could write out vTAv =\naij vi(x)vj (x) and take the deriva\ntive of each term by the product rule.\nAdd Gauss law div D = 0 and div B = 0 with D = δE and B = μH."
        },
        {
          "category": "Resource",
          "title": "am54.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/5db29e69494eb09a26f7224d43adc6f6_am54.pdf",
          "content": "5.4. THE HEAT EQUATION AND CONVECTION-DIFFUSION c2006 Gilbert Strang\n5.4\nThe Heat Equation and Convection-Diffusion\nThe wave equation conserves energy. The heat equation ut = uxx dissipates energy.\nThe starting conditions for the wave equation can be recovered by going backward in\ntime. The starting conditions for the heat equation can never be recovered. Compare\nikx:\nut = cux with ut = uxx, and look for pure exponential solutions u(x, t) = G(t) e\nWave equation: G 0 = ickG\nG(t) = eickt has |G = 1 (conserving energy)\n|\nt\nHeat equation: G 0 = -k2G\nG(t) = e-k2\nhas G < 1 (dissipating energy)\nDiscontinuities are immediately smoothed out by the heat equation, since G is ex\nponentially small when k is large. This section solves ut = uxx first analytically and\nthen by finite differences. The key to the analysis is the beautiful fundamental\nsolution starting from a point source (delta function). We will show in equation (7)\nthat this special solution is a bell-shaped curve:\nu(\np\ne-x2/4t\nu(x,\n∂(x) .\n(1)\nx, t) =\nt\ncomes from the initial condition\n0) =\nNotice that ut = cux + duxx has convection and diffusion at the same time. The\nwave is smoothed out as it travels. This is a much simplified linear model of the\nnonlinear Navier-Stokes equations for fluid flow. The relative strength of convection\nby cux and diffusion by duxx will be given below by the Peclet number.\nThe Black-Scholes equation for option pricing in mathematical finance also has\nthis form. So do the key equations of environmental and chemical engineering.\nFor difference equations, explicit methods have stability conditions like t\n2 (x)2\n≈\n. This very short time step is more expensive than ct ≈ x. Implicit\nmethods can avoid that stability condition by computing the space difference 2U\nat the new time level n + 1. This requires solving a linear system at each time step.\nWe can already see two major differences between the heat equation and the wave\nequation (and also one conservation law that applies to both):\n1. Infinite signal speed. The initial condition at a single point immediately\naffects the solution at all points. The effect far away is not large, because of the\nvery small exponential e-x2/4t in the fundamental solution. But it is not zero.\n(A wave produces no effect at all until the signal arrives, with speed c.)\n2. Dissipation of energy. The energy 2\n1 R\n(u(x, t))2 dx is a decreasing function\nof t. For proof, multiply the heat equation ut = uxx by u. Integrate uuxx by\nparts with u(√) = u(-√) = 0 to produce the integral of -(ux)2:\nd Z 1 1\nZ 1\nZ 1\nEnergy decay\nu 2 dx =\nuuxx dx =\n(ux)2 dx ≈ 0 .\n(2)\ndt\n-\n-1\n-1\n-1\n\nZ 1\nZ 1\nh\ni 1\nZ\nZZ 1\nc2006 Gilbert Strang\n3. Conservation of heat (analogous to conservation of mass):\nd\nHeat is conserved\nu(x, t) dx =\nuxx dx = ux(x, t)\n= 0 .\n(3)\ndt\nx=\n-1\n-1\n-1\nAnalytic Solution of the Heat Equation\nStart with separation of variables to find solutions to the heat equation:\nE 00\nAssume u(x, t) = G(t)E(x).\nThen ut = uxx gives G 0E = GE 00 and G 0\n=\n.\nG\nE\n(4)\nThe ratio G0/G depends only on t. The ratio E00/E depends only on x. Since\nequation (4) says they are equal, they must be constant. This produces a useful\nfamily of solutions to ut = uxx:\nt\nE 00\n= G 0\nis solved by E(x) = eikx and G(t) = e-k2 .\nE\nG\nTwo x-derivatives produce the same -k2 as one t-derivative. We are led to exponential\nsolutions of eikxe-k2t and to their linear combinations (integrals over different k):\nikx e-k2t dx.\n(5)\nGeneral solution\nu(x, t) =\n(k)\nu\ne\n\n-1\nAt t = 0, formula (5) recovers the initial condition u(x, 0) because it inverts the\nFourier transform\n(Section 4.4.) So we have the analytical solution to the heat\nu0\n\nequation--not necessarily in an easily computable form ! This form usually requires\ntwo integrals, one to find the transform\n(k) of\n(\n0), and the other to find the\nu\nu x,\n\ninverse transform of\nk\n(k) -\nu\ne\n\nt in (5).\nExample 1 Suppose the initial function is a bell-shaped Gaussian u(x, 0) = e-x2/2 .\nThen the solution remains a Gaussian. The number λ that measures the width of the\nbell increases to λ + 2t at time t, as heat spreads out. This is one of the few integrals\ninvolving e-x that we can do exactly. Actually, we don't have to do the integral.\nThat function e-x2 /2 is the impulse response (fundamental solution) at time t = 0\nto a delta function ∂(x) that occurred earlier at t =\n2 λ. So the answer we want (at\n-\ntime t) is the result of starting from that ∂(x) and going forward a total time 2 λ + t:\np\n(2λ)\np\n(2λ + 4t)\ne-x2/(2 + 4t)\nWidening Gaussian\nu(x, t) =\n.\n(6)\nThis has the right start at t = 0 and it satisfies the heat equation.\n\n5.4. THE HEAT EQUATION AND CONVECTION-DIFFUSION c2006 Gilbert Strang\nThe Fundamental Solution\nFor a delta function u(x, 0) = ∂(x) at t = 0, the Fourier transform is u0(k) = 1. Then\n\nthe inverse transform in (5) produces u(x, t) = 2\n\nR\neikxe-k2t dk One computation of\nthis u uses a neat integration by parts for @u/@x. It has three -1's, from the integral\nt\nof ke-k2 and the derivative of ieikx and integration by parts itself:\nxu\n@u = 1 Z 1\n(e-k2tk)(ieikx) dk =\n1 Z 1\n(e-k2t)(xe ikx) dk =\n.\n(7)\n@x\n- 4t\n- 2t\n-1\n-1\nThis linear equation @u/@x = -xu/2t is solved by u = ce-x2/4t . The constant\nc = 1/\np\n4t is determined by the requirement\nR\nu(x, t) dx = 1. (This conserves\nthe heat\nR\nu(x, 0) dx =\nR\n∂(x) dx = 1 that we started with. It is the area under a\nbell-shaped curve.) The solution (1) for diffusion from a point source is confirmed:\nFundamental solution from\nu(\np\ne-x2/4t .\nx, t) =\nt\n(8)\nu(x, 0) = (x)\nIn two dimensions, we can separate x from y and solve ut = uxx + uyy :\nFundamental solution from\ne-x2/4t e-y2/4t\nu(x, y, t) =\n.\n(9)\nu(x, y, 0) = (x)(y)\np\n4t\nequations (Problem\nu(\nu(\nt !\ne-c/t\n/\np\nt\nR\nWith patience you can verify that\nx, t) and\nx, y, t) do solve the 1D and 2D heat\ninitial conditions away from the origin\ncorrect as\n0, because\ngoes to zero much faster than 1\nblows up.\nsince the total heat remains at\nu dx = 1 or\nRR\nu dx dy = 1, we have a valid solution.\n). The zero\nare\nAnd\nu\nIf the source is at another point x = s, then the response just shifts by s. The\nexponent becomes -(x-s)2 /4t instead of -x2/4t. If the initial u(x, 0) is a combination\nof delta functions, then by linearity the solution is the same combination of responses.\nBut every u(x, 0) is an integral\nR\n∂(x-s) u(s, 0) ds of point sources ! So the solution to\nt = uxx is an integral of the responses to ∂(x - s). Those responses are fundamental\nsolutions starting from all points x = s:\nu(x, 0)\nu(\np\nZ 1\n-1\nu(s, 0) e-(x - s)2/4t\n(10)\nSolution from any\nx, t) =\nt\nds .\nu\nNow the formula is reduced to one infinite integral--but still not simple. And for a\nproblem with boundary conditions at x = 0 and x = 1 (the temperature on a finite\ninterval, much more realistic), we have to think again. Similarly for an equation\nt = (c(x)ux)x with variable conductivity or diffusivity. That thinking probably\nleads us to finite differences.\nI see the solution u(x, t) in (10) as the convolution of the initial function u(x, 0)\nwith the fundamental solution. Three important properties are immediate:\n\nc2006 Gilbert Strang\n1.\nIf u(x, 0) 0 for all x then u(x, t) 0 for all x and t. Nothing in\nformula (10) will be negative.\n2.\nThe solution is infinitely smooth. The Fourier transform u0(k) in (5) is\n\nmultiplied by e-k2t . In (10), we can take all the x and t derivatives we want.\nu\n3.\nThe scaling matches x2 with t. A diffusion constant d in the equation\nt = duxx will lead to the same solution with t replaced by dt, when we write\nthe equation as @u/@(dt) = @2 u/@x2 . The fundamental solution has e-x2/4dt\nand its Fourier transform has e-dk2t .\nExample 2 Suppose the initial temperature is a step function u(x, 0) = 0. Then for\nnegative x and u(x, 0) = 1 for positive x. The discontinuity is smoothed out immediately,\nas heat flows to the left. The integral in formula (10) is zero up to the jump:\nu(x, t) = p\n4t\nZ\ne-(x - s)2/4t ds .\n(11)\nNo luck with this integral ! We can find the area under a complete bell-shaped curve\n(or half the curve) but there is no elementary formula for the area under a piece of the\ncurve. No elementary function has the derivative e-x . That is unfortunate, since those\nintegrals give cumulative probabilities and statisticians need them all the time. So they\nhave been normalized into the error function and tabulated to high accuracy:\nx\n2 Z\nError function\nerf(x) = p\ne-s ds .\n(12)\nThe integral from -x to 0 is also erf(x). The normalization by 2/p gives erf(√) = 1.\nWe can produce this error function from the heat equation integral (11) by setting\nS = (s - x)/\np\n4t. Then s = 0 changes to S = -x/\np\n4t as the lower limit on the integral,\nand dS = ds/\np\n4t. Split into an integral from 0 to √, and from -x/\np\n4t to 0:\np\n4t Z 1\ne-S2\nx\nu(x, t) =\ndS =\np\n4t\n1 + erf\n.\n(13)\n-x/\np\n4t\np\n4t\nGood idea to check that this gives u = 2 at x = 0 (where the error function is zero).\nThis is the only temperature we know exactly, by symmetry between left and right.\nExplicit Finite Differences\nThe simplest finite differences are forward for @u/@t and centered for @2u/@x2:\ntU\nt = 2\nx U\n(x)2\nUj,n+1 - Uj,n\nt\n= Uj+1,n - 2Uj,n + Uj-1,n\n(x)2\n. (14)\nExplicit method\n\n5.4. THE HEAT EQUATION AND CONVECTION-DIFFUSION c2006 Gilbert Strang\nEach new value Uj,n+1 is given explicitly by Uj,n + R(Uj+1,n - 2Uj,n + Uj,n-1). The\ncrucial ratio for the heat equation ut = uxx is now R = t/(x)2 .\nWe substitute Uj,n = Gn eikjx to find the growth factor G = G(k, t, x):\nOne-step\nG = 1 + R(e ikx - 2 + e-ikx) = 1 + 2R(cos kx - 1) . (15)\ngrowth factor\nG is real, just as the exact one-step factor e-k2t is real. Stability requires G ≈ 1.\n| |\nAgain the most dangerous case is when the cosine equals -1 at kx = :\nt\nStability condition\nG = 1 - 4R ≈ 1 which requires R =\n. (16)\n| |\n|\n|\n(x)2 2\nIn many cases we accept that small time step t and use this simple method. The\naccuracy from forward t and centered 2 is U - u = O(t + (x)2). Those two\nx\n|\n|\nerror terms are comparable when R is fixed.\nWe could improve this one-step method to a multistep method. The \"method\nof lines\" calls an ODE solver for the system of differential equations (continuous in\ntime, discrete in space). There is one equation for every meshpoint x = jh:\ndU\n2 U\ndUj = Uj+1 - 2Uj + Uj-1\nMethod of Lines\n=\nx\n.\n(17)\ndt\n(x)2\ndt\n(x)2\nThis is a stiff system, because its matrix -K (second difference matrix) has a large\ncondition number: max(K)/min(K) N 2 . We could choose a stiff solver like ode15s\nin MATLAB.\nImplicit Finite Differences\nA fully implicit method for ut = uxx computes 2 U at the new time (n + 1)t:\nx\ntUn\nUj+1,n+1 - 2Uj,n+1 + Uj-1,n+1\nImplicit\n=\nx Un+1\nUj,n+1 - Uj,n =\n. (18)\nt\n(x)2\nt\n(x)2\nThe accuracy is still first-order in time and second-order in space. But stability no\nlonger depends on the ratio R = t/(x)2 . We have unconditional stability, with a\ngrowth factor 0 < G ≈ 1 for all k. Substituting Uj,n = Gneijkx into (18) and then\ncanceling those terms from both sides leaves an extra G on the right side:\nG = 1 + RG(e ikx - 2 + e-ikx) leads to G = 1 + 2R(1 - cos kx) .\n(19)\nThe denominator is at least 1, which ensures that 0 < G ≈ 1. The time step is\ncontrolled by accuracy, because stability is no longer a problem.\n\nc2006 Gilbert Strang\nThere is a simple way to improve to second-order accuracy. Center everything at\nstep\nfamous Crank-Nicolson method (like the trapezoidal rule):\nn + 1 . Average an explicit 2 Un with an implicit 2 Un+1. This produces the\nx\nx\nCrank-Nicolson\nUj,n+1 - Uj,n\nxUj,n+1) .\n(20)\nxUj,n + 2\nt\n= 2(x)2 (2\nNow the growth factor G, by substituting Uj,n = Gneijkx into (20), solves\nG + 1\nG - 1 = 2(x)2 (2 cos kx - 2) .\n(21)\nt\nSeparate out the part involving G, write R for t/(x)2, and cancel the 2's:\nUnconditional stability\nG = 1 + R(cos kx - 1) has |G| 1 .\n(22)\n1 - R(cos kx - 1)\nThe numerator is smaller than the denominator, since cos kx ≈ 1. We do notice\nthat cos kx = 1 whenever kx is a multiple of 2. Then G = 1 at those frequencies,\nso Crank-Nicolson does not give the strict decay of the fully implicit method. We\ncould weight the implicit 2 Un+1\nby a > 2 and the explicit 2 Un\nx\nx\ngive a whole range of unconditionally stable methods (Problem\n).\n-\nby 1\na < , to\nNumerical example\nFinite Intervals with Boundary Conditions\nWe introduced the heat equation on the whole line -√ < x < √. But a physical\nproblem will be on a finite interval like 0 ≈ x ≈ 1. We are back to Fourier series\n(not Fourier integrals) for the solution u(x, t). And second differences bring back the\ngreat matrices K, T, B, C that depend on the boundary conditions:\nAbsorbing boundary at x = 0: The temperature is held at u(0, t) = 0.\nInsulated boundary: No heat flows through the left boundary if ux(0, t) = 0.\nIf both boundaries are held at zero temperature, the solution will approach u(x, t) = 0\neverywhere as t increases. If both boundaries are insulated as in a freezer, the solution\nwill approach u(x, t) = constant. No heat can escape, and it is evenly distributed as\nt ! √. This case still has the conservation law\nR 1 u(x, t) dx = constant.\nExample 3 (Fourier series solution) We know that eikx is multiplied by e-k2 t to give\na solution of the heat equation. Then u = e-k2t sin kx is another solution (combining\n+k with -k). With zero boundary conditions u(0, t) = u(1, t) = 0, the only allowed\n\n5.4. THE HEAT EQUATION AND CONVECTION-DIFFUSION c2006 Gilbert Strang\nfrequencies are k = n (then sin nx = 0 at both ends x = 0 and x = 1). The complete\nsolution is a combination of these exponential solutions with k = n:\nComplete solution\nu(x, t) =\nX\nbn e-n\nt sin nx .\n(23)\nn=1\nThe Fourier sine coefficients bn are chosen to match u(x, 0) = P bn sin nx at t = 0.\nYou can expect cosines to appear for insulated boundaries, where the slope (not\nthe temperature) is zero. This gives exact solutions to compare with finite difference\nsolutions. For finite differences, absorbing boundary conditions produce the matrix\nK (not B or C). The choice between explicit and implicit decides whether we have\nsecond differences -KU at time level n or level n + 1:\nExplicit method\nUn+1 - Un = -RKUn\n(24)\nFully implicit\nUn+1 - Un = -RKUn+1\n(25)\nCrank-Nicolson\nUn+1 - Un =\n1 RK(Un + Un+1) .\n(26)\n-\nThe explicit stability condition is again R ≈ 1 (Problem\n). Both implicit meth\nods are unconditionally stable (in theory). The reality test is to try them in practice.\nAn insulated boundary at x = 0 changes K to T . Two insulated boundaries\nproduce B. Periodic conditions will produce C. The fact that B and C are singular\nno longer stops the computations. In the fully implicit method (I + RB)Un+1 = Un,\nthe extra identity matrix makes I + RB invertible.\nThe two-dimensional heat equation describes the temperature distribution in\na plate. For a square plate with absorbing boundary conditions, the difference matrix\nK changes to K2D. The bandwidth jumps from 1 (triangular matrix) to N (when\nmeshpoints are ordered a row at a time). Each time step of the implicit method\nnow requires a serious computation. So implicit methods pay an increased price for\nstability, to avoid the explicit restriction t ≈ 1\n4 (x)2 + 1\n4 (y)2 .\nConvection-Diffusion\nPut a chemical into flowing water. It diffuses while it is carried along by the flow. A\ndiffusion term d uxx appears together with a convection term c ux. This is the simplest\nmodel for one of the most important differential equations in engineering:\n= c\n+ d @2u\n2 .\n(27)\nConvection-diffusion equation\n@u\n@t\n@u\n@x\n@x\nOn the whole line -√ < x < √, the flow and the diffusion don't interact. If the\nvelocity is c, convection just carries along the diffusing solution to ht = d hxx:\nDiffusing traveling wave\nu(x, t) = h(x + ct, t) .\n(28)\n\nc2006 Gilbert Strang\nSubstituting into equation (27) confirms that this is the solution (correct at t = 0):\n@u\n@h\n@h\n@h\n@2h\n@u\n@2u\nChain rule\n= c\n+\n= c\n+ d\n= c\n+ d\n.\n(29)\n@t\n@x\n@t\n@x\n@x2\n@x\n@x2\nExponentials also show this separation of convection eikct from diffusion e-dk2 t:\nt ik(x + ct)\nStarting from eikx\nu(x, t) = e-dk2 e\n.\n(30)\nConvection-diffusion is a terrific model problem, and the constants c and d clearly\nhave different units. We take this small step into dimensional analysis:\ndistance\n(distance)2\nConvection coefficient c:\nDiffusion coefficient d:\n(31)\ntime\ntime\nSuppose L is a typical length scale in the problem. The Peclet number P e = cL/d\nis dimensionless. It measures the relative importance of convection and diffusion. This\nPeclet number for the linear equation (27) corresponds to the Reynolds number for\nthe nonlinear Navier-Stokes equations (Section\n).\nIn the difference equation, the ratios r = ct/x and 2R = 2dt/(x)2 are also\ndimensionless. That is why the stability conditions r ≈ 1 and 2R ≈ 1 were natural for\nthe wave and heat equations. The new problem combines convection and diffusion,\nand the cell Peclet number P uses x/2 as the length scale in place of L:\nr\nc x\nCell Peclet Number\nP =\n=\n.\n(32)\n2R\n2d\nWe still don't have agreement on the best finite difference approximation! Here\nare three natural candidates (you may have an opinion after you try them):\n1.\nForward in time, centered convection, centered diffusion\n2.\nForward in time, upwind convection, centered diffusion\n3.\nExplicit convection (centered or upwind ), with implicit diffusion.\nEach method will show the effects of r and R and P (we can replace r/2 by RP ):\n1. Centered explicit\nUj,n+1 - Uj,n\nUj+1,n - Uj-1,n\nxUj,n\n= c\n+ d\n.\n(33)\nt\n2x\n(x)2\nEvery new value Uj,n+1 is a combination of three known values at time n:\nUj,n+1 = (1 - 2R)Uj,n + (R + RP )Uj+1,n + (R - RP )Uj-1,n .\n(34)\nThose three coefficients add to 1, and U = constant certainly solves equation (33). If\nall three coefficients are positive, the method is surely stable. More than\nthat, oscillations cannot appear. Positivity of the middle coefficient requires R ≈ 2 ,\n\nZ\n5.4. THE HEAT EQUATION AND CONVECTION-DIFFUSION c2006 Gilbert Strang\nas usual for diffusion. Positivity of the other coefficients requires P 1. Of course\n|\n|\nP will be small when x is small (so we have convergence as x ! 0). In avoiding\noscillations, the actual cell size x is crucial to the quality of U .\nFigure 5.12 was created by Strikwerda [59] and Persson to show the oscillations for\nP > 1 and the smooth approximations for P < 1. Notice how the initial hat function\nis smoothed and spread and shrunk by diffusion. Problem\nfinds the exact\nsolution, which is moved along by convection. Strictly speaking, even the oscillations\nmight pass the stability test G ≈ 1 (Problem\n). But they are unacceptable.\n| |\nFigure 5.12: Convection-diffusion with and without numerical oscillations: R =\n, r =\nand\n.\nUj,n+1 - Uj,n\nUj+1,n - Uj,n\n2. Upwind convection\n= c\n+ d\nxUj,n .\n(35)\nt\nx\n(x)2\nThe accuracy in space has dropped to first order. But the oscillations are eliminated\nwhenever r + 2R ≈ 1. That condition ensures three positive coefficients when (35) is\nsolved for the new value Uj,n+1:\nUj,n+1 = (RP + R)Uj+1,n + (1 - RP - 2R)Uj,n + RUj-1,n .\n(36)\nArguments are still going, comparing the centered method 1 and the upwind method 2.\nThe difference between the two convection terms, upwind minus centered, is ac\ntually a diffusion term hidden in (35) !\nUj+1 - Uj\nUj+1 - Uj-1\nx Uj+1 - 2Uj + Uj-1\nExtra diffusion\n=\n.\n(37)\nx\n-\n2x\n(x)2\nSo the upwind method has this extra numerical diffusion or \"artificial viscosity \"\nto kill oscillations. It is a non-physical damping. If the upwind approximation were\nincluded in Figure 5.12, it would be distinctly below the exact solution. Nobody is\nperfect.\n3. Implicit diffusion\nUj,n+1 - Uj,n = c Uj+1,n - Uj,n + d\nxUj,n+1 .\n(38)\nt\nx\n(x)2\nMORE TO DO\nProblem Set 5.4\nSolve the heat equation starting from a combination u(x, 0) = ∂(x + 1) - 2∂(x) +\n∂(x - 1) of three delta functions. What is the total heat\nR\nu(x, t) dx at time t ?\nDraw a graph of u(x, 1) by hand or by MATLAB.\nIntegrating the answer to Problem 1 gives another solution to the heat equation:\nx\nShow that w(x, t) =\nu(X, t) dX solves wt = wxx .\nGraph the initial function w(x, 0) and sketch the solution w(x, 1).\n\nc2006 Gilbert Strang\nIntegrating once more solves the heat equation ht = hxx starting from h(x, 0) =\nR\nw(X, 0) dX = hat function. Draw the graph of h(x, 0). Figure 5.12 shows the\ngraph of h(x, t), shifted along by convection to h(x + ct, t).\nIn convection-diffusion, compare the condition R ≈ 1 , P ≈ 1 (for positive coef\nficients in the centered method) with r + 2R ≈ 1 (for the upwind method). For\nwhich c and d is the upwind condition less restrictive, in avoiding oscillations ?\nThe eigenvalues of the n by n second difference matrix K are k = 2 - 2 cos k .\nn+1\nThe eigenvectors yk in Section 1.5 are discrete samples of sin kx. Write the\ngeneral solutions to the fully explicit and fully implicit equations (14) and (18)\nafter N steps, as combinations of those discrete sines yk times powers of k .\nAnother exact integral involving e-x2/4t is\nZ 1\nx e-x2/4t dx =\nh\n-2t e-x2/4ti1\n= 2t .\nFrom (17), show that the temperature is u =\np\nt\nx\nat the center point\n= 0\nstarting from a ramp u(x, 0) = max(0, x).\nA ramp is the integral of a step function. So the solution of ut = uxx starting\nfrom a ramp (Problem 6) is the integral of the solution starting from a step\nThen\np\nt must be the total amount of heat\nfunction (Example 2 in the text).\nthat has crossed from x > 0 to x < 0 in Example 2 by time t. Explain each of\nthose three sentences."
        },
        {
          "category": "Resource",
          "title": "am55.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/1d170d72359b7d27052c960197b425e5_am55.pdf",
          "content": "c\n5.5. DIFFERENCE MATRICES AND EIGENVALUES\n2006 Gilbert Strang\n5.5\nDifference Matrices and Eigenvalues\nThis brief section collects together useful notes on finite difference matrices, and\nalso finite element matrices. Certainly those special matrices K, T, B, C from the\nstart of the book are the building blocks for approximations to uxx and uyy . Second\nderivatives and fourth derivatives lead to symmetric matrices. First derivatives are\nantisymmetric. They present more difficulties.\nA symmetric matrix has orthogonal eigenvectors. For those special matrices, the\neigenvectors are discrete samples of sines and cosines and eikx . The eigenvalues are\nikx -2+e-ikx\nreal, and they often involve e\n. That is the discrete factor 2 cos kx-2.\nDivided by (x)2, it is close for small k to the factor -k2 from the second derivative\nof e ikx . The von Neumann approach using e ikx matches the eigenvectors of these\nmatrices, and the growth factors G match the eigenvalues.\nFor a one-sided (upwind) difference, the matrix eigenvalues are not always reliable.\nFor a centered difference they do follow von Neumann. Compare\n⎡\n⎡\n-1\n-1\n⎢\n1 6 -1\n0 -1\n⎢\n+ = 6\n⎢\n0 =\n⎢\n-1\n1⎣\n2 4\n-1\n0 -1 ⎣\n-1\n-1\nThe eigenvalues of the triangular upwind matrix + are all -1 (useless). The eigen\nvalues of the antisymmetric 0 are guaranteed to be imaginary like the factor ik from\nthe derivative of eikx . The eigenvalues = -1 for + do not make upwind differences\nuseless. They only mean that the von Neumann test, which produces eikx - 1, is\nbetter than relying on eigenvalues.\nAs it stands, + is exactly in \"Jordan form.\" The matrix has only one line of\neigenvectors, not n. It is an extreme example of a nondiagonalizable (and somehow\ndegenerate) matrix. If the diagonals of -1's and 1's are extended to infinity, then\nFourier and von Neumann produce vectors with components eikjx and with eigen\nvalues e ikx - 1. In summary: For normal matrices, eigenvalues are a reliable guide.\nFor other constant-diagonal matrices, better to rely on von Neumann.\nBriefly, the discrete growth factors G are exactly the eigenvalues when the matrices\nare called \"normal \" and the test is AAT = ATA (for complex matrices take the\nconjugate transpose A). This test is passed by all symmetric and antisymmetric\nand orthogonal matrices.\nOptions for First Differences\nUpwind elements\nStreamline diffusion\nDG\nBoundary conditions\nConvection-diffusion"
        },
        {
          "category": "Resource",
          "title": "am56.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/3762c803fd3f27bcc47cc76341162824_am56.pdf",
          "content": "@\n@\n@\n@\nc2006 Gilbert Strang\n5.6\nNonlinear Flow and Conservation Laws\nNature is nonlinear. The coefficients in the equation depend on the solution u. In\nplace of ut = c ux we will study ut + uux = 0 and more generally ut + f(u)x = 0.\nThese are \"conservation laws\" and the conserved quantity is the integral of u.\nThe first part of this book emphasized the balance equation: forces balance and\ncurrents balance. For steady flow this was Kirchhoff's Current Law: flow in equals\nflow out. The net flow was zero. Now the flow is unsteady --the \"mass inside\" is\nchanging. So a new @/@t term will enter the conservation law.\nThere is \"flux\" through the boundaries. In words, the rate of change of mass\ninside a region equals that incoming flux. For an interval [a, b], the incoming\nflux is the difference in fluxes at the endpoints a and b:\nd\ndt\nZ b\na\nu(\n) dx = f (u(\n)) - f (u(\n)) .\n(1)\nIntegral form\nx, t\na, t\nb, t\nIn applications, u can be a density (of cars along a highway). The integral of u gives\nthe mass (number of cars) between a and b. This number changes with time, as cars\nflow in at point a and out at point b. The flux is density u times velocity v.\nThe integral form is fundamental. We can get a differential form by allowing b to\napproach a. Suppose b - a = x. If u(x, t) is a smooth function, its integral over a\ndistance x will have leading term x u(a, t). So if we divide equation (1) by x,\nthe limit as x approaches zero is @u/@t = -@f(u)/@x:\n+ @ f(u\n+ f (u)\n(2)\nDifferential form\n@u\n@t\n@x\n) = @u\n@t\n@u\n@x = 0\nWhen f(u) = density u times velocity v(u), we can solve this single conservation\nlaw. For traffic flow, the velocity v(u) can be measured (it will decrease as density\nincreases). In gas dynamics there are also conservation laws for momentum and\nenergy. The velocity v becomes another unknown, along with the pressure p. The\nEuler equations for gas dynamics in one space dimension include two additional\nequations:\nConservation of momentum\n(uv) +\n(uv 2 + p) = 0\n(3)\n@t\n@x\nConservation of energy\n(E) +\n(Ev + Ep) = 0 .\n(4)\n@t\n@x\nSystems of conservation laws are more complicated, but our scalar equation (2) al\nready has the possibility of shocks. A shock is a discontinuity in the solution u(x, t),\nwhere the differential form breaks down and we need the integral form (1).\n\nc\n5.6. NONLINEAR FLOW AND CONSERVATION LAWS\n2006 Gilbert Strang\nThe other outstanding example, together with traffic flow, is Burger's equation,\nfor u = velocity. The flux f (u) is 2 u2 . The \"inviscid\" form has no uxx:\n@u\n@\nu2\n@u\n@u\nBurger's equation\n+\n=\n+ u\n= 0 .\n@t\n@x\n@t\n@x\nWhen both the density and velocity are unknowns, these examples combine into\nconservation of mass and conservation of momentum. Typically we change density to\nν. For small disturbances of a uniform density ν0, we could linearize the conservation\nlaws and reach the wave equation (Problem\n). But the Euler and Navier-Stokes\nequations are truly nonlinear, and we begin the task of solving them.\nWe will approach conservation laws (and these examples) in three ways:\n1.\nBy following characteristics until trouble arrives: they separate or collide\n2.\nBy a special formula ( )\n3.\nBy finite difference and finite volume methods, which are the practical choice.\nCharacteristics\nThe one-way wave equation ut = c ux is solved by u(x, t) = u(x + ct, 0). Every initial\nvalue u0 is carried along a characteristic line x + ct = x0. Those lines are parallel\nwhen the velocity c is a constant.\nThe conservation law ut = +u ux = 0 will be solved by u(x, t) = u(x - ut, 0).\nEvery initial value u0 = u(x0, 0) is carried along a characteristic line x - u0t = x0.\nThose lines are not parallel because their slopes depend on the initial value u0.\nNotice that the formula u(x, t) = u(x - ut, 0) involves u on both sides. It gives\nthe solution \"implicitly.\" If the initial function is u(x, 0) = 1 - x, for example, the\nformula must be solved for u:\nu = 1 - (x - ut) gives (1 - t)u = 1 - x and u = 1 - x .\n(5)\n1 - t\nThis does solve Burger's equation, since the time derivative ut = (1 - x)/(1 - t)2 is\nequal to -uux. The characteristic lines (with different slopes) can meet. This is an\nextreme example, where all characteristics meet at the same point:\nx - u0t = x0\nor x - (1 - x0)t = x0\nwhich goes through x = 1, t = 1\n(6)\nYou see how the solution u = (1 - x)/(1 - t) becomes 0/0 at that point x = 1, t = 1.\nBeyond their meeting point, the characteristics cannot completely decide u(x, t).\nA more fundamental example is the Riemann problem, which starts from two\nconstant values u = A and u = B. Everything depends on whether A > B or A < B.\nOn the left side of Figure 5.13, with A > B, the characteristics meet. On the right\n\nZ\nc2006 Gilbert Strang\nside, with A < B, the characteristics separate. Both cases present a new (nonlinear)\nproblem, when we don't have a single characteristic that is safely carrying the correct\ninitial value to the point. This Riemann problem has two characteristics through the\npoint, or none:\nShock Characteristics collide\n(light goes red: speed drops from 60 to 0)\nFan\nCharacteristics separate (light goes green: speed up from 0 to 60)\nThe problem is how to connect u = 60 to u = 0, when the characteristics don't give\nthe answer. A shock will be sharp breaking (drivers only see the car ahead in this\nmodel). A fan will be gradual acceleration.\nTO DO...\nFigure 5.13: A shock when characteristics collide, a fan when they separate.\nFor the conservation law ut + f(u)x = 0, the characteristics are x - f (u0)t = x0.\nThat line has the right slope to carry the constant value u = u0:\nd\n@u\n@u\nu(x0 + St, t) = S\n+\n= 0 when S = f (u) .\n(7)\ndt\n@x\n@t\nThe solution until trouble arrives is u(x, t) = u(x - f (u)t, 0).\nShocks\nAfter trouble arrives, it will be the integral form that guides the choice of the correct\nsolution u. If there is a jump in u (a shock ), that integral from tells where the jump\nmust occur. Suppose u has different values uL and uR at points xL and xR on the\nleft and right sides of the shock:\nd\nxR\nIntegral form\nu dx + f(uR) - f(uL) = 0 .\n(8)\ndt\nxL\nIf the position of the shock is x = X(t), we take xL and xR very close to X. The\nvalues of u(x, t) inside the integral are close to the constants uL and uR:\nd\n\n(x - xL) uL + (xR - X) uR + f(uR) - f(uL) 0 .\ndt\nThis gives the speed s = dX/dt of the shock curve:\nL -\nR + f(uR) - f(uL\n= f (uR) - f (uL)\nuR - uL\n= [ f ]\n[ u ] .\n(9)\nJump condition\ns u\ns u\n) = 0\nshock speed\n\nc\n5.6. NONLINEAR FLOW AND CONSERVATION LAWS\n2006 Gilbert Strang\nFor the Riemann problem, the left and right values uL and uR will be constants A\nand B. The shock speed s is the ratio between the jump [ f ] = f(B) - f(A) and\nthe jump [ u ] = B - A. Since this ration gives a constant slope, the shock line is\nstraight. For other problems, the characteristics are carrying different values of u into\nthe shock. So the shock speed s is not constant and the shock lines is curved.\n= 1\n2 u\nThe shock gives the solution when characteristics collide. With f(u)\n2 in\nBurger's equation, the shock speed is halfway between uL and uR:\n1 u\nL\n2 - u\nR\nBurger's equation\nShock speed s =\n=\n(uR + uL) .\n(10)\n2 uR - uL\nThe Riemann problem has uL = A and uR = B, and s is their average. Figure 5.14\nshows how the integral form of Burger's equation is solved by the right placement of\nthe shock.\nFans\nYou might expect a similar picture (just flipped) when A < B. Wrong. The integral\nform is still satisfied, but it is also satisfied by a fan. The choice between shock and\nfan is made by the \"entropy condition\" that as t increases, characteristics must go\ninto the shock. The wave speed is faster than the shock speed on the left, and slower\non the right:\nf (u)\n(uR)\n(11)\nEntropy condition\n> s > f\nu\nSince Burger's equation has f (u) = u, it only has shocks when uL is larger than uR.\nIn the Riemann problem that means A > B. In the opposite case, the smaller value\nL = A has to be connected to uR = B by the fan in Figure 5.14:\nx\nFan (or rarefaction)\nu =\nfor At < x < Bt .\n(12)\nt\nuse fig 6.28 p. 592 of IAM (reverse left and right figs)\nFigure 5.14: Characteristics collide in a shock and separate in a fan.\nNotice especially that in the traffic flow problem, the velocity v(u) decreases as\nthe density u increases. A good model is linear between v = vmax at zero density\nand v = 0 at maximum density. Then the flux f(u) = u v(u) is a downward parabola\n(concave instead of Burger's convex u2/2):\nTraffic speed\nu\nu\nand flux\nv(u) = vmax 1 - umax\nand f(u) = vmax u -\n. (13)\numax\nTypical values for a single lane of traffic show a maximum flux of f = 1600 vehicles\nper hour, when the density is u = 80 vehicles per mile. This maximum flow rate\n\nc2006 Gilbert Strang\nis attained when the velocity f/u is v = 20 miles per hour! Small comfort at that\nspeed, to know that other cars are getting somewhere too.\nProblems\nand\ncompute the solution when a light goes red (shock trav\nels backward) and when a light goes green (fan moves forward). Please look at the\nfigures, to see how the vehicle trajectories are entirely different form the characteris\ntics.\nA driver keeps adjusting the density to stay safely behind the car in front. (Hitting\nthe car would give u < 0.) We all recognize the frustration of braking and accelerating\nfrom a series of shocks and fans. This traffic crawl happens when the green light is\ntoo short for the shock to make it through.\nA Solution Formula for Burger's Equation\nLet me comment on three nonlinear equations. They are useful models, quite special\nbecause each one has an exact solution formula:\nConservation law\nut + u ux = 0\nBurger's with viscosity\nut + u ux = uxx\nKorteweg-de Vries\nut + u ux = -a uxxx\nThe conservation law can develop shocks. This won't happen in the second equation\nbecause the uxx viscosity term prevents it. That term can stay small when the solution\nis smooth, but it dominates when a wave is about to break. The profile is steep but\nit stays smooth.\nAs starting function for the conservation law, I will pick a point source: u(x, 0) =\n∂(x). We can guess a solution with a shock, and check the jump condition and entropy\ncondition. Then we find an exact formula when uxx is included, by a neat change of\nvariables that produces ht = hxx. When we let ! 0, the limiting formula solves\nthe conservation law--and we can check that the following solution is correct.\nSolution with u(x, 0) = (x)\nWhen u(x, 0) jumps upward, we expect a fan.\nWhen it drops we expect a shock. The delta function is an extreme case (very big\njumps up and down, very close together!). So we look for a shock curve x = X(t)\nimmediately in front of a fan!\nu(\nx\nt for 0 ≈ x ≈ X(t); otherwise u\n(14)\nExpected solution\nx, t) =\n= 0.\n⎪\nThe total mass at the start is\n∂(x) dx = 1. This never changes, and already that\nlocates the shock position X(t):\nZ X x\nX2\nMass at time t =\ndt =\n= 1\nso X(t) =\np\n2t .\n(15)\nt\n2t\n\nZ 1\n<\n:\nc\n5.6. NONLINEAR FLOW AND CONSERVATION LAWS\n2006 Gilbert Strang\nDoes the drop in u, from X/t =\np\n2t/t to zero, satisfy the jump condition?\ndX\np\nJump [ u2/2 ]\nX2/2t2\np\n2t\nShock speed s =\n=\nequals\n=\n=\n.\ndt\np\nt\nJump [ u ]\nX/t\n2t\nThe entropy condition uL > s > uR = 0 is also satisfied, and the solution ( ) looks\ngood. It is good, but because of the delta function we check it another way.\nBegin with ut + u ux = uxx, and solve that equation exactly. If u(x) is @U/@x,\nthen integrating our equation gives Ut + 2 U 2 = Uxx. The initial value U0(x) is now\nx\na step function. Then the great change of variables U = -2 log h produces the heat\nequation ht = hxx (Problem\n). The initial value becomes h(x, 0) = e-U0 (x)/2 .\nSection 5.4 found the solution to the heat equation ut = uxx from any starting function\nh(x, 0) and we just change t to t:\n⎨\n⎩\nU (x, t) = -2 log h(x, t) = -2 log p\nt\n-1\ne -U0(y)/2 e -(x-y)2 /4t dy .\n(16)\nform e\nIt doesn't look easy to let ! 0, but it can be done. That exponential has the\n-B(x,y)/2 . This is largest when B is smallest. An asymptotic method called\n\"steepest descent\" shows that as ! 0, the bracketed quantity in (16) approaches\nc e -B-min/2 . Taking its logarithm and multiplying by -2, (16) becomes U = Bmin\nin the limit:\n\nlim U (x, t) = Bmin = min U0(y) +\n(x -y)2 .\n(17)\n!0\ny\n2t\nThis is the solution formula for Ut + 2 U 2 = 0. Its derivative u = Ux solves the\nx\nconservation law ut + u ux = 0. By including the viscosity uxx with ! 0, we are\nfinding the u(x, t) that satisfies the jump condition and the entropy condition.\nExample\nStarting from u(x, 0) = ∂(x), its integral U0 is a step function. The minimum\nof B is either at y = x or at y = 0. Check each case:\n⎨\n⎩\n⎧0\nfor x ≈0\nU (t, x) = Bmin = miny\n(y ≈0) + (x -y)2\n=\nx2/2t for 0 ≈x ≈\np\n2t\n(y > 0)\n2t\n⎧\nfor x √\np\n2t\nThe result u = dU/dx is 0 or x/t or 0. This agrees with our guess in equation ( )--a\nfan rising from 0 and a shock back to 0 at x =\np\n2t."
        },
        {
          "category": "Resource",
          "title": "am57.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/49baea85bf92b8bd0c73c9a313fd3f33_am57.pdf",
          "content": "p\np\nc2006 Gilbert Strang\n5.7\nLevel Sets and the Fast Marching Method\nThe level sets of f(x, y) are the sets on which the function is constant. For example\nf(x, y) = x2 + y2 is constant on circles around the origin. Geometrically, a level plane\nz = constant will cut through the surface z = f(x, y) on a level set. One attractive\nfeature of working with level sets is that their topology can change (pieces of the level\nset can separate or come together) just by changing the constant.\nStarting from one level set, the signed distance function d(x, y) is especially\nimportant. It gives the distance to the level set, and also the sign: typically d > 0\noutside and d < 0 inside. For the unit circle, d = r - 1 =\nx2 + y2 - 1 will be the\nsigned distance function. In the mesh generation algorithm of Section 2.\n, it was\nconvenient to describe the region by its distance function d(x, y).\nA fundamental fact of calculus: The gradient of f(x, y) is perpendicular to its\nlevel sets. Reason: In the tangent direction t to the level set, f(x, y) is not changing\nand (grad f) · t is zero. So grad f is in the normal direction. For the function x2 + y ,\nthe gradient (2x, 2y) points outward from the circular level sets. The gradient of\nd(x, y) =\nx2 + y2 - 1 points the same way, and it has a special property: The\ngradient of a distance function is a unit vector. It is the unit normal n(x, y)\nto the level sets. For the circles,\ngrad( x2 + y2 - 1) = ( x, y )\nand\n| grad |2 =\n+ y\n= 1 .\n(1)\np\nx\nr r\nr2\nr2\nYou could think of the level set d(x, y) = 0 as a wall of fire. This firefront will move\nnormal to itself. If it has constant velocity 1 then at time T the fire will reach all\npoints on the level set d(x, y) = T .\nThat \"wall of fire\" example brings out an important point when the zero level set\nhas a corner (it might be shaped like a V). The points at distance d outside that set\n(the firefront at time d) will lie on lines parallel to the sides of the V, and also on a\ncircular arc of radius d around the corner. For d < 0 the V moves inward. It remains\na V (with no smoothing of the corner).\nThe central problem of the level set method is to propagate a curve like the\nfirefront. A velocity field v = (v1, v2) gives the direction and speed of each point for\nthe movement. At time t = 0, the curve is the level set where d(x, y) = 0. At later\ntimes the curve is the zero level set of a function (x, y, t). The fundamental level\nset equation in its first form is\nd + v · grad = 0, with = d(x, y) at t = 0 .\n(2)\ndt\nIn our wall of fire example, v would be the unit vector in the normal direction to\nthe firefront: v = n = grad /| grad |. In all cases it is only the normal component\nF = v · n that moves the curve! Tangential movement (like rotating a circle around\nits center) gives no change in the curve as a whole. By rewriting v · grad , the level\n\nc\n5.7. LEVEL SETS AND THE FAST MARCHING METHOD\n2006 Gilbert Strang\nset equation takes a second form that is more useful in computation:\ngrad\nd\nv · grad = v ·\n| grad | = F | grad |\nleads to\n+ F | grad | = 0 . (3)\n| grad |\ndt\nWe only need to know the velocity field v (and only its normal component F )\nnear the current location of the level curve-not everywhere else. We are propagating\na curve. The velocity field may be fixed (easiest case) or it may depend on the local\nshape of the curve (nonlinear case). An important example is motion by mean\ncurvature: F = -. The neat property | grad | = 1 of distance functions simplifies\nthe formulas for the normal n and curvature :\nWhen is a\nn = grad\nbecomes n = grad\ndistance\n| grad |\n(4)\nfunction\n= div n\nbecomes = div(grad ) = Laplacian of\nBut here is an unfortunate point for t > 0. Constant speed (F = 1) in the normal\ndirection does maintain the property | grad | = 1 of a distance function. Motion\nby mean curvature, and other motions, will destroy this property. To recover the\nsimple formulas (4) for distance functions, the level set method often reinitializes\nthe problem--restarting from the current time t0 and computing the distance function\nd(x, y) to the current level set (x, y, t0) = 0. This reinitialization was the Fast\nMarching Method, which finds distances from nearby meshpoints to the current\nlevel set.\nWe describe this quick method to compute distances to meshpoints, and then\ndiscuss the numerical solution of the level set equation (3) on the mesh.\nFast Marching Method\nThe problem is to march outward, computing distances from meshpoints to the in\nterface (the current level set where = 0). Imagine that we know these distances for\nthe grid points adjacent to the interface. (We describe fast marching but not the full\nalgorithm of reinitialization.) The key step is to compute the distance to the next\nnearest meshpoint. Then the front moves further outward with velocity F = 1. When\nthe front crosses a new meshpoint, it will become the next nearest and its distance\nwill be settled next.\nSo we accept one meshpoint at a time. Distances to further meshpoints are tenta\ntive (not accepted). They have to be recomputed using the newly accepted meshpoint\nand its distance. The Fast Marching Method must quickly take these steps recur\nsively:\n1. Find the tentative meshpoint p with smallest distance (to be accepted).\n2. Update the tentative distances to all meshpoints adjacent to p.\n\nc2006 Gilbert Strang\nTo speed up step 1, we maintain a binary tree of unaccepted meshpoints and their\ntentative distances. The smallest distance is at the top of the tree, which identifies\np. When that value is removed from the tree, others move up to form the new tree.\nRecursively, each vacancy is filled by the smaller of the two distance values below\nit. Then step 2 updates those values at points adjacent to p. These updated values\nmay have to move (a little) up or down to reset the tree. In general, the updated\nvalues should be smaller (they mostly move up, since they have the latest meshpoint\np as a new candidate in finding the shortest route to the original interface).\nThe Fast Marching Method finds distances to N meshpoints in time O(N log N ).\nThe method applies when the front moves in one direction only. The underlying\nequation is F |rT | = 1 (Eikonal equation with F > 0). The front never crosses a\npoint twice (and the crossing time is T ). If the front is allowed to move in both\ndirections, and F can change sign, we need the initial value formulation (3).\nLagrangian versus Eulerian\nA fundamental choice in analyzing and computing fluid flow is between Lagrange\nand Euler. For the minimizing function in optimization, they arrived at the same\n\"Euler-Lagrange equation\". In studying fluids, they chose very different approaches:\nLagrange follows the path of each particle of fluid. He moves.\nEuler sees which particles pass through each point. He sits.\nLagrange is more direct. He \"tracks\" the front. At time zero, points on the front have\npositions x(0). They move according to vector differential equations dx/dt = V (x). If\nwe mark and follow a finite set of points, equally spaced at the start, serious difficulties\ncan appear. Their spacing can get very tight or very wide (forcing us to remove or add\nmarker points). The initial curve can split apart or cross itself (changes of topology).\nThe level set method escapes from these difficulties by going Eulerian.\nFor Euler, the x-y coordinate system is fixed. He \"captures\" the front implicitly,\nas a level set of (x, y, t). When the computational grid is also fixed, we are con\nstantly interpolating to locate level sets and compute distance functions. Squeezing\nor stretching or tangling of the front appear as changes in , not as disasters for the\nmesh.\nThe velocity v on the interface determines its movement. When the level set\nmethod needs v at a meshpoint off the interface, a good candidate is the value of v\nat the nearest point on the interface.\nUpwind Differencing\nThe level set finite difference method is properly developed in the books by its origina\ntors: Sethian [ ] and Osher and Fedkiw [ ]. Here we concentrate on an essential point:\n\nc\n5.7. LEVEL SETS AND THE FAST MARCHING METHOD\n2006 Gilbert Strang\nupwind differencing. Recall from Section 1.\nthe three simplest approximations\nF, B, Cto the first derivative d/dx-Forward, Backward, Centered :\n(x + h) - (x)\n(x) - (x - h)\n(x + h) - (x - h)\nF\nB\nC\nh\nh\n2h\nWhich do we use in the simple convection equation d/dt + a d/dx = 0? Its true\nsolution is (x - at, 0). The choice of finite differences depends on the sign of a.\nThe flow moves left to right for a < 0. Then the backward difference is natural--the\n\"upwind\" value (x-h, t) on the left should contribute to (x, t+t). The downwind\nvalue (x + h, t) on the right moves further downwind during the time step, and has\nno influence at x.\nWhen the movement of the solution (and the wind) is right to left (with a > 0),\nthen the forward difference will use the appropriate upwind value (x + h, t) along\nwith (x, t), in computing the new (x, t + t).\nNotice the time-step limitation |a|t h. In time t, the \"wind\" will bring\nthe true value of from x + at to the point x. If a > 0 and finite differences reach\nupwind to x + h, that must be far enough to include information at x + at. So the\nCourant-Friedrichs-Lewy condition is at h. The numerical waves must propagate\nat least as fast as the physical waves (and in the right direction!). Downwind differ\nencing is looking for that information on the wrong side of the point x, and is doomed\nto failure. Centered differencing in space is unstable for ordinary forward Euler.\nBy careful choice of the right finite differences, Osher has constructed higher-order\nessentially non-oscillatory (ENO) schemes. A central idea in nonlinear problems,\nwhere the differential equation has multiple solutions (see Section\n), is to choose\nthe \"viscosity solution.\" This physically correct solution appears in the limit as an\nextra u xx diffusion term goes to zero. With good differencing the viscosity solution\nis the one that appears as x ! 0.\nAt this point, the level set method does not appear in large production codes.\nIn research papers it has successfully solved a great variety of difficult nonlinear\nproblems."
        }
      ],
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/",
      "course_info": "18.086 | Graduate",
      "subject": "General"
    },
    {
      "course_name": "Integral Equations",
      "course_description": "This course emphasizes concepts and techniques for solving integral equations from an applied mathematics perspective. Material is selected from the following topics: Volterra and Fredholm equations, Fredholm theory, the Hilbert-Schmidt theorem; Wiener-Hopf Method; Wiener-Hopf Method and partial differential equations; the Hilbert Problem and singular integral equations of Cauchy type; inverse scattering transform; and group theory. Examples are taken from fluid and solid mechanics, acoustics, quantum mechanics, and other applications.",
      "topics": [
        "Mathematics",
        "Applied Mathematics",
        "Calculus",
        "Differential Equations",
        "Mathematical Analysis",
        "Science",
        "Physics",
        "Theoretical Physics",
        "Mathematics",
        "Applied Mathematics",
        "Calculus",
        "Differential Equations",
        "Mathematical Analysis",
        "Science",
        "Physics",
        "Theoretical Physics"
      ],
      "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nPrerequisite\n\nBasic theory of one complex variable and ordinary differential equations (for example, either course Functions of a Complex Variable (\n18.112\n) or Advanced Calculus for Engineers (18.075) or Complex Variables with Applications (18.04)), or permission of Instructor.\n\nMain Textbook\n\nMasujima, M.\nApplied Mathematical Methods of Theoretical Physics - Integral Equations and Calculus of Variations\n. Weinheim, Germany: Wiley-VCH, 2005. ISBN: 3527405348.\n\nHomework\n\nThere are eight homework assignments, each due 1.5 to 2 weeks after they are handed out. There are also two practice sets that will not be graded.\n\nExams\n\nThere are no exams in this course.\n\nGrading\n\nGrading is based entirely on the homework.",
      "files": [
        {
          "category": "Resource",
          "title": "ps_1.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/91f7f1dc5c27991f4de063141af387e4_ps_1.pdf",
          "content": "Z\nZ\n18.307: Integral Equations\nM.I.T. Department of Mathematics\nSpring 2006\nProblem Set 1\nDue: Wednesday, 03/01/06\n1. (Prob. 2.5 in text by M. Masujima.) Consider the integral equation\nZ\nn\nn\nx - y\nu(x) = 1 +\ndy\nu(y),\n0 x 1.\nx - y\n(a) Solve this equation for n = 2. For what values of does the equation\nhave no solutions?\n(b) Discuss how you would solve this integral equation for arbitrary positive\ninteger n.\n2. (Prob. 2.4 in text by M. Masujima.) Solve the equation\nu() = 1 +\ndλ sin(λ - ) u(λ),\n0 < 2,\nwhere u() is periodic with period 2. Does the kernel of this equation have\nany real eigenvalues?\n3. (Prob. 8.17 in text by M. Masujima.) Solve the nonlinear equation\nu(x) -\ndy u 2(y) = 1.\nIn particular, identify the bifurcation points of this equation. What kind\nof point is = 0? What are the non-trivial solutions of the corresponding\nhomogeneous equation?"
        },
        {
          "category": "Resource",
          "title": "sol1.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/93ee36b2ba76d4c2e632ee6a1ecdf742_sol1.pdf",
          "content": ""
        },
        {
          "category": "Resource",
          "title": "ps_2.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/86d7a1986276850ec41ee84df1d021c7_ps_2.pdf",
          "content": "Z\n18.307: Integral Equations\nM.I.T. Department of Mathematics\nSpring 2006\nHomework 2\nDue: Wednesday, 03/08/06\n4. (Read Sec. 2.4 in text by M. Masujima.) (a) Find a suitable Green's function for the time-\ndependent, one-dimensional Schr odinger equation\ni t(x, t) + xx(x, t) = V (x, t) (x, t),\nwith the initial value (x, 0) = a(x).\n(b) Express this initial-value problem in terms of an integral equation. In what case and how\nwould you proceed to solve this equation approximately? Explain.\n5. (Prob. 2.14 in text by M. Masujima.) (a) Find the Green's function for the partial differential\nequation\nutt - uxx = p - uxx u x,\nu = u(x, t), -1 < x < 1, t > 0,\nwith the initial conditions u(x, 0) = a(x) and ut(x, 0) = b(x); is a constant. This equation\ndescribes the displacement of a vibrating string under the distributed load p = p(x, t).\n(b) Express this initial-value problem in terms of an integral equation. Explain how you would\nfind an approximate solution if were small.\nHint: Find a function u0(x, t) that satisfies the wave equation, i.e., u0,tt - u0,xx = 0, and the\ngiven initial conditions.\n6. (Prob. 3.4 in text by M. Masujima.) Consider the Volterra equation of the first kind\nx\nf(x) =\ndy K(x - y) u(y),\nf(0) = 0.\n\nSolve the equation for K(x) = ln x and arbitrary (admissible) f(x). Hint:\n1 dx e-x ln x =\n-, where = 0.5772156649 . . . is Euler's constant."
        },
        {
          "category": "Resource",
          "title": "sol2.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/1ce95ed2183f4b3796ea9bda922c5382_sol2.pdf",
          "content": ""
        },
        {
          "category": "Resource",
          "title": "ps_3.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/9fb37a3f0b3e981c750d7323720c5560_ps_3.pdf",
          "content": "(\nZ 1\nZ\n18.307: Integral Equations\nM.I.T. Department of Mathematics\nSpring 2006\nHomework 3\nDue: Wednesday, 03/15/06\n7. (Similar to Prob. 3.9 in text by M. Masujima.) Solve the integral equation\n1 dy K(x, y) u(y) =\n1, where\nxy + (x - y)-1/2 ,\nx > y\nK(x, y) =\nxy,\nx < y.\n8. (From Prob. 3.5 in text by M. Masujima.) Consider the integral equation\nu(x) = f(x) +\ndy e a(x-y) u(y),\na > 0, -1 < x < 1.\n(1)\nx\n(a) Is the kernel square integrable? Explain.\n(b) Consider the homogeneous counterpart of (1), i.e., set f 0, and determine the 's for\nwhich the resulting equation has non-trivial solutions, if there are any. Is the kernel spectrum\n(i.e., this set of 's) discrete or continuous?\n(c) Solve Eq. (1) for f(x) = 1. How many arbitrary constants does the solution have when\n> 0? How about < 0?\n(d) Consider the integral equation stemming from (1) by replacing the kernel by its transpose,\nand find the non-trivial solutions w(x) and the corresponding 's.\n9. (Prob. 2.11 in text by M. Masujima.) In solid-state physics, the effect of periodic forces in\ncrystals on the steady-state motion of electrons is usually described by the time-independent\nodinger equation with the periodic potential V (x) = -(a2 + k2 cos x):\nSchr\nd2 (x)\n+ (a 2 + k2 cos x) (x) = 0.\ndx2\nShow directly that even periodic solutions of this equation, which are called even Mathieu\nfunctions, satisfy the homogeneous integral equation\ndy e k cos x cos y (y).\n(x) =\n-\n10. Antennas fed by transmission lines are often modeled by tubular dipoles with a current I(x)\nthat satisfies the Hall en integral equation:\nZ h\ndy K(x - y) I(y) = A sin(k x ) + C cos(kx),\nx < h,\n-h\n| |\n| |\nwhere A and C are constants, h is the length of the dipole and k > 0 is proportional to fre\nquency. The kernel K(x) is a known yet complicated function. In order to apply numerical\nmethods to this equation, the exact kernel K is sometimes replaced by the simpler (approxi\nmate) kernel\nik\np\nx2 +a2\n1 e\nKap(x) =\n,\n4 p\nx2 + a\nwhere a is the radius of the dipole tube. Give an argument to show that, with this approximate\nkernel, the equation for I(x) has no solution."
        },
        {
          "category": "Resource",
          "title": "sol3.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/79d43157a02d46df60287118a4dad327_sol3.pdf",
          "content": ""
        },
        {
          "category": "Resource",
          "title": "ps_4.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/b28624e722459272aaa1c86e00d3ac32_ps_4.pdf",
          "content": "18.307: Integral Equations\nM.I.T. Department of Mathematics\nSpring 2006\nHomework 4\nDue: Wednesday, 03/22/06\n11. (Similar to Prob. 4.1 in text by M. Masujima.) Show the following correspondence between\nthe kernel K(x, y) of the Fredholm equation and the determinant D() defined in class. What\nare the kernel eigenvalues in each case? Explain.\n(a) K(x, y) = ± 1,\nx ≥ [0, 1]\n≤\nD() = 1 inf .\n(b) K(x, y) = g(x) g(y),\nx ≥ [a, b]\n≤\nD() = 1 -\nR b dx g(x)2 .\na\n(c) K(x, y) = x + y,\nx ≥ [0, 1]\n≤\nD() = 1 - - 12 .\n(d) K(x, y) = x2 + y ,\nx ≥ [0, 1]\n≤\nD() = 1 - 3 - 4 2 .\n(e) K(x, y) = xy(x + y),\nx ≥ [0, 1]\n≤ D() = 1 - 2 - 1 2 .\n12. Consider the Fredholm equation of the second kind\nZ b\nu(x) = f (x) +\ndx0 K(x, x ) u(x ),\na x b.\na\nx\n(a) For b = +≡ , make the changes of variable t = 1+x and t0 =\nx0\n0 , which in turn renders\n1+x\nthe integration range finite. Write the original equation in terms of t and t0 .\n(b) Symmetrize the resulting kernel \"as much as possible\" by defining (1 - t) (1 - t0) (t, t0)\nK(x, x0). Show then that | | = | K| and that the norm of the new inhomogeneous term also\nremains the same.\n13. Consider the integral equation for the scattering of a non-relativistic electron by a potential,\nZ 1\nik|x-y|\ne\nκ(x) = e ikx +\ndy\nV (y) κ(y),\n-≡ < x < ≡ .\n-1\n2ik\nSymmetrize the kernel and find the first 2 terms of the Taylor series for the functions D()\nand N (x, y; ) defined in class. The ratio of these two series yields the improved Born series\nof the scattering amplitude κ. Calculate this amplitude.\n14. (Prob. 4.17 in text by M. Masujima.) In the theoretical search for \"supergain antennas,\"\nmaximizing the directivity in the far field of axially invariant currents j = j(α) that flow along\nthe surface of infinitely long, circular cylinders of radius a leads to the following Fredholm\nequation for the (unknown) density j:\nj(α) = e\nZ 2 dα0\nα - α0\nika sin -\nJ0 2ka sin\nj(α0),\n0 α < 2λ;\n2λ\nα is the polar angle of the circular cross section, k is a positive constant proportional to\nfrequency, is a parameter (Lagrange multiplier) that expresses a constraint on the current\nmagnitude, 0, and J0(x) is the Bessel function of zeroth order.\n(a) Determine the eigenvalues of the homogeneous equation.\n(b) Solve the given inhomogeneous equation in terms of Fourier series.\nR 2\nHints for (a), (b): Use the integral formula Jn(x) = 2\n\ndα0 eix sin 0 e-in0 , n: integer and\nJn: Bessel function of nth order."
        },
        {
          "category": "Resource",
          "title": "sol4.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/3ae5bc5bb1dd4483a93cbc8cf1bc9e44_sol4.pdf",
          "content": ""
        },
        {
          "category": "Resource",
          "title": "ps_5.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/16a50f945649795ea68e6185c0f2a857_ps_5.pdf",
          "content": ".\n18.307: Integral Equations\nM.I.T. Department of Mathematics\nSpring 2006\nHomework 5\nDue: Wednesday, 04/05/06\n15. (Probs. 4.3 & 4.4 in text by M. Masujima.) By using Fourier transform, solve the integral\nequation\n\nu(x) = f(x) +\ndy e-|x-y| u(y),\n-1 < x < 1,\n-1\nfor the following cases: (a) f(x) = x, x > 0; 0, x < 0, and (b) f(x) = x, -1 < x < 1.\nHint: In (a) you must define the Fourier transform f (k) of f(x) in the suitable part of\nthe complex plane, so that the integral for f (k) converges. In (b), you may write f(x) =\nf1(x) + f2(x) where fi(x) (i = 1, 2) is zero for either x < 0 or x > 0, use the solution of part\n(a), and then superimpose to get the final solution.\n16. (Prob. 5.9 in text by M. Masujima.) By using the bilinear formula for a symmetric kernel\n(which was given in class) show that, if ˇ is an eigenvalue of the symmetric kernel K(x, y),\nthen the integral equation\nb\nu(x) = f(x) + ˇ\ndy K(x, y)u(y),\na x b,\na\nhas no solution, unless f(x) is orthogonal to all the eigenfunctions corresponding to ˇ\n17. Consider the Fredholm integral equation of the 2nd kind\nu(x) = f(x) +\ndy min{x, y} u(y),\nwhere min{x, y} denotes the smallest of x and y.\n(a) Find all non-trivial solutions un(x) and corresponding eigenvalues n for f 0.\nHint: Obtain a differential equation for u(x) with the suitable conditions for u(x) and u0(x).\n(b) For the original inhomogeneous equation (f ≡= 0), will the iteration series converge? Ex\nplain.\n(c) Evaluate the series P\nn -2 by using an appropriate integral.\nn"
        },
        {
          "category": "Resource",
          "title": "sol5.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/cbac9afbb2907e44e2f03335b02b9e84_sol5.pdf",
          "content": ""
        }
      ],
      "source_url": "https://ocw.mit.edu/courses/18-307-integral-equations-spring-2006/",
      "course_info": "18.307 | Graduate",
      "subject": "General"
    },
    {
      "course_name": "Advanced Algorithms",
      "course_description": "No description found.",
      "topics": [
        "Engineering",
        "Computer Science",
        "Algorithms and Data Structures",
        "Theory of Computation",
        "Engineering",
        "Computer Science",
        "Algorithms and Data Structures",
        "Theory of Computation"
      ],
      "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nDescription\n\nThis is a graduate course on the design and analysis of algorithms, covering several advanced topics not studied in typical introductory courses on algorithms.\n\nPrerequisites\n\nPrerequisites include \"Introduction to algorithms\" (at the level of 18.410J / 6.046J), linear algebra (at the level of 18.06 or 18.700), and mathematical maturity (since we'll be doing a lot of correctness proofs). The course is especially designed for doctoral students interested in theoretical computer science.\n\nRequirements\n\nThere will be biweekly problem sets and students will also be expected to take turns to scribe lecture notes. It is thanks to the scribes that we can have a good set of lecture notes with many details.\n\nTextbook\n\nThere is no textbook required for the course.\nLecture notes\nare available for the current term as well as selected lecture notes from a\nprevious term\n. Reference textbooks for each topic are listed in the\nreadings\nsection.\n\nCourse Outline\n\nTopics we will be covering this year include:\n\nNetwork flows (max flow and min-cost flow/circulation)\n\nData structures (fibonacci heaps, splay trees, dynamic trees)\n\nLinear programming (structural results, algorithms)\n\nDealing with intractability: approximation algorithms (techniques for design and analysis)\n\nDealing with large data sets (compression, streaming algorithms, compressed sensing)\n\nComputational geometry",
      "files": [
        {
          "category": "Assignment",
          "title": "Problem Set 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/60796425602758eff3685ba9b179928e_ps1.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set 1\nCollaboration policy: collaboration is encouraged. Here are more precise rules:\n1. You must first think about the problems on your own,\n2. You must write up your own solutions, independently,\n3. You must record the name of every collaborator,\n4. You must actually participate in solving all the problems. This is difficult in\nvery large groups, so you should keep your collaboration group limited to 3 or\n4 people in a given week.\nProblems:\n1. Show a sequence of operations that can lead to a Fibonacci heap on n elements\nwhich is a chain.\n2. Suppose that Fibonacci heaps were modified so that a node is marked after\nlosing k - 1 children (as opposed to just one child in the original setting), and\nthus a node is cut after losing k children.\n(a) Assuming that k is a constant, show that this will improve the amortized\ncost of decrease key (to a better constant) while it will increase the amor\ntized cost of delete-min (by a constant factor). You may need to change\nthe potential function for the analysis.\n(b) Could one take k growing with n and still have logarithmic ranks for every\nnode?\n3. Consider the Ford-Fulkerson augmenting path algorithm for the maximum flow\nproblem seen in lecture, and assume that at every iteration the shortest (in\nterms of number of edges) augmenting path in the residual graph is selected\n(recall that the algorithm first finds the shortest path in the residual graph\ncorresponding to the current flow, then pushes as much flow on it so as not to\nviolate the capacity constraints, and then repeats until there is no augmenting\npath in the residual graph).\nPS1-1\n\n(a) Show that the length of the shortest path in the residual graph does not\ndecrease from one iteration to the next.\n(b) Show that the length of the shortest path in the residual graph must in\ncrease at least every m iterations, where m is the number of edges in the\ngraph.\n(c) Deduce that the total number of augmentations must be O(nm) where n\nis the number of vertices.\n(d) Deduce that the total running time of the algorithm is O(nm 2).\n4. Consider a maximum s-t flow problem with integer capacities u. We saw in lec\nture that the Ford and Fulkerson generic algorithm could take O(mU) iterations,\nwhere U is the maximum capacity of any edge, and this is not polynomial. In\nthis problem, you will show that the algorithm can be made polynomial through\na simple technique known as scaling.\n(a) Suppose we have a maximum flow f for our capacitated network, and we\nincrease the capacity of some of the edges by 1 unit. f may not remain\noptimal. Prove that the maximum flow value increases by at most m,\nwhere m is the number of edges. What is the running time of Ford and\nFulkerson if we start from our given flow f?\n(b) Let k = ∪log2 U⊆ and consider k + 1 maximum flow instances. In the jth\none where j = 0, 1, · · · , k, the capacities u(v, w) are replaced by\nu(v, w)\nuj(v, w) =\n.\n2j\nWhen j = 0, the problem is our original maximum flow instance. When\nj = k, all capacities are either 0 or 1. Describe how we can efficiently\nsolve the maximum flow problem for the original capacities by solving the\nproblem with the capacities uj for j from k down to 1. What is the running\ntime of the resulting algorithm?\n5. Consider a capacitated network G = (V, E). Let uv denote the maximum flow\nvalue from u to v. Prove that\nst min(su, ut)\nfor any s, t, u ≥ V .\n6. We have seen in lecture that the maximum cardinality matching problem in a\nbipartite graph G = (V, E) with V = A [ B can be formulated as a maximum\nflow problem in a network H with vertex set V [{s, t} and appropriately defined\narcs and capacities. For any matching M in G, there is a corresponding 0 - 1\nPS1-2\n\nflow f (i.e. with all (raw) flow values 0 or 1) in H with |f| = |M|, and vice\nversa.\nUse the max-flow min-cut theorem to show that there exists a matching M in\nG of cardinality |A| (i.e. every vertex in A is incident to an edge of M) if and\nonly if for every S ∃ A, we have |N(S)| |S| where N(S) = {v ≥ B : 9u ≥ S\nwith (u, v) ≥ E} is the set of neighbors of S (or neighborhood).\n7. Consider the following 2-player game between Alice and Bob. We are given a\nbipartite graph G = (V, E) where V is partitioned into A [ B (thus A and B\nare disjoint). Alice first chooses a vertex a1 ≥ A. Then Bob selects a vertex\nb1 ≥ B which is adjacent to a1. Alice now needs to select a vertex a2 ≥ A which\nis adjacent to b1, and different from a1. And we keep alternating between the\n2 players. At any stage, a player needs to select a vertex which is (i) adjacent\nto the last vertex selected by its opponent and (ii) distinct from all previously\nselected vertices. A player loses as soon as he/she can't select such a vertex.\nFor example, if the graph is a path u - v - w - x on 4 vertices with A = {u, w}\nand B = {v, x} then Bob has a winning strategy. If Alice first plays u then Bob\nwill play v (and then Alice w and Bob x), and if she plays w, he will play x. It\nis also easy to construct instances where Alice has a winning strategy (e.g. a\ncomplete bipartite graph with |A| > |B|).\nDesign a polynomial-time algorithm which given a bipartite graph decides whether\nAlice or Bob has a winning strategy. And explain what the corresponding win\nning strategy (for that player) is.\n8. Which question did you like the most (excluded this one...)? Which question\ndid you like the least?\nPS1-3"
        },
        {
          "category": "Assignment",
          "title": "Problem Set 1 (2001)",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/f03a89acb3d52b0876da75cad4e1b1fb_homework1.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.864 Advanced Algorithms\nProblem Set 1\n1. C d d e r P = ( z : A z < b+>0),whmeAismxn. Showtha~ifaisavertex\nofP thed we crtn 5 d m t ~1 and J wit31 tbe fo11mbg pmp&ies.\n(a) 1s {l,...lrn), J & (1,...,n} and II)=IJI.\n(b) A$ is invertible where A$ isthesubmatrix of A c o r m s p ~ ~\nto the POW\nin1 a n d & e m 1 ~ i nJ.\n(c) zj = O f a j # J md XJ = (A:)-'@\nnhm??denotes the m&ri&ion of b\nto t h ~ildics in I.\n(Aint: Consider Q ={(z,s):&+l e =b,s> 0, s 2 O).)\n2. In his paper in FOCS 92, Tbrnasz RBdaL needs a d\nt\n\nof the faII+\niorm\n(Page 662 of the hcmdiqp):\nLamma 1 Let c E EP and vk E (0,l)\" !OF k = 1,...,q such thd 2 1 ~ ~ 1 4\nl ~ e l f o . r k = I,...,q-1. Ahmrmethdyqc=l. Thsnqs f(n).\nh other nrords, given any set ofn tposgibbnepkim) numbers, one camat find\nm m\nthan f(n)~ubsumsof thaw nnmbera which chmamin e66Jolpted u e bv\n& factar of at Xm2.\nRadsiL pr-\nthe d\nt fnr f(n)= O(nPlogn) and conjectures. that f(n)=\nW(n]where €Ydeaotes the omisaicm of lo@-c\nt-.\nUelng b w pro-\ngnmtmhg3p are asked taimprove hisWt to f(n)=O(nlogn).\n(a) O m a wetor c and a set oS p\nthi hypothesis af the\nLemma,write &set ofineqmlitieainths~abJ,wx~\n2 L , i = l ...n,mch\nthat zi = 1 ~ is1 a feasible vector, and for asrg Mbl~vmti3~z' thm $\na ctmesponding vector d sslti&kg the hpothwis ofthe Lemmafor ths\na\nm\n\nset of mbsm,\n(b) P m thst &are mast exlst s vector d satisfying the hypotheah of the\nL\nm with c' ofthe forin (dl/d,&/d,. . . ,&/d) fagame intqpm Id[,idll,\n*..*\n=2O(nlw.) *\n(Hint: ase Problem 1.)\n(c) Deduce that 1(n)=0(nlogn).\n\n(d) (Not part of the problem set; only for those who like challenges ... A\nguaranteed A+ for anyone getting this part without outside help.) Show\nthat f(n) = n(n1ogn).\n3. The maximum flow problem on the directed graph G = (V,E) with capacity\nfunction u (and lower bounds 0) can be formulated by the following linear\nprogram:\nsubject to\n(xij represents the flow on edge (2, j); the flow has to be less or equal to the\ncapacity on any edge and flow conservation must be satisfied at every vertex\nexcept the source s, where we try to maximize the flow, and the sink t.)\n(a) Show that its dual is equivalent to:\nsubject to\n(b) A cut is a set of edges of the form {(i,j) E E : i E S,j $! S) for some\nS c V and its value is\nIt separates s from t if s E S and t $! S.\nShow that a cut of value W separating s from t corresponds to a feasible\nsolution y, x of the dual program such that\n\n(c) Given any (not necessarily integral) optimal solution y*, z* of the dual\nlinear program and an optimal solution x* of the primal linear program,\nshow how to construct from z* a cut separating s from t of value equal to\nthe maximum flow.\n(Hint: Consider the cut defined by S = {i : 5 0) and use complementary\nslackness conditions.)\n(d) Deduce the max-flow-min-cut theorem: the value of the maximum flow\nfrom s to t is equal to the value of the minimum cut separating s from t.\n4. Consider the following property of vector sums.\nTheorem 2 Let ul, . . . , un be d- dimensional vectors such that llvi1 1 5 1for\ni = 1,. . . , n (where 11 .I1 denotes any norm) and\nThen there exists a permutation ?r of (1,. . . , n) such that\nfor k = 1,.. . ,n.\nIn this problem, you are supposed to prove this theorem by using linear pro-\ngramming techniques.\n(a) Suppose we have a nested sequence of sets\nwhere lVk 1 = k for k = d, d + 1,. . . , n. Suppose further that we have\nnumbers Xki satisfying:\nfor k = d, . . . , n. Define a permutation n- as follows: set n-(1),. . . , a(d) to\nbe elements of Vd in any order, and set a(k) to be the unique element in\nVk \\VkP1 for k = d + 1,. ..,n.\nShow that this permutation satisfies the conditions of Theorem 2.\n\n(b) Show that there exist Xni, i = 1 . . .n, satisfying (I),(2) and (3) for k = n.\n(c) Suppose we have constructed Vn,. .. ,\nand Xji for j = k +1,.. . ,n and\ni E 1/3 satisfying (I), (2) and (3) for k + 1,.. .,n (where k 2 d). Prove\nthat the following system of d + 1 equalities ((4) contains d equalities),\nk + 1inequalities and k + 1nonnegativity constraints has a solution with\nat least one pi = 0:\nDeduce the existence of the nested sequence and the X's as described in\n(4.\n5. Consider the following optimization problem with \"robust conditions\":\nmin{cTx : x E Rn; Ax 2 b for any A E F),\nwhere b E Rm and F is a set of m x n matrices:\nF = {A: Vi, j ; a F n <- aij 5 azax}.\n(a) Considering F as a polytope in WXn,what are the vertices of F?\n(b) Show that instead of the conditions for all A E F , it is enough to consider\nthe vertices of F. Write the resulting linear program. What is its size? Is\nthis polynomial in the size of the input, namely m, n and the sizes of b, c,\narand a y 7\n(c) Derive a more efficient description of the linear program: Write the condi-\ntion on x given by one row of A, for all choices of A. Formulate this condi-\ntion as a linear program. Use duality and formulate the original problem\nas a linear program. What is the size of this one? Is this polynomial in\nthe size of the input?"
        },
        {
          "category": "Assignment",
          "title": "Problem Set Solution 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/ab2d12f4b9a57fa59e9a0909a36a31e6_solution1.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set Solution 1\n1. Consider P = {x : Ax 5 b, x 2 0), where A is m x n. Show that if\nx is a vertex of P then we can find sets I and J with the following\nproperties.\n(a) I c {I,.. . ,m}, J c {I7... ,n} and 111 = IJI.\n(b) A:\nis invertible where A:\nis the submatrix of A corresponding to\nthe rows in I and the columns in J.\n( c ) xj = 0 for j $- J and XJ = (A:)-lb'\nwhere b' denotes the restriction\nof b to the indices in I.\n(Hint: Consider Q = {(x, s) : Ax + Is = b, x 2 0,s 2 O).)\nUsing the hint we turn our attention to Q = {(x,s) : Ax+ Is = b, x > 0, s 2 0).\nIf we let (x, s) be a pair such that x E P and s is the unique vector of slack\nvariables associated with x (s = b - Ax), it is not hard to show that if x is a\nvertex of P then (x, s) is a vertex of Q. Assume that x is a vertex of P but\nthere is a (y, t) such that (x, s) & (y, t) E Q and (y, t) # 0. Then we have\nA(x * y) + (sh t) = b,x * y 2 0 , s * t 2 0. This implies that A(x + y) 5 b\nand A(x - y) 5 b. Since x is a vertex, this implies y = 0. Then solving for t in\nAx+ (s+t) = b we find that it must be zero as well which contradicts (y, t)# 0.\nWe can now take advantage of the fact that Q is in the special form (Ax =\nb, x 2 0). If (x, s) is a vertex of Q then there is a subset B c (1,.. . , n + m}\nsuch that 1B1 = m and\n(a) (x,s ) ~\n= 0 for N = {I,.. . ,n+ m)\\ B\n(b) (AI I) is non singular\nLet J c B be the set of columns involving A of (A I I). Notice that if I JI = k, k\nof the x variables are basic and m-k of the s variables are basic. So xj = 0 for\nj $ J and and k of the s variables are zero. We take the rows corresponding to\nthe zero components of s as the set I . Then\nand A$ is invertible, so\n\n2. In his paper in FOCS 92, Tomasz Radzik needs a result of the follow-\ning form (Page 662 of the Proceedings):\nLemma 1 Let c E Rn and yk E (0, lInfor k = 1,. ..,q such that 21yk+lcl 5\nIykcl for k = 1,..., q - 1. Assume that ly,cl = 1. Then q 5 f(n).\nIn other words, given any set of n (possibly negative) numbers, one\ncannot find more than f (n) subsums of these numbers which decrease\nin absolute value by a factor of at least 2.\nRadzik proves the result for f (n) = O(n2log n) and conjectures that\nf (n) = O*(n) where 0*denotes the omission of logarithmic terms.\nUsing linear programming, you are asked to improve his result to\nf (n) = O(n1ogn).\n(a) Given a vector c and a set of q subsums satisfying the hypothesis\nof the Lemma, write a set of inequalities in the variables xi 2\n0, i = 1...n, such that xi = lcil is a feasible vector, and for any\nfeasible vector x' there is a corresponding vector d satisfying the\nhypothesis of the Lemma for the same set of subsums.\nWe have a set of inequalities of the form 2 1 yi+lcl 5 1 yicl for 1<- i <- q- 1and\nlylcl = 1,and we have a vector c which satisfies them. To obtain a linear\nsystem, we need to remove the absolute value signs. Let yi = yisgn(yic).\nThen lyicl = yic. The system becomes 2yi+,c 5 y,!c for 1 5 i 5 q - 1\nand yic = 1and the original c is still feasible. To limit the solution space\nto vectors of the form x 2 0 a similar trick is used. We replace elements\ncj that are negative by -xj and non-negative elements by xj. The linear\nsystem that remains has a solution x 2 0, namely xj = I cj 1. A solution c'\nto the original inequalities can be obtained from any feasible x in the newly\nconstructed set of inequalities by negating the value of the ith element of\nx if the ith element of c was negative. This results in the same number of\ninequalities as we had originally, namely q.\n(b) Prove that there must exist a vector d satisfying the hypothesis\nof the Lemma, with d of the form (dl/d, dz/d, ...,dn/d) for some\nintegers ldl, Id1 1 , ...,ldnl = 2O(\"logn).\n(Hint: see Problem 1.)\nThe polytope defined by the inequalities above is nonempty. This means\nthat the polytope has a vertex. Our system looks like Ax 2 b and x 2 0,\nwhere A is a q by n matrix containing entries between -3 and 3 (since\nevery entry is the difference of two integers, one being &2 or 0, the other\nbeing f1or 0) and b has one nonzero element which is f1. From the first\nproblem, we know that the nonzero components, say y, of a vertex satisfy\n\nA'y = b' where A' is an invertible submatrix of A and b' is a subvector\nof b. Notice that 1det(Af)\nI 5 n!3\" = 2°(n logn). As in class (by Cramer's\nrule), we know that we can set d to be Idet(Af)I and the (nonzero) di's\nto be determinants of submatrices of A'. By the same argument, these\ndeterminants are also upper bounded by 2'(\"l0gn), proving the result.\n(c) Deduce from the above that f (n) = O(n log n).\nMultiplying the vector d by d yields an integer solution to 2yl+,x 5 yix\nfor 1 5 i 5 q - 1with elements of value 20(\"1°gn). Thus the largest sum\nthat can be obtained by a subset is 2°(n10gn). As the first subset sums to\nat least one (since the di's are integers), the number of times the sum can\ndouble is at most O(n log n).\n3. The maximum flow problem on the directed graph G = (V,E) with\ncapacity function u (and lower bounds 0) can be formulated by the\nfollowing linear program:\nmax w\nsubject to\n(xij represents the flow on edge (i, j); the flow has to be less or equal\nto the capacity on any edge and flow conservation must be satisfied\nat every vertex except the source s, where we try to maximize the\nflow, and the sink t.)\n(a) Show that its dual is equivalent to:\nsubject to\n\nThis is an immediate consequence of the definition of the dual. If one takes\nthe dual of the system of equations and inequalities above, then one gets\nSince adding a constant to all zi7s doesn't change anything, we can require\nthat z, = 0 and zt = 1.\n(b) A cut is a set of edges of the form {(i,j ) E E : i E S,j $ S) for\nsome S c V and its value is\nIt separates s from t if s E S and t $ S.\nShow that a cut of value W separating s from t corresponds to a\nfeasible solution (y, z) of the dual program such that\nFor a cut defined by S c V, we define zi = 0 for i E S, zi = 1for i $ S,\nyij = 1for i E S,j $ S, (i,j ) E E and yij = 0 otherwise. Obviously, (y ,z)\nis a feasible solution and its value is\n(c) Given any (not necessarily integral) optimal solution y*, z* of the\ndual linear program and an optimal solution x* of the primal\nlinear program, show how to construct from r* a cut separating\ns from t of value equal to the maximum flow.\n(Hint: Consider the cut defined by S = {i : zi 5 0) and use\ncomplementary slackness conditions.)\nWe divide the vertices into two sets defined as follows:\n\nEvery edge (i, j) with i E S and j\nS satisfies 2,' - 2; + ytj 2 0. Since\n2: 5 0 and zj > 0 we have that ytj > 0, which by complementary slackness\nimplies that x,*,= u:~.Every edge (j,i) with i E S and j $ S satisfies\nzj - z,' +~5~ > 0, since zj > 0 and z,' 5 0. By complementary slackness\nwe have that xji= 0. Thus, we can write\nwhich is the value of the maximum flow.\n(d) Deduce the max-flow-min-cut theorem: the value of the maxi-\nmum flow from s to t is equal to the value of the minimum cut\nseparating s from t.\nFrom (b) and weak duality, the value of any cut is greater or equal to the\nmaximum flow value. By the analysis above, we can find a cut which is\nequal to the maximum flow. Thus, the minimum cut value must be the\nsame as the maximum flow value.\n4. Consider the following property of vector sums.\nTheorem 2 Let vl, .. . ,vn be d-dimensional vectors such that llvi1 1 5 1\nfor i = 1,...,n (where 11.11 denotes any norm) and\nThen there exists a permutation a of (1,. . . ,n) such that\nfor k = 1,...,n.\nIn this problem, you are supposed to prove this theorem by using\nlinear programming techniques.\n(a) Suppose we have a nested sequence of sets\nwhere lVkl= k for k = d , d + l , .. . ,n. Suppose further that we have\nnumbers Xki satisfying:\n\nfor k = d, . . . ,n. Define a permutation n as follows: set n(l),.. . ,n ( d )\nto be elements of Vd in any order, and set n ( k ) to be the unique\nelement in Vk\\ Vk-1 for k = d + 1,... ,n.\nShow that this permutation satisfies the conditions of Theorem\n2.\nFor k 5 d, the theorem is trivial. By the definition of n and Xki, for k > d\nwe have\n(b) Show that there exist Xni, i = 1 ...n, satisfying (I), (2) and (3)\nfor k = n.\nWe choose simply\nThen\nand\n(c) Suppose we have constructed V,, . . .,\nand Xji for j = k + l , .. . ,n\nand i E V, satisfying (I), (2) and (3) for k +1,. .. ,n (where k 2 d).\nProve that the following system of d+1 equalities ((4)contains d\nequalities), k + 1 inequalities and k + 1 nonnegativity constraints\nhas a solution with at least one Pi = 0:\nDeduce the existence of the nested sequence and the X's as de-\nscribed in (a).\n\nBy the induction hypothesis, we have\nk - d\nPi= k + l - d\nXk+l,i\nsatisfying our inequalities, so the polytope of feasible solutions is nonempty.\nWe want to find a solution with at least one zero coordinate. Consider a\nvertex of the polytope and suppose that for each i, Pi > 0. There are k +1\ncoordinates summing up to k - d, so at most k - d - 1 of them can be\nequal to 1. (If k -d coordinates are equal to 1,the rest is zero.) Therefore\nwe have at least d + 2 coordinates pi,0 < Pi < 1. Let's denote this set of\ncoordinates by J. The corresponding vectors vj, j E J cannot be affinely\nindependent, so there exists a linear combination with y # 0 such that\nFor a small enough t > 0, we obtain two feasible solutions by replacing the\ncoordinates of pj for j E J by Pj lj CTj, which contradicts the assumption\nthat ,Biis a vertex. Therefore a vertex has always a zero coordinate and by\nremoving this coordinate we obtain the subset Vk and the corresponding\ncoefficients Xki = Pi which completes the induction.\n5. Consider the following optimization problem with \"robust condi-\nt ions\" :\nmin{cTx : x E Rn;Ax > b for any A E F),\nwhere b E Rm and F is a set of m x n matrices:\nF = {A : Qi, j ; amin < aij 5 acax).\n2J\n-\n(a) Considering F as a polytope in RmXn,what are the vertices of\nF?\nF is an (rn x n)-dimensional product of intervals. It has 2\"\" vertices A@)\nwhere each coordinate a:;)\nis either a;'\"\nor a;=.\n(b) Show that instead of the conditions for all A E F, it is enough to\nconsider the vertices of F. Write the resulting linear program.\nWhat is its size?\n\nSuppose that x satisfies\nfor every vertex A(').\nAny A E F can be written as a convex linear\ncombination of the vertices:\nBy taking the corresponding linear combination of inequalities (with non-\nnegative coefficients), we get\nwhich is\nTherefore x is a feasible solution if and only if it satisfies the condition for\nevery vertex of F. We can write the optimization problem in the following\nform:\nrnin{cTx : Vk;A@)X 2 b).\nThis is a linear program; however, it has an exponential number of inequal-\nities, namely m2mn,in n variables.\n(c) Derive a more efficient description of the linear program: Write\nthe condition on x given by one row of A, for all choices of A.\nFormulate this condition as a linear program. Use duality and\nformulate the original problem as a linear program. What is the\nsize of this one?\nLet us consider a fixed vector x. It is feasible if the following condition is\nsatisfied for each row ai of the matrix A:\nWe can regard this condition as a linear programming problem:\nNote that the variables are now aij, while x is fixed! By duality, we get an\nequivalent condition for a linear program with variables pi j 7qij:\n\nThis means that x is feasible if there exist pij 2 0, qij 2 0 such that\npij - qij = X j\nand\nAll together, we can write our optimization problem as the following:\nwhich is a linear program in variables X j , pij, qij. It has 2mn +n variables,\nmn equalities, m inequalities and 2mn nonnegativity constraints. The\nlinear program from part (b) has size which is exponential in the size of\nthis one."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/349667bfcb9a02721d104e645926b5d5_ps2.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set 2\nIf you have any doubt about the collaboration policy, please check the\ncourse webpage.\nProblems:\n1. Given a directed graph G = (V, E), a source s ∈ V , a sink t ∈ V and a length\nfunction l : E → R, the fattest path problem is to find a simple path P from s\nto t which maximizes min(v,w)∈P l(v, w).\n(a) Give a modification of Dijkstra's algorithm for the shortest path problem\nwhich solves the fattest path problem. Argue correctness of your algorithm.\n(b) Suppose that all arcs lengths are integer-valued between 1 and m where\nm = |E|. Can you provide an implementation of your algorithm that runs\nin O(m) time? (Hint: Do not use any fancy priority queue.)\n2. In this problem, you will show that the fattest augmenting path algorithm for\nthe maximum flow problem can be implemented to run in O(m) time per itera\ntion after some basic preprocessing. Remember that in the fattest augmenting\npath algorithm, the augmenting path with largest minimum residual capacity\nis chosen at every iteration.\n(a) Show that if we have a total ordering of the residual capacities then the\nfattest augmenting path can be found in O(m) time.\n(b) Show that, this total ordering of the residual capacities can be maintained\nin O(m) time after pushing flow along one augmenting path (how do the\nresidual capacities change)?\n(c) What is the running time ofthe resulting inplementation of the fattest\naugmenting path algorithm?\n3. Consider a directed graph G = (V, E) with a length function l : E → Z and\na specified source vertex s ∈ V . The Bellman-Ford shortest path algorithm\ncomputes the shortest path lengths d(v) between s and every vertex v ∈ V ,\nassuming that G has no directed cycle of negative length (otherwise the problem\nis NP-hard). Here is a description of this algorithm:\nThe Bellman-Ford algorithm computes d(v) by computing dk(v) = the shortest\nwalk1 between s and v using exactly k edges. dk(v) can be computed by the\n1A walk is like a path except that vertices might be repeated.\nPS2-1\n\nP\nrecurrence\ndk(v) = min [dk-1(w) + l(w, v)] .\n(w,v)∈E\nLet hl(v) = min dk(v). It can be shown that if the graph has no negative cycle\nk=1,...,l\nthen hn-1(v) = d(v) for all v ∈ V . Moreover, the graph has no negative cycle\niff, for all v, dn(v) ≥ hn-1(v).\n(You are not required to prove any of the above facts.)\n(a) Let μ ∗ be the minimum average length of a directed cycle C of G, i.e.,\nl(u, v)\nμ ∗(G) =\nmin\nμ(C) = min\n(u,v)∈C\n.\ndirected cycles C\nC\n|C|\nUsing the Bellman-Ford algorithm, show how to compute μ ∗ in O(nm)\ntime.\n(Hint: Use the fact that if we decrease the length of each edge by μ the\naverage length of any cycle decreases by μ.)\n∗\n(b) Can you find the cycle C with μ(C) = μ using only O(n2) additional time?\n(In other words, suppose you are given all the values that the Bellman-\nFord algorithm computes. Can you find a minimum mean cost cycle using\nthis information in O(n2)?)\n4. In this problem, we will propose another way to solve the minimum mean cost\ncycle problem. The resulting algorithm will be quite slow, but the technique\nis widely applicable (and for other problems, this will give the fastest known\napproach). The problem of finding μ ∗ is equivalent to the problem of finding\nthe largest value of μ such that the graph with lengths lμ(u, v) = l(u, v) -μ has\nno negative cycles.\n(a) Argue that for a given value of μ, we can decide whether μ ∗ ≥ μ or\nμ ∗ < μ by performing at most O(A(m, n)) additions of 2 numbers and\nO(C(m, n)) comparisons involving 2 numbers (and no other operations\nexcept control statements).\nPlease state the values you can obtain for\nA(m, n) and C(m, n). Observe that, as we are performing only additions\nand comparisons, all the numbers involved are linear functions of the input\nlengths and μ.\n(b) Now suppose we run the above algorithm with μ equal to the unknown\nvalue μ ∗ . We can easily perform the additions provided that we store all\nthe numbers (including the inputs) as linear functions of μ ∗ (i.e. of the\nform a + bμ∗). Explain how we can resolve the comparisons (even though\nwe do not know μ ∗). (It is normal if the solution requires a fair amount of\ntime to resolve each comparison.) As a function of A(m, n) and C(m, n),\nwhat is the total running time of your algorithm to compute μ ∗?\nPS2-2\n\n5. We argued in lecture that for the maximum flow problem, there always exists a\nmaximum flow which is integer-valued if the capacities are integral. Prove that\na corresponding statement for minimum cost circulations also holds, namely\nthat if the capacities and the costs are integer-valued then (i) the minimum\ncost circulation can be chosen to be integer-valued and (ii) the vertex potentials\nproving optimality can also be chosen to be integer-valued.\n6. In this problem, we will add a time dimension to network flows. Suppose we\nhave a network G = (V, E) in which each arc has unit capacity (u(v, w) = 1 for\nall arcs (v, w)), and we have two special vertices, a source s and a sink t. Our\nnetwork for example could be a computer network and our unit of flow could be\na packet. Each arc also has an integer-valued transit time τ(v, w) ∈ Z+ which\nrepresents the time it takes (a unit of flow or packet) to travel through the arc.\nAt every unit of time, say at time d, only one packet can enter the arc (there\nmight be several packets already travelling through the arc since there could\nhave been packets injected in it at times d-1, d-2, etc.). We can assume that\nvertices can instantaneously accept packets on its incoming arcs and also inject\none packet (if available) on each of its outgoing arcs (and if there are remaining\npackets, they can be queued at the vertex).\nThe first problem we consider is, given a deadline D, to find the maximum\nnumber k(D) of packets that can enter the network at s at time 1 or later and\nleave the network at vertex t at time D or earlier. As an example, suppose that\nour graph has only 3 arcs (s, a), (a, t) and (s, t) each with a transit time of 2.\nThen, if D = 5, the answer should be k(5) = 4 packets. Indeed, we can send 3\npackets along the arc (s, t), entering at times 1, 2 and 3 and leaving at time 3,\n4 and 5 ≤ D. We can also send a 4th packet, along the path (s, a) and (a, t);\nit will enter the arc (s, a) at time 1, arrive at a at time 3 and arrive at t at\ntime 5. (Observe by the way that in this example, no packet had to wait at\nintermediate vertices.)\n(a) Construct a maximum flow instance on a bigger network G ′ = (V ′ , E ′) such\nthat the solution of this maximum flow instance allows you to find k(D)\nand the scheduling (when they travel through each arc) of the packets in\nthe original network G. |V ′| can be of the order of D|V |.\n(b) The solution above is not polynomial when D is part of the input (since\nthe size of the network grows linearly in D). To find a polynomial time\nalgorithm, consider the following circulation problem. Take the original\ngraph G = (V, E) with all arcs of capacity 1 and give arc (v, w) ∈ E a cost\nc(v, w) = τ(v, w). Add one arc (t, s) of infinite capacity and cost equal to\n-D. Let -C∗ be the cost of the minimum cost circulation f ∗ . Prove the\nfollowing claim: C∗ is precisely k(D). Also explain how one can find the\nscheduling of the packets from the minimum cost circulation f ∗ .\nPS2-3\n\n(It might be helpful to first see what happens on the simple example with\n3 arcs given above.)\n(c) Now, suppose that we want to solve the converse problem.\nWe would\nlike to send k packets from s to t so that the last packet arrives at t as\nearly as possible. Propose a polynomial-time algorithm which given k finds\nD(k), the minimum time at which all packets have arrived at t. What is\nthe running time of your algorithm as a function of n = |V |, m = |E|,\nT = max τ(v, w) and k?\n(d) Your algorithm for (c) is probably not strongly polynomial, in the sense\nthat its running time depends on log(T) and/or log(k).\nCan you pro\npose a strongly polynomial-time algorithm? Just sketch it (a few lines are\nenough); do not give all the details. (Kind of hint: this solution will be\nmuch slower than (c) when T and k are small.)\n(By the way, all the results above are still true if the capacities are integers\npossibly greater than 1; in such a case, at every time d, at most u(v, w) packets\ncan be injected in arc (v, w). Arguing about the validity of (b) is slightly more\ndifficult.)\n7. Which question did you like the most (excluded this one...)? Which question\ndid you like the least?\nPS2-4"
        },
        {
          "category": "Assignment",
          "title": "Problem Set 2 (2001)",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/8bf6e97ac73e0c97bedcc48aa086e150_homework2.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/8,854 Advanced Algorithms\nProblem Set 2\n1. The Min s-&Cut problem is the following:\nGiven an undimted gmph G = (V,E), a weight firaction w : E +W, and two\nvertices 8, t E V ,find\nMin 8 -t -Cut(G)=min{w(d(S)) :S C I/,s f S,t$3).\nwhere &($) denote the cut\n(a) Argue (i jwt afew lines) that there is a polynomial-timealgorithmto find\na Min s - t-cut based on linear programming (remember Problem Set 1).\n(Be careful; problem set 1 dehed the Min s-t-cut problem for a directed\ngraph, while this problem considera undirected graphs.) [We will see a\nmuch more efficient algorithm for it (not based on Iinem programming)\nhter this semester,]\nWe are going to develop an algorithm for a generdization of the problem:\nGiven an uadimted graph G = (lr, E), w :E +R? , and an even cardinality\nsubset of vertices T G V ,find\nMzn T-Odd -Cut(G)=min{w(5(S)) :S C V,ISnTI = o d 4\nThat is, we want to optimize over all cuts that separate T into two parts of odd\nsize (since IT1 is even, ISn TI odd implies that T \\ SI odd as well).\n(b) Suppose that IT1= 2,say T = { s , t ) . Wbat is the Min T-Odd-Cut then?\n(c) For a given T\nV,call a cut d(S) T-splitting if 0# SnT # T.\nUsing a s-t-&-Cut\nalgorithm, show how we can 6nd the minimum T-\nsplitting cut in polynomial time. Can you do it in at most IT1 cab to a\nMin s-t-Cut algorithm?\n\n(d) For any lam sets C and D (0# C,D c V ) ,prove the inequally that\n(o) Prove that if 6(C)is sminimum T-splitting cut then there is a minimum\nT-odd-cut 6(D)such that either D E C or C D.\nHint: UE the inquality proved above.\n(f) Use the previous ohemtion to design a recursive algorithm which wlvm\nMin T-Odd-Cut in polynomial time. (Hint: pwibly think about mod-\nm\ng the graph.) HOW many & (inO(.) notation) to s Mig &-Cut\nalgorithm does your algorithm perfom?\n2. Usethe ellipacidmethod to solve the minimummight perfect matchingproblem\n(there: is a more &cient mmbinatorial dgoritlm for it, but here we will use\nthe power of the ellipsoid dgorithm]:\nGiven an undimtai gmph G = (V,E ) and a weight finctiopa w :E -+ N,find a\nset of dga M wvm-nganwy vertex ea;actiy once (a perfect matching) with the\nminim~srntotalweight.\nIn order to formdab this problem w a linear program, we d&e\nthe V-join\npolytope:\nP = catu{XM E {0, lIE :M is a perfect matching}\nwhere XM is the c.har%ter&tic vector of M (xM(e)= 1 if e E M and 0 other-\nwise). ?\"he C Q ~ W\nis d h d =(x,\nE A, & >_ O,Ci& =\nhuU WTIUCA)\nAiai :\n1) (where t b m a t i o n ia finite:).\n(a) Argue that thevertices of P are the chractedsticvector~of perf& match-\ninp. Deduce that if we can optimize\nw,~, over P, we would fud a\nminimumweight perfect matching.\n(b) Suppose now that we can dwide (via linw pro^^ or same other\nway) whether Pi-74~:wTx 5 A) isempty or not, for any dvenX (remember\naJI wei&ts weare integers). Show that by calling an algorithm for this\ndecision problem a polynomial number of fhna(in the &e of the input,\ni.e. IVI, IEl and log(w-) 1, we canfind tbeweight of the mhimm-weight\nperfect matchingp\n(c) With the same amumptiona as in the previous part, can you also fbd\na minimtrm-weight perfect matching (not just its weight, but a h which\nedges are in it) in polynomid time? (There might be merd perfect match-\ning having the m\ne\n\nminFmnm .weightTbut here you need to produce only\none of them. &o,\nthe algorithm does not need to be extremely efficient,\njust palgnomial.)\n\nDue to Jack Edmonds, the perfect matching polytope can be described by the\nfollowing inequalities:\n(d) Show that every vector in P satisfies the above inequalities.\nTake the other implicationfor granted (everyvector satisfying these inequalities\nis in P).\n(e) How many inequalities do we have in this complete description of P? Can\nwe just use any polynomial-time algorithm for linear programming to o p\nt G e over P?\n(f) Show how we can use the ellipsoid method to decide if there exists a perfect\nmatching of weight at most X in polynomial time. How would you select\nthe initial ellipsoid? How would you tike care of the equality constraints\nin the description of P? When can you stop?"
        },
        {
          "category": "Assignment",
          "title": "Problem Set Solution 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/7c1e4b71e427918687f9560c1f5fc500_solution2.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set Solution 2\n1. The Min s-t-Cut problem is the following:\nGiven an undirected graph G = (V,E), a weight function w : E +R+,\nand two vertices s, t E V, find\nMin s - t - Cut(G) = min{w(S(S)) : S c V, s E S,t $ S).\nwhere 6(S) denotes the cut\n6(S) = {(i,j) E E : I{i,j) nSI = 1)\nand\n(a) Argue (in just a few lines) that there is a polynomial-time al-\ngorithm to find a Min s - t-cut based on linear programming\n(remember Problem Set 1). (Be careful; problem set 1defined\nthe Min s-t-cut problem for a directed graph, while this problem\nconsiders undirected graphs.) [We will see a much more efficient\nalgorithm for it (not based on linear programming) later this\nsemester.]\nThe description of the (directed) Min s-t-cut problem as a linear program\nfrom Problem 1shows immediately that we can solve it in polynomial time,\nfor example by the ellipsoid algorithm. Given an undirected graph, we can\ntransform it into a directed graph simply by producing two directed edges\n(one each way) for every undirected edge. Then cuts in the directed graph\ncorrespond exactly to cuts in the undirected graph, with the same weight.\nWe are going to develop an algorithm for a generalization of the\nproblem:\nGiven an undirected graph G = (V,E), w : E -t R+, and an even\ncardinality subset of vertices T\nV, find\nMin T - Odd - Cut(G) = min{w (b(S)) : S c V,IS fl TI = odd}\nThat is, we want to optimize over all cuts that separate T into two\nparts of odd size (since IT1 is even, ISnTI odd implies that IT\\ SI odd\nas well).\n\n(b) Suppose that IT1 = 2, say T = {s,t). What is the Min T-Odd-Cut\nthen?\nSince any T-odd cut must split T into two odd parts, there is only one\nway to do that - have s and t on opposite sides of the cut. Thus the Min\nT-odd cut is exactly the Min s -t cut.\n(c) For a given T C V, call a cut 6(S) T-splitting if 0 # SnT # T.\nUsing a s-t-Min-Cut algorithm, show how we can find the min-\nimum T-splitting cut in polynomial time. Can you do it in at\nmost IT1 calls to a Min s-t-Cut algorithm?\nChoose a fixed s E T. For every cut b(S), we can assume s E S (the\ngraph is undirected and V \\ S defines the same cut). Moreover, for every\nT-splitting cut, there exists a vertex t E T\\ S and such a cut is an s-t-cut\nas well. It is sufficient to find the minimum s-t-cut for every t E T \\ {s}\nand take the minimum of all these cuts.\n(d) For any two sets C and D (0 # C,D c V), prove the inequality\nthat\nw(b(C \\ D)) +w(b(D \\ C))5 w(b(C)) +w(b(D)).\nWe apply the definition of a cut - 6(X) is the set of all edges between X\nand V \\ X:\n(e) Prove that if 6(C) is a minimum T-splitting cut then there is a\nminimum T-odd-cut 6(D) such that either D\nC or C & D.\nHint: Use the inequality proved above.\nSuppose b(C) is the minimum T-splitting cut. If IC nTI is odd then C\nis also the minimum T-odd cut and we can choose D = C. Otherwise\nsuppose IC nTI is even, while b(D') is the minimum T-odd cut.\n\nSince I D' nTI is odd, either I (Dl n C)nTI or I (D' \\ C)n TI is odd. Denote\nby C' either C or V \\ C so that I (Dl n C') nTI is odd. Note that in any\ncase 6(C1) = S(C) and IC' n TI is even.\nSince S(C1) is a T-splitting cut, (V \\ C') n T is nonempty which implies\nthat either (D' \\ C') n T is nonempty or ((V \\ D') \\ C') n T is nonempty.\nWithout loss of generality, we can assume that (Dl \\ C') nT is nonempty,\notherwise we rename D' to V \\ D' which doesn't change the cut (and the\nnew D' still satisfies I (Dl n C') n TI is odd since IC' n TI is even).\nBecause 6(C1) is the smallest T-splitting cut and S(D1 \\ C') is a T-splitting\ncut, we have w(S(C1)) 5 w(6(D1\\ C')). Since I(C' n Dl) n TI is odd,\nI (C'\\ Dl) nTI is odd as well. The smallest T-odd cut is 6(D1), so w (6(D1))5\nw(S(C1\\ Dl)). Comparing these two inequalities with the one given in the\nhint, w(6(C1)) + w(S(D1))2 w(S(C1\\ Dl)) + w(S(D1\\ C')), we find that\nthey must all hold with equality and 6(C1 \\ D') is a minimum T-odd cut\nas well.\nIn case C' = C, we can choose D = C1\\D' and we have a minimum T-odd\ncut D c C. In case C' = V \\ C, we can choose D = V \\ (C' \\ D') and we\nhave a minimum T-odd cut such that C c D.\n(f) Use the previous observation to design a recursive algorithm\nwhich solves Min T-Odd-Cut in polynomial time. (Hint: pos-\nsibly think about modifying the graph.) How many calls (in O(*)\nnotation) to a Min s-t-Cut algorithm does your algorithm per-\nform?\nMinOddCut (G, T )\n{\nC = MinCut (G, T ) ;\ni f (IC n TI = odd) return C ;\nG1 = Contract (G, C) ;\nG2 = Contract (G, V \\ C) ;\nC1 = MinOddCut (GI,\n\\ C) ;\nC2 = MinOddCut (G2, T fl C) ;\ni f (weight ( 6 ( 4 ) ) < weight (6(C2)) return C1;\nelse return C2;\n}\nHere, MinCut (G, T) is supposed to return the minimum T-splitting cut in\nG and Contract (G, C) should merge C into a single vertex and update\nthe edges accordingly (i.e. any edge between a vertex u of C and a vertex\n\nv not in C becomes a new edge between the new shrunk vertex and v; if\nthere are multiple edges between two vertices, we can replace them by one\nedge with weight equal to the sum of the weights).\nThe correctness of the algorithm follows from the previous observations.\nEither the minimum cut S(C) is T-odd, or we can assume that the mini-\nmum T-odd-cut is S(D) where C C_ D or D\nC. Cuts 6(D) where C C_ D\nare equivalent to cuts in the graph G1 where C is contracted to a single\nvertex. Cuts 6(D) where D\nC are equivalent to cuts in the graph G2\nwhere V \\ C is contracted to a single vertex. The smaller of the two cuts\nmust be the minimum T-odd cut.\nFinally, let's analyze the running time of this algorithm. The body of the\nfunction (excluding the recursive calls) runs in time polynomial in the size\nof the input graph (MinCut algorithm + elementary graph operations). It\nremains to estimate the number of recursive calls to MinOddCut. Denote\nthe size of the input set T by t. Note that if the function is called with\nparameter T and it produces recursive calls with parameters TI and T2,\nthen IT1 = ITlI +IT21. Since lz 1 2 2 in the leaves of the recursion tree, the\nnumber of leaves is at most i. The tree is binary, so the number of nodes\nis at most t. Therefore the total number of recursive calls to MinOddCut is\nlinear in IT I.\nEach call to MinOddCut will require a number of calls to a Min s - t-cut\nalgorithm less than t = ITI. Hence, the tot a1 number of calls to a Min\ns-t-cut algorithm is O(IT 1 2 ) .\n(By studying the problem, one can actually\nsolve the Min T-odd-cut problem with O(IT1) calls to a Min s - t-cut\nalgorithm.)\n2. Use the ellipsoid method to solve the minimum weight perfect match-\ning problem (there is a more efficient combinatorial algorithm for it,\nbut here we will use the power of the ellipsoid algorithm):\nGiven an undirected graph G = (V,E) and a weight function w :\nE tN, find a set of edges M covering every vertex exactly once (a\nperfect matching) with the minimum total weight.\nIn order to formulate this problem as a linear program, we define the\nperfect matching polytope:\nP = conv{xM E {O, 1IE: M is a perfect matching)\nwhere XM is the characteristic vector of M (xM(e) = 1 if e E M and 0\notherwise). The convex hull conv(A) is defined as {xiXixi : xi E A, Xi >\n0, xiXi = 1) (where the summation is finite).\n\n(a) Argue that the vertices of P are the characteristic vectors of\nperfect matchings. Deduce that if we can optimize Cewexe over\nP, we would find a minimum weight perfect matching.\nAny point in P can be written as\nwhere AM 2 0 and EMAM = 1. Clearly, x can be a vertex only if we\nhave exactly one AM = 1. We will show that all such vectors are indeed\nvertices. For a given M , consider the hyperplane\nwhere n = IVI (an even number). Note that every perfect matching has\nexactly 5 edges. Then for any x E P,\nbecause 0 5 xe 5 1. Equality can hold only if Ve E M;xe = 1but then x\nis the characteristic vector of M. Therefore P n H M = { x M ) which proves\nit is a vertex.\nThe optimum of zewese can be assumed to be a vertex X M which means\nthat for any other perfect matching MI, w(M1) 2 w(M).\n(b) Suppose now that we can decide (via linear programming or some\nother way) whether Pn{x : wTx 5 A ) is empty or not, for any given\nX (remember all weights we are integers). Show that by calling\nan algorithm for this decision problem a polynomial number of\ntimes (in the size of the input, i.e. IVI, I EI and log(w,,,)),\nwe can\nfind the weight of the minimum-weight perfect matching.\nWe can find the minimum weight by binary search. If the graph has n\nvertices and maximum edge weight w,,,,\nthe maximum possible weight\nof a perfect matching is inw,,,.\nFor any X E [O; inw,,,],\nwe are able to\ntest whether there exists a perfect matching of weight at most X (that's\nexactly when Pn{x : wTx 5 A ) # a)). The weights are integers, so we can\npinpoint the smallest such X in O(log(nw,,,))\nsteps.\n(c) With the same assumptions as in the previous part, can you also\nfind a minimum-weight perfect matching (not just its weight, but\nalso which edges are in it) in polynomial time? (There might be\nseveral perfect matching having the same minimum weight, but\n\nhere you need to produce only one of them. Also, the algorithm\ndoes not need to be extremely efficient, just polynomial.)\nFor any edge, we can determine if we need it for the optimal perfect match-\ning. First, find the minimum weight W*. Then pick an edge el, remove\nit from the graph and test if there is still a perfect matching of weight\nW*. If yes, we don't need edge el and we continue on the graph G \\ {el).\nOtherwise we know that el appears in any optimal perfect matching, so\nwe remember it, remove its two vertices from the graph, and continue on\nthe remaining graph with modified optimum weight W' = W* - w(el). In\nI E 1 steps, we determine the optimal perfect matching.\nDue to Jack Edmonds, the perfect matching polytope can be de-\nscribed by the following inequalities:\n(d) Show that every vector in P satisfies the above inequalities.\nSuppose x is the characteristic vector of a perfect matching. Then the first\ntwo inequalities are satisfied by definition. For the last inequality, consider\nand odd-size subset W c V. All vertices of W cannot be covered by edges\ninside W because these edges cover disjoint pairs of vertices. At least one\nvertex must be covered by an edge e E 6(W) and therefore\nSince these inequalities are valid for the vertices of P, they are also valid\nfor any point inside P.\nTake the other implication for granted (every vector satisfying these\ninequalities is in P).\n(e) How many inequalities do we have in this complete description\nof P? Can we just use any polynomial-time algorithm for linear\nprogramming to optimize over P?\nUnfortunately, the third condition generates 2\n\"\n~\n~\ninequalities (one for each\nodd subset, the same equality for W and V \\ W). Therefore a straightfor-\nward linear programming approach would be very inefficient (not polyno-\nmial in n, the number of vertices, and log w,,,).\n\n(f) Show how we can use the ellipsoid method to decide if there exists\na perfect matching of weight at most X in polynomial time. How\nwould you select the initial ellipsoid? How would you take care\nof the equality constraints in the description of P ? When can\nyou stop?\nBy adding the inequality wTx 5 A, we get a polytope PAwhich is nonempty\nexactly if there exists a perfect matching of weight at most A.\nThe ellipsoid algorithm can be used to test whether PA= 0 whenever we\ncan:\na find a suitable bounding ellipsoid to start with,\na have a polynomial-time separation oracle, and\na estimate the minimum volume of PA,if it's nonempty.\nThe bounding ellipsoid here is simple. We can take for example the sphere\nwith center in the origin and radius fl.This contains all characteristic\nvectors of perfect matching.\nIf a point x doesn't lie in PA,it's because it violates some of the conditions.\nThe condition wTx 5 X is easy, as well as the first two inequalities in the\ndescription of P . The third inequality seems to require an exponential\nnumber of inequality checks but here's where Problem 1comes into play.\nFor a given x, we can calculate in polynomial time\nbecause this is just a min-V-odd-cut problem. Then we check whether\ny(x) 2 1. In case y(x) < 1,we can report that x violates the inequality for\nW where S(W)is the minimum V-odd cut. Otherwise, we are guaranteed\nthat no such cut exists.\nFinally, we have to make sure that PAhas some volume so we know when\nto stop. We do this by employing the theorem given in class (theorem 2 of\nthe scribe notes of lecture 5) which states that {x : A x 5 b) is nonempty\nif and only if {x : Ax 5 b+ ~ e )is nonempty as well, where E can be chosen\nas 2-L. The value L as defined in class involves the number of rows as\nwell, which is enormous, but this is not needed here. We can simply redo\nthe proof more carefully. We have to consider a vertex of y 2 0, AT = 0,\nand bTy = -1, where the matrix ( t: ) has entries all 0 and I except for\none column containing wj's and X (which can be assumed to be at most\nmw,,,).\nMost of the entries of such a basic feasible solution y will be 0, the\nones that are non-zero (basic, and thus at most m of them) will be at most\nm!mw,,.\nHence, following the proof of Theorem 5, we can choose E to be\n\nsay 2m!m2~,,,\n= 2-Q with Q = O(m log rn + log w,,,),\nwhich happens to\nbe polynomial in rn and log w,,,.\nTherefore we replace each equality by a\npair of inequalities and increase the right-hand size by E = 2-Q. We have\nto slightly modify our separation algorithm (since now we are separating\nover this slightly modified polytope) but this is trivial since we simply\ncompare the value of the minimum V-odd-cut to 1- & instead of 1. In\nsummary, this guarantees that we can stop after a polynomial number of\nsteps (0(m2Q)= 0(m3 log rn +m2 log w,,,)\n) and either find a point in P'\nor declare it empty."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 3",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/553efdbb9b04e5eee039d975b9ecccf4_ps3.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n!\n18.415/6.854 Advanced Algorithms\nProblem Set 3\nIf you have any doubt about the collaboration policy, please check the\ncourse webpage.\nProblem 1. Consider the symmetric traveling salesman path problem. We are\ngiven a set V of vertices, a (symmetric) distance d(i, j) = d(j, i) ≥ 0 for every pair\ni, j of vertices, 2 special vertices s and t, and we would like to find a path from s to t\npassing through all vertices (a Hamiltonian path) and of minimum total length. This\nproblem is NP-hard, and so we will not try to solve it exactly in polynomial time.\nOne simple heuristic is known as 2-OPT. Start from any ordering v1 = s, v2,\n, vn =\n· · ·\nt and remove 2 non-consecutive edges of the Hamiltonian path, say (vi, vi+1) and\n(vj , vj+1) where i +1 < j. Now, there is a unique way to form a new Hamilonian path\nby adding 2 other edges, namely (vi, vj ) and (vi+1, vj+1). If this results in a path of\nshorter length, this so-called 2-swap is performed, and this is repeated until no more\nimprovements are possible.\nNow suppose we would like to create a data structure to maintain the ordering\nof the vertices on the path. For any vertex v, we would like to be able to find\nNext(v), the vertex following v on the way to it, and similarly Previous(v), the\nvertex preceding v (i.e. closer to s). The tricky thing is that when we perform a\n2-swap, the vertices vi+1 to vj are now visited in the reverse order (from vi we go\nto vj , then to vj-1 and continue all the way to vi+1, and then continue at vj+1).\nThis means we would also like to have an operation Reverse(v, w) that reverses\nthe ordering from v to w; in our case above we would perform Reverse(vi+1, vj ).\nIf we were maintaining the ordering as a doubly-linked list (with pointers next and\nprevious), a Reverse operation would require Θ(n) time in the worst-case. Show\nhow to use splay trees to maintain the ordering and perform any sequence of m\noperations (Next, Previous, or Reverse) in O((m + n) log n) time.\n(If you augment the splay tree with additional information at every node, you\nmust indicate how this information is maintained while performing operations.)\nProblem 2. In lecture, we argued the static optimality property of splay trees by\nshowing that the time T required for a splay tree to access element i mi ≥ 1 times\nfor i = 1,\n, n is within a constant of the time required by any static BST. In this\n· · ·\nproblem, you need to argue that this time T for splay trees is\nX\nm\nO\nmi 1 + log\n,\nmi\ni\nPS3-1\n\nP\nwhere m =\ni mi is the total number of accesses.\nProblem 3. In the blocking flow problem (which arises in the blocking flow al\ngorithm for the maximum flow problem), we are given a directed acyclic graph\nG = (V, E), 2 vertices s and t and capacities on the edges. The goal is to find a\nflow f such that for every path P in E from s to t at least one of the edges of P is\nsaturated (i.e. f(v, w) = u(v, w)).\n1. Is the following argument correct?\nA blocking flow f is maximum since, if we take S to be the set of\nvertices reachable from s in (V, E) by non-saturated edges, we get\n\na cut (S : S) whose value equals the blocking flow, and hence the\nblocking flow must be optimal.\nIf the argument is fallacious, show a blocking flow which is not maximum.\n2. Show how to find a blocking flow in a graph G = (V, E) with n vertices and m\nedges in O(m log n) time. (Your solution can be quite short.)\n(FYI, Dinitz showed that one can solve a maximum flow problem in a general directed\ngraph by solving at most n blocking flow problems in directed acyclic graphs.)\nProblem 4. A team of n members would like to travel a distance d from A to B\nas quickly as possible. All of them can walk and have also one scooter (which can\ncarry only one person at a time) that they can use. For each person i (1 ≤ i ≤ n), we\nknow his/her walking speed wi and his/her speed si when travelling on the scooter.\nThe goal is to find a way to bring all n people to destination so as to minimize the\ntime at which the last person arrives. The scooter can be left by any member of the\ngroup on the side of the road, and picked up by anyone else of the group. Members\nof the group can also walk or use the scooter backwards (towards A) if that helps.\n1. Consider the case where n = 3, w1 = w2 = 1, s1 = s2 = 6, w3 = 2, s3 = 8 and\nd = 100. Find the fastest way for everyone to travel the distance d.\n2. For a general instance (general n and arbitrary speeds), write a linear program\nwhose value is always a lower bound on the time needed for the n-person team\nto travel a distance d. This should be a small linear program; the number\nof variables and constraints should be O(n) (and not dependent on d, or the\nnumber of 'legs' of the solution).\n3. Use the linear programming routines in matlab (or any LP software like CPLEX)\nto compute your lower bound for the instance given in 1. (You don't have to use\na linear programming software, provided you can exhibit an optimum solution,\nwith a proof of optimality.)\nPS3-2\n\nX\nP\nIf the bound you obtain is not equal to the value you found in 1., either improve\nyour solution to 1., or find a stronger linear program in 2.\nMatlab is available on athena, see http://web.mit.edu/olh/Matlab/Matlab.html\nfor more info. Type help linprog for information on how to use the LP rou\ntine.\nProblem 5. We will rederive the max-flow min-cut theorem from linear program\nming duality. Consider the maximum flow problem on a directed graph G = (V, E)\nwith source s ∈ V , sink t ∈ V and edge capacities u : E → R. The max-flow problem\nis a linear program:\nmax w\nsubject to\n⎧\nX\nX\n⎨ w\ni = s\nxij -\nxji = ⎩ 0\ni =6\ns, t\nj\nj\n-w\ni = t\nxi,j ≤ ui,j\n(i, j) ∈ E\n0 ≤ xi,j\n(i, j) ∈ E.\nThe variables are the xij 's (one per edge) and w.\n1. Show that its dual is equivalent to:\nmin\nuijyij\n(i,j)∈E\nsubject to\nzi - zj + yij ≥ 0\n(i, j) ∈ E\nzs = 0, zt = 1\nyij ≥ 0\n(i, j) ∈ E.\n2. Given a cut (S : S) with s ∈ S and t ∈/ S, show that a feasible solution\ny, z to the dual can be obtained of value equal to the capacity of the cut:\nU(S : S) =\n(i,j)∈E uij yij .\n3. Given any (not necessarily integral!) optimal solution y∗, z∗ of the dual linear\nprogram and an optimal solution x∗ of the primal linear program, show how to\nconstruct from z∗ a cut separating s from t of value equal to the maximum flow.\nThis shows the max-flow-min-cut theorem.\nPS3-3"
        },
        {
          "category": "Assignment",
          "title": "Problem Set 3 (2001)",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/15c42bf554389b217e86310365de7765_homework3.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set 3\n1. Consider the following optimization problem:\nGiven c E Rn, c 2 0, n even, find\nn\nmin{cTx: C,,,xi 2 1 VS c {I,...,n), IS1 = 5,\nIn class, it was shown that this can be solved by the ellipsoid method because\nthere is an efficient separation algorithm. However, this problem has a more\nstraightforward solution.\nDevelop an algorithm which finds the optimum in O(n logn) time. Prove its\ncorrectness.\n2. Fill a gap in the analysis of the interior point algorithm:\nSuppose that (x, y, s) is a feasible vector, i.e. x > 0, s > 0,\nand we perform one Newton step by solving for Ax, Ay, As:\nAAx = O\nA\n~\nA\n~\n\n+ As =\nVjj'; X j S j\nAxjsj + xjA5'j = P\nwhere p > 0. The proximity function is defined as\nX j S j\nI)?\nP\nProve that if\no(x + Ax, s + As, ,!A) < 1\nthen (x + Ax, y + Ay, s + As) is a feasible vector for Ax = b,x > 0 and\nA T Y + s = c , s > 0.\n\n3 b Given a directed graph G = (V,E) and two vertices s and t, we d\nd like to\nfind the maximum number of edge-disjoint paths between s and t (two paths\nare edge-disjoint if they don't share an edge). Denote the number of vertices\nby n and the number of edges by m.\n(a) Argue that this problem can be solved as a m h u m flow problem with\nunit capacitia. Explain.\n(b) Consider now the mashurn ~ Q W\n=\nproMan on directed graphs G (V,E)\nwith unit capacity edges (dthoughsome of the questions below would also\nappIy to the more general case).\nGiven afeasible flow f,we cmconstruct the midual network Gf = (V, E!)\nwhere\nThe residual capacity of an edge (i,j ) E Ef is equal to - fV or to fji\ndepending on the case above. Since we are dealingwith the unit capwits\ncase, an the q*'s are 1 and therefore for 0- 1flows f (i,e. flowsfor which\nthe d u e on any edge is 0 or I),dl residual capwitia wilI be I.\nWe d&e the distance of .a vertex If(v) as the Iengkh of the shortest path\nfrom s to v in Ep (w for vertices which are not reachable from s in Ef).\nFurther, d&e\nthe lmelled residual network as\nd a sabmtingflow g in E) as a flow in E'f (with capacities being the\nresidual capacities) such that every directed s - t path in El has at least\none saturated edge (i.e. an edge whose fiflow equals the residual capacity].\nFor a unit capacity graph and a given 0 -1 flow f,show how we can .find\nthe leverIIed residual network and a saturatisg flaw in O(m)time.\n(c) Prove that ifthe le~1ledreaidualnetwork has no path from s to t (kf (t)=\nm),then the fiow f iis mmhum.\nEd] For a flm f,d&e\n4fl =b($1\n(the distance from s to 5 in the residual network). Prove that if g is a\nsaturating flow far f then\nwhere f+g denotes the b w obtained from f by either incr-\nthe flow\nfij by gij or decreasing the flow fjiiby gg for e m\nedge (i,j ) E GI.\n\n(e) Prove that if f is a feasible 0 - 1flow with distance d = d(f) and f * is an\noptimum flow, then\nand also\n(f) Design a maximum flow algorithm (for unit capacities) which proceeds by\nfinding a saturating flow repeatedly. Try to optimize its running time.\nUsing the observations above, you should achieve a running time bounded\nby o(min (mn2I3, m3I2)).\n(g) Can we now justify that, for 0 - 1capacities, there is always an optimum\nflow that takes values 0 or 1on every edge?"
        },
        {
          "category": "Assignment",
          "title": "Problem Set Solution 3",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/4004fe272abc830c274663ab7d107daf_solution3.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set Solution 3\n1. Consider the following optimization problem:\nGiven c E Rn,c 2 0, n even, find\nn\nmin{cTx : xi,,\nxi 2 1 VS c (1,. . . ,n), IS1 = 2,\nIn class, it was shown that this can be solved by the ellipsoid method\nbecause there is an efficient separation algorithm. However, this\nproblem has a more straightforward solution.\nDevelop an algorithm which finds the optimum in O(nlog n) time.\nProve its correctness.\nLet\nWe would like to describe the structure of P,which is an unbounded polyhedron.\nWe prove that x E P exactly when x can be written as\nwhere XA denotes the characteristic vector of A, X A 2 0, and additionally\nFirst, suppose x satisfies this and consider S of size n/2. Any set A of size\nIAl > n/2 intersects S in at least IAl - n/2 elements, therefore\nConversely, let x E P . Let 7-r be a permutation such that\nG(1) I G ( 2 ) I . I X,(n).\nPSS3-1\n\nSet\nand\nfor k = 1...n. Then obviously XI, > 0 and\nFinally, we verify condition (*):\nNow we can optimize over P much more easily. First, observe that for any\noptimal solution\nwe can assume X A = 0 for IAI 5 n/2 and\notherwise we decrease the coefficients until the equality holds. This won't in-\ncrease the objective function C cixi, since c > 0. Therefore an optimal solution\nalways exists in the convex hull of {pA : IAl > n/2} where\nWe could evaluate the objective function at all these points but there are still\ntoo many of them. However, we can notice that for a given k = IAl, the\nonly candidate for an optimum pa is the set A which contains the k smallest\ncomponents of c. Therefore the algorithm is the following:\nSort the components of c and let Ak denote the indices of the k smallest\ncomponents of c, for each k > n/2. This takes O(n logn) time.\n\nFor each k > n/2, calculate sk = CiEAk\nck. This can be done in O(n)time,\nbecause the sets Ak form a chain and we can use sk to calculate sk+l in\nconstant time.\nFind the smallest value of\nfor k > n/2. Return this as the optimum.\nThe algorithm runs in 0( nlog n)time and its correctness follows from the anal-\nysis above.\n2. Fill a gap in the analysis of the interior point algorithm:\nSuppose that (x,y, s) is a feasible vector, i.e. x > 0, s > 0,\nAx = b,\nA T y + s = c\nand we perform one Newton step by solving for Ax, Ay, As:\nV\nXjSj+Axj~j+xjAsj =/l\nwhere p > 0. The proximity function is defined as\nProve that if\nthen ( x + Ax, y + Ay, s + As) is a feasible vector for Ax = b, x > 0 and\nATy+s=c,s>O.\nThe equalities are satisfied directly by the assumptions:\nWe have to verify the positivity conditions. First we prove that at least one of\nxj +A x j , sj +Asj is positive. We have xj > 0,sj > 0 and\n\ntherefore either xj +Axj or sj+Asjmust be positive.\nSecond, we use the proximity condition:\nIn particular, for each j\nwhich means that xj + Axj and sj+ Asjhave the same sign. We know they\ncan't be negative so they must be positive.\nGiven a directed graph G = (V,E) and two vertices s and t, we would\nlike to find the maximum number of edge-disjoint paths between s and\nt (two paths are edge-disjoint if they don't share an edge). Denote\nthe number of vertices by n and the number of edges by m.\n(a) Argue that this problem can be solved as a maximum flow prob-\nlem with unit capacities. Explain.\nLet F be a union of k edge-disjoint paths from s to t. We define a flow of\nvalue k in a natural way - an edge gets a flow of value 1if it is contained\nin F and and 0 otherwise. Since each path enters and exits any vertex\n(except s and t) the same number of times, flow conservation holds. The\nvalue of the flow is the number of edges in F leaving s (or entering t) which\nis k.\nConversely, let f be the maximum flow with unit capacities. As we shall\nprove, there is always a 0 -1maximum flow, therefore we can assume that\nf, is either 0 or 1for each edge. Let\nand k be the value of the flow. Then we can decompose F into k edge-\ndisjoint paths in the following way: We start from s and follow a path\nof edges in F until we hit t. (This is possible due to flow conservation.)\nWhen we have found such a path, we remove it from F and consider the\nremaining flow of value k - 1. By induction, we find exactly k such paths.\n(b) Consider now the maximum flow problem on directed graphs G =\n(V,E) with unit capacity edges (although some of the questions\nbelow would also apply to the more general case).\nGiven a feasible flow f , we can construct the residual network\nGf = (V,Ef) where\nEf = {(i,j) : ((i,j) E E & fq < uij) or ((j,i) E E & fji> 0)).\n\nThe residual capacity of an edge (i,j) E Ef is equal to uij - fij or\nto fji depending on the case above. Since we are dealing with\nthe unit capacity case, all the uij's are 1and therefore for 0 - 1\nflows f (i.e. flows for which the value on any edge is 0 or I), all\nresidual capacities will be 1.\nWe define the distance of a vertex lf(v) as the length of the short-\nest path from s to v in Ef (cafor vertices which are not reachable\nfrom s in Ef). Further, define the levelled residual network as\nElf = {(i,j) E Ef : lf(j) = lf (i) + 1)\nand a saturating flow g in Ei as a flow in E; (with capacities\nbeing the residual capacities) such that every directed s -t path\nin Elf has at least one saturated edge (i.e. an edge whose flow\nequals the residual capacity).\nFor a unit capacity graph and a given 0 - 1flow f,show how we\ncan find the levelled residual network and a saturating flow in\nO(m) time.\nFirst, we can find Ef in O(m) time simply by testing each edge and adding\nthe edge or its reverse to Ef,depending on the current flow. Then we can\nlabel the vertices by If (v) by a breadth-first search from s. This takes time\nO(m), also. At the same time we find d(f) as the length of the shortest\npath from s to t.\nThen, we create E$ by keeping only the edges between successive levels.\nThus all paths between s and t in Ei have length d( f). Now we produce\nflow g by finding as many edge-disjoint s-t paths as possible. We start with\nE' = Ei and we perform a depth-first search from s. If we get stuck, we\nbacktrack and remove edges on the dead-end branches since these are not\nin any s-t path anyway. When we find an s-t path, we set gij = 1along\nthat path, and remove it from E'. We continue searching for paths until\nE' is empty. We spend a constant time on each edge before it's removed,\nwhich is O(m) time total. When we are done, there is no s-t path in E$\nwithout a saturated edge, otherwise it would still be in E'.\n(c) Prove that if the levelled residual network has no path from s to\nt (If (t)= co),then the flow f is maximum.\nSuppose there is a flow f * of greater value. Then f*- f (where the dif-\nference is produced by either decreasing flow along an edge and increasing\nflow in the opposite direction) is a feasible flow in the residual network\nwhich has a positive value. This is easy to see because if f; > fij then\n(i,j) appears in Ef and f; - f, 5 uij - fo which is the capacity of this\nedge in Ef. If f; < fq then fu > 0 and therefore the opposite edge (j,i)\nappears in Ef. Also, fi, - f; 5 fij which is the capacity of (j,i) in Ef.\n\nWhen a non-zero flow exists in Ef, there exists a path from s to t using\nonly edges in Ef. The shortest of these paths would appear in Ef,as well,\nwhich is a contradiction.\n(d) For a flow f , define\nd(f = If (t)\n(the distance from s to t in the residual network). Prove that if\ng is a saturating flow for f then\nwhere f +g denotes the flow obtained from f by either increasing\nthe flow f, by gij or decreasing the flow fji by gij for every edge\n(i,j) E Gf.\nConsider Ef and the labeling of vertices if (v). For every edge (i, j) of Ef\nwe have that if (j)5 If (i)+ 1. Since g is a saturating flow in ~ f , ,the only\nedges (u, v) which are in Ef+, and not in Ef are such that (v, u) E E;,\nwhich implies that lf (v) = lf (u) -I. In summary, every edge (i, j ) of Ef+,\nsatisfies if (j)5 If (i)+ 1 and, furthermore, the edges which are not in\nEf actually satisfy the inequality strictly 1 (j)< lf(i) + 1. Consider now\nany path P in Ef+, Adding up lf(j) 5 lf(i) + 1over the edges of P, we\nget that d(f) 5 IPI. Moreover, we can have d(f) = IPI only if all edges\nof P also belong to Ef, which is impossible since g is a saturating flow.\nHence, d(f) < lPl and this is true for any path P of Ef+, implying that\nd(f) < d(f + g).\n(e) Prove that if f is a feasible 0 - 1 flow with distance d = d(f) and\nf * is an optimum flow, then\nand also\nSuppose f has distance d and f * is an optimal flow. As noted before,\ng = f * - f is a feasible flow in the residual network Ef.\nConsider s-t cuts Cl,C2,... Cddefined by\nck = {(i,j) E Ef : lf(i) 5 k,lf(j) > k).\nThere are at most m edges in total and these cuts are disjoint, therefore\n\nSince the value of g cannot be greater than any s-t cut in Ef,\nm\nvalue(f*) - value(f) = value(g) 5 -.d\nSimilarly, define d + 1sets of vertices Vo,V17V2,.. .,Vd:\n= {i E V : lf(i)= k ) .\nBy double counting,\nSuppose that IVk-l 1 = a, IV,I 5 9 - a. Note that the edges of Ck belong\nto Vk-l x Vk. Therefore\n(f) Design a maximum flow algorithm (for unit capacities) which\nproceeds by finding a saturating flow repeatedly. Try to opti-\nmize its running time. Using the observations above, you should\nachieve a running time bounded by 0(min(mn2l3,m3I2)).\nThe algorithm starts with a zero flow f . Then we repeat the following:\nFind the levelled residual network ~ l f .\nFind a saturating flow g.\na Add g to f , reset the residual network and continue.\nEach iteration takes O(m) time. Since d(f) increases every time and it\ncannot reach more than n (the maximum possible distance in G), the\nrunning time is clearly bounded by O(mn). However, we can improve this.\nSuppose we iterate only d times and our flow after d iterations is f . We\nknow d(f) 2 d, and iff* is an optimal flow,\nBecause the flow increases by at least 1 in each iteration, the remaining\nnumber of iterations is bounded by min{y, $). We choose d in order to\noptimize our bound. It turns out that the best choice is dl = m1I2 for the\nbound based on m and d2 = n2I3for the bound based on n. Thus the total\nrunning time is 0(min{m3I2,mn2I3)).\n(g) Can we now justify that, for 0 - 1 capacities, there is always an\noptimum flow that takes values 0 or 1 on every edge?\nOur algorithm finds a 0 -1flow and we have a proof of optimality, therefore\nthere is always a 0 - 1 optimal flow. This justifies our reasoning in part\n(a)."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 4",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/18565004ba2bba2e5974156903dd320e_ps4.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nProblem Set 4\nProblem 1. In a 0-sum 2-player game, Alice has a choice of n so-called pure strate\ngies and Bob has a choice of m pure strategies. If Alice picks strategy i and Bob picks\nstrategy j, then the payoff is aij , meaning aij dollars are transfered from Alice to\nBob. So Bob makes money if aij is positive, but Alice makes money if aij is negative.\nThus, Alice wants to pick a strategy that minimizes the payoff while Bob wants a\nstrategy that maximizes the payoff. The matrix A = (aij ) is called the payoff matrix.\nIt is well known that to play these games well, you need to use a mixed strategy--\na random choice from among pure strategies. A mixed strategy is just a particular\nprobability distribution over pure strategies: you flip coins and then play the selected\npure strategy. If Alice has mixed strategy x, meaning he plays strategy i with prob\nability xi, and Bob has mixed strategy y, then it is easy to prove that the expected\npayoff in the resulting game is xT Ay. Alice wants to minimize this expected payoff\nwhile Bob wants to maximize it. Our goal is to understand what strategies each\nplayer should play.\nWe'll start by making the pessimal assumption for Alice that whichever strategy\nshe picks, Bob will play best possible strategy against her. In other words, given\nAlice's strategy x, Bob will pick a strategy y that achieves maxy xT Ay. Thus, Alice\nwants to find a distribution x that minimizes maxy xT Ay. Similarly, Bob wants a y\nto maximize minx xT Ay. So we are interested in solving the following 2 problems:\nP min\nPmax\nx T Ay\nx:\nxi=1,x≥0 y:\nyj =1,y≥0\nPmax\nP min\nx T Ay\ny:\nyj =1,y≥0 x:\nxi=1,x≥0\nUnfortunately, these look like nonlinear programs!\n1. Show that if Alice's mixed strategy is known, then Bob has a pure strategy\nserving as his best response.\n2. Show how to convert each program above into a linear program, and thus find\nan optimal strategy for both players in polynomial time.\n3. Use strong duality (applied to the LP you built in the previous part) to argue\nthat the above two quantities are equal.\nPS4-1\n\nX\nP\nThe second statement shows that the strategies x and y, besides being optimal, are\nin Nash Equilibrium: even if each player knows the other's strategy, there is no point\nin changing strategies. This was proven by Von Neumann and was actually one of\nthe ideas that led to the discovery of strong duality.\nProblem 2. Consider the linear program\nmin\nxj\nj\nsubject to\nX\naij xj ≥ 1 ∀i\nj\nxj ≥ 0 ∀j\nand its dual\nX\nmax\nyi\ni\nsubject to\nX\naijyi ≤ 1\ni\nxi ≥ 0.\nAssume that A = [aij ] is m × n and has only nonnegative entries.\nIn this problem, you'll have to show that a continuous algorithm solves (almost\nmiraculously) the above pair of dual linear programs. We shall define a series of\nfunctions whose argument is the \"time\" and you'll show that some of these functions\ntend to the optimal solution as time goes to infinity. (For simplicity of notation, we\ndrop the dependence on the time.)\n- Initially, we let sj = 0 for j = 1, . . . , n and LB = 0. The vector s will (sort\nof) play the role of primal solution, and LB the role of a lower bound on the\nobjective function.\n- At any time, let\nP\nti = e-\nj aij sj\nfor i = 1, . . . , m. Also, let dj =\ni aij ti for j = 1, . . . , n, D = maxj dj and k\nbe an index j attaining the maximum in the definition of D. The algorithm\ncontinuously increases sk.\nObserve that when sk is increased, the vectors t and d as well as D change also,\nimplying that the index k changes over time.\nPS4-2\n\nP\nP\n1. Let α = mini(\nj aij sj ). Let xj = sj /α for j = 1, . . . , n, yi = ti/D for i =\nP\n1, . . . , m and LB = max(LB, D\nti ). Show that x is primal feasible, y is dual\nfeasible and LB is a lower bound on the optimal value of both primal and dual.\n2. Prove that\nm\nX\nP n\nti ≤ me-\nj=1 sj /LB .\ni=1\nHint: Show that initially the inequality holds and that it is also maintained\nwhenever we have equality.\n3. Deduce from (b) that\ni ti tends to 0 as time goes to infinity.\n4. Using (b), give an upper bound on the value of the primal solution x, and\nusing (c), show that this upper bound tends to LB as time goes to infinity.\nThis shows that as time goes to infinity, both x and y tend to primal and dual\noptimal solutions!\nProblem 3. We would like to find a function f(n) such that, given any set of\nn (possibly negative) numbers, c1,\n, cn, one cannot find more than f(n) subsums\n· · ·\nof these numbers which decrease in absolute value by a factor of at least 2. More\nformally:\nLemma 1 Let c ∈ Rn and yk ∈{0, 1}n for k = 1, . . . , q such that 2|yT\n| ≤|yk\nT c|\nk+1c\nfor k = 1, . . . , q - 1. Assume that yT c = 1. Then q ≤ f(n).\nq\nUsing linear programming, you are asked to prove that f(n) = O(n log n).\n1. Given a vector c and a set of q subsums satisfying the hypothesis of the Lemma,\nwrite a set of inequalities in the variables xi ≥ 0, i = 1 . . . n, such that xi = |ci|\nis a feasible vector, and for any feasible vector x0 there is a corresponding vector\nc0 satisfying the hypothesis of the Lemma for the same set of subsums.\n2. Prove that there must exist a vector c0 satisfying the hypothesis of the Lemma,\nwith c0 of the form (d1/d, d2/d, . . . , dn/d) for some integers d , d1 , . . . , dn =\n2O(n log n) .\n| | |\n|\n|\n|\n3. Deduce that f(n) = O(n log n).\n4. (Not part of the problem set; only for those who find the problem sets too\neasy...) Show that f(n) = Ω(n log n) (as a tiny step, can you find a set of\nnumbers such that f(n) > n?).\nPS4-3\n\nProblem 4. Let K be a bounded convex set in Rn . In this problem, you'll prove\nthat there exists an ellipsoid E contained within K such that if you blow it up by\na factor of n (the dimension) then the corresponding ellipoid contains K; in short,\nE ⊆ K and K ⊆ nE.\n1. Suppose that we have an ellipsoid E(a, A) = {x ∈ Rn : (x - a)T A-1(x - a) ≤ 1}\nand we have a point b /∈ nE(a, A). Argue that the convex hull of b and E(a, A),\nconv({b}, E(a, A)), contains an ellipsoid E0 of larger volume than E(a, A).\n(You do not need to explicitly give a0 and A0 corresponding to E0 = E(a0, A0),\nif that helps. It might be easier to deal with a particular case for a, A and b,\nand argue why you can.)\n2. Argue that the maximum volume ellipsoid E contained in K (it is actually\nunique, although you do not need this) is such that nE ⊇ K.\n3. (Optional. Assume that K = {x ∈ Rn : Cx ≤ d} is bounded, where C is m × n\nand d ∈ Rm . Formulate the problem of finding the largest volume ellipsoid\ncontained within K as a convex program (minimizing a convex function over\na convex set, or maximizing a concave function over a convex set. One could\ntherefore use the ellipsoid algorithm to find (a close approximation to) this\nmaximum volume ellipsoid.)\nProblem 6. Given an undirected graph G = (V, E), a set T ⊆ V with |T | even\nand a weight function w : E\nQ+, the minimum (weight) T -cut problem is to find\nS ⊆ with |S ∩ T | odd1 such that d(S) := w(S : S ) :=\ne∈(S:S ) we is minimized.\n→\nP\n\nHere (S : S) denotes the set of edges of E with exactly one endpoint in S (since our\ngraph is undirected, observe that d(S) = d(S )). For T = {s, t}, the minimum T -cut\nproblem reduces to the minimum s - t cut problem (in an undirected graph). In this\nproblem, you will show that the minimum T -cut problem can be solved efficiently.\n1. Argue that the minimum s - t cut problem in an undirected graph G can be\nsolved efficiently by using an algorithm for a minimum s - t cut problem in a\ndirected graph H.\n\n2. A T - T cut is a cut (S : S) with S ∩ T 6= ∅ and S ∩ T 6= ∅. Show that the\nminimum weight T - T cut can be obtained by solving a polynomial number of\nminimum s - t cut problems. Can you do it with O(|T |) such minimum s - t\ncut computations?\n3. Prove that for any A, B ⊆ V , we have\nd(A) + d(B) ≥ d(A ∩ B) + d(A ∪ B).\n1thus, S ∩ T | = |T \\ S| is also odd.\nPS4-4\n\n4. To solve the minimum T -cut problem, suppose we first solve the minimum\n\nT - T cut problem and obtain the cut (S : S). If |S ∩ T | is odd, we are done\n(right?). If |S ∩ T | is even, use the previous inequality to argue that there exists\n\na minimum T -cut (C : C) such that C ⊆ S or C ⊆ S. Deduce from this an\nefficient algorithm for solving the minimum T -cut problem. How many calls to\nyour minimum T - T cut algorithm are you using?\nPS4-5"
        },
        {
          "category": "Resource",
          "title": "Approximaion Algorithms: MAXCUT",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/0eef690adf10b98bffd65d5415516412_lec18.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nX\nX\n18.415/6.854 Advanced Algorithms\nNovember 19, 2008\nApproximaion Algorithms: MAXCUT\nLecturer: Michel X. Goemans\nMAX-CUT problem\nMAX-CUT Problem: Given a graph G = (V, E) and weights on the edges w : E\nR+, find a\ncut (S : S ), S ⊆ V that maximizes w(S : S ) = P\nS) w(e).\n→\ne∈(S:\nMIN-CUT Problem: find a cut (S : S ) that minimizes w(S : S ).\nThere is a polynomial algorithm for the MIN-CUT problem: use the min s - t cut algorithm\non each pair of vertices (or, better, for a fixed s), and take the smallest of them. However, the\nMAX-CUT problem is NP-hard, and we'll try several ways of designing approximation algorithms\nfor it.\nIdea #1: Local Search\nAlgorithm: Start from any cut (S : S ). Define the neighborhood N(S : S ) of the cut to be the\nMOVE neighborhood: all the cuts that result from moving one vertex from one side of the cut to\nthe other side. Consider a locally maximum cut for this neighborhood.\nLemma 1 If (S : S ) is a local maximum for the MOVE neighborhood, then w(S : S ) ≥ 2 w(E) ≥\n1 OPT .\nProof of lemma 1:\nLook at a vertex i ∈ V . Let Ci be the set of all edges (i, j) ∈ E that are\npart of the cut (S : S ) (that is if i ∈ S then j ∈ S and vice versa). Let Ai be the set of all edges\n(i, j) ∈ E that are not part of the cut (S : S ). Since moving any single vertex i to the other side of\nthe cut does not improve the weight of the cut, we know that:\nw(Ci) ≥ w(Ai).\nSumming over all vertices i, we get:\nw(Ci) ≥\nw(Ai),\ni∈V\ni∈V\nor 2w(S : S ) ≥ 2w(E\\(S : S )). Rearranging, we get:\n4w(S : S ) ≥ 2w(E)\nor\nw(S :\nS) ≥ 1\n2 w(E) ≥ 1\n2 OP T.\n\nRemarks:\n(a) The bound of 1/2 cannot be improved for this MOVE neighborhood: Consider a k-vertex\ncycle, where k is a multiple of 4, as the graph G (with unit weights). The best cut will include\nLec18-1\n\nall edges. However, if we start from a cut in which the edges of the cycle alternate in and out\nof the cut, we have a locally optimum solution with only k/2 edges in the cut.\n(b) The local search algorithm based on the MOVE neighborhood for MAX-CUT takes expo\nnentially many steps in the worst-case. This is true even for graphs that are 4-regular (each\nvertex has exactly 4 neighbors) (Haken and Luby [1]). For 3-regular graphs the algorithm is\npolynomial (Poljak [4]).\n(c) To capture the complexity of local search, Johnson, Papadimitriou and Yannakakis [3] have\ndefined the class PLS (Polynomial Local Search). Members of this class are optimization\nproblems of the form max{f(x) : x ∈ S} together with a neighborhood N : S\n2S . We say\n→\nthat v ∈ S is a local optimum if c(v) = max{c(x) : x ∈ N(v)}. To be in PLS, we need to\nhave polynomial-time algorithms for (i) finding a feasible solution, (ii) deciding if a solution is\nfeasible and if so computing its cost, and (iii) deciding if a better solution in the neighborhood\nN(v) of a solution v exists and if so finding one. They introduce a notion of reduction, and\nthis leads to PLS-complete problems for which any problem in PLS can be reduced to it. Their\nnotion of reduction implies that if, for one PLS-complete problem, one has a polynomial-time\nalgorithm for finding a local optimum then the same true for all PLS problems. In particular,\nMAX-CUT with the MOVE neighborhood is PLS-complete [5]. Furthermore, it follows from\nJohnson et al. [3] that the obvious local search algorithm is not an efficient way of finding\na local optimum for a PLS-complete problem; indeed, for any PLS-complete problem, there\nexist instances for which the local search algorithm of repeatedly finding an improved solution\ntakes exponential time. The result of Haken and Luby above is thus just a special case. Still,\nthis does not preclude other ways of finding a local optimum.\nIdea #2: Random Cut\nAlgorithm: There are 2|V | possible cuts. Sample a cut randomly using a uniform distribution over\nall possible cuts in the graph: ∀v ∈ V, Pr(v ∈ S) = 1 , independently for all vertices v ∈ V .\nLemma 2 This randomized algorithm gives a cut with expected weight that is ≥ 1 OPT .\nProof of lemma 2:\nE[w(S :\nS)] = E[\nX\nw(e)I(e ∈ (S :\nS))] =\nX\nw(e) · P r(e ∈ (S :\nS))\n=\ne∈E\nX\nw(e) · 1\n2 = 1\n2 w(E).\ne∈E\ne∈E\n\nUsing the method of conditional expectations, we can transform this randomized algorithm into\na deterministic algorithm. The basic idea is to use the following identity for a random variable f\nand event A:\nE[f]\n= E[f A]Pr(A) + E[f A ]Pr(A ) = E[f|A]Pr(A) + E[f|A ](1 - Pr(A))\n≤ max\n|\n{E[f|A], E[f|A ]\n|\n}.\nIn our setting, we consider the vertices in a specific order, say v1, v2,\n, and suppose we have\n· · ·\nalready decided/conditioned on the position (i.e. whether or not they are in S) of v1,\n, vi-1.\n· · ·\nNow, condition on whether vi ∈ S. Letting f = w(S : S ), we get:\nE[f|{v1, · · · , vi-1} ∩ S = Ci-1]\n≤ max(E[f|{v1, · · · , vi-1} ∩ S = Ci-1, vi ∈ S], E[f|{v1, · · · , vi-1} ∩ S = Ci-1, vi ∈/ S]).\nLec18-2\n\nX\nX\nX\nX\nBoth terms in the max can be easily computed and we can decide to put vi on the side of the cut\nwhich gives the maximum, i.e. we set Ci to be either Ci-1 or Ci-1 ∪{vi} in such a way that:\nE[f|{v1, · · · , vi-1} ∩ S = Ci-1 ≤ E[f|{v1, · · · , vi} ∩ S = Ci].\nWhen we have processed all inequalities, we get a cut (Cn : C n) such that\n2 w(E) ≤ E[f] ≤ w(Cn : C n),\nand this provides a deterministic 0.5-approximation algorithm.\nExamining this derandomized version more closely, we notice that we will place vi on the side\nof the cut that maximizes the total weight between vi and the previous vertices {v1, v2,\n, vi-1}.\n· · ·\nThis is therefore a simple greedy algorithm.\nRemarks:\n(a) The performance guarantee of the randomized algorithm is no better than 0.5; just consider\nthe complete graph on n vertices with unit weights. Also, the performance guarantee of the\ngreedy algorithm is no better than 0.5 int he worst-case.\nIdea #3: LP relaxation\nAlgorithm: Start from an integer-LP formulation of the problem:\nSince we have a variable xe for each edge (if xe = 1 than e ∈ (S : S)), we need the second type of\nmax\nw(e)xe\ne∈E\ns.t.\nxe ∈ {0, 1} ∀e ∈ E\nX\nX\ne∈F\ne∈C\\F\nxe +\n(1 - xe) ≤ |C| - 1\nX\nX\n∀cycle C ⊆ E ∀F ⊆ C, |F | odd\n⇔\ne∈F\ne∈C\\F\nxe -\nxe ≤ |F | - 1 ∀cycle C ⊆ E ∀F ⊆ C, |F | odd\n\nconstraints to guarantee that S is a legal cut. The validity of these constraints comes from the fact\nthat any cycle and any cut must intersect in an even number of edges. even number of edges that\nare in the cut.\nNext, we relax this integer program into a LP:\nmax\nw(e)xe\ne∈E\ns.t.\n0 ≤ xe ≤ 1 ∀e ∈ E\nxe -\nxe ≤|F | - 1 ∀cycle C ⊆ E ∀F ⊆ C, |F | odd.\ne∈F\ne∈C\\F\nThis isa relaxation of the maximum cut problem, and thus provides an upper bound on the value\nof the optimum cut. We could try to solve this linear program and devise a scheme to \"round\" the\npossibly fractional solution to a cut.\nRemarks:\nLec18-3\n\n(a) This LP can be solved in a polynomial time. One possibility is to use the ellipsoid algorithm\nas the separation problem over these inequalities can be solved in polynomial time (this is not\ntrivial). Another possibility is to view the feasible region of the above linear program as the\nprojection of a polyhedral set Q ⊆ Rn 2 with O(n3) number of constraints; again, this is not\nobvious.\n(b) If the graph G is planar, then all extreme points of this linear program are integral and\ncorrespond to cuts. We can therefore find the maximum cut in a planar graph in polynomial\ntime (there is also a simpler algorithm working on the planar dual of the graph).\n(c) There exist instances for which OP T ∼ 1 (or ∃G = (V, E), w(e) = 1, OPT ≤ n( 1 + ), LP ≥\nLP\nn(1 - )), which means that any rounding algorithm we could come up with will not guarantee\na factor better than 1 .\nIdea #4: SDP relaxation\nThe idea is to use semidefinite programming to get a more useful relaxation of the maximum cut\nproblem. This is due to Goemans and Williamson [2].\nInstead of defining variables on the edges as we did in the previous section, let's use variables on\nthe vertices to denote which side of the cut a given vertex is. This leads to the following quadratic\ninteger formulation of the maximum cut problem:\nmax\nX\nw(i, j)1 - yiyj\n(i,j)∈E\ns.t.\nyi ∈{1, -1}n ∀i ∈ V.\nHere we have defined a variable yi for each vertex i ∈ V such that yi = 1 if i ∈ S and yi = -1\notherwise. We know that an edge (i, j) is in the cut (S : S ) iff yiyj = -1, and this explains the\nquadratic term in the objective function.\nWe can rewrite the objective function in a slightly more convenient way using the Laplacian of\nthe graph. The Laplacian matrix L is defined as follows:\n⎧\n⎪0\n(i, j) /\n⎨\n∈ E\nlij =\n-w(i, j)\ni = j, (i, j) ∈ E\n⎪P\n⎩\n=i w(i, k) i = j.\nk:k\nthat is, the off-diagonal elements are the minus the weights, and the diagonal elements correspond\nto the sum of the weights incident to the corresponding vertex. Using the Laplacian matrix, we can\nrewrite equivalently the objective function in the following way:\nn\nX n\nX\nn\nX\nX\nX\ny T Ly =\nyiyj lij =\ny 2\ni\nw(i, k) -\nyiyj w(i, j)\ni=1 j=1\ni=1\nk6=i\n⎛\n(i,j)∈E\n⎞\n= 2w(E) -\nX\nyiyj w(i, j) = 4 ⎝ X\nw(i, j) 1 - yiyj\n⎠ ,\n(i,j)∈E\n(i,j)∈E\nand thus\nX\nw(i, j) 1 - yiyj\n= 1\n4y T Ly.\n(i,j)∈E\nLec18-4\n\nThus the maximum cut value is thus equal to\nmax{ 4 y T LY : y ∈{0, 1}n}.\nIf the optimization was over all y ∈ Rn with ||y||2 = n then we would get that\nn\nmax{ 4y T LY : y ∈ Rn , ||y||2 = n} = 4 λmax(L),\nwhere λmax(L) is the maximum eigenvalue of the matrix L. This shows that OPT ≤ n λmax(L);\nthis is an eigenvalue bound introduced by Delorme and Poljak.\nUsing semidefinite programming, we will get a slightly better bound. Using the Frobenius inner\nproduct, we can again reformulate the objective function as:\ny T Ly = L (yy T ),\n4 -\nor as\n1 L\nY\n4 -\nif we define Y = yyT . Observe that Y 0, Y has all 1's on its diagonal, and its rank is equal to\n1. It is easy to see that the coverse is also true: if Y 0, rank(Y ) = 1 and Yii = 1 for all i then\nY = yyT where y ∈ {-1, 1}n . Thus we can reformulate the problem as:\nmax\nL\nY\n4 -\ns.t.\nrank(Y ) = 1,\n∀i ∈ V : Yii = 1,\nY 0.\nThis is almost a semidefinite program except that the rank condition is not allowed. By removing\nthe condition that rank(Y ) = 1, we relax the problem to a semidefinite program, and we get the\nfollowing SDP:\nSDP = max\nL\nY\n4 -\ns.t.\n∀i ∈ V : Yii = 1,\nY 0.\nObviously, by removing the condition that rank(Y ) = 1 we only increase the space on which we\nmaximize, and therefore the value (simply denoted by SDP ) to this semidefinite program is an\nupper bound on the solution to the maximum cut problem.\nWe can use the algorithms we described earlier in the class to solve this semidefinite program\nto an arbitrary precision. Either the ellipsoid algorithm, or the interior-point algorithms for conic\nprogramming. Remember that semidefinite programs were better behaved if they satisfied a regular\nity condition (e.g., they would satisfy strong duality). Our semidefinite programming relaxation of\nMAXCUT is particularly simple and indeed satisfies both the primal and dual regularity conditions:\n(a) Primal regularity conditions ∃Y 0 s.t. Yii = 1 ∀i. This condition is obviously satisfied\n(consider Y = I).\nLec18-5\n\nX\nX\n(b) Dual regularity condition: First consider the dual problem\nmin\nzi\ni∈V\nz1\n...\nz2\n...\n. . .\n. . .\n.. .\n. . .\n...\nzn\n⎛\n⎞\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎠- L 0,\ns.t.\nwhere zi ∈ R for all i\nz1\n...\nz2\n...\n⎞\n⎛\n∈ V . The regulation condition is that there exist zi's such that\n⎜\n⎜\n⎜\n⎝\n⎟\n⎟\n⎟\n⎠- L 0. This is for example satisfied if, for all i, zi > λmax(L).\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n...\nzn\nRemark: If we add the condition that z1 = z2 = ... = zn to the dual then the smallest value zi\ncan take is equal to λmax(L), and we derive that:\nn\nOPT ≤ SDP ≤ 4 λmax(L),\nand therefore this SDP bound improves upon the eigenvalue bound.\nWe will start the next lecture by proving the following theorem.\nTheorem 3 ([2]) For all w ≥ 0, we have that OP T ≥ 0.87856.\nSDP\nIn order to prove this theorem, we will propose an algorithm which derives a cut from the solution\nto the semidefinite program. To describe this algorithm, we first need some preliminaries. From the\nCholesky's decomposition, we know that:\nY 0 ⇔\n∃V ∈ Rk×n , k = rank(Y ) ≤ n, s.t. Y = V T V\n⇔\n∃v1, ..., vn s.t. Yij = v T vj , vi ∈ Rn .\ni\nTherefore, we can rewrite the SDP as a 'vector program':\nmax\nw(i, j)1 - vi\nT vj\nTo be continued...\ns.t.\n(i,j)∈E\n∀i ∈ V :\n∀i ∈ V :\nkvik = 1\nvi ∈ Rn .\nReferences\n[1] A. Haken and M. Luby, \"Steepest descent can take exponential time for symmetric connection\nnetworks\", Complex Systems, 1988.\n[2] M.X. Goemans and D.P. Williamson, Improved Approximation Algorithms for Maximum Cut\nand Satisfiability Problems Using Semidefinite Programming, J. ACM, 42, 1115-1145, 1995.\n[3] D.S. Johnson, C.H. Papadimitriou and M. Yannakakis, \"How easy is local search\", Journal of\nComputer and System Sciences, 37, 79-100, 1988.\nLec18-6\n\n[4] S. Poljak, \"Integer Linear Programs and Local Search for Max-Cut\", SIAM J. on Computing,\n24, 1995, pp. 822-839.\n[5] A.A. Sch affer and M. Yannakakis, \"Simple local search problems that are hard to solve\", SIAM\nJournal on Computing, 20, 56-87, 1991.\nLec18-7"
        },
        {
          "category": "Resource",
          "title": "Fibonacci heaps",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/16cda5a1eae18c7a26ec0c0498966dde_lec1.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 3rd, 2008\nFibonacci heaps\nLecturer: Michel X. Goemans\nIntroduction\nToday we will describe Fibonacci heaps, a data structure that provides a very ecient implementa-\ntion of a priority queue. By priority queue we mean a data structure that stores a set S of elements\nwhere with each element s we associate a key k(s) being priority of that element. Now, we want\nthe queue to handle three operations on set S:\ninsert Adding a new element s0 with a key k(s0) to S\n-\nextract-min Returning an element s∗ of S having minimal key and removing s∗ from S\n-\ndecrease-key Replacing the value of a key of some element s by a new, smaller value.\n-\nThe motivation behind the search for fast implementation of priority queues can be observed on\nthe example of two classical graph problems: Single-source Shortest Paths and Minimum Spanning\nTree.\n1.1\nSingle-source Shortest Paths problem\nWe are given a directed graph G = (V, E), some vertex s ∈ V and a length function l : E\nR+ on\n→\nthe arcs. Observe that we impose that the lengths are nonnegative. Now, for each vertex v ∈ V we\nwant to compute the length ds(v) of the shortest path from s to v.\nThe classical solution for this problem is Dijkstra's algorithm. The algorithm is:\n1. Maintain a priority queue containing some subset S of vertices of G with keys k(v). Initially,\nS = V , k(s) = 0 and k(v) = +inf.\n2. As long as S is nonempty:\n- Extract a vertex u from S with minimum key. Output k(u) as the value of ds(u).\nFor each out-neighbor v ∈ S of u, we update (i.e. possibly decrease) the key k(v) of v to\n-\nbe min{k(v), k(u) + l((u, v))}.\nIn this algorithm, k(v) represents the length of the shortest path from s to v using only intermediate\nvertices not in S, and represents ds(v) when extracted. The algorithm can be adapted to output\nthe shortest paths.\n1.2\nMinimum Spanning Tree problem\nGiven an undirected graph G = (V, E) and a weight function w : E\nR on edges, we would like a\n→\nspanning tree of G of minimal weight. Surprisingly, one of the classical solutions of this problem -\nPrim's algorithm - is very similar to the approach of Dijkstra's algorithm for Single-source Shortest\nPath problem. The algorithm is as follows:\nlect-1\n\n-\n1. Maintain a priority queue containing some subset S of vertices of G with keys k(v) and a tree\nT spanning V \\S. Initially, T = ∅, S = V , k(s) = 0 for some arbitrary vertex s and k(v) = +inf\nfor v = s.\n2. As long as S is nonempty:\nExtract a vertex u from S with minimum key. If u = s (rst iteration), add to T the\ncorresponding edge (i.e. the minimum-weight edge connecting u to T of weight k(u)).\n- For each neighbor v /∈ S of u, we update (i.e. possibly decrease) k(v) to be min{k(v), w((u, v))}.\n1.3\nNumber of priority queue operations\nWe will not prove the correctness of the algorithms. However, for the sake of the running time\nanalysis that we will do later, we notice that in both cases the algorithm uses |V | insert operations,\n|V | extract-min operations and, since each edge can enforce at most one decrease-key operation, at\nmost |E| decrease-key operations.\nBinary heaps\nThe classical implementation1 of priority queues are binary heaps. A binary heap T is a binary tree\nwhose nodes correspond to elements of the set S and has two properties:\nit is almost complete i.e. if T has depth h then it has exactly 2i vertices on depth i if i < h\n-\nand the last level is lled from the left.\nheap-ordering: the key of every child is not smaller than the key of its parent.\n-\nKeeping this properties in mind it is relatively easy (see [CLRS]) to develop procedures for\ninserting, extracting the minimal element and decreasing the key that execute any of these operations\nin O(log n) time where n is the number of items in the priority queue. Therefore, since the number\nof elements is at most |V | in our applications, we obtain the total running time of both algorithms to\nbe O((|V |+|E|) log |V |). Obviously, |E| ≥|V | in case of connected graphs and therefore the running\ntime is dominated by the O(|E| log |V |) term corresponding to decrease-key operations. The question\nis: can we do better ?\nd-ary heaps\nOne of the ideas to get a better running time is increasing the arity of the tree that we are using. If we\nuse a d-ary tree instead of a binary one then we reduce the depth of our tree and thus our inserts and\nbottlenecking decrease-key operations execute in O(logd |S|) time. On the other hand, the execution\nof extract-min operation requires O(d logd |S|) time. So, by choosing the best possible d = d|E|/|V |e\nwe get the total running time of our algorithms to be O((|E|+d|V |) logd |V |) = O(|E|logd|E|/|V |e|V |),\nwhich is a signicant improvement for dense graphs. However, it turns out that we can do even\nbetter. Namely, we can implement priority queue in such a way that from the point of view of running\ntime analysis of our algorithms the cost of decrease-key will be constant and costs of insert and\nextract-min will be logarithmic. This leads to the essentially optimal O(|E| + |V | log |V |) running\ntime of both algorithms and for some present-day applications (think graphs with billions of edges)\nthis improvement can make a huge dierence.\n1 A comprehensive coverage of binary heaps (as well as Fibonacci heaps) can be found in [CLRS].\nlect-2\n\nFibonacci heaps\nThe Fibonacci heaps were proposed by Fredman and Tarjan in 1984 giving a very ecient imple-\nmentation of the priority queues. The main motto of this construction is laziness - we do work only\nwhen we must, and then use it to simplify the structure as much as possible so that the future work\nis easy. This way, we enforce that any sequence of operations has to contain a lot of cheap ones\nbefore we need to do something computationally expensive - the formalization of this intuition will\nbe given later.\n4.1\nConstruction\nA Fibonacci heap consists of a collection of heap-ordered trees (of variable arity) with following\nproperties:\n1. nodes of the trees correspond to elements being stored in the queue,\n2. roots of heap-ordered trees are arranged in a doubly-linked list,\n3. we keep a pointer to the root of a tree that corresponds to the element with minimum key\n(note that heap-ordering of the trees implies that such minimum element has to be a root of\nsome tree),\n4. for each node we keep track of its rank (degree), i.e. the number of its children, as well as\nwhether it is marked (the purpose of marking will be dened later on),\n5. size requirement: if a node u has rank k then the subtree rooted at u has at least Fk+2 nodes,\nwhere Fi is the i-th Fibonacci number, i.e. F0 = 0, F1 = 1 and Fi = Fi-1 + Fi-2 for i ≥ 2.\nWe proceed now to describing how do we perform priority queue operation on our Fibonacci\nheap.\n4.1.1\ninsert\nInserting is very simple. We just add the new element s as a new heap-ordered tree to our collection\nand check whether k(s) is smaller that the current minimum for the queueif so then we change\nthe pointer to the minimum accordingly (see Figure 1).\n4.1.2\ndecrease-key\nWhen we decrease the key of an element s, if the heap-ordering is still satised then we do not need\nto do anything else. Otherwise, we just cut s out of the tree in which it resides and put it as a root\nof a new tree in our collection (note that all the descendants of s are now in this new tree as well).\nWe compare the new key of s and the previously minimum key and change the pointer accordingly\n(see Figure 1).\nThis way we end up with something that looks like a desired Fibonacci heap. However, the\nproblem with simply cutting each such s is that, when we perform in this manner many decrease-\nkey operations, we may end up violating the size requirement that we wanted to preserve. Therefore,\nto alleviate this issue we introduce an additional rule that when we cut s we check whether its parent\nis marked. If so then we cut the parent as well (and we unmark it). Otherwise, we just mark the\nparent. Note that we do this cutting recursively, so if the parent of marked parent of s is also marked\nthen we cut it as well, and so on. Obviously, if we cut a root we are not doing anything, and so it\nis useless to mark a root. This (potentially cascading) cutting procedure therefore always ends.\nlect-3\n\nFigure 1: Illustration of: (left side) inserting a new element to the Fibonacci heap; (right side)\ncutting a vertex in the rst step of decrease-key operation. In both examples we assumed that\nthe newly created root has smaller key than the keys of all the other elements.\n4.1.3\nextract-min\nFinally, we can describe extracting the minimum element s∗. We start with removing s∗ (recall that\nwe stored the pointer to it) and putting all the children of s∗ as roots of new trees in our collection.\nNext, we scan the entire list of roots in our collection to nd the new minimum element and we set\nthe relevant pointer accordingly.\nIn principle at this point we could be done, because we obtain once again a valid Fibonacci\nheap. However, it is not hard to see that so far executing of any of our queue operations makes the\nlist longer and longer. So, going through the whole list of roots during extract-min can be very\nexpensive computationally. Therefore, in the spirit of laziness, if we have to do this work anyway\nthen we can use this opportunity to do some cleaning as well, and avoid in this way the necessity of\ndoing the whole work again when doing the next extract-min . What we do is, as long as there\nare two trees whose roots have the same rank, say k, we merge these trees to obtain one tree of rank\nk + 1. Merging consist of just comparing the keys of the roots and setting the root of the tree with\nlarger key as a new child of the other root (see Figure 2). Note that since merging can introduce a\nsecond tree of rank k + 1 in the collection, one root can take part in many merges.\n4.2\nRunning-time Analysis\nNow we want to analyze the worst-case performance of the described Fibonacci heap data structure.\n4.2.1\nA worst-case example\nLet's imagine the following scenario: We do n consecutive insert operations into the Fibonacci\nheap such that it is a circular linked list containing all elements as singleton heaps. If we perform an\nextract-min operation on this Fibonacci heap, this operation will have to go through the entire\nlist to determine the new minimum. This takes O(n) time an unbearable performance for just\none operation.\nlect-4\n\nFigure 2: Illustration of merging of two trees of the same rank.\n4.2.2\nAre Fibonacci heaps useless?\nDoes this mean that Fibonacci heaps are inecient? No! Intuitively such heavy operations can occur\nonly very rarely and make no big contribution to the overall running time of an algorithm using the\nheap. Being not able to give worst-case performance guarantees for each individual operation, we\nwant to consider a sequence of operations and give a proof that, for any such sequence, the total\nrunning time is small, in the sense that this running time can be apportioned between the individual\noperations so that each has a small contribution. This type of analysis is called amortized analysis\n[CLRS]. More precisely, if we have ` dierent types of operations and we claim that the amortized\nrunning time of an operation of type j is at most tj , this means that for any sequence of operations\ncomposed of kj operations of type j for all j = 1,\n, ` (with operations of dierent types interlaced\n· · ·\nP\nin any way), the total running time is upperbounded by\nj kj tj .\n4.2.3\nExcursion: Amortized Analysis via the Potential Method\nThe most common way to perform amortized analysis is using the potential method. The idea of\nthe potential method allows cheap operations to save up time for the use of heavy operations. This\nfunctions like a bank account with time deposited in it. The potential function Φ represents the\nbalance in the account. Initially, the balance is zero, and remains nonnegative during the whole\nsequence. Now operations are performed having costs (i.e. running times) of c1, c2, c3, ..., ck. Every\noperation is allowed to either pay more than its actual cost ci thereby increasing its amortized cost,\nplacing the credit/savings in the bank account thus increasing the balance Φ, or pay less than the\nactual cost by withdrawing the dierence from Φ. This gives the amortized cost.\nOften one can think of the potential function as a measurement of the complexity of the data\nstructure or conguration within an algorithm. In this case cheap operations are allowed to increase\nthe internal complexity, while operations which simplify or clean up the data are allowed to take\nmore time.\nMaking this formal, a potential function, Φ, maps a conguration Di of an evolving algorithm\nor data structure D into a nonnegative number. The start conguration is normalized to have the\nvalue 0: Φ0 = Φ(D0) = 0. Consider a sequence of operations o1, o2, o3, ..., ok and let Di be the\nconguration of the data structure after performing the ith operation. We impose that the potential\nlect-5\n\nX\nX\nX\nX\nX\nX\nX\nfunction remains nonnegative throughout:\n∀t : Φt = Φ(Dt) ≥ 0.\nIf operation oi has cost (running time) ci then its amortized cost is dened by:\nai = ci + ΔΦi = ci + Φi - Φi-1.\nGiven this, it is easy to see that the sum of the amortized costs upperbounds the original total cost:\nk\nk\nk\nk\nai =\n(ci + Φi - Φi-1) =\nci + Φk - Φ0 ≥\nci.\ni=1\ni=1\ni=1\ni=1\nThus amortized analysis provides an upper bound on the worst-case cost of any sequence of opera-\ntions.\nThe diculty in performing amortized analysis is in choosing the right potential function.\n4.2.4\nFibonacci heaps obey the size requirement\nThe rst important observation regarding the heap-ordered trees in a Fibonacci heap is that the\nrestriction to cut o at most one child prevents cutting down the nice binomial tree like structure\nbuilt up through the combination steps. This guarantees that the size requirement we want to have\nis preserved.\nLemma 1 Consider a node x with rank (number of children) d. Let y1, y2, ..., yd be those children\nin the order they were added to the tree. Then every child yi has rank at least i - 2.\nProof:\nWhen yi was added to x, at least the i - 1 children y1 to yi-1 were present. Since only\nroots of the same rank get combined, yi had at least i - 1 children at this time. At most, one of\nthese children could have been cut away since otherwise yi would have qualied for a cascading cut.\nThus yi has at least i - 2 children.\n\nA simple counting argument given in the next lemma reveals that the number of nodes in a\nsubtree rooted at a node of rank d is at least Fd+2. This exponential growth upperbounds the heap\ndegrees to be logarithmic.\nLemma 2 Let N (d) be the smallest possible number of nodes in a subtree rooted at a node of rank\nd. Then N(d) ≥ Fd+2. Thus, the rank of any node in a Fibonacci heap with n elements is O(log n).\nProof:\nFor N, it holds that N(0) = 1, N(1) = 2 and we have the recurrence relation:\nd\nN(d) ≥ 2 +\nN(i - 2)\ni=2\nbecause of Lemma 1 (counting one for the root, one for the rst child y1 and N(i - 2) for each\nremaining child yi). Proceeding by induction on d (thus assuming that N(j) ≥ Fj+2 for j < d), we\nget that\nd\nd\nN(d) ≥ 2 +\nFi = 1 +\nFi.\ni=2\ni=0\nThe right-hand-side is Fd+2; this can be shown again by induction on d: 1+ P\ni\nd\n=0 Fi = Fd+1 + Fd =\nFd+2. Thus we have shown the rst part of the lemma that N (d) ≥ Fd+2.\nUsing the closed-form expression for the Fibonacci numbers, we get that\n⎛\n⎞\n⎝\n⎠\nN(d) ≥ Fd+2 = √1\n\n1 +\n√\n!d+2\n-\n\n1 -\n√\n!d+2\n≥ 1.61d .\nSince N(d) ≤ n, all ranks of nodes in the heap are at most log1.61 n.\n\nlect-6\n\n4.2.5\nAmortized Analysis of the Fibonacci heap operations\nEach individual adding, combining and cutting step takes only O(1) time. Thus the only two critical\nsituations occur when we have to search through many roots for nding the minimum and when we\nhave a long chain of cascading cuts. The length of a cascading cut corresponds to the number of\nnodes being unmarked. With this intuition, we choose the potential function to be\nΦt = rt + 2mt\nwhere rt is the number of roots and mt the number of marked nodes at time t. The reason for the\nfactor of 2 will become clear in the analysis. Here is the amortized analysis of each operation.\ninsert\n-\nInserting a new root in the list takes ct = O(1) time and increases the number of roots\nrt = rt-1 + 1 by one. Thus the amortized cost for an insert operation is also constant:\nat = ct + (rt - rt-1) + 2(mt - mt-1) = O(1) + 1 + 0 = O(1).\nextract-min\n-\nDuring an extract-min operation, we start with rt roots, cut away the minimum root (say\nof rank d) leaving rt-1 + d - 1 roots in the list. These get combined to rt roots, having\ndierent ranks. Since, by Lemma , the maximum possible rank is O(log n), there are in the\nend only rt = O(log n) roots left. Since the cut and each of the combining steps takes O(1)\ntime and eliminates one root the actual time spend on an extract-min operation is at most\nct = rt-1+d-1 units (where the 'unit' may need to be redened to take into account constants).\nPutting this together the amortized cost for an extract-min operation is logarithmic:\nat = ct + (rt - rt-1) + 2(mt - mt-1) = (rt-1 + d - 1) + (rt - rt-1) + 0 = rt + d - 1 = O(log n).\ndecrease-key\n-\nLet's assume that during a decrease-key operation we do k cuts, k ≥ 1. Each (but the rst)\ncut unmarks a node and each cut introduces a new root. Thus the increase in the number\nof roots, rt - rt-1, is equal to the number k of cuts performed. The decrease mt-1 - mt of\nmarked nodes is either k - 1 or k (depending on whether the node itself was marked); thus, in\nany case, mt-1 - mt ≥ k - 1. The key decreasing, cutting and reinserting takes 1 + k units of\ntime (redening the unit, if needed), and thus its amortized cost is:\nat = ct + (rt - rt-1) + 2(mt - mt-1) ≤ 1 + k + k - 2(k - 1) = O(1).\nThis last relation justies the constant 2 in the denition of the potential function.\nSummarizing, in a Fibonacci heap, every insert and decrease-key takes O(1) amortized time,\nand every extract-min takes O(log n) amortized time.\n4.2.6\nUsing Fibonacci heaps to speed up Prim's and Dijkstra's algorithm\nUsing Fibonacci heaps in the two algorithms mentioned in the introduction leads to improved running\ntimes of O(|E| + |V | log |V |).\nlect-7\n\nReferences\n[CLRS] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Cli Stein. Introduction\nto Algorithms (Second Edition). MIT Press and McGraw-Hill.\nlect-8"
        },
        {
          "category": "Resource",
          "title": "Goldberg-Tarjan Min-Cost Circulation Algorithm",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/6c53462d9af606e71fa8950a51dfccfa_lec4.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 15, 2008\nGoldberg-Tarjan Min-Cost Circulation Algorithm\nLecturer: Michel X. Goemans\nIntroduction\nIn this lecture we shall study Klein's cycle cancelling algorithm for finding the circulation of minimum\ncost in greater detail. We will pay particular attention to the choice of cycle to cancel and we will\nrigorously prove two bounds on the number of iterations required, the first of which depends on the\nmagnitude of the cost and is valid only for integer-valued costs, and the second of which is strongly\npolynomial and works even for irrational costs.\nRecall from last time that for a given circulation f, the following are equivalent:\ni. f is of minimum cost\nii. There is no negative cost cycle in the residual graph Gf\niii. There exist potentials p : V ! R such that the reduced costs\ncp(v, w) = c(v, w) + p(v) - p(w) 0\nfor all (v, w) ≥ Ef , where Ef = {e : uf (e) > 0}.\nKlein's cycle cancelling algorithm\nAlgorithm 1 Kleins-Cycle-Cancel(Gf )\nLet f be any circulation (e.g., f = 0)\nwhile there exists a negative cost cycle ≥ Gf do\nPush (f) = min uf (v, w) along\n(v,w)2\nend while\nIt is important to note that the Ford-Fulkerson algorithm for the maximum flow problem is a\nspecial case of Klein's cycle cancelling algorithm, by defining zero costs for all edges in the original\ngraph and by adding an extra edge from the sink to the source with cost -1.\n2.1\nChoice of cycle\nAs in the Ford-Fulkerson algorithm, the question is which negative-cost cycle to choose.\n1. (Weintraub 1972). One idea is to try choosing the maximum improvement cycle, where\nthe difference in cost is as large as possible. One can show that the number of iterations is\npolynomial for rational costs, but finding such a cycle is NP-hard. For irrational costs, one\ncan show that this algorithm may never terminate (Queyranne 1980) even for the maximum\nflow problem (the fattest augmenting path algorithm of Edmonds and Karp), although the\nsolution converges to a minimum cost flow.\nlect-1\n\n||\n||\n2. (Goldberg-Tarjan 1986). Alternatively, we can choose the cycle of minimum mean cost,\ndefined as follows:\nμ(f) =\nmin\ndirected cycles ≥ Gf\nc()\n||\nP\nwhere c() =\n(v,w)2 c(v, w) and || is the number of edges in the cycle.\nNotice that there exists a negative cost cycle in Gf if and only if μ(f) is negative.\nTo see that we can indeed find the minimum mean-cost cycle efficiently, suppose we replace the\ncosts c with c0 such that c0(v, w) = c(v, w) + for each edge (v, w). Then μ0(f) = μ(f) + ,\nso if = -μ(f) then we would have μ0(f) = 0. In particular,\nμ(f) = - inf{ : there is no negative cost cycle in Gf with respect to costs c + }.\nFor any , we can decide if there is a negative cost cycle by using the Bellman-Ford algorithm.\nNow, perform binary search to find the smallest for which no such cycle exists. In the next\nproblem set we will show a result by Karp, which finds the cycle of minimum mean cost in\nO(nm) time by using a variant of Bellman-Ford.\n2.2\nBounding the number of iterations\nWe will give two bounds on the number of iterations for the algorithm. The first depends on the\nmagnitude of the cost and is valid only for integer-valued costs; it is polynomial but not strongly\npolynomial. The second bound is strongly polynomial and works even for irrational costs.\nWe first need a measure of 'closeness' to the optimal circulation. The following definition gives\nsuch a measure, and will be key in quantifying the progress of the algorithm.\nDefinition 1 (Relaxed optimality) A circulation f is said to be -optimal if there exists a po\ntential p : V ! R such that cp(v, w) - for all edges (v, w) ≥ Ef .\nNote that an 0-optimal circulation is of minimum cost.\nDefinition 2 For a circulation f, let\n(f) = min{ : f is -optimal}.\nOne important thing about this that we will prove soon is that when we push some flow in a\ncirculation f along some cycle and obtain a new circulation f 0, we get that (f 0) (f). This means\nthat is monotonically non-increasing in general. First, we need the following strong relationship\nbetween (f) and μ(f), and this really justifies the choice of cycle of Goldberg and Tarjan.\nTheorem 1 For all circulations f, (f) = -μ(f).\nProof:\nWe first show that μ(f) -(f). From the definition of (f) there exists a potential\np : V ! R such that cp(v, w) -(f) for all (v, w) ≥ Ef . For any cycle ≤ Ef the cost c() is\nequal to the reduced cost cp() since the potentials cancel. Therefore c() = cp() -||(f) and\nc()\nso || -(f) for all cycles . Hence μ(f) -(f).\nNext, we show that μ(f) -(f). For this, we start with the definition of μ(f). For every\nc()\ncycle ≥ Ef it holds that || μ(f). Let c0(v, w) = c(v, w) - μ(f) for all (v, w) ≥ Ef . Then,\nc ()\nc()\n=\n- μ(f) 0 for any cycle . Now define p(v) as the cost of the shortest path from an\nadded source s to v with respect to c0 in Gf (see Fig. 1); the reason we add a vertex s is to make sure\nthat every vertex can be reached (by the direct path). Note that the shortest paths are well-defined\nsince there are no negative cost cycles with respect to c0 . By the optimality property of shortest\nlect-2\n\nc'(v,w)\ns\nv\nw\nFigure 1: p(v) is the length of the shortest path from s to v.\npaths, p(w) p(v) + c0(v, w) = p(v) + c(v, w) - μ(f). Therefore cp(v, w) μ(f) for all (v, w) ≥ Ef\nwhich implies that f is -μ(f)-optimal and thus (f) -μ(f).\nBy combining μ(f) -(f) and (f) -μ(f) we conclude (f) = -μ(f) as required.\n\nThe nature of the algorithm is to push flow along negative cost cycles. We would like to know if\nthis actually gets us closer to optimality. This is shown in the following remark.\nRemark 1 (Progress) Let f be a circulation. If we push flow along the minimum mean cost cycle\nin Gf and obtain circulation f 0 then (f) (f 0).\ncp()\nc()\nProof: By definition\n|| = || = μ(f). Now, (f) = -μ(f) implies that there exists a potential\np such that cp(v, w) μ(f) for all (v, w) ≥ Ef . Furthermore for all (v, w) ≥ the reduced cost\ncp(v, w) = μ(f) = -(f). If flow is pushed along some arcs may be saturated and disappear from\nthe residual graph. On the other hand, new edges may be created with a reduced cost of +(f). More\nformally, Ef 0 ≤ Ef →{(w, v) : (v, w) ≥ }. So for all (v, w) ≥ Ef 0 it holds that cp(v, w) -(f).\nThus we have that (f 0) (f).\n\n2.3\nAnalysis for Integer-valued Costs\nWe now prove a polynomial bound on the number of iterations for an integer cost function c : E ! Z.\nAt the start, for any circulation, the following holds for all (v, w) ≥ E:\n(f) C = max |c(v, w)|.\n(v,w)2E\nNow we can continue with the rest of the analysis.\nLemma 2 If costs are integer valued and (f) < n\n1 then f is optimal.\nProof: Consider -(f) = μ(f) > - n\n1 . For any cycle ≥ Gf we have c() = cp() > - n\n1 || -1.\nSince the cost is an integer, c() 0. By the optimality condition, if there is no negative cycle in\nthe graph, the circulation is optimal.\n\nLemma 3 Let f be a circulation and let f 0 be the circulation after m iterations of the algorithm.\nThen (f 0) (1 - n\n1 )(f).\nProof:\nLet p be the potential such that cp(v, w) -(f) for all (v, w) ≥ Ef and let i and fi\nbe the cycle that is cancelled and the circulation obtained at the ith iteration, respectively. Let\nA be the set of edges in Efi such that cp(v, w) < 0 (we should emphasize that this is for the p\ncorresponding to the circulation f we started from). We now show that as long as i ≤ A, then\n|A| strictly decreases. This is because cancelling a cycle removes at least one arc with a negative\nreduced cost from A and any new arc added to Efi must have a positive reduced cost. Hence after\nlect-3\n\n||\nk m iterations we will find an edge (v, w) ≥ k+1 such that cp(v, w) 0. So by Theorem 1, -(fk)\nis equal to the mean cost of k+1 and thus\nc(k+1)\ncp(k+1)\n(fk) = -μ(fk) = -\n= -\n|k+1|\n|k+1|\n0 + (-(f))(|k+1| - 1)\n\n-\n|k+1|\n\n1 -\n(f).\nn\nCorollary 4 If the costs are integer, then the number of iterations is at most mn log(nC).\nProof:\nWe have that\n\nn log(nC)\n(fend) 1 - 1\n(f = 0) < e- log(nC)|C| =\n1 |C| = 1 ,\nn\nnC\nn\nand thus the resulting circulation is optimal.\n\nThe time per iteration will be shown to be O(nm) (see problem set), hence the total running\ntime of the algorithm is O(m2n2 log(nC)).\n2.4\nStrongly Polynomial Analysis\nIn this section we will remove the dependence on the costs. We will obtain a strongly polynomial\nbound for the algorithm for solving the minimum cost circulation problem. In fact we will show\nthat this bound will hold even for irrational capacities. The first strongly polynomial-time analysis\nis due to Tardos; the one here is due to Goldberg-Tarjan. This result was very significant, since it\nwas the most general subclass of Linear Programming (LP) for which a strongly polynomial-time\nalgorithm was shown to exist. It remains a big open problem whether a strongly polynomial-time\nalgorithm exists for general LP.\nDefinition 3 An edge e is -fixed if for all -optimal circulations f we have that f(e) maintains the\nsame value.\nNote that (v, w) is -fixed if and only if (w, v) is -fixed, by skew-symmetry of edge-costs.\nTheorem 5 Let f be a circulation and p be a potential such that f is (f)-optimal with respect to\np. Then if |cp(v, w)| 2n for some edge (v, w) ≥ E, the edge (v, w) is -fixed.\nProof:\nSuppose (v, w) is not (f)-fixed. There exists an f 0 that is (f)-optimal and f 0(v, w) =∪\nf(v, w); without loss of generality assume f 0(v, w) < f(v, w). Let E< = {(x, y) : f 0(x, y) < f(x, y)}.\nWe can see that E< ≤ Ef 0 by definition of Ef 0 . Furthermore, from flow conservation, we know that\nthere exists a cycle ≥ Ef 0 containing the edge (v, w). Indeed, by flow decomposition, we know\nthat the circulation f - f 0 can be decomposed into (positive net) flows along cycles of Ef 0 , and thus\none of these cycles must contain (v, w)\nNow we have the following,\nc() = cp() -2n(f) + (n - 1)(f) < -n(f).\nConsequently, c() < - and so μ(f 0) < -. As a result, f 0 is not (f)-optimal and thus we have a\ncontradiction.\n\nlect-4\n\n||\n||\nLemma 6 After O(mn log n) iterations, another edge becomes fixed.\nProof: Let f be a circulation and f 0 be another circulation after application of mn log(2n) iterations\nof the Goldberg-Tarjan algorithm. Also suppose that is the first cycle cancelled and p, p0 are the\npotentials for f, f 0 respectively. From the previous lemma, we have that (f 0) (1- n\n1 )n log(2n)(f) <\ne- log(2n) = 2\nn (f). Now from the definition of μ we get the following,\ncp0 ()\nc()\n=\n= μ(f) = -(f) < -2n(f 0)\nThis means that there exists an edge (v, w) ≥ such that cp0 (v, w) < -2n(f 0) which means that it\nwas not (f)-fixed. Thus (v, w) becomes (f 0)-fixed and the claim is proven.\n\nNotice that if e is fixed, it will remain fixed as we iterate the algorithm. An immediate con\nsequence of the above lemma then is a bound on the number of iterations in the Goldberg-Tarjan\nalgorithm.\nCorollary 7 The number of iterations of the Goldberg-Tarjan algorithm, even with irrational costs,\nis O(m2n log n).\nlect-5"
        },
        {
          "category": "Resource",
          "title": "Network Flows",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/10ad543e5ab70c642a82341b3fb0d5f3_lec2.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\nX\n18.415/6.854 Advanced Algorithms\nSeptember 8th, 2008\nNetwork Flows\nLecturer: Michel X. Goemans\nIntroduction\nIn the previous lecture, we introduced Fibonacci heaps, which is a data structure that provides an\necient implementation of priority queues. In this lecture, we switch our attention from eciency\nto algorithm design. In particular, for the next few lectures we study Network Flows.\nNetwork ows are a family of problems that are concerned with a directed graph and properties of\nfunctions dened on the graph. A ow is an abstraction of elements which typically do not disappear\nwhile travelling through the edges of the directed graph; it could be current in an electrical network,\npackets in a computer network, cars/trains in a transportation network, or some purely abstract\nobject. In the maximum ow problem, we try to obtain a ow on the graph such that the ow going\nfrom a given source vertex to a given sink vertex is maximized.\nIn today's lecture, we focus on two instances of network ow problems: the Shortest Path Problem\nand the Maximum Flow Problem. There are other variants of network ow problems that we\ncover later in this class. For example, we will talk about the minimum cost ow or minimum cost\ncirculation problem, which is a generalization of both the shortest path problem and the maximum\now problem. We will also cover the bipartite matching problem, which has two versions: cardinality\nbipartite matching (a special case of the maximum ow problem) and weighted bipartite matching\n(a special case of the minimum cost ow problem). There are still other network ow problems that\nwe do not discuss such as the multi-commodity ow problem. Figure 1 illustrates how these network\now problems are related to one another.\nShortest Path Problem\nLet G = (V, E) be a directed graph, where V denotes the set of vertices and E denotes the set of\nedges. Let `: E\nR be a length function dened on the edges of G.1 Given two vertices s and t in\n→\nV , the s - t shortest path problem is the problem of nding a simple directed path on G from s to\nt of minimum total length. The length of a path P is dened to be the sum of the lengths of all the\nedges in P :\n`(P ) =\n`(v, w).\n(v,w)∈P\nIn this problem, we refer to s as the source vertex and t as the sink vertex.\nWe note that if the length function `(e) is non-negative for every edge e ∈ E, then Dijkstra's\nalgorithm using Fibonacci heaps provides a O(m + n log n) solution to this problem, where m = |E|\nand n = |V |. On the other hand, if some edges of G have negative lengths, but the graph has\nthe property that for every cycle C the total length of the cycle is non-negative, then we can use\nthe Bellman-Ford algorithm to solve the s - t shortest path problem in polynomial time. For more\ninformation on Dijkstra's and the Bellman-Ford algorithm, see Chapter 24 in [CLRS].\n1 For v, w ∈ V , we use the notation `(v, w) to mean the length of the edge e = (v, w). In these notes, we use the\ntwo notations `(e) and `(v, w) interchangeably.\nlect-1\n\nFigure 1: Some instances of network ow problems and how they are related to one another, where\nthe arrow indicates is a special case of. In this lecture we only cover the shaded boxes: the shortest\npath problem and the maximum ow problem.\nRemark 1: In this lecture, we consider directed graphs only. For undirected graphs with non-\nnegative edge lengths, we can still apply Dijkstra's algorithm by transforming every (undirected)\nedge into two edges of opposite directions with the same length, as illustrated in Figure 2.\n(a) Original undirected edge.\n(b) Directed edges after the transfor-\nmation.\nFigure 2: Transformation of an undirected edge into two directed edges to apply Dijkstra's algorithm.\nHowever, the same trick does not apply for the Bellman-Ford algorithm, because even if the original\nundirected graph satises the constraint that every cycle has non-negative length, the new directed\ngraph resulting from the transformation might violate this constraint. An example of this case is\ngiven in Figure 3.\nThe problem of nding the shortest path between two vertices in an undirected graph where every\ncycle has non-negative length is still solvable in polynomial time, but it is a much harder problem.\nWe will discuss this problem later in the class if time permits.\nRemark 2: In directed graphs with non-negative, given a shortest path P between two vertices, the\npath between any two vertices in P is also the shortest path between those two vertices. However, this\nis not necessarily true in the case of undirected graphs (and this prevents the use of a transformation\nto a directed graph). For example, in the graph given in Figure 3(a), the shortest path between\nv and w is P = {(v, z), (z, w)} with length 0. However, the shortest path between v and z is not\n{(v, z)} as it appears in P , but rather {(v, w), (w, z)} with length 0.\nlect-2\n\nX\nX\nX\nX\n(a) Original undirected graph. Every\n(b) Directed graph after the transfor-\ncycle has non-negative length.\nmation. The cycle {(w, z), (z, w)} has\nnegative length.\nFigure 3: An example where the given transformation creates a negative cycle so that the Bellman-\nFord algorithm cannot be applied.\nMaximum Flow Problem\nThe second instance of network ow problems that we study in this lecture is the maximum ow\nproblem. In this problem, we want to nd a ow from a source vertex to a sink vertex with maximum\now value.\nMore precisely, we dene the problem framework as follows. Let G = (V, E) be a directed graph,\nwhere V is the set of vertices and E is the set of edges of G. Let n denote the cardinality of V and\nm denote the cardinality of E. Given a vertex v ∈ V , let N +(v) (resp. N -(v)) denote the set of\nendpoints of edges coming out (resp. into) v:\nN +(v) = {w ∈ V : (v, w) ∈ E},\nN -(v) = {w ∈ V : (w, v) ∈ E}.\nFurthermore, let u: E\nR+ be a capacity function that limits the amount of ow that we can send\n→\nthrough each edge of G. We refer to the graph G and the capacity function u collectively as the\nnetwork G. Given a source vertex s ∈ V and a sink vertex t ∈ V , we are interested in determining\nhow much ow we can push from s to t through this network.\n3.1\nNotions of Flow\nLoosely speaking, a ow is an assignment of quantity to the edges of G under certain constraints.\nThere are two notions of ow that we use in this class: raw ow and net ow.\nDenition 1 A raw ow on a network G is a function r : E\nR satisfying the following properties:\n→\n1. Capacity constraint: For all (v, w) ∈ E, 0 ≤ r(v, w) ≤ u(v, w).\n2. Conservation constraint: For all v ∈ V \\ {s, t},\nr(v, w) -\nr(w, v) = 0.\nw∈V :(v,w)∈E\nw∈V :(w,v)∈E\nGiven a raw ow r, the ow value of r is dened to be the total excess of ow at the source vertex\ns, i.e.\n|r| =\nr(s, w) -\nr(w, s).\nw∈N+(s)\nw∈N- (s)\nlect-3\n\nX\nP\nWe now give the second denition of ow, which is the one we primarily use for the rest of these\nnotes.\nDenition 2 Given a raw ow r on a network G, the net ow f with respect to r is the function\nf : E → R given by f(v, w) = r(v, w) - r(w, v).\nAn example of raw ow and the corresponding net ow is illustrated in Figure 4.\n(a) Raw ow.\n(b) Net ow.\nFigure 4: An example of a raw ow and its corresponding net ow.\nBefore we go any further, we rst note that from the denition given above, to compute f(v, w) we\nneed both r(v, w) and r(w, v). However, there is a slight diculty because even if (v, w) ∈ E, (w, v)\nmight not be an edge of G. To resolve this issue, we assume that the graph G has the property that\nif (v, w) ∈ E then (w, v) ∈ E. Given a directed graph G, we can achieve this property by modifying\nG as follows:\n1. Consider the set E0 = {(v, w) ∈ E : (w, v) ∈/ E}.\n2. For every (v, w) ∈ E0, create a new edge (w, v) with edge capacity 0 and add it to E.\nSimilar to the denition of the ow value of raw ow, the ow value of f is dened to be the total\namount of net ow that comes out from the source vertex s:\n|f| =\nf(s, w),\n(1)\nw∈N(s)\nwhere we now use N(s) to denote N +(s) = N -(s), the common set of out-neighbors and in-neighbors\nof s.\nFrom the denition of net ow, it is easy to check that the net ow f satises the following properties:\n1. Skew symmetry: For all (v, w) ∈ E, f(v, w) = -f(w, v).\n2. Capacity constraint: For all (v, w) ∈ E, f(v, w) ≤ u(v, w).\n3. Flow conservation: For all v ∈ V \\ {s, t},\nw∈N (v) f(v, w) = 0.\nNote that, unlike r, the ow f has no restriction on being negative. In fact, f will be negative for\nsome edges, unless it is the 0 ow everywhere. For example, if the original graph G has an edge\n(v, w) with positive raw ow r(v, w) such that (w, v) is not an edge, then in the modied graph, the\nedge (w, v) has negative net ow f(w, v) = -r(v, w). Note that this does not violate the capacity\nconstraint since f(w, v) ≤ u(w, v) = 0. Figure 5 illustrates an example of a net ow.\nFor the maximum ow problem, we use the notion of net ow. For the rest of these notes, unless\nspecied otherwise, the term ow refers to net ow. We can now dene the maximum ow problem\nproperly.\nDenition 3 (Maximum Flow Problem) Given a network G, a source vertex s ∈ V , and a sink\nvertex t ∈ V , the maximum ow problem is the problem of nding a ow through G of maximum\now value.\nNotice that modifying G by adding to E the new edges needed to dene the net ow does not aect\nthe maximum ow problem, since the new edges all have zero capacity.\nlect-4\n\nX\nFigure 5: An example of a ow of a network. The label x/y on each edge e is such that x = f(e)\nand y = u(e). Here the ow value is |f| = 3.\n3.2\ns - t Cut\nWe now dene the notion of cut, which helps us to construct the solution of the maximum ow\nproblem.\nDenition 4 Suppose that we have a network G with source vertex s and sink vertex t. Let S be a\nsubset of V such that s ∈ S and t /∈ S, and let S = V \\ S. Then the s - t cut with respect to S is\ndened to be\n(S : S) = {(v, w) ∈ E : v ∈ S and w ∈ S}.\nWe can also denote an s - t cut by δ+(S) or δ-(S), but in this class the preferred notation is (S : S)\nas introduced above. Figure 6 shows an example of an s - t cut.\nFigure 6: An example of an s - t cut. The solid arrows represent the edges in (S : S).\nDenition 5 Given an s - t cut (S : S), then its cut capacity is dened to be the total capacity\nof the edges across the cut:\nu(S : S) =\nu(v, w).\n(v,w)∈(S:S)\n3.3\nConnection between Flows and Cuts\nWe have the following lemma that connects ows and cuts.\nlect-5\n\nX\nX\nX\nX\nX\nX\nX\nX X\nX\nX\nX\nLemma 1 Let G be a network with source s and sink t. Then for every ow f and every s - t cut\n(S : S), we have\nX\n|f| =\nf(v, w).\n(2)\n(v,w)∈(S:S)\nIn particular, this implies that |f| ≤ u(S : S).\nProof:\nFrom the ow conservation property of f, for every vertex v ∈ S \\ {s}, we have\nf(v, w) = 0.\nw∈N(v)\nTaking the sum over all vertices v ∈ S \\ {s} gives us\nf(v, w) = 0.\nv∈S\\{s} w∈N(v)\nAdding the denition of the ow value of f (Eq. (1)) to the equation above yields\n|f| =\nf(s, w) +\nf(v, w).\nw∈N (s)\nv∈S\\{s} w∈N(v)\nNow notice that if an edge (v, w) appears in either of the summations above and w ∈ S, then (w, v)\nalso appears in the summations. Therefore, we can rewrite the equation above in a slightly dierent\nway:\n|f| =\nf(v, w) +\nf(v, w).\n(v,w)∈(S:S)\nv∈S w∈S\nBy the skew-symmetry property of f, the second summation in the equation above is equal to 0\nsince f(v, w) and f(w, v) cancel each other out. Therefore, we conclude that\n|f| =\nf(v, w),\n(v,w)∈(S:S)\nas desired.\nFurthermore, by the capacity constraint of f, we can write\n|f| =\nf(v, w) ≤\nu(v, w) = u(S : S).\n(v,w)∈(S:S)\n(v,w)∈(S:S)\nThis completes the proof of the lemma.\nIn particular, if we take S = V \\ {t} and S = {t}, then Eq. (2) from Lemma 1 tells us that the\now coming from s is equal to the ow going to t. In other words, there is no loss in the ow of the\nnetwork.\nAn important corollary to Lemma 1 comes from the observation that since the value of any ow f is\nalways less than equal to the capacity of any s - t cut (S : S), then it also holds for the case when f\nis a maximum ow and (S : S) is a minimum cut. This fact is known as the Weak-Duality Lemma.\nCorollary 2 (Weak-Duality Lemma) Let G be a network with source vertex s and sink vertex\nt. Then\nmax f ≤ min u(S : S),\nf | |\n(S:S)\nwhere the maximum is taken over all possible ows and the minimum is taken over all possible s - t\ncuts in G.\nlect-6\n\nThe Max-Flow and Min-Cut Theorem\nIn this section, we show that the inequality in the Weak-Duality Lemma is actually an equality, that\nis, the maximum value of a net ow is equal to the minimum value of an s - t cut. This fact was\nrst discovered in 1956 by by Elias, Feinstein, and Shannon (see [EFS]), and independently by Ford\nand Fulkerson in the same year.\nTheorem 3 (Duality Theorem/Maxow Mincut Theorem) In a network G, the following\nequality holds:\nmax f = min u(S : S).\nf | |\n(S:S)\nIn order to prove the theorem, we rst have to introduce some new denitions. The rst one is\nresidual capacity, which denotes the extent to which a ow on some edge is less than the capacity\non that edge.\nDenition 6 The residual capacity of G with respect to f is the function uf : E\nR dened by\n→\nuf (v, w) = u(v, w)- f(v, w) for all (v, w) in E. Hence, the residual capacity on the edge (v, w) is the\namount of additional ow that we can push from v to w, without violating the capacity constraint.\nWe observe that the capacity constraint implies that uf (v, w) = u(v, w) - f(v, w) = u(v, w) +\nf(w, v) ≤ u(v, w) + u(w, v). Moreover, since f is a ow, u(v, w) ≥ f(v, w), so that uf (v, w) ≥ 0.\nHence, the following inequality holds for any edge (v, w) in E:\n0 ≤ uf (v, w) ≤ u(v, w) + u(w, v).\nAll the edges with positive residual capacities are members of a set that we call the residual arcs.\nDenition 7 The residual arcs Ef of G with respect to f is the set given by Ef = {(v, w) ∈ E :\nuf (v, w) > 0}. Intuitively, the residual arcs is the subset of E that contains those edges through\nwhich we can push a non-zero additional ow.\nGiven the vertices of a network G, its residual arcs, and its residual capacity, we can make a new\nnetwork, the residual network.\nDenition 8 The residual network Gf of the network G with respect to f is the network given\nby the graph Gf = (V, Ef ) together with the capacity function uf .\nThe residual network is used to understand to what extent a ow is not maximal, and we do that\nby dening a certain kind of path in the residual network that we call augmenting path.\nDenition 9 An augmenting path of G with respect to f is a directed simple path from the source\ns to the sink t in the residual network Gf .\nIn fact, the existence of an augmenting path in a residual network for a given ow indicates that\nthe ow is not maximal, as we prove in the following lemma.\nLemma 4 If a residual network Gf has at least one augmenting path P , then f is not a maximum\now.\nProof: By denition, the residual network Gf includes only edges with non-zero residual capacity\nwith respect to f. Therefore, an augmenting path P of Gf is a path through which we can push\nmore ow in the original network G, and the additional amount of ow is upper bounded by the\nbottleneck of P .\nlect-7\n\nX\nX\nMore precisely, consider the quantity given by\n(P ) = min uf (v, w).\n(v,w)∈P\nObserve that (P ) > 0, because P ⊂ Ef so that P is a nite set of positive real numbers.\nThen, construct the ow f 0 given by\nf 0(v, w) =\n⎧\n⎪\n⎨\n⎪\n⎩\nf(v, w) + (P ) if (v, w) ∈ P ,\nf(v, w) - (P ) if (w, v) ∈ P ,\nf(v, w)\notherwise.\nNote that f 0 is satises all the ow constraints for G. Moreover, |f 0| = |f| + (P ) > |f|, so that the\now f is not a maximum ow.\nUsing Lemma 4 and the Weak-Duality Lemma, we prove now the Maxow Mincut Theorem.\nProof of Theorem 3: Let f be a ow of maximal value for G = (V, E). By Lemma 4, the residual\nnetwork Gf has no augmenting path, since, if it did, then f would not be of maximal value.\nConsider the set S of vertices v ∈ V such that there exists a directed path from the source s to v in\nGf . By denition, s ∈ S. Moreover, Gf has no augmenting path, so that t 6∈ S. Therefore, (S : S)\nis an s - t cut.\nNow notice that uf (v, w) = 0 for any (v, w) ∈ (S : S). By denition, uf (v, w) = u(v, w) - f(v, w),\nso that f(v, w) = u(v, w) for any (v, w) ∈ (S : S). Thus, we can compute that\n|f| =\nf(v, w) =\nu(v, w) = u(S : S).\n(v,w)∈(S:S)\n(v,w)∈(S:S)\nThe Weak-Duality Lemma tells us that the value of any ow is upper bounded by the capacity of\nany s - t cut, so we can conclude that\nmax f = min u(S : S).\nf | |\n(S:S)\nWe summarize all of the results in the following theorem.\nTheorem 5 (Max-Flow Min-Cut Theorem) Let G be a network and f be a ow on G. Then,\nthe following statements are equivalent:\n1. f is a ow of maximal value;\n2. Gf has no augmenting path; and\n3. |f| = u(S : S) for some s - t cut (S : S).\nProof:\nWe prove the equivalence of the statements by showing that (1)\n(2)\n(3)\n(1), that\n⇒\n⇒\n⇒\nis:\n(1)\n(2): This implication is the contrapositive of the implication proved in Lemma 4.\n-\n⇒\n(2)\n(3): This implication follows from the proof of the Maxow Mincut Theorem.\n-\n⇒\n(3)\n(1): This implication follows from the Weak Duality Lemma.\n-\n⇒\nlect-8\n\nThe Ford-Fulkerson Algorithm\nIn 1956 Ford and Fulkerson used the Max-Flow Min-Cut Theorem to design an algorithm, called the\nFord-Fulkerson algorithm, to compute the maximal ow of a network (see [FF]). The idea of their\nalgorithm is very simple: as long as there is an augmenting path in the residual network we push\nmore ow along that path in the original network. This idea is illustrated as pseudocode below.\nFord-Fulkerson(G)\n1 start with a zero ow f (or any feasible ow)\n2 while Gf has an augmenting path P\ndo push (P ) more units of ow through P , so that |f| ←|f| + (P )\nBefore we declare the idea above an algorithm, there are two issues that need to be addressed:\n1. Does the algorithm ever halt?\n2. If there is more than one augmenting path in the residual network, which one should we choose?\nAnd how does our decision aect the correctness and running time of the algorithm?\nWe consider three cases.\nCase 1: Assume that the capacity function u of G is integer valued. Then we can make the\nfollowing observations:\n1. At every iteration of Ford-Fulkerson, the ow f is integer valued, and therefore so are the\nresidual capacities. Indeed, this is the case at the beginning when f = 0, and by induction,\nthis is maintained since (P ) is the minimum of a set of positive integers and thus a positive\ninteger, and therefore the resulting ow after an augmentation is also integer valued.\nFurthermore, since (P ) ≥ 1 (being a positive integer) and since the minimum-cut value (and\nthus the maximum ow value) is nite, it follows that the Ford-Fulkerson always halts.\n2. Since the algorithm halts and every intermediate ow is integer valued, the maximum ow\noutput will also be integer valued. That is, if the capacities of a network are integral then\nthere is a maximum ow that is also integral. This is a very useful property that has many\napplications. One such application is the cardinality bipartite problem, as we will see in the\nnext lecture.\n3. The number of iterations is bounded by |f| ≤|N(s)|U ≤ nU , where U = max{u(s, w) : w ∈\nN(v)}. Note that U may not be polynomial in the size of G. In fact, Figure 7 shows an example\nof a graph where Ford-Fulkerson takes exponential time to halt. The dotted and dashed\nlines represent paths from the source to the sink. The algorithm might choose alternatively\nand repeatedly the two paths as augmenting paths. In such a case, the algorithm will take\nO(2L) time to terminate. Thus, we need a better policy to choose the augmenting path.\nCase 2: Assume that the capacity function u of G is rational valued. Then, a similar discussion\nas the one carried out in Case 2 shows that Ford-Fulkerson always halts, that the value of the\nmaximal ow is rational, and that there exists an example of a network for which the running time\nis exponential. The arguments are similar because the rational capacities behave like integers if we\nconsider them all as written with the same least common multiple.\nCase 3: Assume that the capacity function u of G is real valued. In the general case (i.e. u(E) ⊂ Q+\nis not necessarily true) there exist instances of networks such that Ford-Fulkerson never halts.\nMoreover, in such cases, the value of |f| may converge to a sub-optimal value.\nlect-9\n\nFigure 7: An example of a network for which the Ford-Fulkerson algorithm may not halt in polyno-\nmial time (the reverse edges and the corresponding ows are not shown for clarity).\nFixing the Ford-Fulkerson Algorithm\nThe problems of the Ford-Fulkerson algorithm that we examined at the end of Section 5 can be\naddressed, at least in part, by specifying a policy for choosing the augmenting path at every iteration.\nA good policy must satisfy two properties:\n1. It is possible to eciently (e.g. in polynomial time) nd the augmenting path specied by the\npolicy; and\n2. The maximum number of augmentations (and thus the total time) is polynomial.\nIn fact, we should be precise when we say that a running time is polynomial, because it means\ndierent things depending on the model of computation. Also, ideally, we would like algorithms for\nwhich the number of operations does not depend on the size of the numbers involved in the input\n(e.g. the capacities in a maximum ow instance); such algorithms could be used even if the data was\nirrational (provided our model allows (arithmetic) operations on irrational data).\nGiven an instance I of a number problem (a computational problem involving numbers as input),\nlet size(I) denote the number of bits needed to represent the input and number(I) denote the\nnumber of numbers involved in the input. For example, for a maximum ow instance, number(I)\ncorresponds to the number m of edges while size(I) corresponds to the number of bits needed to\nrepresent all edge capacities. For the solution of an n × n system of linear equations, number(I) will\nbe n2 + n (n2 for the matrix and n for the right-hand-side) while size(I) is the sum of the binary\nsizes of all the entries of the matrix and the right-hand-side.\nWe say that an algorithm A running on an instance I is (weakly) polynomial if\n- the number of operations performed by A is at most polynomial in size(I) and\n- the size of any number obtained during the execution of A is at most polynomial in size(I).\nFor an algorithm to be strongly polynomial, we require that\n- the number of operations performed by A is at most polynomial in number(I) and\n- the size of any number obtained during the execution of A is at most polynomial in size(I).\nThus, the two notions dier only in whether the number of operations performed depends on the\nsize of the numbers in the input. For example, Gaussian elimination can be shown to be strongly\npolynomial for solving a system of equations (it is clear that the number of operations is at most\nO(n3), but one can also show that the size of the numbers obtained through the algorithm are\npolynomially bounded in the size of the input). On the other hand, Euclid's algorithm for computing\nthe gcd is clearly not strongly polynomial (as only 2 numbers are involved), but is polynomial.\nlect-10\n\nWe now consider two policies for choosing the augmenting path in the Ford-Fulkerson algorithm.\nBoth were proposed by Edmonds and Karp in 1972 [EK]. Both lead to polynomial algorithms, while\nthe second leads to a strongly polynomial algorithm.\nPick the Fattest: Suppose that, in the case of integral capacities, at every iteration of the Ford-\nFulkerson algorithm, we pick the fattest augmenting path, that is, a path P such that (P ) is\nmaximized. Given this policy:\nBy adapting Dijkstra's algorithm to nd this bottleneck path rather than the shortest path, it\n-\nis possible to nd the augmenting path that maximizes (P ) in O(m + n log n) time;\n- It can be shown that the number of iterations is O(m log U), where U is a bound for the\ncapacity function, yielding a running time for this fattest augmenting path algorithm of O((m+\nn log n)m log U).\nA similar argument works for rational capacities as well. However, for irrational capacities, the time\ncomplexity given above does not apply, and this analysis does not even show whether the algorithm\nterminates.\nPick the Shortest: Suppose that, in the case of integral capacities, at every iteration of the Ford-\nFulkerson algorithm, we pick the shortest augmenting path, that is, a path P such that its number\nof edges is minimized. Given this policy, we observe that:\nUsing breadth-rst search, it is possible to nd the augmenting path with a minimum number\n-\nof edges in O(m) time (by breadth-rst-search);\n- It can be shown that the number of iterations is O(nm), yielding a running time for the\nalgorithm of O(nm2). Thus this shortest augmenting path algorithm is strongly polynomial\nand therefore halts even if capacities are irrational.\nNext time we will discuss more network ow problems.\nReferences\n[CLRS] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Cliord Stein, Introduction\nto Algorithms, Second Edition, MIT Press and McGraw-Hill, 2001.\n[EFS] P. Elias, A. Feinstein, and C. E. Shannon, Note on maximum ow through a network, IRE\nTransactions on Information Theory IT-2, 117119, 1956.\n[EK] Jack Edmonds, and Richard M. Karp, Theoretical improvements in algorithmic eciency for\nnetwork ow problems, Journal of the ACM 19 (2): 248264, 1972.\n[FF] L. R. Ford, D. R. Fulkerson, Maximal ow through a network, Canadian Journal of Mathematics\n8: 399404, 1956.\nlect-11"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 3",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/84ea6cf153a2841c6fb7d25812bd69ff_lec3.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 10, 2008\nLecture 3\nLecturer: Michel X. Goemans\nIntroduction\nToday we continue our discussion of maximum flows by introducing the fattest path augmenting\nalgorithm, an improvement over the Ford-Fulkerson algorithm, to solve the max flow problem. We\nalso discuss the minimum cost circulation problem.\nMaximum Flow\nIn a maximum flow problem, the goal is to find the greatest rate (flow) at which material can be\nsent from a source s to a sink t. Several problems can be modeled as a max-flow problem, including\nbipartite matching, which will be discussed today. We will also discuss flow decomposition and the\nfattest augmenting path algorithm.\n2.1\nMaximum Cardinality Matching in Bipartite Graphs\nA bipartite graph is a graph G = (V, E) whose vertex set V can be partitioned into two disjoint sets,\nA and B, such that every edge connects a vertex in A to one in B. A matching M is a subset of E\nsuch that the endpoints of all the edges in M are distinct. In other words, two edges in M cannot\nshare a vertex. We are interested in solving the following problem: Given an undirected bipartite\ngraph G = (V, E) where V = A ∪ B, find a matching M of maximum cardinality.\nWe can formulate this maximum cardinality matching problem as a max-flow problem. To do\nthat, consider the network shown in Figure 1.\nFigure 1: The figure on the left represents a matching in a bipartite graph. The figure on the right\nshows how the bipartite graph can be converted into a max-flow network by imposing a capacity of\n1 on arcs out of s and into t.\n3-1\n\nThe network is constructed as follows: We orient each edge in G from A to B and assign them a\ncapacity of 1 (any capacity greater than 1 works too). We also add two new vertices, s and t, and\narcs from s to every vertex in A, and from every vertex in B to t. All the new arcs are given unit\ncapacity.\nTheorem 1 Let G = (V, E) be a bipartite graph with vertex partition V = A ∪ B, and let G0 =\n(V 0, E0) be the capacitated network constructed as above. If M is a matching in G, then there is an\ninteger-valued flow f in G0 with value |f| = |M|. Conversely, if f is an integer-valued flow in G0,\nthen there is a matching M in G with cardinality |M| = |f|.\nProof:\nGiven M, define a flow f in G0 as follows: if (u, v) ∈ M, then set f(s, u) = f(u, v) =\nf(v, t) = 1 and f(u, s) = f(v, u) = f(t, v) = -1. For all other edges (u, v) ∈ E0, let f(u, v) = 0.\nEach edge (u, v) ∈ M corresponds to 1 unit of flow in G0 that traverses the path s → u → v → t.\nThe paths in M have distinct vertices, aside from s and t. The net flow across the cut (A ∪ s : B ∪ t)\nis equal to |M|. We know that the net flow across any cut is the same, and equals the value of the\nflow. Thus, we can conclude that |M| = |f|. To prove the converse, let f be an integer-valued flow\nin G0. By flow conservation and the choice of capacities, the net flow in each arc must be -1, 0 or\n1. Let M be the set of edges (u, v), with u ∈ A, v ∈ B for which f(u, v) = 1. It is easy to see, by\nflow conservation again, that M is indeed a matching and, using the same argument as before, that\n|M| = |f|.\n\nSince all the capacities of this maximum flow problem are integer valued, we know that there\nalways exists an integer-valued maximum flow, and therefore the theorem shows that this maximum\nflow formulation correctly models the maximum cardinality bipartite matching.\n2.2\nFlow Decomposition\nIn an (raw) s-t flow, we have the following building blocks:\n- Unit flow on an s-t directed path.\n- Unit flow on a directed cycle.\nAny (raw) s-t flow can be written as a linear combination of these building blocks.\nTheorem 2 Any (raw) s-t flow r can be decomposed into at most m flows along either paths from s\nto t or cycles, where m is the number of edges in the network. More precisely, it can be decomposed\ninto at most |{e : r(e) > 0}| ≤ m paths and cycles.\nProof:\nBy tracing back the flow on an edge e and tracing forward the flow on e, we either get an\ns-t path T , or a cycle T with r(e) > 0 for all e ∈ T . Denote the min flow on T by Δ(T ):\nΔ(T ) = min r(e).\ne∈T\nWe want to decrease the flow on T such that at least one edge goes to 0 (by subtracting out Δ(T )),\nand keep doing that until there are no more edges with non-zero flows. More precisely, the following\nalgorithm extracts at most m paths and cycles.\n(i) While there is a directed cycle C with positive flow:\n(a) Decrease the flow on this cycle by Δ(C)\n(b) Add this cycle as an element of the flow decomposition\n(ii) (The set of arcs with positive flow now form an acyclic graph.) While there is a path P from\ns to t with positive flow:\n3-2\n\n(a) Decrease the flow on this path by Δ(P ).\n(b) Add this path as an element of the flow decomposition.\nEach time we decrease the flow on a path or a cycle T , we zero out the flow on some edge.\nWhen we do this, the new raw flow is rnew(e) = r(e) - Δ(T ) if e ∈ T , or r(e) otherwise. Since\nthere are |{e : r(e) > 0}| ≤ m edges with positive flow in the graph, there will be at most that\nnumber of decreases in the flow, and consequently, at most that number of paths or cycles in the\nflow decomposition.\n\n2.3\nFattest Augmenting Path Algorithm (Edmonds-Karp '72)\nFlow decomposition is a key tool in the analysis of network flow algorithms, as we will illustrate\nnow.\nAs we saw in the last lecture, the Ford-Fulkerson algorithm for finding a maximum flow in a\nnetwork may take exponential time, or even not terminate at all, if the augmenting path is not\nchosen appropriately. We proposed two specific choices of augmenting paths, both due to Edmonds\nand Karp, that provide a polynomial running time. One was the shortest augmenting path, the\nother was the fattest augmenting path or maximum-capacity augmenting path: the augmenting path\nthat increases the flow the most. This is the variant we analyze now.\nFor an augmenting s-t path P ∈ Gf , define\nε(P ) = min uf (v, w)\n(v,w)∈P\nwhere the uf are the residual capacities. The minimum residual capacity ε(P ) (the bottleneck) is\nthe maximum flow that can be pushed along the path P . We wish to find the fattest augmenting\npath P such that ε(P ) is maximized. The fattest augmenting path P can be efficiently found with\nDijkstra's algorithm in O(m + n log n) time 1 .\nTheorem 3 Assuming that capacities are integral and bounded by U, the optimal flow for a network\ncan be found in O(m log(mU )) = O(m log(nU )) iterations of augmenting along the fattest path.\nProof: Start with a zero flow, f = 0. Consider a maximum flow f ∗. Its value is at most the value\nof any cut, which is bounded by mU:\n|f ∗| ≤ mU.\nConsider the flow f ∗ - f (this is, f ∗(e) - f(e) for all edges e) in the residual graph Gf with residual\ncapacities uf = u - f.\nWe can decompose f ∗ - f into ≤ m flows using flow decomposition. As a result, at least one of\nthese paths carry a flow of value at least 1 (|f ∗| -|f|). Suppose now that we push ε(P ) units of\nm\nflow along the fattest path in the residual graph Gf and obtain a new flow f new of value:\n|f new| = |f| + ε(P ).\nSince the fattest path provides the greatest increase in flow value, we must have that ε(P ) ≥\nm (|f ∗| -|f|). Thus we have the following inequality\n|f new| ≥|f| + m (|f ∗| -|f|),\n1Actually, it can be found in O(m) time under the condition that we have the capacities sorted beforehand, see\nthe forthcoming problem set.\n3-3\n\n|\n\nP\nwhich implies\n|f ∗| -|f new| = |f ∗| -|f + |f| -|f new|\n≤\n1 - m (|f ∗| -|f|) .\nAfter k iterations, we get a flow fˆ such that\n\nk\n|f ∗| -|fˆ| ≤ 1 - m\nmU.\nEventually f ∗\nfˆ < 1 which implies f ∗ = fˆ since, for integral capacities, all intermediate flows\n|\n| -| |\nwill be integral. Since (1 - m )m ≤ e for all m ≥ 2, the number of iterations required for the\ndifference to go below 1 is\nk = m log(mU).\nCombining the results mentioned above we have the following corollary.\nCorollary 4 We can find a maximum flow in an integer-capacitated network with maximum capacity\nU in O((m + n log n)m log(nU )) time 2 .\nMinimum Cost Circulation Problem (MCCP)\nA circulation is simply a flow where the net flow into every vertex (there are no sources or sinks) is\nzero. Notice that we can easily transform an s - t flow to a circulation by adding one arc from t to\ns (with infinite capacity) which carries a flow equal to the s - t flow value.\nDefinition 1 A circulation f satisfies\n(i) Skew-Symmetry: ∀ (v, w) ∈ E, f(v, w) = -f(w, v).\n(ii) Flow Conservation: ∀ v ∈ V ,\nf(v, w) = 0.\nw\n(iii) Capacity Constraints: ∀ (v, w) ∈ E, f(v, w) ≤ u(v, w).\nDefinition 2 A cost function c : E 7→ R assigns a cost per unit flow to each edge. We assume the\ncost function satisfies skew symmetry: c(v, w) = -c(w, v). For a set of edges C (e.g. a cycle), we\ndenote the total cost of C by :\nX\nc(C) =\nc(v, w).\n(v,w)∈C\nDefinition 3 The goal of the Minimum Cost Circulation Problem (MCCP) is to find a circulation\nf of minimum cost c(f) where\nX\nc(f) =\nc(v, w)f(v, w).\n(v,w)\nThe MCCP is a special case of a Linear Programming (LP) problem (an optimization problem\nwith linear constraints and a linear objective function). But while no strongly polynomial time\nalgorithms are known for linear programming, we will be able to find one for MCCP.\n2Using the previous footnote, we can do this in O(m2 log(nU)) time.\n3-4\n\nX\nX\nX\nX\nX\n3.1\nVertex Potentials\nBefore we can solve MCCP, it is necessary to introduce the concept of vertex potentials, or simply\npotentials.\nDefinition 4 A vertex potential is a function p : V 7→ R that assigns each vertex a potential. The\nvertex potential defines a reduced cost function cp such that\ncp(v, w) = c(v, w) + p(v) - p(w).\nProposition 5 The function cp satisfies the following properties:\n(i) Skew-Symmetry: cp(v, w) = -cp(w, v).\n(ii) Cycle Equivalence: for a cycle C, c(C) = cp(C); i.e., the reduced cost function agrees with\nthe cost function.\n(iii) Circulation Equivalence: for all circulations, the reduced cost function agrees with the cost\nfunction, c(f) = cp(f).\nProof:\nThe first property is trivial. The second property follows since all the potential terms\ncancel out. And we'll prove the third property. By definition\ncp(f)\n=\n(c(v, w) + p(v) - p(w))(f(v, w))\n(v,w)\n= c(f) +\np(v)\nf(v, w) -\np(w)\nf(v, w).\nv\nw:(v,w)∈E\nw\nv:(w,v)∈E\nNow by flow conservation, the inner sums are all zero. Hence cp(f) = c(f). (The third property also\nfollows easily from flow decomposition, as the decomposition of a circulation only contains cycles\nand thus the cost and the reduced cost of a circulation are the same because of (ii).)\n\n3.2\nKlein's Cycle-Cancelling Algorithm\nWe present a pseudo-algorithm for removing negative-cost cycles. While there exists a negative-cost\ncycle C in Gf , push a flow ε along the cycle C, where ε is the minimum residual flow:\nε = min uf (v, w).\n(v,w)∈C\nOf course, this doesn't lead to a straight-forward implementation, since we haven't specified which\nnegative-cost cycle to select or how to find them. We should also consider whether the algorithm is\nefficient and whether it will terminate. We'll answer these questions in the next lecture. However,\nwe will show now that if it terminates, then the circulation output is of minimum cost.\n3.3\nOptimality Conditions\nWe now present a theorem that specifies the conditions required for f to be a minimum cost circu\nlation.\nTheorem 6 (Optimality Condition) Let f be a circulation. The following are equivalent:\n(i) f is of minimum cost.\n(ii) There exists no negative-cost cycle in the residual graph Gf .\n3-5\n\nX\nX\n(iii) There exists a potential function p such that for all (v, w) ∈ Ef , cp(v, w) ≥ 0.\nProof: To show that (i) implies (ii), we'll prove the contrapositive. Suppose there exists a negative\ncost cycle C in the residual graph Gf where f is the optimal circulation. Denote by C0 the reverse\ncycle (i.e. following the arcs in the reverse order). We define a new circulation f 0 for any edge e as\nfollows. If e ∈ C, f 0(e) = f(e)+ ε. And if e ∈ C0, then f 0(e) = f(e) - ε. Otherwise, let f 0(e) = f(e).\nThen we compute the cost of this new flow as\nc(f 0)\n= c(f) + (ε)(c(C)) + (-ε)(-c(C))\n= c(f) + 2εc(C)\n<\nc(f),\nwhere the last step follows since C is a negative cost cycle. Thus we've shown that f is indeed not\noptimal. Hence (i) implies (ii).\nNow we show that (ii) implies (iii). Add zero-cost (or of arbitrary cost) arcs from a new vertex\ns to every vertex in Gf (this is to make sure that s can reach every vertex in V ). Define a potential\np such that p(v) is the length of the shortest simple path from s to v. Then, since there are no\nnegative cost cycle, we have the optimality conditions for the shortest-path lengths:\np(w) ≤ p(v) + c(v, w) ∀ (v, w) ∈ Ef ,\nas one way to go from s to w is to go to v by a shortest path and then go directly to w.\nHere, we have implicitly used the fact that Gf has no negative cost cycles. For if the shortest\npath from s to v already goes through w then adding (v, w), we create a cycle C (and the resulting\npath is not simple). However, this cycle can't be of negative cost by assumption. Thus, by removing\nit, we obtain a simple path to w of cost less or equal to p(v) + c(v, w). Rearranging the inequality\ngives the desired result\ncp(v, w) ≥ 0 ∀ (v, w) ∈ Ef .\nNow we prove that (iii) implies (i) by showing the contrapositive. Suppose we have an optimal\ncirculation f ∗ and a suboptimal one f: c(f ∗) < c(f). Consider the cost of the circulation f ∗ - f:\nc(f ∗ - f)\n= cp(f ∗ - f)\n=\ncp(v, w)[f ∗(v, w) - f(v, w)]\n(v,w)∈E\n=\ncp(v, w)[f ∗(v, w) - f(v, w)]\n(v,w):f ∗-f>0\n≥ 0\nby (iii). Note that in the second to last step, we utilized the skew-symmetry of the cost of reverse\narcs (with flows of opposite parity). But since f ∗ is supposed to be strictly better than f, we have\na contradiction.\n\nReferences\n[EK72] Jack Edmonds, and Richard M. Karp, Theoretical improvements in algorithmic efficiency\nfor network flow problems, Journal of the ACM 19 (2): 248-264, 1972.\n[Klein67] Klein, M. A primal method for minimum cost flows with application to the assignment and\ntransportation problem. Management Science 14: 205-220, 1967.\n3-6"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 5",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/0d3338683064d96b5174095829043b93_lec5.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 17, 2008\nLecture 5\nLecturer: Michel X. Goemans\nToday, we continue the discussion of the minimum cost circulation problem. We first review the\nGoldberg-Tarjan algorithm, and improve it by allowing more flexibility in the selection of cycles.\nThis gives the Cancel-and-Tighten algorithm. We also introduce splay trees, a data structure which\nwe will use to create another data structure, dynamic trees, that will further improve the running\ntime of the algorithm.\nReview of the Goldberg-Tarjan Algorithm\nRecall the algorithm of Golberg and Tarjan for solving the minimum cost circulation problem:\n1. Initialize the flow with f = 0.\n2. Repeatedly push flow along the minimum mean cost cycle Γ in the residual graph Gf , until\nno negative cycles exist.\nWe used the notation\nc(Γ)\nμ(f) =\nmin\ncycle Γ⊆Ef\nΓ\n| |\nto denote the minimum mean cost of a cycle in the residual graph Gf . In each iteration of the\nalgorithm, we push as much flow as possible along the minimum mean cost cycle, until μ(f) ≥ 0.\nWe used (f) to denote the minimum such that f is -optimal. In other words\n(f) = min{ : ∃ potential p : V\nR such that cp(v, w) ≥- for all edges (v, w) ∈ Ef }.\n→\nWe proved that for all circulations f,\n(f) = -μ(f).\nA consequence of this equality is that there exists a potential p such that any minimum mean cost\ncycle Γ satisfies cp(v, w) = -(f) = μ(f) for all (v, w) ∈ Γ, since the cost of each edge is bounded\nbelow by mean cost of the cycle.\n1.1\nAnalysis of Goldberg-Tarjan\nLet us recall the analysis of the above algorithm. This will help us to improve the algorithm in order\nto achieve a better running time. Please refer to the previous lecture for the details of the analysis.\nWe used (f) as an indication of how close we are to the optimal solution. We showed that (f)\nis a non-increasing quantity, that is, if f 0 is obtained by f after a single iteration, then (f 0) ≤ (f).\nIt remains to show that (f) decreases \"significantly\" after several iterations.\nLemma 1 Let f be any circulation, and f 0 be the circulation obtained after m iterations of the\nGoldberg-Tarjan algorithm. Then\n\n(f 0) ≤ 1 -\n(f).\nn\nWe showed that if the costs are all integer valued, then we are done as soon as we reach (f) < 1 .\nn\nUsing these two facts, we showed that the number of iterations of the above algorithm is at most\nO(mn log(nC)). An alternative analysis using -fixed edges provides a strongly polynomial bound\nof O(m2n log n) iterations. Finally, the running time per a single iteration is O(mn) using a variant\nof Bellman-Ford (see problem set).\n5-1\n\n1.2\nTowards a faster algorithm\nIn the above algorithm, a significant amount of time is used to compute the minimum cost cycle.\nThis is unnecessary, as our goal is simply to cancel enough edges in order to achieve a \"significant\"\nimprovement in once every several iterations.\nWe can improve the algorithm by using a more flexible selection of cycles to cancel. The idea of\nthe Cancel-and-Tighten algorithm is to push flows along cycles consisting entirely of negative cost\nedges. For a given potential p, we push as much flow as possible along cycles of this form, until no\nmore such cycles exist, at which point we update p and repeat.\nCancel-and-Tighten\n2.1\nDescription of the Algorithm\nDefinition 1 An edge is admissible with respect to a potential p if cp(v, w) < 0. A cycle Γ is\nadmissible if all the edges of Γ are admissible.\nCancel and Tighten Algorithm (Goldberg and Tarjan):\n1. Initialization: f ← 0, p ← 0, ← max(v,w)∈E c(v, w), so that f is -optimal respect to p.\n2. While f is not optimum, i.e., Gf contains a negative cost cycle, do:\n(a) Cancel: While Gf contains a cycle Γ which is admissible with respect to p, push as much\nflow as possible along Γ.\n(b) Tighten: Update p to p0 and to 0, where p0 and 0 are chosen such that cp0 (v, w) ≥-0\nfor all edges (v, w) ∈ Ef and 0 ≤ 1 - n\n1 .\nRemark 1 We do not update the potential p every time we push a flow. The potential p gets updated\nin the tighten step after possibly several flows are pushed through in the Cancel step.\nRemark 2 In the tighten step, we do not need to find p0 and 0 such that 0 is as small as possible;\nit is only necessary to decrease by a factor of at least 1 - 1 . However, in practice, one tries to\nn\ndecrease by a smaller factor in order to obtain a better running time.\nWhy is it always possible to obtain improvement factor of 1 - 1 in each iteration? This is\nn\nguaranteed by the following result, whose proof is similar to the proof used in the analysis during\nthe previous lecture.\nLemma 2 Let f be a circulation and f 0 be the circulation obtained by performing the Cancel step.\nThen we cancel at most m cycles, and\n(f 0) ≤ 1 - n\n(f).\nProof:\nSince we only cancel admissible edges, after any cycle is canceled in the Cancel step:\n- All new edges in the residual graph are non-admissible, since the edge costs are skew-symmetric;\n- At least one admissible edge is removed from the residual graph, since we push the maximum\npossible amount of flow through the cycle.\n5-2\n\nSince we begin with at most m admissible edges, we cannot cancel more than m cycles, as each cycle\ncanceling reduces the number of admissible edges by at least one.\nAfter the cancel step, every cycle Γ contains at least one non-admissible edge, say (u1, v1) ∈ Γ\nwith cp(u1, v1) ≥ 0. Then the mean cost of Γ is\nc(Γ)\nX\ncp(u, v) ≥-(|Γ| - 1) (f) = - 1 - 1\n1 - 1\n(f).\n|Γ| ≥|Γ| (u1,v1 )=(u,v)∈Γ\n|Γ|\n|Γ| (f) ≥-\nn\nTherefore, (f 0) = -μ(f 0) ≤ 1 - n\n1 (f).\n2.2\nImplementation and Analysis of Running Time\n2.2.1\nTighten Step\nWe first discuss the Tighten step of the Cancel-and-Tighten algorithm. In this step, we wish to find\na new potential function p0 and a constant 0 such that cp0 (v, w) ≥-0 for all edges (v, w) ∈ Ef\nand 0 ≤ 1 - n\n1 . We can find the smallest possible 0 in O(mn) time by using a variant of the\nBellman-Ford algorithm. However, since we do not actually need to find the best possible 0, it is\npossible to vastly reduce the running time of the Tighten step to O(n), as follows.\nWhen the Cancel step terminates, there are no cycles in the admissible graph Ga = (V, A), the\nsubgraph of the residual graph with only the admissible edges. This implies that there exists a\ntopological sort of the admissible graph. Recall that a topological sort of a directed acyclic graph\nis a linear ordering l : V →{1, . . . , n} of its vertices such that l(v) < l(w) if (v, w) is an edge of the\ngraph; it can be achieved in O(m) time using a standard topological sort algorithm (see, e.g., CLRS\npage 550). This linear ordering enables us to define a new potential function p0 by the equation\np0(v) = p(v) - l(v)/n. We claim that this potential function satisfies our desired properties.\nClaim 3 The new potential function p0(v) = p(v)-l(v)/n satisfies the property that f is 0-optimal\nwith respect to p0 for some constant 0 ≤ (1 - 1/n).\nProof:\nLet (v, w) ∈ Ef , then\ncp0 (v, w) = c(v, w) + p0(v) - p0(w)\n= c(v, w) + p(v) - l(v)/n - p(w) + l(w)/n\n= cp(v, w) + (l(w) - l(v))/n.\nWe consider two cases, depending on whether or not l(v) < l(w).\nCase 1: l(v) < l(w). Then\ncp0 (v, w) = cp(v, w) + (l(w) - l(v))/n\n≥- + /n\n= -(1 - 1/n).\nCase 2: l(v) > l(w), so that (v, w) is not an admissible edge. Then\ncp0 (v, w) = cp(v, w) + (l(w) - l(v))/n\n≥ 0 - (n - 1)/n\n= -(1 - 1/n).\nIn either case, we see that f is 0-optimal with respect to p0, where 0 ≤ (1 - 1/n).\n5-3\n\n2.2.2\nCancel Step\nWe now shift our attention to the implementation and analysis of the Cancel step. Na ıvely, it takes\nO(m) time to find a cycle in the admissible graph Ga = (V, A) (e.g., using Depth-First Search) and\npush flow along it. Using a more careful implementation of the Cancel step, we shall show that each\ncycle in the admissible graph can be found in an \"amortized\" time of O(n).\nWe use a Depth-First Search (DFS) approach, pushing as much flow as possible along an ad\nmissible cycle and removing saturated edges, as well as removing edges from the admissible graph\nwhenever we determine that they are not part of any cycle. Our algorithm is as follows:\nCancel(Ga = (V, A)): Choose an arbitrary vertex u ∈ V , and begin a DFS rooted at u.\n1. If we reach a vertex v that has no outgoing edges, then we backtrack, deleting from A the\nedges that we backtrack along, until we find an ancestor r of v for which there is another child\nto explore. (Notice that every edge we backtrack along cannot be part of any cycle.) Continue\nthe DFS by exploring paths outgoing from r.\n2. If we find a cycle Γ, then we push the maximum possible flow through it. This causes at\nleast one edge along Γ to be saturated. We remove the saturated edges from A, and start\nthe depth-first-search from scratch using G0\na = (V, A0), where A0 denotes A with the saturated\nedges removed.\nEvery edge that is not part of any cycle is visited at most twice (since it is removed from the\nadmissible graph the second time), so the time taken to remove edges that are not part of any cycle\nis O(m). Since there are n vertices in the graph, it takes O(n) time to find a cycle (excluding the\ntime taken to traverse edges that are not part of any cycle), determine the maximum flow that\nwe can push through it, and update the flow in each of its edges. Since at least one edge of A is\nsaturated and removed every time we find a cycle, it follows that we find at most m cycles. Hence,\nthe total running time of the Cancel step is O(m + mn) = O(mn).\n2.2.3\nOverall Running Time\nFrom the above analysis, we see that the Cancel step requires O(mn) time per iteration, whereas\nthe Tighten step only requires O(m) time per iteration. In the previous lecture, we determined\nthat the Cancel-and-Tighten algorithm requires O(min(n log(nC), mn log n)) iterations. Hence the\noverall running time is O(min(mn2 log(nC), m2n2 log n)).\nOver the course of the next few lectures, we will develop data structures that will enable us to\nreduce the running time of a single Cancel step from O(mn) to O(m log n). Using dynamic trees, we\ncan reduce the running time of the Cancel step to an amortized time of O(log n) per cycle canceled.\nThis will reduce the overall running time to O(min(mn log(nC) log n, m2n log2 n)).\nBinary Search Trees\nIn this section, we review some of the basic properties of binary search trees and the operations\nthey support, before introducing splay trees. A Binary Search Tree (BST) is a data structure that\nmaintains a dictionary. It stores a collection of objects with ordered keys. For an object (or node)\nx, we use key[x] to denote the key of x.\nProperty of a BST. The following invariant must always be satisfied in a BST:\n- If y lies in the left subtree of x, then key[y] ≤ key[x]\n- If z lies in the right subtree of x, then key[z] ≥ key[x]\n5-4\n\nOperations on a BST. Here are some operations typically supported by a BST:\n- Find(k): Determines whether the BST contains an object x with key[x] = k; if so, returns the\nobject, and if not, returns false.\n- Insert(x): Inserts a new node x into the tree.\n- Delete(x): Deletes x from the tree.\n- Min: Finds the node with the minimum key from the tree.\n- Max: Finds the node with the minimum key from the tree.\n- Successor(x): Find the node with the smallest key greater than key[x].\n- Predecessor(x): Find the node with the greatest key less than key[x].\n- Split(x): Returns two BSTs: one containing all the nodes y where key[y] < key[x], and the\nother containing all the nodes z where key[z] ≥ key[x].\n- Join(T1, x, T2): Given two BSTs T1 and T2, where all the keys in T1 are at most key[x], and\nall the keys in T2 are at least key[x], returns a BST containing T1, x and T2.\nFor example, the procedure Find(k) can be implemented by traversing through the tree, and\nbranching to the left (resp. right) if the current node has key greater than (resp. less than) k. The\nrunning time for many of these operations is linear in the height of the tree, which can be as high\nas O(n) in the worst case, where n is the number of nodes in the tree.\nA balanced BST is a BST whose height is maintained at O(log n), so that the above operations\ncan be run in O(log n) time. Examples of BSTs include Red-Black trees, AVL trees, and B-trees.\nIn the next lecture, we will discuss a data structure called splay trees, which is a self-balancing\nBST with amortized cost of O(log n) per operation. The idea is that every time a node is accessed,\nit gets pushed up to the root of the tree.\nThe basic operations of a splay tree are rotations. They are illustrated the following diagram.\nA\nB\nC\nx\ny\nA\nB\nC\nx\ny\nzig (right rotation)\nzag (left rotation)\n5-5"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 6 - Splay Trees",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/921232cb9a69015c50002ff5ea6a9824_lec6.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 24, 2008\nLecture 6 - Splay Trees\nLecturer: Michel X. Goemans\nIntroduction\nIn this lecture, we investigate splay trees, a type of binary search tree (BST) first formulated by\nSleator and Tarjan in 1985. Splay trees are self-adjusting BSTs that have the additional helpful\nproperty that more commonly accessed nodes are more quickly retrieved. They have good behavior\nwhen compared to many other types of self-balancing BSTs, even when the operations are unknown\nand non-uniform. While in the worst case, operations can take O(n) time, splay trees maintain\nO(log n) amortized cost for basic BST operations, and are within a constant factor to the cost of\nany static BST.\nWe first give an overview of the operations used in splay trees, then give an amortized analysis of\nits behavior. We conclude by noting its behavior relative to other Binary Search Trees.\nSplay Tree Structure\nA splay tree is a dynamic binary search tree, meaning that it performs additional operations to\noptimize behavior. Because they are BSTs, given a node x in a splay tree and a node y in the left\nsubtree of x, we have key(y) < key(x). Similarly, for a node z in the right subtree of x, we have\nkey(x) < key(z). This is the binary search tree property. A well-balanced splay tree will have height\nΘ(log(n), where n is the number of nodes.\nSplay trees achieve their efficiency through use of the following operations:\n2.1\nRotation\nThe basic operation used in splay trees (or any other dynamic BST) is the rotation. A rotation\ninvolves rearranging the nodes of a subtree rooted at y so that one of the children x of y becomes\nthe new root of the subtree, while maintaining the binary search tree property. This is illustrated\nin Figure 1.\nWhen the left child becomes the new root, the rotation is a right rotation. When the right child\nbecomes the new root, the rotation is a left rotation. We call a right rotation a zig and a left rotation\na zag.\nThe key idea of the splay tree is to bring node x to the root of the tree when accessing x via rotations.\nThis brings the most recently accessed nodes closer to the top of the tree.\nHowever, there are many ways of bringing a node to the root via rotations, and we must therefore\nspecify in which order we perform them. Consider a linear tree (effectively a linked list) of the values\n1, . . . , n, rooted at n. Suppose we access the value 1. If we use the naive (and most natural) method\nof repeatedly performing a zig to bring 1 at the top, we proceed as illustrated in Figure 2. The\nresulting tree has the same height as the original tree, and is clearly not better balanced. We must\ntry a more clever approach than successive, single rotations.\n6 - Splay Trees-1\n\nFigure 1: Rotation via zigs and zags.\nFigure 2: When we access node 1 and try to bring it up via pure rotations, the result is a tree that\nis just as unbalanced as before.\n2.2\nSplay-Step\nWe now define an operation called splay-step. In one splay-step on a node x, x is brought up 2\nlevels with rotations (or just 1 level if x's parent is the root). When some node x is accessed in the\nsplay tree, we bring x up with a series of splay-steps until it is the root.\nWe separate the actions performed for the splay-step into the following categories. Call the node\nthat we are trying to access x, its parent y, and y's parent z.\n- Case 0: x is the root. Do nothing in this case.\n- Case 1: y is the root. If x is the left child of the root, perform a zig on x and y. If not,\nperform a zag.\n- Case 2: x and y are both left children (or both right children). Let us look at the case when\nboth x and y are left children. We first do a zig on the y-z connection. Then, we do a zig on\nthe x-y connection. If x and y are right children, we do the same thing, but with zags instead.\n(See Figure 3.)\n- Case 3: x is a left child and y is a right child, or vice versa. Consider the case where x is a\nright child, and y is a left child. We first do a zag on the x-y edge, and then a zig on the x-z\nedge. In the case where x is a left child and y a right child, we do the same thing, but with a\nzig on the first move, followed by a zag. (See Figure 4.)\n6 - Splay Trees-2\n\nFigure 3: Case 2 of the splay-step is when x and y are the same type of children. In this figure, we\nfirst do a zig on y - z, and then a zig on x - z.\nFigure 4: In Case 3, x and y are not the same type of children. In this case, we do a zag on the\nx - y edge, and then a zig on the x - z edge.\nNote that in the case of the earlier example with the chain of nodes, using splay-step instead of\ndirect rotations results in a much more balanced tree, see Figure 5.\n2.3\nSplay\nWith the splay-step operation, we can bring the node x to the root of the splay tree with the\nprocedure:\nsplay(x):\nWHILE x=root:\nDO splay-step(x)\nThe described procedure performs the splay operation in a bottom-up order. It is possible to perform\nthe splay operation in a top down fashion, which would result in the same running time.\n6 - Splay Trees-3\n\nX\nFigure 5: When splaying node 1, the resulting tree has half its original height.\nRunning-Time Analysis\n3.1\nPotential Function\nWe define a class of potential functions for the amortized analysis of operations on a splay tree. The\npotential function depends on weights that we can choose. For each node x in the tree, make the\nfollowing definitions:\n- T (x) is the subtree rooted at x (and it includes teh node x itself),\n- weight function: w(x) > 0 is the weight of node x (we can choose what this is; we'll often take\nw(x) = 1 for all nodes x)\n- weight-sum function: s(x) =\nw(y),\ny∈T (x)\n- rank function: r(x) = log2 s(x).\n6 - Splay Trees-4\n\nX\n\nThen we define the potential function as:\nφ =\nr(x).\nx∈T (root)\n3.2\nAmortized Cost of Splay(x)\nUsing the potential function described above, we can show that the amortized cost of the splay\noperation is O(log n). For the purposes of cost analysis, we assume a rotation takes 1 unit of time.\nLemma 1 For a splay-step operation on x that transforms the rank function r into r0, the amortized\ncost is ai ≤ 3(r0(x) - r(x)) + 1 if the parent of x is the root, and ai ≤ 3(r0(x) - r(x)) otherwise.\nProof of Lemma 1: Let the potential before the splay-step be φ and the potential after the splay-\nstep be φ0. Let the worst case cost of the operation be ci. The amortized cost ai is ai = ci + φ0 - φ.\nWe consider the three cases of splay-step operations.\nCase 1: In this case, the parent of x is the root of the tree. Call it y. After the splay-step, x\nbecomes the root and y becomes a child of x. The operation involves exactly one rotation, so ci = 1.\nThe splay step only affects the rank for x and y. Since y was the root of the tree and x is now the\nroot of the tree, r0(x) = r(y). Additionally, since y is now a child of x, (the new) T (x) contains (the\nnew) T (y), so r0(y) ≤ r0(x). Thus the amortized cost is:\nai\n= ci + φ0 - φ\n= 1 + r0(x) + r0(y) - r(x) - r(y)\n= 1 + r0(y) - r(x)\n≤ 1 + r0(x) - r(x)\n≤ 1 + 3(r0(x) - r(x)),\nsince r0(x) ≥ r(x).\nCase 2: In this case, we perform two zigs or two zags, so ci = 2. Let the parent of x be y and the\nparent of y be z. Node x takes the place of z after the splay-step, so r0(x) = r(z). Also, we see in\nFigure 3 that r(y) ≥ r(x) (since y was the parent of x) and r0(y) ≤ r0(x) (since y is now a child of\nx). Then the amortized cost is:\nai\n= ci + φ0 - φ\n= 2 + r0(x) + r0(y) + r0(z) - r(x) - r(y) - r(z)\n= 2 + r0(y) + r0(z) - r(x) - r(y)\n≤ 2 + r0(x) + r0(z) - r(x) - r(x).\nNext, we use the fact that the log function is concave, or log a+log b ≤ log ( a+b ). If the splay-step\noperation transforms the weight-sum function s into s0, we have:\nlog2 (s(x)) + log2 (s0(z))\ns(x) + s0(z)\n≤ log2\n.\n6 - Splay Trees-5\n\nThe left side is equal to r(x)+\nr0 (z) . On the right side, note that\ns(x) + s0(z) ≤ s0(x);\nindeed the old subtree T (x) and the new subtree T 0(z) cover all nodes of T 0(x), except y (thus\ns(x) + s0(z) = s0(x) - w(y)). Thus, we have:\nr(x) + r0(z)\nlog2 (s0(x))\n≤\n= r0(x) - 1,\nor\nr0(z) ≤ 2r0(x) - r(x) - 2.\nTherefore, the amortized cost is:\nai\n≤ 2 + r0(x) + 2r0(x) - r(x) - 2 - r(x) - r(x)\n= 3(r0(x) - r(x)).\nCase 3: In this case, we perform a zig followed by a zag, or vice versa, so ci = 2. Let the parent\nof x be y and the parent of y be z. Again, r0(x) = r(z) and r(y) ≥ r(x). Then the amortized cost is:\nai\n= ci + φ0 - φ\n= 2 + r0(x) + r0(y) + r0(z) - r(x) - r(y) - r(z)\n≤ 2 + r0(y) + r0(z) - r(x) - r(x).\nNote in Figure 4 that s0(y) + s0(z) ≤ s0(x). Using the fact that the log function is concave as before,\nwe find that r0(y) + r0(z) ≤ 2r0(x) - 2. Then we conclude\nai\n≤ 2 + 2r0(x) - 2 - r(x) - r(x)\n≤ 2(r0(x) - r(x))\n≤ 3(r0(x) - r(x)).\nLemma 2 The amortized cost of the splay operation on a node x in a splay tree is O(1+log s(root) ).\ns(x)\nProof of Lemma 2: The amortized cost a(splay(x)) of the splay operation is the sum of all of the\nsplay-step operations performed on x. Suppose that we perform k splay-step operations on x. Let\nr0(x) be the rank of x before the splay operation. Let ri(x) be the rank of x after the ith splay-step\noperation. Then we have rk(x) = r0(root) and:\na(splay(x)) ≤ 3(rk(x) - rk-1(x)) + 3(rk-1(x) - rk-2(x)) + ... + 3(r1(x) - r0(x)) + 1\n= 3(rk(x) - r0(x)) + 1\n= 3(r0(root) - r0(x)) + 1.\nThe added 1 comes from the possibility of a case 1 splay-step at the end. The definition of r gives\nthe result.\n\nThe above lemma gives the amortized cost of a splay operation, for any settings of the weights. To\nbe able to get good bounds on the total cost of any sequence of operations, we set w(x) = 1 for all\nnodes x. This implies that s(root) ≤ n where n is the total number of nodes ever in the BST, and\nby Lemma 2, the amortized cost of any splay operation is a(splay(x)) = O(log n).\n6 - Splay Trees-6\n\n3.3\nAmortized Cost of BST operations\nWe now need to show how to implement the various BST operations, and analyze their (amortized)\ncost (still with the weights set to 1).\n3.3.1\nFind\nFinding an element in the splay tree follows the same behavior as in a BST. After we find our node,\nwe splay it, which is O(log n) amortized cost. The cost of going down the tree to find the node can\nbe charged to the cost of splaying it. Thus, the total amortized cost of Find is O(log n). (Note: if\nthe node is not found, we splay the last node reached.)\n3.3.2\nFind-Min\nThis operation will only go down the left children, until none are left, and this cost will be charged\nto the subsequent splay operation. After we find the min node, we splay it, which takes O(log n)\namortized cost. The total amortized cost is then O(log n).\n3.3.3\nFind-Max\nThe process for this is the same as for Find-Min, except we go down the right child. The total\namortized cost of this is O(log n) as well.\n3.3.4\nJoin\nGiven two trees T1 and T2 with key(x) < key(y) ∀x ∈ T1, y ∈ T2, we can join T1 and T2 into one tree\nwith the following steps:\n1. Find-Max(T1). This makes the max element of T1 the new root of T1.\n2. Make T2 the right child of this.\nThe amortized cost of the first step is O(log n). For the second step, the actual cost is 1, but we\nneed to take into account in the amortized cost the increase in the potential function value. Before\nstep 2, T1 and T2 had a potential function value of φ(T1) and φ(T2). After it, the resulting tree has\na potential function value ≤ φ(T1) + φ(T2) + log n, since the rank of the new root is ≤ log(n). So\nthe amortized cost of Join is O(log n).\n3.3.5\nSplit\nGiven a tree T and a pivot i, the split operation partitions T into two BSTs:\nT1 : {x | key(x) ≤ i},\nT2 : {x | key(x) > i}.\nWe split the tree T by performing Find(i). This Find will then splay on a node, call it x, which\nbrings it to the root of the tree. We can then cut the tree; everything on the right of x belongs to\n6 - Splay Trees-7\n\nX\nX\nX\nX\nX\nX\n\nT2, and everything on the left belongs to T1. Depending on its key, we add x to either T1 or T2.\nThus, we either make the right child or the left child of x a new root by simply removing its pointer\nto its parent.\nThe amortized cost of the Find operation is O(log n). The actual cost of creating the second BST\n(by cutting off one of the children) is just O(1), and the potential function does not increase (as the\nrank of the root does not increase). Thus the total amortized time of a Split is also O(log n) time.\nJoin and Split make insertion and deletion very simple.\n3.3.6\nInsert\nLet i be the value we want to insert. We can first split the tree around i. Then, we let node i be\nthe new root, and make the two subtrees the left and right subtrees of i respectively. The amortized\ncost again is O(logn).\n3.3.7\nDelete\nTo delete a node i from a tree T , we first Find(i) in the tree, which brings node i to the root.\nWe then delete node i, and are left with its left and right subtrees. Because everything in the left\nsubtree has key less than everything in the right subtree, we can then join them. It is easy to see\nthat this has amortized cost O(log n) as well.\n3.3.8\nTotal cost of m operations\nThe next theorem shows that the cost of any sequence of operations on a splay tree has worst-case\ntime similar to any balanced BST (unless the number of operations m is o(n) where n is the number\nof keys).\nTheorem 3 For any sequence of m operations on a splay tree containing at most n keys, the total\ncost is O((m + n) log n).\nProof of Theorem 3:\nLet ai be the amortized cost of the ith operation. Let ci be the real cost\nof the ith operation. Let φ0 be the potential before and φm be the potential after the m operations.\nThe total amortized cost of m operations is:\nm\nm\nai =\nci + φm - φ0.\ni=1\ni=1\nThen we have:\nm\nm\nci =\nai + φ0 - φm.\ni=1\ni=1\nSince we chose w(x) = 1 for all x, we have that, for any node x, r(x) ≤ log n. Thus φ0 -φm ≤ n log n,\nso we conclude:\nm\nm\nci =\nai + O(n log n) = O(m log n) + O(n log n) = O((m + n) log n).\ni=1\ni=1\n6 - Splay Trees-8\n\nX\nX\nX\n\n!\nP\nP\nComparison to other BSTs\n4.1\nStatic Optimality Property\nWe will show that splay trees are competitive against any binary search tree that does not involve\nany rotations. We consider BSTs containing n keys, and sequences of operations that contain only\nFind operations (thus, no Insert or Delete for example).\nTheorem 4 Define a static binary search tree to be one that uses no rotation operations. Let mi\nbe the number of times element i is accessed for i = 1, . . . , n. We assume mi ≥ 1 for all i. Then the\ntotal cost for accessing every element i mi times is at most a constant times the total cost of any\nstatic binary search tree.\nProof of Theorem 4:\nConsider any binary search tree T rooted at t. Let l(i) be the height\nof of i in T , or the number of nodes on the path from i to the root of T , so l(t) = 1. In T , the\ncost for accessing an element i is l(i), so the total cost for accessing every element i mi times is\nl(i)mi. We want to show that the total cost of operations on a splay tree, irrespective of the\ni\nX\nstarting configuration, is O(\nl(i)mi).\ni\nWe choose a different weight function that earlier. Here, we define the weights to be w(i) = 3-l(i)\nfor all i. Note that s(t) ≤ 1 + 2( 3\n2 ) + 22( 3\n3 ) + . . . = 1. Then, by Lemma 2, the amortized cost of\nfinding i is:\ns(t)\na(i) = O(1 + log2\n) = O(1 + log2\n) = O(1 + l(i)).\ns(i)\n3-l(i)\nThe total amortized cost of accessing every element i mi times on a splay tree is thus:\nO(m +\nl(i)mi) = O\nl(i)mi\n.\ni\ni\nThis is the amortized cost, we now need to argue about the actual cost. Let φ be the potential\nbefore the beginning of the sequence, and φ' be the potential after the sequence of operations. For\na node i, let r(i) be the rank of i before and r0(i) be the rank after the operations. Note that (since\nr(i) ≤ log2 1 and r0(i) ≥ log2 w(i)):\n\n!\nφ - φ0 =\nX\ni\nr(i) - r0(i) ≤\nX\ni\nlog2\n3-l(i) = O\nX\ni\nl(i) .\nThen we have:\n\n!\n\n!\n\n!\nX\nX\nX\nX\nX\n=\nci\nai + φ - φ0 = O\nl(i)mi\n+ O\nl(i)\n= O\nl(i)mi\n,\ni\ni\ni\nsince our assumption mi ≥ 1 implies that\ni l(i) ≤\ni l(i)m(i).\n\n4.2\nDynamic Optimality Conjecture\nThe Dynamic Optimality Conjecture claims that Splay Trees are efficient up to a constant factor to\nany self-adjusting Binary Search Tree (allowing an arbitrary number of (arbitrary) rotations between\naccesses). This conjecture was first put forth in the Tarjan and Sleater's original Splay Tree paper\nin 1985, and has withstood attempts to prove or disprove it since.\n6 - Splay Trees-9\n\n4.3\nScanning Theorem\nThe scanning theorem states that, for a splay tree that contains the values [1, 2, . . . , n], accessing\nall of those elements in sequential order takes O(n) time, regardless of the initial arrangement of\nthe tree. An interesting point is that, even though the Scanning Theorem has been proved, if the\nDynamic Optimality Conjecture were true, then it would follow directly from the fact that one can\ncreate dynamic BST's that perform sequential access in linear time.\n6 - Splay Trees-10"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 7 - Dynamic Trees",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/ef1228eac504ad517af51a2a3e8e1a93_lec7.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nSeptember 29, 2008\nLecture 7 - Dynamic Trees\nLecturer: Michel X. Goemans\nOverview\nIn this lecture, we discuss dynamic trees, a sophisticated data structure introduced by Sleator and\nTarjan. Dynamic trees allow to provide the fastest worst-case running times for many network flow\nalgorithms. In particular, it will allow us to efficiently perform the Cancel operation in the Cancel\nand Tighten algorithm. Dynamic trees build upon splay trees, which we introduced in the previous\nlecture.\nDynamic trees manage a set of node-disjoint (not necessarily binary) rooted trees. With each\nnode v is associated a cost. In our use of dynamic trees, the cost will be coming from the edge\n(p(v), v), where p(v) denotes the parent of v; the cost of the root in that case will be set arbitrarily\nlarge (larger than the cost of any other node), say +inf.\nFigure 1: Example of Dynamic Tree.\nDynamic trees will support the following operations:\n- make-tree(v): Creates a tree with a single node v, whose cost is +inf.\n- find-root(v): Finds and returns the root of the tree containing the node v.\n- find-cost(v): Returns the cost of node v. (This may sound like a trivial operation, but\nin fact there is real work to be done, because we will not explicitly maintain the costs of all\nnodes.)\n- find-min(v): Finds and returns the ancestor of w of v with minimum cost. Ties go to the\nnode closest to the root.\n- add-cost(v, x): Adds x to the cost of all nodes w on the path from find-root(v) to v.\n- cut(v): Breaks the rooted tree in two by removing the link to v from its parent. The node v\nis now the root of a new tree, and its cost is set to +inf.\n- link(v, w, x): Assumes that (1) w is a root, and (2) v and w are not in the same tree, i.e.\nfind-root(v)\nw. Combines two trees by adding an edge (v, w), i.e. p(w) = v.\n=\nSets the cost\nof w equal to x.\nWe will later show that all of these operations run in O(log n) amortized time.\n7 - Dynamic Trees-1\n\nv\nv\nFigure 2: cut(v) operation.\nLINK\nW\nV\nCOST(W) = X\nFigure 3: link(v, w, x) operation.\nTheorem 1 The total running time of any sequence of m dynamic tree operations is O((m +\nn) log n), where n is the number of nodes.\nWe defer the proof of this theorem until the next lecture.\nImplementation of Cancel with dynamic trees\nRecall the setting for the Cancel step in the algorithm Cancel and Tighten for the minimum cost\nflow problem. We have a circulation f and node potentials p in an instance defined on graph G.\nRecall that an edge (v, w) is admissible if cp(v, w) < 0, and the admissible graph (V, Ea), is the\nsubgraph of Ef (the residual graph corresponding to our circulation) containing only the admissible\nedges. Our aim is to repeatedly find a cycle in the admissible graph and saturate it. Each time we\ndo this, all of the saturated edges disappear from the graph. Also recall that no edges are added\nto the admissible graph during this process, because any new edge in the residual graph must have\npositive reduced cost and are therefore is not admissible.\nWe represent the problem with dynamic trees, where the nodes in the dynamic trees correspond\nto nodes in G and the edges of the dynamic trees are a subset of the admissible edges. We maintain\ntwo (disjoint) sets of admissible edges: those which are currently in the dynamic tree, and those\nwhich still need to be considered. The cost of a node v will correspond to the residual capacity\nuf (p(v), v) of the edge (p(v), v), unless v is a root node, in which case it will have cost +inf. We\nwill also mark some of the roots (denoted graphically with a (∗)) to indicate that we dealt with\nthem and concluded they can't be part of any cycle. For the edges not in the dynamic tree, we also\nmaintain the flow value. (We don't need to maintain the flow explicitly for the edges in the trees,\nsince we can recover the flow from the edge capacities in G and the residual capacity.)\nTo summarize, we begin with a set of n singleton trees. All of the edges start out in the remaining\npool. In each iteration, we try to find an admissible edge leading to the root r of one of the dynamic\ntrees. If we fail to find such an edge, this implies there are no admissible cycles which include r,\n7 - Dynamic Trees-2\n\nand so we mark it and remove it from consideration. Suppose, on the other hand, that we do find\nan edge (w, r) leading into the root. If w is in a different tree, we join the two trees by adding an\nedge connecting w and r. On the other hand, if w and r are part of the same tree, it means we have\nfound a cycle. In this case, we push flow along the cycle and remove the saturated edges from the\ndata structure.\nIn more detail, we keep repeating the following procedure as long as there still exist unmarked\nroots:\n⊲\nChoose an unmarked root r.\n⊲\nAmong admissible edges, try to find one which leads to r.\n⊲\nCASE 1: there is no such (v, r) ∈ Ea.\n⊲\nMark r, since we know it cannot possibly be part of a cycle.\n⊲\nCut all the children v of r.\n⊲\nSet\nf(r, v)\n← u(r, v) - uf (r, v)\n=\nu(r, v) - find-cost(v)\n⊲\nCASE 2: there is an admissible edge (w, r) from a different tree, i.e.\nfind-root(w) =\nr.\n⊲\nLink the two trees: link(w, r, u(w, r) - f(w, r))\n⊲\nCASE 3: there is an admissible edge (w, r) from the same tree, i.e.\nfind-root(w) =\nr.\n⊲\nWe've found a cycle, so push flow along the cycle. The amount we can push is\nδ = min(u(w, r) - f(w, r), find-cost(find-min(w)))\n⊲\nadd-cost(w, -δ)\n⊲\nIncrease f(w, r) by δ\n⊲\nIf f(w, r) = u(w, r), then (u, r) is inadmissible, so we get rid of it.\n⊲\nWhile find-cost(find-min(w)) = 0:\n⊲\nz ← find-min(w)\n⊲\nf(p(z), z) ← u(p(z), z)\n⊲\ncut(z)\nThe last while loop is to delete all the edges that became inadmissible along the path from r to w.\n2.1\nRunning time\nIn a cancel step, we end up cancelling at most O(m) cycles, where m is the number of edges. In\naddition, each edge gets saturated at most once (if it does, it becomes inadmissible); therefore the\nnumber of cut(z) and find-min(w) over all cases 3 is O(m). Thus the total number of dynamic\ntree (and also other arithmetic or control) operations is at most O(m). Hence, by Theorem 1, the\nrunning time of each Cancel operation is O((m + n) log n) = O(m log n). The overall running time\nof Cancel-and-Tighten is therefore O(m2n log2 n) (strongly polynomial running time bound) or\nO(mn log n log(nC)).\nDynamic trees implementation\nWe now turn to the implementation of dynamic trees. Here we present the definitions; we will cover\nthe running time analysis in the next lecture. The dynamic trees data structure is a collection of\nrooted trees. We decompose each rooted tree into a set of node-disjoint (directed) paths, as shown\nin Figure 4. Each node is in precisely one path (possibly containing that node only). We will refer\n7 - Dynamic Trees-3\n\nFigure 4: Decomposition of rooted tree.\nto the edges on these paths as solid edges, and we will refer to the remaining edges as dashed\nedges, or middle edges. Each path is directed from its tail (highest in the tree) to its head lowest\nin the tree).\nThere are many possible ways to partition a tree into solid paths. For instance, if we are given a\nsolid edge and a dashed edge which are both children of a single parent, we can swap the solid and\ndashed edges. This follows from the basic observation that, for any middle edge (v, w), w is the tail\nof a solid path. This operation is known as splicing as shown in Figure 5.\nSplicing\nFigure 5: Splicing in the rooted tree.\nIn a dynamic tree, each solid path is represented by a splay tree, where the nodes are sorted\nin increasing order from the head to the tail, as shown in Figure 6. In other words, the node with\nsmallest key is the head (the lowest in the tree), and the node with largest key is the tail (the highest\nin the tree)\nIn addition, we will maintain links between the different splay trees. The root of each splay\ntree is attached to the parent of the tail of the path in the rooted tree, as shown in Figure 7. For\nexample, the edge (e, f) in the original rooted tree becomes the edge (e, i) linking e to the root i of\nthe splay tree corresponding to the solid path f → i. The entire data structure -- with the splay\ntrees corresponding to the same rooted tree being connected to each other -- forms what is called\na virtual tree. Any given node of the virtual tree may have at most one left child and at most one\nright child (of a splay tree), as well as any number of children attached by dashed edges. Children\nattached by dashed edges are known as middle children, and we draw them in between the left\nand right children.\nNotice that we can reconstruct the rooted tree from the virtual tree. Each splay tree corresponds\nto a solid path from the node of lowest key to the node of highest key. In addition, for any middle\n7 - Dynamic Trees-4\n\nf\ne\nd\nc\nb\na\nc\nb\na\ne\nf\nd\nHEAD\nTAIL\nFigure 6: Representation of solid path from head to tail in BST (Splay Tree).\na\nb\nc\nd\ne\nf\ni\ng\nh\nb\na\nc\ne\ni\nf\ng\nh\nd\nFigure 7: Rooted tree on the left and corresponding virtual tree on the right.\nedge, we get an edge of the original rooted tree; for example, to (e, i) in the virtual tree, corresponds\nthe edge (e, f) in the original tree where f is the node with highest key in the splay tree in which i\nresides.\nNote that there are many different ways to represent rooted trees as virtual trees, and we can\nmodify virtual trees in various ways which don't affect the rooted trees.\nIn particular, we define the Expose(v) operation, which brings a given node v to the root of the\nvirtual tree. This operation involves three main steps:\n1. Make sure that the path from v to the root only uses roots of splay trees. This can be done\nby performing splay operations whenever we enter a new splay tree.\n2. Make sure that the path from v to the root consists entirely of solid edges. We can ensure this\nthrough repeated splicing.\n3. Do the splay operation to bring v to the top of the resulting splay tree. This is justified since\nv is now in the same splay tree as the root of the original rooted tree.\n7 - Dynamic Trees-5"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 8",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/5cccdb7dca0aa81810781ba75d0a1d46_lec8.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nOctober 1, 2008\nLecture 8\nLecturer: Michel X. Goemans\nPreviously, we introduced the dynamic tree data structure and the operations that dynamic\ntrees must support. Today, we take a more detailed look at dynamic trees and describe the efficient\nimplementation of the operations. In doing so, much of our focus will be on the Expose method,\nan extended splay operation that is essential in all these operations. We show that any sequence of\nm operations on a dynamic tree with n nodes takes O((m + n) log n) time.\nDynamic Trees\nDynamic trees (also known as link-cut trees) introduced by Sleator and Tarjan are a data structure\nintended to maintain a representation of a set of rooted trees. We will be able to perform various\noperations on these trees, to be discussed later. Figure 1 shows an example tree as a virtual tree\n(left) and a rooted tree (right).\n1.1\nRooted Trees\nWe view rooted trees as unions of node-disjoint (directed) paths. This divides the edges of the tree\ninto two sets. Solid edges are those that are on the node-disjoint paths that the tree is composed\nof, and dashed edges are those that are not on these paths. Note that each path consisting of solid\nedges is a directed path (we omit the arrows here) from top to bottom.\n1.2\nVirtual Trees\nThe union of disjoint paths described above can be used to represent virtual trees. In a virtual tree,\neach solid path is represented by a splay tree such that the following conditions hold:\nA successor node in a splay tree is an ancestor in the rooted tree.\n-\nFor each splay tree, its largest node is linked to the parent of the root in the rooted tree.\n-\nIn the virtual tree, each node has at most one left child, at most one right child, and any\n-\nnumber of middle (virtual) children.\nThere are three kinds of edges in a virtual tree, corresponding to the three types of children a\nnode can have. Left and right children of a node are connected to the node by solid edges, and\nmiddle children of a node are connected to it by dashed edges. Note that there can be many virtual\ntrees corresponding to a rooted tree, because there are two different degrees of freedom involved in\nconstructing a virtual tree -- the union of disjoint paths could be different, as could the structure\nof the splay trees corresponding to the paths.\nAn important consequence of this setup is that rotations in a splay tree do not affect the structure\nof the rooted tree.\nThe Expose Operation\nThe Expose(v) operation is an extended splay operation that brings v to the root of the virtual\ntree without changing the structure of the rooted tree. The important parts of this operation are to\n8-1\n\nFigure 1: Virtual tree (left) and corresponding rooted tree (right).\nmake sure that the path from v to the root is solid and that the splay tree representing the path to\nwhich v belongs is rooted at v. We can describe this operation in three steps. In our example, we\nrun Expose on node 15.\n2.1\nStep 1\nStep 1 consists of walking from v to the root of the virtual tree. Whenever the walk enters a splay\ntree (solid edges) at some node w, a Splay(w) operation is performed, bringing w to the root of\nthat tree. Middle children are not affected in this step. For instance, we splay nodes 11 and 5 in\nour example tree as in figure 2. Note that at the end of step 1 of an Expose(v) operation, v will be\nconnected to the root of the virtual tree only by dashed edges.\n2.2\nStep 2: Splicing\nStep 2 consists of walking from v to the root of the virtual tree exchanging along the way each\nmiddle edge with the left subtree of the parent. This is illustrated in Figure 3 and called splicing.\nA middle child of a node w and its left child can be exchanged (without changing the rooted tree)\nonly if w is the root of its splay tree. This justifies our execution of step 1 first since at the end of\nstep 1 all edges from v to the root are middle edges.\nSplicing is a valid operation on virtual trees. Indeed, referring to Figure 3, the left subtree of\nw in the splay tree corresponds to the part of the solid path that is below w in the rooted tree;\nthis is because w is the root of its splay tree. Exchanging that solid subpath with the solid path\ncorresponding to the splay tree rooted at v still leaves the rooted tree decomposed into a node-disjoint\nunion of paths.\nNote that after performing this operation on every edge to the root of the virtual tree, there will\nbe a solid path from the root of the rooted tree to the node being exposed.\n8-2\n\nFigure 2: Walking Up and Splaying. The virtual tree after splaying 15 and 11 is shown on the left.\nThe virtual tree on the right is at the end of step 1, after splaying also node 5.\nFigure 3: Splicing. w needs to be the root of its splay tree.\n8-3\n\nFigure 4: Left virtual tree is after first splicing, the right virtual tree is the one at the end of step 2.\nThe result of splicing every node on the path to the root for our example is illustrated in Figure\n4.\n2.3\nStep 3\nStep 3 consists of walking from v to the root in the virtual tree, splaying v to the root. Note that\nin the analysis, we can charge the entire cost of step 2 to the final splaying operation in step 3.\nFigure 5 shows the relevant splay tree before and after this step.\nOperations on Dynamic Trees\nWe will now describe the desired operations on a dynamic tree and how to implement them efficiently\nusing the Expose method just defined. Some of these operations require keeping track of different\ncosts in the tree, so we first consider an efficient way of doing this.\n3.1\nMaintaining Cost Information\nWhen performing operations on the dynamic tree, we need to keep track of cost(x) for each node x,\nand we need to be able to find the minimum cost along paths to the root of the rooted tree. If such\na path is the prefix of a path corresponding to a splay tree, it seems that, knowing the minimum\ncost in any subtree of any our splay trees might be helpful. So, in addition to cost(x), we would like\nto keep track of the value mincost(x), given by\nmincost(x) = min{cost(y) | y in the subtree rooted at x of x's splay tree}.\nWe'll see that, instead of maintaining cost(x) and mincost(x), that it will be easier to maintain the\nfollowing two quantities for every node x:\nΔ min(x) = cost(x) - mincost(x)\n8-4\n\nFigure 5: Splaying on Virtual Tree.\nv\nv\nc\nw\na\nb\nb\nc\nw\na\nFigure 6: Rotation.\nand\n(\ncost(x)\nif x is the root of a splay tree,\nΔ cost(x) =\ncost(x) - cost(p(x)) otherwise.\nObserve that, if x is the root of a splay tree, then cost(x) = Δ cost(x) and mincost(x) = Δ cost(x) -\nΔ min(x). This fact, combined with the Expose operation, shows that we can find cost(x) and\nmincost(x) given Δ min(x) and Δ cost(x), so it is sufficient to maintain the latter.\nWe now claim that we can update Δ min(x) and Δ cost(x) in O(1) time after a rotation or a\nsplice, which will allow us to maintain cost(x) and mincost(x) in O(1) time.\nWe first consider a rotation, see Figure 6 for the labelling of the nodes. Let Δ cost(x) and\nΔ cost0(x) correspond to before and after the rotation, respectively. Similarly define Δ min(x) and\nΔ min0(x). Observe that during a rotation, only the nodes b, w and v have their Δ cost(x) change.\nOne can check that the updates are as follows:\nΔ cost0(v)\n= Δ cost(w) + (cost(v) - cost(w))\n= Δ cost(w) + Δ cost(v),\nΔ cost0(w)\n= -Δ cost(v),\nΔ cost0(b)\n= Δ cost(b) + (cost(v) - cost(w)) = Δ cost(b) + Δ cost(v).\nBefore showing the corresponding updates for Δ min(x), observe that Δ min(x) and Δ cost(x)\n8-5\n\nsatisfy the following equation; here x is any node and l is its left child and r is its right child:\nΔ min(x) = cost(x) - mincost(x)\n= cost(x) - min(cost(x), mincost(l), mincost(r))\n= max(0, cost(x) - mincost(l), cost(x) - mincost(r))\n= max(0, Δ min(l) - Δ cost(l), Δ min(r) - Δ cost(r)).\n(1)\nFurthermore, the minimum of the subtree can be located by knowing which term attains the maxi\nmum in the last expression.\nBack to the updates for Δ min(x). The only subtrees that change are those of w and v, and so\nonly those Δ min values change. Using (1), one can see that\nΔ min0(w) = max(0, Δ min(b) - Δ cost0(b), Δ min(c) - Δ cost(c))\nΔ min0(v)\n= max(0, Δ min(a) - Δ cost(a), Δ min0(w) - Δ cost0(w)).\nNotice that Δ min0(v) depends on Δ min0(w) that was just computed.\nSimilar when we perform the splicing step given in Figure 3, Δ cost only change for v and u and\nonly Δ min(w) changes. The updates are:\nΔ cost0(v)\n=\nΔ(cost(v)) - Δ(cost(w)),\nΔ cost0(u)\n= Δ cost(u) + Δ cost(w),\nΔ min0(w) = max(0, Δ min(v) - Δ cost0(v), Δ min(z) - Δ cost(z)).\n3.2\nImplementation of Operations\nWe now describe the implementation of each of the desired operations on a dynamic tree, making\nextensive use of the Expose operation.\n- make-tree(v)\nSimply create a tree with the single node v.\n- find-root(v)\nFirst, run Expose(v). Then follow right children until a leaf w of the splay tree containing v\nis reached. Now, splay(w), and then return w.\n- find-cost(v)\nFirst, run Expose(v). Now v is the root, so return Δ cost(v) = cost(v). Note that the actual\ncomputations here were done by the updates of Δ cost(v) and Δ min(x) within the splay and\nsplice operations.\n- find-min(v)\nFirst, run Expose(v). Now, let's rewrite (1):\nΔ min(v) = max{0, -Δ cost(left(v)) + Δ min(left(v)), -Δ cost(right(v)) + Δ min(right(v))}.\nIf Δ min(v) = 0, then splay(v) and then return v, as the minimum is achieved at v. Else,\nif -Δ cost(left(v)) + Δ min(left(v)) > -Δ cost(right(v)) + Δ min(right(v)), then the minimum\nis contained in the left subtree and we walk down it recursively. Otherwise, the minimum is\ncontained in the right subtree, so we recurse down the right. Once we have found the minimum,\nwe splay it.\n8-6\n\nX\nX\n- add-cost(v, x)\nFirst, run Expose(v). Add x to Δ cost(v) and subtract x from Δ cost(left(v)). Also update\nΔ min(v) (using (1)). (The Δ min value of other nodes is unchanged.)\n- cut(v)\nFirst, run Expose(v). Add Δ cost(v) to Δ cost(right(v)). Remove the edge (v, right(v)).\n- link(v, w, x)\nFirst, run Expose(v) and Expose(w). Then, add the root w as a middle child of v. Add\nΔ cost(w) - x to Δ cost(right(v)) and to Δ cost(left(v)). Also update Δ min(w).\nAnalysis of Dynamic Trees\nWe now give an amortized analysis of cost of operations in these dynamic trees. We will see that\nany sequence of m dynamic tree operations on n nodes will take O((m + n) log n) time.\n4.1\nPotential Function\nWe will use the following potential function in our analysis, motivated by our analysis of splay trees.\nFor each node x, let w(x) = 1 be the weight assigned to x, and define\ns(x) =\nw(y),\ny∈Tx\nwhere Tx is the entire virtual tree subtree attached at x. Then, consider r(x) = log2 s(x) and take\nour final potential function to be\nX\nφ(T ) = 3\nr(x).\nx∈T\nThis differs from the potential function for splay trees in 2 ways. First Tx is defined over the entire\nvirtual tree and secondly we have this additional factor 3. We will see later why the constant factor\nof 3 was chosen here.\n4.2\nRuntime of the Expose Operation\nWe first analyze the runtime of Expose(v), since it is used in all other operations. We look at each\nstep of Expose(v) separately. Let k be the number of middle edges separating v from the root of\nthe entire virtual tree. Equivalently, k is the number of splay operations performed during Step 1.\n- Step 1: Let t(v) be the root of the splay tree containing v. Recall that the amortized cost of\nsplay(v) was 3(r(t(v)) - r(v)) + 1 when we used the potential function\nφsplay(T ) =\nr(x).\nx∈T\nWe now have the potential function φ(T ) = 3φsplay(T ), so the 3(r(t(v)) - r(v)) term here\nshould be multiplied by 3 to obtain an amortized runtime of 9(r(t(v)) - r(v)) + 1 for each call\nof splay(v) (the +1 corresponds to the cost of the last zig, if any, and so we do not need to\nmultiply it by 3).\n8-7\n\nX\nX\nWe are using the splay operation on the k nodes v, p(t(v)), . . . , (p\nt)k-1(v) in this step,\nmeaning that we get a total amortized runtime of\n*\nkX\n-1\n\n9 r(t((p * t)i(v))) - r((p * t)i(v)) + 1 ≤ 9[r(root) - r(v)] + k,\ni=0\nsince we have that r(t(p * t)i-1(v)) ≤ r((p * t)i(v)), so the sum telescopes. The amortized cost\nof step 1 is therefore O(log n) + k (since r(root) - r(v) ≤ log n).\n- Step 2: Splicing does not change the value of φ(T ), so the amortized cost for this step is the\nsame as its actual cost of k.\n- Step 3: We are using the splay operation once on node v at distance k from the root, so this\nhas an actual cost of k. Using the fact that our potential φ has an additional factor 3 in its\ndefinition compared to the splay tree version, we get from the amortized analysis of splaying\nthat:\nk + 3Δφ(T ) ≤ 3[r(root) - r(v)] + 1 = O(log n).\nMultiplying by 3, we see that we can also account for the additional cost of 2k from steps 1\nand 2, and have an amortized time of O(log n).\n- Total: We get O(log n) + k in step 1, k in step 2, and these 2k plus step 3 gives O(log n), for\na total of O(log n).\n4.3\nRuntimes of all Operations\nWe can now briefly summarize the runtimes of all other operations in terms of Expose.\n- find-cost, find-root, find-min, add-cost\nEach of these operations requires at most one use of Expose, at most one run of splay, and\nat most one search of the tree which can be charged to the last splay. Therefore, they each\nrun in O(log n) amortized time.\ncut\n-\nWe again use Expose once. We now consider the effect of the other actions on the poten\ntial function. Removing the edge (v, right(v)) decreases s(v) by s(right(v)) and leaves s(x)\nunchanged for all other x, so it decreases φ(T ), which we can safely ignore. This gives an\namortized runtime of O(log n).\nlink\n-\nWe use Expose twice. Now, when we link w to v, we see that r(v) increases by O(log n), and\nall other r(x) remain unchanged. Hence, this operation increases φ(T ) by O(log n), giving a\ntotal amortized runtime of O(log n).\nWith this analysis, we see that every operation has amortized time O(log n). A sequence of m\noperations has therefore amortized time O(m log n). Furthermore, the potential function satisfies\nφ(T ) =\nr(x) ≤\nlog n ≤ n log n,\nx∈T\nx∈T\nmeaning that any increase in potential is at most O(n log n), implying that the total cost is at most\nO((m + n) log n). We now have the following theorem.\nTheorem 1 Any m operations on a dynamic tree with n nodes run in O((m + n) log n) time.\n8-8"
        },
        {
          "category": "Lecture Notes",
          "title": "Lecture 9",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/c61e649bf05c7123da984519955da0bd_lec9.pdf",
          "content": "MIT OpenCourseWare\nhttp://ocw.mit.edu\n6.854J / 18.415J Advanced Algorithms\nFall 2008\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.\n\n18.415/6.854 Advanced Algorithms\nOctober 6, 2008\nLecture 9\nLecturer: Michel X. Goemans\nLinear Programming\nLinear programming is the class of optimization problems consisting of optimizing the value of a\nlinear objective function, subject to linear equality or inequality constraints. These constraints are\nof the form\na1x1 + · · · + anxn\n{≤, =, ≥} b,\nwhere ai, b ∈ R, and the goal is to maximize or minimize an objective function of the form\nc1x1 +\n+ cnxn.\n· · ·\nIn addition, we constrain the variables xi to be nonnegative.\nThe problem can be expressed in matrix form. Given these constraints\nAx\nb\n{≤, =, ≥}\nx\n0,\n≥\nmaximize or minimize the value of\nT\nc x,\nwhere x ∈ Rn , A ∈ Rm×n , b ∈ Rm , c ∈ Rn .\nLinear programming has many applications and can also be used as a proof technique. In\naddition, it is important from a complexity point-of-view, since it is among the hardest of the class\nof polynomial-time solvable problems.\n9.1\nAlgorithms\nResearch in linear programming algorithms has been an active area for over 60 years. In this class,\nwe will discuss three major (classes of) algorithms:\nSimplex method (Dantzig 1947).\n-\n- Fast in practice.\n- Still the most-used LP algorithm today.\n- Can be nonpolynomial (exponential) in the worst case.\nEllipsoid algorithm (Shor, Khachian 1979).\n-\n- Polynomial time; this was the first polynomial-time algorithm for linear programming.\n- Can solve LP (and other more general) problems where the feasible region P = {x : Ax =\nb, x ≥ 0} is not explicitly given, but instead, given a vector x, one can efficiently decide\nwhether x ∈ P or if not, find an inequality satisfied by P but not by x.\n- Very useful for designing polynomial time algorithms for other problems.\n- Not fast in practice.\n9-1\n\n- Interior-point algorithms (Karmarkar 1984).\n- This is a class of algorithms which maintain a feasible point in the interior of P ; many\nvariants (by many researchers) have been developed.\n- Polynomial time.\n- Fast in practice.\n- Can beat the simplex method for larger problems.\n9.2\nEquivalent forms\nA linear programming problem can be modified to fit a preferred alternate form by changing the\nobjective function and/or the linear constraints. For example, one can easily transform any linear\nprogram into teh standard form: min{cT x : Ax = b, x ≥ 0}. One can use the following simple\ntransformations.\nMaximize to minimize\nmax{cT x}\nmin{-cT x}\n-→\naT\nEquality to inequality\nai\nT x = bi\na\ni\nT\nx ≤ bi\n-→\ni x ≥ bi\nInequality to nonnegativity constraint\nai\nT x ≤ bi\n-→\ns\nai\nT\n≥\nx\n+ s = bi (s ∈ Rn)\n⎧ replace xj everywhere by x + - x-\n⎨\nj\nj\n+\nVariables unrestricted in sign\nxj unrestricted in sign -→ ⎩ xj ≥ 0\nx-\nj ≥ 0\n9.3\nDefinitions\nHere is some basic terminology for a linear program.\nDefinition 1 A vector x is feasible for an LP if it satisfies all the constraints.\nDefinition 2 An LP is feasible if there exists a feasible solution x for it.\nDefinition 3 An LP is infeasible if there is no feasible solution x for it.\nDefinition 4 An LP min{cT x : Ax = b, x ≥ 0} is unbounded if, for all λ ∈ R, ∃x ∈ Rn such that\nAx = b\nx ≥ 0\ncT x ≤ λ.\n9.4\nFarkas' lemma\nIf we have a system of equations Ax = b, from linear algebra, we know that either Ax = b is\nsolvable, or the system AT y = 0, bT y = 0 is solvable.\nIndeed, since Im(A) = ker(AT )⊥, either b\nis orthogonal to ker(AT ) (in which case it is in the image of A, i.e. Ax = b is solvable) or it is\nnot orthogonal to it in which case one can find a vector y ∈ ker(AT ) with a non-zero inner product\nwith b (i.e. AT y = 0, bT y = 0 is solvable).\nFarkas' lemma generalizes this when we have also linear inequalities:\nLemma 1 ((Farkas' lemma)) Exactly one of the following holds:\n1. ∃x ∈ Rn : Ax = b, x ≥ 0,\n9-2\n\n2. ∃y ∈ Rm : AT y ≥ 0, bT y < 0.\nClearly, both cannot simultaneously happen, since the existence of such an x and a such a y\nwould mean:\nyT Ax = yT (Ax) = y T b < 0,\nwhile\nyT Ax = (AT y)T x ≥ 0,\nas the inner product of two nonnegative vectors is nonnegative. Together this gives a contradiction.\n9.4.1\nGeneralizing Farkas' Lemma\nBefore we provide a proof of the (other part of) Farkas' lemma, we would like to briefly mention\nother possible generalizations of the solvability of system of equations.\nFirst of all, consider the case in which we would like the variables x to take integer values,\nbut don't care whether they are nonnegative or not. In this case, the natural condition indeed is\nnecessary and sufficient. Formally, suppose we take this set of constraints:\nAx = b\nx\nZn\n∈\nThen if yT Ax = yT b, and we can find some yT A ∈ Zn and some yT b that is not integral, then the\nsystem of constraints is infeasible. The converse is also true.\nTheorem 2 Exactly one of the following holds:\n1. ∃x ∈ Zn : Ax = b,\n2. ∃y ∈ Rm : AT y ∈ Zn and bT y ∈/ Z.\nOne could try to combine both nonnegativity constraints and integral restrictions but in that case,\nthe necessary condition for feasibility is not sufficient. In fact, for the following set of constraints:\nAx = b\nx\n≥\nx\nZn ,\n∈\ndetermining feasibility is an NP-hard problem, and therefore we cannot expect a good characteriza\ntion (a necessary and sufficient condition that can be checked efficiently).\n9.4.2\nProof of Farkas' lemma\nWe first examine the projection theorem, which will be used in proving Farkas' lemma (see Figure\n1).\nTheorem 3 (The projection theorem) If K is a nonempty, closed, convex set in Rm and b ∈/\nK, define\np = projK (b) = arg min\n(1)\nz∈K kz - bk2.\nThen, for all z ∈ K : (z - p)T (b - p) ≤ 0.\n9-3\n\nFigure 1: The projection theorem.\nProof of Lemma 1:\nWe have seen that both systems cannot be simultaneously solvable.\nSo, now assume that @x : Ax = b, x ≥ 0 and we would like to show the existence of y satisfying\nthe required conditions. Define\nK = {Ax : x ∈ Rn , x ≥ 0} ⊆ Rm .\nBy assumption, b ∈/ K, and we can apply the projection theorem. Define p = projK (b). Since\np ∈ K, we have that p = Ax for some vector x ≥ 0. Let y = p - b ∈ Rm . We claim that y satisfies\nthe right conditions.\nIndeed, consider any point z ∈ K. We know that ∃w ≥ 0 : z = Aw. By the projection theorem,\nwe have that (Aw - Ax)T y ≥ 0, i.e.\n(w - x)T AT y ≥ 0,\n(2)\nfor all w ≥ 0. Choosing w = x + ei (where ei is the ith unit vector), we see that AT y ≥ 0. We still\nneed to show that bT y < 0. Observe that bT y = (p - y)T y = pT y - yT y < 0 because pT y ≤ 0\nand yT y > 0. The latter follows from y = 0 and the former from (2) with\nw = 0: -xT AT y ≥ 0,\ni.e. -pT y ≥ 0.\n\n9.4.3\nCorollary to Farkas' lemma\nFarkas' lemma can also be written in other equivalent forms.\nCorollary 4 Exactly one of the following holds:\n1. ∃x ∈ Rn : Ax ≤ b,\n2. ∃y ∈ Rm : y ≥ 0, AT y = 0, bT y < 0.\nAgain, x and y cannot simultaneously exist. This corollary can be either obtained by massaging\nFarkas' lemma (to put the system of inequalities in the right form), or directly from the projection\ntheorem.\n9.5\nDuality\nDuality is one of the key concepts in linear programming. Given a solution x to an LP of value z,\nhow do we decide whether or not x is in fact an optimum solution? In other words, how can we\ncalculate a lower bound on min cT x given that Ax = b, x ≥ 0?\n9-4\n\nSuppose we have y such that AT y ≤ c. Then observe that yT b = yT Ax ≤ cT x for any feasible\nsolution x. Thus yT b provides a lower bound on the value of our linear program. This conclusion\nis true for all y satisfying AT y ≤ c, so in order to find the best lower bound, we wish to maximize\nyT b under the constraint of AT y ≤ c.\nWe can see that this is in fact itself another LP. This new LP is called the dual linear program\nof the original problem, which is called the primal LP.\n- Primal LP: min cT x, given Ax = b, x ≥ 0,\n- Dual LP: max bT y, given AT y ≤ c.\n9.5.1\nWeak Duality\nThe argument we have just given shows what is known as weak duality.\nTheorem 5 If the primal P is a minimization linear program with optimum value z, then it has a\ndual D, which is a maximization problem with optimum value w and z ≥ w.\nNotice that this is true even if either the primal or the dual is infeasible or unbounded, provided\nwe use the following convention:\ninfeasible min. problem -→ value = +inf\nunbounded min. problem -→ value = -inf\ninfeasible max. problem -→ value = -inf\nunbounded max. problem -→ value = +inf\n9.5.2\nStrong Duality\nWhat is remarkable is that one even has strong duality, namely both linear programs have the same\nvalues provided at least one of them is feasible (it can happen that both the primal and the dual\nare infeasible).\nTheorem 6 If P or D is feasible, then z = w.\nProof:\nWe assume that P is feasible (the argument if D is feasible is analogous; or one could also\nargue that the dual of the dual is the primal and therefore one can exchange the roles of primal and\ndual).\nIf P is unbounded, z = -inf, and by weak duality, w ≤ z. So it must be that w = -inf and thus\nz = w.\nOtherwise (if P is not unbounded), let x∗ be the optimum solution to P, i.e.:\nz = cT x∗\nAx∗\n= b\nx∗\n≥\nWe would like to find a dual feasible solution with the same value as (or no worse than) x∗. That\nis, we are looking for a y satisfying:\nAT y ≤ c\nbT y ≥ z\nIf no such y exists, we can use Farkas' lemma to derive: ∃x ∈ Rn , x ≥ 0, and ∃λ ∈ R, λ ≥ 0 :\nAx - λb = 0 and cT x - λz < 0.\nWe now consider two cases.\n9-5\n\n-\n\nIf λ = 0, we can scale by λ, and therefore assume that λ = 1. Then we get that\n∃x ∈ Rn :\n⎧\n⎨\n⎩\nAx = b,\nx ≥ 0\ncT x < z.\nThis result is a contradiction, because x∗ was the optimum solution, and therefore we should\nnot be able to further minimize z.\nIf λ = 0 then\n-\n∃x ∈ Rm :\n⎧\n⎨\n⎩\nx ≥ 0\nAx = 0\ncT x < 0.\nConsider now x∗ + μx for any μ > 0. We have that\nx∗ + μx ≥ 0\nA(x∗ + μx)\n= Ax∗ + μAx = b + 0 = b.\nThus, x∗ + μx is feasible for any μ ≥ 0. But, we have that\ncT (x∗ + μx) = cT x∗ + μcT x < z,\na contradiction.\n9-6"
        }
      ],
      "source_url": "https://ocw.mit.edu/courses/6-854j-advanced-algorithms-fall-2008/",
      "course_info": "18.415J | Graduate",
      "subject": "General"
    },
    {
      "course_name": "Seminar in Analysis: Applications to Number Theory",
      "course_description": "No description found.",
      "topics": [
        "Mathematics",
        "Algebra and Number Theory",
        "Mathematical Analysis",
        "Mathematics",
        "Algebra and Number Theory",
        "Mathematical Analysis"
      ],
      "syllabus_content": "Course Meeting Times\n\nLectures: 2 sessions / week, 1.5 hours / session\n\nPrerequisite\n\nThe prerequisite for this class is Analysis I (18.100A, 18.100B, or 18.100C). We will emphasize those topics which use methods taught in a typical first semester course in Analysis.\n\nTextbook\n\nYoung, Robert M.\nExcursions In Calculus: An Interplay of The Continuous and The Discrete.\nWashington, DC: Mathematical Association of America, 1992. ISBN: 0883853175.\n\nThere is also a recently published (2006) book, which I find very suitable for a topics course like ours:\n\nMiller, Steven J., and Ramin Takloo-Bighash.\nAn Invitation to Modern Number Theory.\nPrinceton, NJ: Princeton University Press, March 6, 2006. ISBN-10: 0691120609, ISBN-13: 9780691120607.\n\nThe following books may be useful for reference:\n\nDavenport, Harold.\nThe Higher Arithmetic\n:\nAn Introduction to The Theory of Numbers.\n7th ed. Cambridge, UK: Cambridge University Press, 1999. ISBN: 0521632692, 0521634466. (pbk)\n\nNiven, Ivan, Herbert S. Zuckerman, and Hugh L. Montgomery.\nAn Introduction to The Theory of Numbers\n. 5th ed. New York, NY: Wiley, c1991. ISBN: 0471625469.\n\nFormat and Expectations\n\nThe two main components of this seminar course are:\n\nPresentations\n\nThe students will take turn in giving presentations. One such presentation (in the second part of the semester) will use slides (computer or overhead projector). The recitations will be interactive, therefore attendance is required. The students are allowed two unexcused absences during the semester.\n\nPaper\n\nOne paper on a topic related to the course of approximately ten pages will be required. It must be written in Latex. See some instructions and helpful information about writing mathematics and using Latex in the\nrelated resources section\n.\n\nThe writing should be aimed at a typical MIT math major, and it should reflect your own understanding of the subject. Please also review the MIT published handbook on academic integrity and avoiding plagiarism, and the guide on general scientific writing (both found in the related resources section).\n\nAssignments and Exams\n\nIn addition, there will be 3 homework assignments to complement the lectures. There are no exams.\n\nGrading\n\nACTIVITIES\n\nPERCENTAGES\n\nHomework\n\n20%\n\nClass Work\n\n40%\n\nPaper\n\n40%",
      "files": [
        {
          "category": "Resource",
          "title": "p1.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/bdb201c2878420fb950eee935c0167f0_p1.pdf",
          "content": "18.104. Problem Set 1\nDue date: Session #3. Late homeworks\nwill be accepted only with a medical note or for some other MIT ap\nproved reason. You may work with others, but the final write-up should\nbe entirely your own and based on your own understanding.\nSquare roots:\n1. Text, problem 7, page 10. (Extra question: can you find a closed\nformula which gives the entries on the nth rung of the ladder?)\n2. Text, problem 10, page 19.\n3. Text, problem 8, page 96.\n4. Can you construct a generalization of the ladder in problem 1,\nwhich produces in the limit pm for m 3 (instead of\np\n2)?"
        },
        {
          "category": "Resource",
          "title": "p2.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/32cdcfe402307960b0a0322e787fce84_p2.pdf",
          "content": "18.104. Problem Set 2\nDue date: Session #7. Late homeworks will\nbe accepted only with a medical note or for some other MIT approved\nreason. You may work with others, but the final write-up should be\nentirely your own and based on your own understanding.\nZeta function:\nWrite an exposition in LaTex on one proof (of your choice) of the\nfact that (2) = 2/6. It should reflect your own understanding of the\nproof, should be less than two pages, and the exposition should be\nself-contained. For a selection of possible references, see the textbook,\npage 348, footnote 23."
        },
        {
          "category": "Resource",
          "title": "p3.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/90d585b558e48bdcf3d615acb6a89ab9_p3.pdf",
          "content": "18.104. Problem Set 3\nLate homeworks will\nbe accepted only with a medical note or for some other MIT approved\nreason. You may work with others, but the final write-up should be\nentirely your own and based on your own understanding.\n1. ex. 24 p. 119 (D. Zagier's \"one line proof\" of Fermat's great\ntheorem).\n2. The exercise from lecture 10/12 about the RSA algorithm.\n3. ex. 23, 24/p. 243.\nDue date: Session #10."
        },
        {
          "category": "Resource",
          "title": "berke.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/ba06bc66574b1ea6affcaec23ef5c33b_berke.pdf",
          "content": "An Introduction to The Twin Prime Conjecture\nAllison Berke\nDecember 12, 2006\nAbstract\nTwin primes are primes of the form (p, p + 2). There are many proofs\nfor the infinitude of prime numbers, but it is very difficult to prove whether\nthere are an infinite number of pairs of twin primes. Most mathemati\ncians agree that the evidence points toward this conclusion, but numerous\nattempts at a proof have been falsified by subsequent review. The prob\nlem itself, one of the most famous open problems in mathematics, has\nyielded a number of related results, including Brun's conjecture, Mertens'\ntheorems, and the Hardy-Littlewood Conjecture. Along with these con\njectures, there are a number of results which are easier to arrive at, but\nnevertheless help mathematicians think about the infinitude of primes,\nand the special properties of twin primes. This paper will introduce the\naforementioned conjectures associated with the twin prime conjecture, and\nwork through some exercises that illuminate the difficulties and intricacies\nof the twin prime conjecture.\nIntroduction: The Original Conjecture and\nFailed Proofs\nThe term twin prime was coined by Paul Stackel in the late nineteenth cen\ntury. Since that time, mathematicians have been interested in the properties of\nrelated primes, both in relation to number theory as a whole, and as specific,\nwell-defined problems. One of the first results of looking at twin primes was\nthe discovery that, aside from (3, 5), all twin primes are of the form 6n ± 1.\nThis comes from noticing that any prime greater than 3 must be of the form\n6n ± 1. To show this, note that any integer can be written as 6x + y, where\nx is any integer, and y is 0, 1, 2, 3, 4 or 5. Now consider each y value individ\nually. When y = 0, 6x + y = 6x and is divisible by 6. When y = 1 there\nare no immediately recognizable factors, so this is a candidate for primacy.\nWhen y = 2, 6x + 2 = 2 (3x + 1), and so is not prime. For the case when\n·\ny = 3 : 6x + 3 = 3 (2x + 1) and is not prime. When y = 4 : 6x + 4 = 2 (3x + 2)\n·\n·\nand is not prime. When y = 5, 6x + 5 has no immediately recognizable factors,\nand is the second candidate for primacy. Then all primes can be represented as\neither 6n + 1 or 6n - 1, and twin primes, since they are separated by two, will\nhave to be 6n - 1 and 6n + 1.\n\nQ\nTwin Prime Conjecture\nFurther research into the conjecture has been concerned with finding expres\nsions for a form of the prime counting function π(x) that depend on the twin\nprime constant. The prime counting function is defined as\nπ(x) = {N(p)|p 5 x}\nwhere N(p) denotes the number of primes, p. One motivation for defining the\nprime counting function is that it can be used to determine a formula for the\nsize of the intervals between primes, as well as giving us an indication of the\nrate of decay by which primes thin out in higher numbers. It has been shown\nalgebraically that the prime counting function increases asymptotically with\nthe logarithmic integral [12]. In the following expression, π2(x) refers to the\nnumber of primes of the form p and p + 2 greater than x, and\nis the twin\nQ\nprime constant, which is defined by the expression\n(1 - 9p-\n1)2 ) over primes\np = 2. The term O(x), meaning \"on the order of x,\" is defined as follows: if\nf(x) and g(x) are two functions defined on the same set, then f(x) is O(g(x))\nas x goes to infinity if and only if there exists some x0 and some M such that\n|f(x)| 5 M|g(x)| for x greater than x0. This expression for the twin prime\ncounting function is\nπ2(x) 5 cΠ2\nx\n[1 + O(ln(ln(x)))]\n(1)\n(ln(x))2\nln(x)\nwhich is the best that has been proven thus far. The constant c in (1) has been\nreduced to 6.8325, down from previous values as high as 9 [12]. The formation\nof this inequality involves two of Merten's theorems which will be discussed in\nthe following section. Hardy and Littlewood [3] have conjectured that c = 2,\nand using this assumption have formulated what is now called the Strong Twin\nPrime Conjecture. In the following expression, a ∼ b means that a approaches 1\nb\nat the limits of the expressions a and b. In this case, the limit is as x approaches\ninfinity.\nZ x\ndx\nπ2(x) ∼ 2Π2\n(ln(x))2 .\n(2)\nA necessary condition for the strong conjecture (2) is that the prime gaps con\nstant, Δ ≡ lim supn→inf\npn+1-pn be equal to zero. The most recent attempted\npn\nproof of the twin prime conjecture was that of Arenstorf, in 2004 [1], but an\nerror was found shortly after its publication, and it was withdrawn, leaving the\nconjecture open to this day.\nMertens' Theorems\nA number of important results about the spacing of prime numbers were derived\nby Franz Mertens, a German mathematician of the late nineteenth and early\ntwentieth century. The following proofs of Mertens' conjectures lead up to the\nresult that the sum of the reciprocals of primes diverges, which will contrast\n\nZ\nZ\nZ\nX\nTwin Prime Conjecture\nwith Brun's conjecture, that the sum of the reciprocals of twin primes con\nverges. First, we should briefly show that the primes are infinite, for otherwise\nthe implications of Mertens' theorems are not obvious. Euclid's proof of this\npostulate, his second theorem, is as follows.\nLet 2, 3, 5, ..., p be an enumeration of all prime numbers up to p, and let\nq = (2 3 5 · ... p) + 1. Then q is not divisible by any of the primes up\n·\n·\n·\nto and including p. Therefore, it is either prime or divisible by a prime between\np and q. In the first case, q is a prime greater than p. In the second case, the\ndivisor of q between p and q is a prime greater than p. Then for any prime p,\nthis construction gives us a prime greater than p. Thus, the number of primes\nmust be infinite [4]. Now we can resume with Mertens' theorems.\nMertens Theorem 1:\nFor any real number x ≥ 1,\nX\nx\n0 ≤\nln( n ) < x.\n(3)\nn≤x\nProof:\nThe function f(t) = ln( x\nt ) is decreasing on the interval [1, x], so\nx\nX\nx\nZ\nx\nln( ) < ln(x) +\nln( )dt\n1≤n≤x\nn\nt\nWe can rewrite the right-hand side of the inequality as the following:\nx\nx\nx\nln(x) +\nln( )dt = x ln(x) -\nln(t)dt.\nt\nSimilarly, we can rewrite this:\nx\nx ln(x) -\nln(t)dt = x ln(x) - (x ln(x) - x + 1) < x.\nFor Mertens' second theorem, we introduce the Von Mangoldt's function, Λ(n),\nwhere\nm\nΛ(n) = ln(p) if n = p\nis a prime power, and zero otherwise.\nThen the psi function of the prime number theorem is defined as follows\nΨ(x) =\nΛ(m).\n1≤m≤x\n\nTwin Prime Conjecture\nMertens Theorem 2:\nFor any real number x ≥ 1,\nX Λ(n) = ln(x) + O(1).\n(4)\nn\nn≤x\nProof:\nLet N = [x]. Then\nN\nX\nx\nX\n0 ≤\nln( ) = N ln(x) -\nln(n) = x ln(x) - ln(N!) + O(ln(x)) < x\nn\nn≤x\nn=1\nln(N!) = x ln(x) + O(x).\nThe proof of this equation, which is integral to the theorem, is shown on the\nnext page. Let vp(n) denote the highest power of p, a prime, that divides n.\nThen\nX\nln(N!) =\nvp(N) ln(p)\np≤N\nWe can rewrite this as a single summation, by combining the limits on p and k:\n[ ln(N )\nln(p) ]\nX X N\nln(N!) =\n[ pk ] ln(p).\np≤N\nk=1\nFor ease of notation, we replace N with x, and substitute Von Mangoldt's\nfunction:\nX x\nln(x!) =\n[ ]Λ(n).\nn\nn≤x\nNow we can remove the floor function brackets by introducing an error term\non the order of one, since the maximum error obtained by removing the floor\nfunction is less than one:\nX x\nln(x!) =\n(\n+ O(1))Λ(n).\nn\nn≤x\nWe can distribute this term, forming two sums, one in the error term:\nX Λ(n)\nX\nln(x!) = x\n+ O(\nΛ(n)).\nn\nn≤x\nn≤x\nNow we can substitute in the Psi function defined earlier:\nX Λ(n)\nln(x!) = x\n+ O(Ψ(x)).\nn\nn≤x\n\nX\nX\nX\nX\nX\nTwin Prime Conjecture\nSince the Psi function is of the same order as a linear function in x, we can\nreplace it in the error term, obtaining the following:\nX Λ(n)\nln(x!) = x\n+ O(x).\nn\nn≤x\nTherefore,\nX Λ(n)\nx\n+ O(x) = x ln(x) + O(x)\nn\nn≤x\nX Λ(n) = ln(x) + O(1)\nn\nn≤x\nMertens Theorem 3:\nFor any real number x ≥ 1,\nX ln(p) = ln(x) + O(1).\n(5)\np\np≤x\nProof:\nFrom the previous theorem,\n0 ≤\nX Λ(\nn\nn) -\nX ln(\np\np)\nX\nln(\npk\np)\n=\nn≤x\np≤x\npk ≤x,k≥2\ninf\na\nln(p)\n≤\nln(p)\npk ≤\np(p - 1)\np≤x\nk=2\np≤x\n≤ 2\nln(\np2\np) ≤ 2\ninfln(\nn2\nn) = O(1)\np≤x\nn=1\nIt then follows from the previous theorem that\nX ln(p) =\nX Λ(n) + O(1) = ln(x) + O(1).\np\nn\np≤x\nn≤x\nFinally, the last and longest of Mertens' theorems presented here brings us to\nour desired conclusion.\nMertens Theorem 4:\nThere exists a constant b1 > 0 such that\nX 1\np = ln(ln(x)) + b1 + O(ln(x)), x ≥ 2.\n(6)\np≤x\n\nR\nZ\nP\nZ\nZ\nZ\nZ\nZ\nZ\nZ\nZ\nZ\nZ\nZ\nZ\nZ\nTwin Prime Conjecture\nProof:\nWe can write\nX 1 =\nX ln(p)\n=\nX\nu(n)f(n)\np\np ln(p)\np≤x\np≤x\nn≤x\nwhere u(n) = ln(\np\np) if n = p, and 0 otherwise, and f(t) = ln(\nt) . We define new\nfunctions U(t) and g(t) as follows\nX\nX ln(p)\nU(t) =\nu(n) =\n= ln(t) + g(t)\np\nn≤t\np≤t\nThen U(t) = 0 for t < 2 and g(t) = O(1) by the previous theorem. Then the\ng(t)\nintegral 2\ninf\n(t ln(t)2) dt converges absolutely, and\ninf g(t)dt\n= O(\n).\nt(ln(t))2\nln(x)\nx\nWe know that f(t) is continuous and U(t) is increasing, so we can express\n1 as a Riemann integral. U(t) = 0 for t < 2, so by partial sums,\np≤x p\nNathanson [8] shows the conclusion of this theorem:\nX 1\nX\nx\n=\nu(n)f(n) =\n+\nf(t)dU(t)\np\np≤x\nn≤x\nIntegrating by parts, we obtain the following:\nx\nx\nln(x) + g(x)\nx\n2 +\nf(t)dU(t) = f(x)U(x) -\nU(t)df(t) =\nln(x)\n-\nU(t)f 0(t)dt.\nNow we can simplify the term outside the integral, and substitute in for U(t):\nx\nx ln(t) + g(t)\n+\nf(t)dU(t) = 1 + O(\n) +\ndt.\nln(x)\nt(ln(t))2\nWe can split the integral in order to simplify the result:\nx\nx\ninf\ng(t)\ninf\ng(t)\n2+\nf(t)dU(t) =\n2 t ln(t) dt+\nt(ln(t))2 dt-\nx\nt(ln(t))2 dt+1+O(ln(x)).\nNow we can evaluate two of the integrals:\nx\ninf\ng(t)\nt ln(t) dt +\nt(ln(t))2 dt = ln(ln(x)) - ln(ln(2))\nFinally, we can simplify this result in terms of a variable b1:\nln(ln(x))-ln(ln(2))+\ninf\ng(t)\ndt+1+O( 1 ) = ln(ln(x))+b1 +O(\n)\nt(ln(t))2\nln(x)\nln(x)\n\nX\nX\nX\nX\nTwin Prime Conjecture\nwhere\nZ\nb1 = 1 - ln(ln(2)) +\ninf\ng(t)\ndt.\nt(ln(t))2\nNow thanks to Mertens, we not only know that the sum of reciprocals of primes\ndiverges, we can see that it diverges like the function ln(ln(x)).\nBrun's Conjecture\nAn important step towards proving the twin prime conjecture is the realization,\nfirst made by Brun in 1919, that the sum of the reciprocals of odd twin primes\nconverges to a definite number. While this fact does not limit the number of\ntwin primes, it shows that they are distributed infrequently among the real\nnumbers.\nBrun's Conjecture:\nLet p1, p2, ... be the sequence of prime numbers p such that p + 2 is also prime.\nThen\ninf\n( pn\n+ pn + 2 ) = ( 3 + 5) + ( 5 + 7) + ( 11 + 13) + . . . < inf\n(7)\nn=1\nProof:\nThe proof of the theorem depends on a result which will be shown in the exercises\nand assumed here, namely that π2(x) <<\nx\n2 for all x ≥ 2. Then\n(ln(x))\npn\npn\nn = π2(pn) <\n≤\n(ln(pn))\n(ln(n))\nfor n ≥ 2. Then\n<\npn\nn(ln(n))\n2 .\nIt follows that the series defined above is convergent:\ninf\ninf\ninf\n≤ 3 +\npn\n<< 3 +\n2 .\nn=2 n(ln(n))\npn\nn=1\nn=2\nThen we have a stronger bound for π2(x) than the one discussed earlier: we\nx\ncan now write that π2(x) = O( ln(ln(x)) ), as shown in Nathanson [8] The exact\nconstant to which Brun's series converges is unknown, but its value has been\ncalculated to be approximately 1.9021605824 [5]. The constant's value gives an\nidea of how infrequent twin primes actually are. Had Brun's series diverged,\nit would have indicated immediately that there are an infinite number of twin\nprimes. The fact that the series converges does not allow one to reach a con\nclusion about the infinitude of twin primes, but it makes the problem more\ndifficult.\n\nZ\nX\nX\nX\nX\n\nTwin Prime Conjecture\nThe Hardy-Littlewood Conjecture\nThe first Hardy-Littlewood Conjecture, also known as the k - tuple conjecture,\nstates that the number of prime constellations can be computed. A prime\nconstellation is a sequence of primes p1, p2, ..., pn where the difference between\nthe first and last primes, pn - p1, is s. This s is the smallest number such\nthat there exist n integers in an interval of length s, and, for every prime q, at\nleast one of the residues pi (mod q) is not one of these n integers. Hardy and\nLittlewood have then conjectured that\nx\ndt\nπ2(x) ∼ 2Π2\nln(t)2\n(8)\nHere Π2 refers to the twin prime constant introduced before, which is\nΠ2 =\nY p\n(p\n(p\n-\n-\n1)\n2)\n2 ≈ 0.6601618158468695739278121100145 . . . [12]\np>3\nThe formulation of the Hardy-Littlewood conjecture builds upon some of the\ntechniques used to prove Brun's conjecture, namely the Brun sieve techniques.\nThe Brun sieve can be constructed as follows: Let X be a nonempty, finite set\nof N objects, and let P1, . . . , Pr be r different properties that the elements of\nthe set X might have. Let N0 denote the number of elements of X that have\nnone of these properties. For any subset I = {i1, . . . , ik} of {1, 2, . . . , r}, let\nN(1) = N(i1, . . . , ik) denote the number of elements of X that have each of the\nproperties Pi1 , Pi2 , . . . , Pik . Let N(∅) = |X| = N . If m is a nonnegative even\ninteger, then\nm\nN0 ≤\n(-1)k\nN(I).\n(9)\nk=0\n|I|=k\nIf m is a nonnegative odd integer, then\nm\nN0 ≥\n(-1)k\nN(I).[8]\n(10)\nk=0\n|I|=k\nThe proof given in Nathanson [8] is as follows. Let x be an element of the set\nX, and suppose that x has exactly l properties Pi. If l = 0, then x is counted\nonce in N0 and once in N(∅), but is not counted in N(I) if I is nonempty. If\nl ≥ 1, then x is not counted in N0. By renumbering the properties, we can\nassume that x has the properties P1, P2, . . . , Pl. Let I ⊆{1, 2, . . . , l, . . . , r}. If\ni ∈ I for some i > l, then x is not counted in N(I). If I ⊆{1, 2, . . . , l} then\nx contributes 1 to N(I). For each k = 0, 1, . . . , l, there are exactly\nk\nl\nsuch\nsubsets with |I| = k. If m ≥ l, then the element x contributes\nX\nl\nl\n(-1)k\n= 0\nk\nk=0\n\nTwin Prime Conjecture\nto the right sides of the inequalities. If m < l, then x contributes\nm\n\nX\nl\n(-1)k\nk\nk=0\nto the right sides of the inequalities. This contribution is positive if l is even\nand negative if l is odd.\nThe Hardy-Littlewood conjecture makes use of this sieve by considering a set\nof primes with properties corresponding to the size of the intervals between\nconsecutive members. Then the Brun sieve is used to make generalizations\nabout these properties based on the primes themselves.\nThe Hardy-Littlewood conjecture is compatible with the twin prime conjecture\nas originally stated, and is in fact sometimes substituted in as the strong twin\nprime conjecture, because it is understood that its proof would involve the\nintegral going to infinity, and therefore π2 going to infinity as well.\nExercises\nAn interesting exercise involving the distribution of primes is determining the\nnumber of primes in an interval. To formulate a corresponding expression, we\ncan start out by rewriting π(x + x) - π(x) in terms of Euler's approximation.\nThen\nx + x\nx\nx\nπ(x + x) - π(x) = ln(x) + ln(1 + ) - ln(x) + O(ln(x)).\nWe can rewrite the right-hand side as\nx\nx\n+ O(\n)\nln(x)\nln(x)\nThen if we let = 1,\nx\nx\nπ(2x) - π(x) = ln(x) + O(ln(x))\nWhere O( ln(\nx\nx) ) ∼ π(x). At first, it appears that this is saying that the number\nof primes in an interval is equal to the number of primes in a sequential interval\nof equal length, which is misleading, because we know from experience and\ncomputation that the primes thin out as we progress through the numbers.\nHowever, a better formulation of the result of this exercise would be that\nπ(2x) - 2π(x) = O(π(x))\nand this result is not inconsistent with known tables of primes.\nEarlier, we used a result discovered by the German mathematician Brun, which\nwe can prove here. Brun's conjecture is as follows:\nx(ln(ln(x)))2\nπ2(x) <<\n(11)\n(ln(x))2\n\nX\nX\nX\nTwin Prime Conjecture\nTo prove this, we use the Brun sieve to find an upper bound for N0(x, y), where\nhere N0 is the number of integers such that n(n + 2) is prime, and x is a prime\nless than y but greater than 5. Then by the Brun sieve, with m an even integer\nsuch that 1 ≤ m ≤ r, it is shown in Narkiewicz [7] that\nm\nN0(x, y) ≤\n(-1)k\nN(I)\nk=0\n|I|=k\nm\nX\nX\n2ky\n≤\n(-1)k\n( pi1 , . . . , pik\n+ O(2k))\nk=0\n{i1,...ik }\nm\nm\n\nX\nX\n(-2)k\nX\nr\n≤ y\npi1 , . . . , pik\n+\n(-1)k\nk O(2k)\nk=0 {i1,...ik }\nk=0\nr\nr\nm\nX\nX\n(-2)k\nX\nX\n(-2)k\nX r\n≤ y\npi1 , . . . , pik\n- y\npi1 , . . . , pik\n+ O(\nk (2k)).\nk=0 {i1,...ik }\nk=m+1 {i1,...ik }\nk=0\nBy another of Brun's theorems,\nTheorem:\nr\nX\nX\n(-2)ky\ny\ny\n<<\npi1 , . . . , pik\n(ln(x))2\nk=0 {i1 ,...ik }\nNathanson [8] supplies the bound for the second term: Let sk(x1, . . . , xr) be the\nelementary symmetric polynomial of degree k in r variables. For any nonnega\ntive real numbers x1, . . . , xr, we have\nsk(x1, . . . , xr) =\nxi1 . . . xik\n{i1,...ik }\n(x1 + . . . + xr)k\n= (s1(x1, . . . , xr))k\n≤\nk!\nk!\n< e k\ns1(x1, . . . , xr)k .\nk\nTherefore,\nr\nr\nX\nX\n(-2)k\nX\nX\n(-2)k\n|y\npi1 , . . . , pik\n| ≤ y\npi1 , . . . , pik\nk=m+1 {i1,...ik }\nk=m+1 {i1,...ik }\nr\nX\nX\n≤ y\n(\n) . . . (\n)\npi1\npik\nk=m+1 {i1,...ik}\nr\nX\n= y\nsk(\n, . . . ,\n)\np1\npr\nk=m+1\n\nX\n\nX\nTwin Prime Conjecture\nr\nX\ne\n< y\n( )k s1(\n, . . . ,\n)k\nk\np1\npr\nk=m+1\nr\n2e\nX 1\n< y\n(\n)k(\n)k\nm\np\nk=m+1\np≤x\nr\nX\nc ln(ln(x))\n< y\n( ·\n)k\nm\nk=m+1\nwhere c is a positive constant. If we choose m such that m > 2c ln(ln(x)), then\n·\nX\nr\nr\nc ln(ln(x)))k\nX\nx\ny\n( ·\nm\n≤ x\n2k < 2m\nk=m+1\nk=m+1\nFor the last term,\nX\nm\nm\nr\nm\n2k <\n(2r)k << (2r)m ≤ x .\nk\nk=0\nk=0\nCombining these three terms, we get\nπ2(y) << x + (ln(\ny\nx))2 + 2\ny\nm + x m << (ln(\ny\nx))2 + 2\ny\nm + x m\nwhere again x is any number less than y but greater than 5, and m is any even\ninteger such that m > 2c ln(ln(x)). If we let c0 = max{2c, (ln(2)-1)}, and let\n·\nln(y)\nx = e( 3c0·ln(ln(y)) ) = y 3c0·ln(ln(y))\nm = 2[c0 ln(ln(y))]\n·\nThen since\nln(y)\nln(x) = 3c0 · ln(ln(y))\ny\ny(ln(ln(y)))2\n<<\n(ln(x))2\n(ln(y))2\nSince c0 ≥ (ln(2))-1, and m = 2[c0 · ln(ln(y))] > 2c0 · ln(ln(y)) - 2,\ny\n4y\n4y\n4y\n2m < 22c0 ·ln(ln(y)) = (ln(y))2c0·ln(2) ≤ (ln(y))2\nThen\nm\n2c0 ln(ln(y))\n2c0 · ln(ln(y) ln(y))\nx ≤ x\n·\n= exp(\nln(ln(y))\n) = y\n3c0 ·\nFinally,\nx(ln(ln(x)))2\nπ2(x) <<\n.\n(ln(x))2\n\nTwin Prime Conjecture\nConclusion\nThe twin prime conjecture may never be proven, but studying the properties of\ntwin primes is certainly a rewarding exercise. Recent work on the twin prime\nconjecture by Dan Goldston and Cem Yilidrim has focused on creating ex\npressions for the gap size between primes, and in particular focusing on the\nexpression\nΔ = lim inf pn+1 - pn = 1\nn→inf\nln(pn)\nResearch into better expressions for the interval between consecutive primes\nis currently being conducted at Stanford, sponsored by the American Insti\ntute of Mathematics [12]. Though number theory has been the foundation of\nmany different branches of higher mathematics, its fundamental problems re\nmain interesting and fruitful for researchers interested in the properties of prime\nnumbers.\nReferences\n[1] Arenstorf, R. F. \"There Are Infinitely Many Prime Twins.\" 26 May 2004.\nhttp://arxiv.org/abs/math.NT/0405509.\n[2] Guy, R. K. \"Gaps between Primes. Twin Primes.\" A8 in Unsolved Problems\nin Number Theory, 2nd ed. New York: Springer-Verlag, pp. 19-23, 1994.\n[3] Hardy, G. H. and Littlewood, J. E. \"Some Problems of 'Partitio Numero\nrum.' III. On the Expression of a Number as a Sum of Primes.\" Acta Math.\n44, 1-70, 1923.\n[4] Hardy, G. H. and Wright, E. M. An Introduction to the Theory of Numbers,\n5th ed. Oxford, England: Clarendon Press, 1979.\n[5] Havil, J. Gamma: Exploring Euler's Constant. Princeton, NJ: Princeton\nUniversity Press, pp. 30-31, 2003.\n[6] Miller, S. J. and Takloo-Bighash, R. An Invitation to Number Theory.\nPrinceton, NJ: Princeton University Press ,pp. 326-328, 2006.\n[7] Narkiewicz, W. The Development of Prime Number Theory. Berlin, Ger\nmany: Springer Press, 2000.\n[8] Nathanson, M. B. Additive Number Theory. New York, New York: Springer\nPress, 1996.\n[9] Ribenboim, P. The New Book of Prime Number Records. New York:\nSpringer-Verlag, pp. 261-265, 1996.\n[10] Shanks, D. Solved and Unsolved Problems in Number Theory, 4th ed. New\nYork: Chelsea, p. 30, 1993.\n\nTwin Prime Conjecture\n[11] Tenenbaum, G. \"Re Arenstorf's paper on the Twin Prime Con\njecture.\"8 Jun 2004.\n[12] Weisstein,\nEric\nW.\n\"Twin\nPrime\nConjecture\"\nhttp://mathworld.wolfram.com/TwinPrimeConjecture.html, 2006.\n[13] Young, R. M. Excursions in Calculus. The Mathematical Association of\nAmerica, 1992."
        },
        {
          "category": "Resource",
          "title": "chan.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/58778de0faae6448c3b0bfb1a1ec4f7a_chan.pdf",
          "content": "Z\nTHE SINE PRODUCT FORMULA AND THE GAMMA\nFUNCTION\nERICA CHAN\nDECEMBER 12, 2006\nAbstract. The function sin x is very important in mathematics\nand has many applications. In addition to its series expansion, it\ncan also be written as an infinite product. The infinite product of\nsin x can be used to prove certain values of ζ(s), such as ζ(2) and\nζ(4). The gamma function is related directly to the sin x function\nand can be used to prove the infinite product expansion. Also used\nare Weierstrass' product formula and Legendre's relation.\n1. Introduction\nThere are a few special functions in mathematics that have particular\nsignificance and many applications. The gamma function is one of\nthose functions. The gamma function can be defined as\ninf\nΓ(x) =\ne-ttx-1dt.\nWe can also get the formula\n(1)\nΓ(x + 1) = xΓ(x)\nby replacing x with x + 1 and integrating by parts.\nIn addition, since Γ(1) = 1, using Equation (1), by induction, we can\nrelate the gamma function to the factorial formula\n(2)\nΓ(n) = (n - 1)!.\nThe gamma function has the properties that it is log convex and mono\ntonic, which will be used in a later proof.\nAnother important function in mathematics is the sine function. The\ntrigonometric function sin x can be written as an infinite series\nx\nx\nx\nsin x = x - 3! + 5! - 7! + ...\nDate: December 12, 2006.\n\nERICA CHAN\nThe function sin x can also be written as an infinite product expansion.\nThe gamma function is directly related to the sine function. To derive\nthe infinite product expansion of the sine function, the Weierstrass\nproduct formula, Legendre relation, and the gamma function are all\nused.\nThe sine product formula is important in mathematics because it has\nmany applications, including the proofs of other problems. One such\napplication is the calculation of the values of ζ(2) and ζ(4), where\nX\ninf\nζ(s) =\n= 1 +\n+\n+\n+ ...\nns\n2s\n3s\n4s\nn=1\nis the Riemann zeta function.\nSection 2 derives Weierstrass' product formula and Euler's constant.\nSection 3 introduces Stirling's formulas, Gauss' multiplication formula,\nand the Legendre relation. In Section 4, the sine product formula is\nproduced from the gamma function. Finally, Section 5 discuss the\napplications of the sine product formula, including the calculation of\nζ(2) and ζ(4).\n2. Weierstrass' product formula\nWeierstrass derived a formula which, when applied to the gamma\nfunction, can be used to prove the sine product formula. To find Weier\nstrass' product formula, we first begin with a theorem.\nTheorem 2.1. The function Γ(x) is equal to the limit as n goes to\ninfinity of\nnxn!\n(3)\nΓ(x) = lim\n.\nn→inf x(x + 1)\n(x + n)\n· · ·\nProof. Begin with a difference quotient expressed as the inequality\nlog Γ(-1 + n) - log Γ(n)\nlog Γ(x + n) - log Γ(n)\nlog Γ(1 + n) - log Γ(n) .\n(-1 + n) - n\n≤\n(x + n) - n\n≤\n(1 + n) - n\nThis inequality is true because of the gamma functions properties that\nit is monotonically increasing and log convex. Substitute for Γ(n) using\nEquation (2) and simplify to get\nlog(n - 1) ≤ log Γ(x + n) - log(n - 1)! ≤ log n.\nx\n\nX\nTHE SINE PRODUCT FORMULA AND THE GAMMA FUNCTION\nRearrange the terms of the inequality and since the logarithm is a\nmonotonically increasing function, we get\n(n - 1)x(n - 1)! ≤ Γ(x + n) ≤ n x(n - 1)!.\nUsing Equation (1), we know that Γ(x + n) = (x + n - 1)(x + n -\n2)\n(x + 1)xΓ(x). Substituting it into the inequality gives us\n· · ·\n(n - 1)x(n - 1)!\nnxn!\nx + n.\n(x + n - 1) ≤ Γ(x) ≤\nx(x + 1)\nx(x + 1)\n(x + n)\nn\n· · ·\n· · ·\nIf we replace n with n + 1 on the left hand side and then rearrange the\ninequality, we get\nn\nnxn!\nΓ(x) x + n ≤ x(x + 1)\n(x + n) ≤ Γ(x).\n· · ·\nTaking the limit as n →inf gives us\nnxn!\nΓ(x) = lim\n.\nn→inf x(x + 1)\n(x + n)\n· · ·\nEquation (3) was derived by Gauss. From there, Weierstrass was\nable to derive another form of the same equation\nx(log n-1/1-1/2-...-1/n) 1 ex/1\nex/2\nex/n\nΓ(x) = e\n.\nx 1 + x/1 · 1 + x/2 · · · 1 + x/n\nThe limit of\nlim (\n+\n+\n+ n - log n)\nn→inf 1\n· · ·\nexists, is equal to C, and is often called Euler's constant. So we can\nrewrite Weierstrass' product formula as\nY\nx/i\ninf\ne\n(4)\nΓ(x) = e-Cx\n.\nx\n1 + x/i\ni=1\n3. Multiplication Formula\nThere are three formulas,\nΓ(x) =\n√\n2πxx-1/2 e-x+μ(x), where\ninf\nθ\nμ(x) =\n(x + n + ) log(1 +\n) - 1 =\n, and\nx + n\n12x\nn=0\ne-n+θ/12n\nn! =\n√\n2πnn+1/2\n,\n\nERICA CHAN\nwhich are known as Stirling's formulas. Where θ is a number inde\npendent of other values and where 0 ≤ θ ≤ 1. Stirling's formulas are\napproximations of Γ(x) for large values of x and the accuracy increases\nas x increases.\nGauss discovered a formula, which expresses Γ(x) as a product of its\nfactors. Gauss' multiplication formula is\n(2π)(p-1)/2\nx\nx + 1\nx + p - 1\n(5)\nΓ(x) = Γ( )Γ(\n)\nΓ(\n),\npx-1/2\np\np\n· · ·\np\nwhere p is a positive integer.\nThere is the special case discovered by Legendre, where p = 2, which\nis called Legendre's relation. Legendre's relation states\nx\nx + 1\n√π\n(6)\nΓ( )Γ(\n) =\nΓ(x).\n2x-1\nThe derivation and proof of these formulas can be found at [1]. They\nare based on finding an approximation for Γ(x) in terms of an estimate\nfor n!.\n4. The Sine and and Gamma Functions\nTo derive the sine product formula, we first find a relationship be\ntween the sine and gamma functions. We define a function φ(x) and\nfind that φ(x + 1) = φ(x).\nTheorem 4.1. Define the function φ(x), for nonintegral x, to be\n(7)\nφ(x) = Γ(x)Γ(1 - x) sin πx,\nthen φ(x + 1) = φ(x).\nProof. If we use Equation (1) and substitute -x + 1 for x, then we get\nthat\n(8)\nΓ(-x + 1) = -xΓ(-x).\nFinding φ(x + 1), we get\nφ(x + 1) = Γ(x + 1)Γ(-x) sin(π(x + 1)).\nNote that sin(πx+π) = - sin πx. Use Equation (1), rearrange Equation\n(8), and substitute them in to get\nφ(x + 1) = xΓ(x)Γ(-x + 1) (- sin πx) = Γ(x)Γ(-x + 1) sin πx.\n-x\nThis is equal to the original equation for φ(x), so φ(x+1) = φ(x).\n\nTHE SINE PRODUCT FORMULA AND THE GAMMA FUNCTION\nThe Legendre relation (Equation (6)) can be written as\nx\nx + 1\nΓ( )Γ(\n) = b2-xΓ(x),\nwhere b = 2√π is a constant. Replacing x with 1 - x gives\nx\nΓ(1 - x )Γ(1 - ) = b2x-1Γ(1 - x).\nFrom there, we get that\nφ( x )φ( x + 1 ) = Γ( x )Γ(1 - x ) sin πx Γ( x + 1 )Γ(1 - x ) cos πx .\nSimplifying the above, we find\nx\nx + 1\nb2\nb2\nφ( )φ(\n) =\nΓ(x)Γ(1 - x) sin πx =\nφ(x) = cφ(x)\nwhere c = b2 is a constant.\nUsing Equation (1) and the infinite series expansion of sin x, we get\nthat\n(πx)3\n(πx)5\n(πx)7\nΓ(1 + x)\nφ(x) =\nΓ(1 - x) πx -\nx\n+\n+ ...\n-\n3!\n5!\n7!\nπ3x2\nπ5x4\nπ7x6\n= Γ(1 + x)Γ(1 - x) π - 3! + 5! - 7! + ... .\nThe right hand side of the equation equals π when x = 0. From there\nwe see that φ(0) = π. Let g(x) be a periodic function that is equal\nthe second derivative of log φ(x). It is periodic because log φ(x) =\nlog(Γ(x)Γ(1 - x) sin πx) is periodic and so the second derivative will\nalso be periodic. Since g(x) is periodic, then it satisfies the equation\nx\nx + 1\n(9)\ng(x) = (g( ) + g(\n)).\nSince g(x) is continuous on the interval 0 ≤ x ≤ 1, it is bounded by a\nconstant M, |g(x)| ≤ M. Because g(x) is periodic, it is bounded by M\nfor all x. From Equation (9), we get that\n)\nM\nFrom this we see that g(x) can actually be bounded by 2 . We can\ncontinue to repeat this process until the bound of g(x) goes to 0. There\nfore g(x) = 0, which means that log φ(x) is a linear function, because\ng(x) = 0 is its second derivative. Since log φ(x) is periodic, this implies\nx + 1\nM\nx\ng(x)\ng( )\ng(\n+\n|\n| ≤\n.\n≤ 2\n\nX\nERICA CHAN\nthat it is a constant, which also implies that φ(x) is constant. We know\nthat φ(0) = π and therefore φ(x) must equal π for all x.\nRearranging Equation (7) and using the fact that φ(x) = π,\nπ\nΓ(x)Γ(1 - x) =\n.\nsin πx\nUsing (1), the above equation can be rewritten as\nπ\nsin πx =\n.\n-xΓ(x)Γ(-x)\nThe Weierstrass product formula allows us to replace the gamma func\ntion and rewrite sin πx as an infinite product expansion\nY\ninf\nx\n(10)\nsin πx = πx\n1 - i2\n.\ni=1\n5. Applications of the Sine Product Formula\nApplications of the sine product formula include the calculation of\ncertain values of the Riemann zeta function. The proof that ζ(2) =\nπ2/6 is often called Euler's Theorem.\nTheorem 5.1. The sum of the reciprocal of the perfect squares is π2/6:\ninf\nπ2\n=\n.\nn2\nProof. Consider the function sin x = 0, which has an infinite number\nof roots ±π, ±2π, ±3π, .... Using the infinite series expansion of sin x\nand dividing sin x by x gives us the infinite series\nx\nx\nx\n(11)\n1 - 3! + 5! - 7! + ... = 0.\nApplying Equation (10) and dividing by πx, we get the infinite product\nexpansion\nsin x\nx2\nx2\nx2\n(12)\n= (1 -\n)(1 -\n)(1 -\n)...\nx\nπ2\n4π2\n9π2\nWhen Equation (12) is expanded, the coefficient of x2 will be\n+\n+\n+ ...\nπ2\n4π2\n9π2\nUsing Equation (11), we get\n+\n+\n+ ... =\n.\nπ2\n4π2\n9π2\n3!\n\nX\nX\nX\nTHE SINE PRODUCT FORMULA AND THE GAMMA FUNCTION\nMultiplying both sides of the equation by π2 gives us\nπ2\n+\n+\n+ ... =\n.\nUsing a similar method, we can also calculate ζ(4), which is the sum\nof the reciprocals of numbers to the fourth power.\nTheorem 5.2. The value of ζ(4) is π4/90:\ninf\nπ4\n=\n.\nn4\nProof. From the calculation of ζ(2), we know that\nX\ninf\n(13)\n= .\ni2π2\ni=1\nIf we square Equation (13), we get that\n\n!2\nX\ninf\n(14)\n=\n.\ni2π2\ni=1\nExpanding the left hand side of Equation (14) we get\n\n!2\ninf\nX\n(15)\ni2π2\n=\ni4π4 + 2\ni2j2π4 .\ni=1\ni<j\nIf we use the sine infinite product expansion, Equation (12), we get\nthat the coefficient of x4 for sin\nx\nx is equal to the sum of the product of\ni2\nx\nπ\n2 . In other words, the coefficient of x4 is\nX\n.\ni2j2π4\ni<j\nFrom Equation (11), we also know that the coefficient of x4 must be\n1 , so\n\nX\nX\nP\nX\n\nERICA CHAN\nX\n(16)\n=\n.\ni2j2π4\ni<j\nTherefore, substituting Equation (16) into Equation (15) gives us\n\n!2\n\ninf\n=\n=\n+ 2\n.\ni2π2\ni4π4\ni=0\nTo calculate ζ(4), simply solve for\ni4\nπ4 and multiply both sides by\nπ4, to get\ninf1\nπ4\n=\n.\ni4\ni=1\nReferences\n[1] Artin, Emil. The Gamma Function. New York: Holt, Rinehart and Winston,\n1964.\n[2] Simmons, George F. \"Calculus with Analaytic Geometry.\" New York: McGraw\nHill, Second Edition, 1996.\n[3] Weisstein, Eric W. \"Gamma Function.\" From MathWorld-A Wolfram Web Re\nsource. http://mathworld.wolfram.com/GammaFunction.html\n[4] Young, Robert M. Excursions in Calculus: An Interplay of the Continuous and\nDiscrete. United States of America: The Mathematical Association of America,\n1992."
        },
        {
          "category": "Resource",
          "title": "peter_s.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/64f808cbc8a097dad504f7ef6eb85b6a_peter_s.pdf",
          "content": "X\n\nGenerating Functions and Their Applications\nAgustinus Peter Sahanggamu\nMIT Mathematics Department\nClass of 2007\n18.104 Term Paper\nFall 2006\nAbstract. Generating functions have useful applications in many fields of study. In this paper,\nthe generating functions will be introduced and their applications in combinatorial problems,\nrecurrence equations, and physics will be illustrated.\n1. Introduction.\nWorking with a continuous function is sometimes much easier than working with a\nsequence. For example, in the analysis of functions, calculus is very useful. However, the\ndiscrete nature of sequences prevents us from using calculus on sequences. A generating\nfunction is a continuous function associated with a given sequence. For this reason,\ngenerating functions are very useful in analyzing discrete problems involving sequences\nof numbers or sequences of functions.\nDefinition 1-1. The generating function of a sequence {fn}inf\nis defined as\nn=0\ninf\nf(x) =\nfnx n ,\n(1-1)\nn=0\nfor x < R, and R is the radius of convergence of the series.\n| |\nIt is important that the series has a nonzero radius of convergence, otherwise f(x)\nwould be undefined for all x = 0. Fortunately, for most sequences that we would be\ndealing with, the radius of convergence is positive. However, we can certainly construct\nsequences for which the series (1-1) is divergent for all x = 0; fn = nn is one such\nexample.\nExample 1-2. As a simple example of a generating function, consider a geometric\nsequence, fn = an . Then\ninf\nX\nf(x) =\na n x n =\n.\n(1-2)\nax\nn=0\n1 -\nThis series convergences absolutely whenever ax < 1. Therefore, the radius of conver\n|\n|\ngence is R = 1/ a .\n| |\nFor the rest of the paper, if not mentioned otherwise, x is always chosen to be small\nenough such that any series encountered in our analysis converges absolutely. Now we\nshall discuss an application of generating functions to linear recurrence problems.\n\nX\nX\nX\nX\nX\nX\n18.104 Term Paper\n2. From Recursion to Algebra.\nGenerating functions can be used to solve a linear recurrence problem.\nDefinition 2-1. The problem of linear recurrence is to find the values of a sequence\n{un} satisfying\nm\nckun+k = v,\nfor some constant v and any integer n ≥0,\n(2-1)\nk=0\ngiven the initial values u0, u1, . . . , um-1, where both c0 and cm are nonzero.\nIn order to solve this recurrence problem, we use the following property of generating\nfunctions.\nProposition 2-2. If {un}inf\nis a sequence with a generating function u(x) and k\nn=0\nis a positive integer, then\ninf\n\nk-1\n\nX\nX\nun+kx n =\nu(x) -\nujxj .\n(2-2)\nxk\nn=0\nj=0\nProof: We start from the definition of u(x).\ninf\nk-1\ninf\nu(x) =\nujxj =\nujxj +\nujxj .\n(2-3)\nj=0\nj=0\nj=k\nShifting the summation index in the second term to n = j -k, we obtain\nk-1\ninf\nu(x) =\nujxj +\nun+kx n+k ,\n(2-4)\nj=0\nn=0\nfrom which Eq. (2-2) immediately follows.\n\nBy multiplying both sides of Eq. (2-1) by xn and summing from n = 0 to inf, we find\nm\ninf\ninf\nX\nX\nX\nv\nck\nun+kx n = v\nx n =\n.\n(2-5)\nx\nk=0\nn=0\nn=0\n1 -\nUsing Proposition2-2, each term on the left hand side with fixed k can be expressed in\nterms of u(x) and known constants u0, u1, . . . , um-1. We have now reduced Eq. (2-5)\ninto an algebraic equation for u(x), which can be easily solved.\nAfter finding u(x), we write down the Taylor series expansion of u(x) around x = 0.\nBecause the Taylor series of a function is unique (if it exists), the coefficient of xn in\nthe Taylor series must be un. To illustrate this method, we shall use it on the following\nexample from physics.\nExample 2-3. In special relativity, the usual one dimensional velocity addition for\n′\nmula v = u + v is modified into [1, p. 127]\nu + v\n′ v =\n,\n(2-6)\n1 + uv\n′\nwith v , u, and v measured in units of the speed of light c. We will use this velocity\naddition in the following problem. Suppose there are infinitely many cars labeled by\n\nX\nX\nX\n\nX\nX\nGenerating Functions and Their Applications\nintegers n ≥0. The (n + 1)-th car moves to the right relative to the n-th car with a\nrelative velocity v (0 < v < 1). In our reference frame, we denote the velocity of the\nn-th car by un. Assuming u0 = 0, find un for all n ≥1.\nSolution: Notice that un+1 is the addition of un and v using the addition formula\nin Eq. (2-6).\nun + v\nun+1 =\n,\nfor n ≥0.\n(2-7)\n1 + unv\nThis recurrence is not linear, and therefore we may not apply our previous method\ndirectly. With a little manipulation, however, this recurrence can be transformed into\na linear recurrence.\n1 + unv -un -v\n(1 -un)(1 -v)\nun+1 =\n=\n1 -\n1 + unv\n1 + v -v(1 -un)\n1 + v\nv\n1 -un+1\n=\n1 -v\n1 -un\n-1 -v .\n(2-8)\nDefining\n1 + v\nv\nα =\n, λ =\n, and fn =\n,\nv\nv\nun\n1 -\n1 -\n1 -\nEq. (2-8) can be written as\nfn+1 = αfn -λ,\n(2-9)\nwhich is a linear recurrence in fn. Now multiply both sides by xn and sum from n = 0\nto inf.\ninf\ninf\ninf\nfn+1x n\nn -λ\nn\n= α\nfnx\nx\nn=0\nn=0\nn=0\nλ\nf(x) -f0 = αf(x) -\n,\n(2-10)\nx\n1 -x\nwhere we have used Proposition2-2 to simplify the left hand side. The initial condition\nof fn is given by f0 = 1/(1 -u0) = 1. Solving for f(x) yields\nλx\nf(x) = 1 -αx -(1 -x)(1 -αx)\nλ\nf(x) =\n+\n.\n(2-11)\n1 -αx\nα -1 1 -x -1 -αx\nUsing the definitions of α and λ, we find\nλ\n=\n,\n(2-12)\nα -1\nand hence\n\ninf\ninf\nf(x) =\n+\n=\nαn + 1 x n =\nfnx n .\n(2-13)\n2 1 -αx\n1 -x\nn=0 2\nn=0\nSince f(x) is the sum of two geometric series, we conclude that the Taylor series\naround 0 has a positive radius of convergence. Therefore, 2fn = αn+1 by the uniqueness\nof Taylor series, and\nαn -1\nun = 1 -fn\n= αn + 1 .\n(2-14)\n\nX\nY\nY X\nX\nX\n18.104 Term Paper\nSince α > 1, we conclude that 0 < un < 1 for all n ≥1. Physically, this result shows\nthat any car moves with a speed less than c (remember that we are writing un in units\nof the speed of light). Special relativity predicts that any massive object always travels\nslower than light [4, p. 119].\n\n3. Applications to Combinatorial Problems.\nMany combinatorial problems can be solved with the aid of generating functions. In\nparticular, let's consider the problem of finding the number of partitions of a natural\nnumber.\nDefinition 3-1. [6, p. 169] A partition of a natural number n is a way to write n as\na sum of natural numbers, without regard to the ordering of the numbers.\nExample 3-2. 1 + 1 + 3 + 1 is a partition of 6.\nWith this definition, the generating function of the number of partitions of n has a\nsimple form.\nTheorem 3-3. [6, p. 169] If pn is the number of partitions of n and p0 = 1, then\ninf\ninf\nX\nY\npnx n =\n.\n(3-1)\nn=0\nk=1 1 -xk\nProof: First we need to establish the convergence of the infinite product for x = 0.\ninf\n\nThis infinite product converges absolutely if the series\nxk converges absolutely [3, p.\nk=1\n53]. Thus, the right hand side converges absolutely for x < 1.\n| |\nEach factor in the infinite product can be expressed as a geometric series.\ninf\ninf inf\n\nmk\n=\nx\n.\n(3-2)\nk=1 1 -xk\nk=1 m=0\nIn this form, we can see that the coefficient of the xn term is equal to the number of\ninf\nways to choose integers {mk mk ≥0, k = positive integers} satisfying n =\nmkk. If\n|\nk=1\nn\nwe take the xmk k term from the k-th factor, then we obtain x . By comparing this with\nDefinition 3-1, we conclude that the coefficient of xn is equal to the number of partitions\nof n.\n\nAs a check, let's try expanding the right hand side of Eq. (3-1) up to x4 .\ninf\npnx n = (1 + x + x 2 + x 3 + x 4)(1 + x 2 + x 4)(1 + x 3)(1 + x 4) + · · ·\nn=0\n= (1 + x + x 2 + x 3 + x 4)(1 + x 2 + x 3 + 2x 4) + · · ·\n= 1 + x + 2x 2 + 3x 3 + 5x 4 + O(x 5),\np1 = 1,\np2 = 2,\np3 = 3,\np4 = 5.\n(3-3)\nWe can easily verify that Eq. (3-3) correctly gives the number of partitions of 1 to 4.\nAnother important combinatorial problem that can be easily solved with generating\nfunctions is Catalan's problem [6, p. 260].\n\nX\nX\nX X\nX\nX\nX\nX\nX\nX X\nX X\nGenerating Functions and Their Applications\nExample 3-4 (Catalan's Problem). Given a product of n letters, how many ways can\nwe calculate the product by multiplying two factors at a time, keeping the order fixed?\nAs an example, for n = 3, there are two ways: (a1a2)a3 and a1(a2a3).\nSolution: Denote the solution for n = m by Km. It is clear that K2 = 1. For later\nconvenience, we define K1 = 1. Now consider the n = m + 1 case. Suppose at the\nlast step of the multiplication, we have bc, where b = a1\naj, c = aj+1\nam+1, and\n· · ·\n· · ·\n1 ≤j ≤m. Notice that there are Kj ways to multiply the factors in b, and there are\nKm-j+1 ways to multiply the factors in c. Thus, for a given j, there are KjKm-j+1\nways to multiply a1\nam+1. The total number of ways can be obtained by summing\n· · ·\nover all possible values of j.\nm\nKm+1 =\nKjKm-j+1,\nm ≥1.\n(3-4)\nj=1\nMultiply both sides by xm+1 and sum from m = 1 to m = inf.\ninf\ninf\nm\nKm+1x m+1 =\nKjKm-j+1x m+1 .\n(3-5)\nm=1\nm=1 j=1\nAs usual, we define the generating function for {Kn}inf\nn=1.\ninf\nK(x) =\nKnx n .\n(3-6)\nn=1\nThe left hand side of Eq. (3-5) is\ninf\ninf\nKm+1x m+1 =\nKnx n -K1x = K(x) -x.\n(3-7)\nm=1\nn=1\nNow consider the expression for K(x)2 .\ninf\ninf\nK(x)2 =\nKjxj\nKix i .\n(3-8)\nj=1\ni=1\nLet i = m -j + 1, where m ≥1. For a given m, j can be any integer from 1 to m, since\ni ≥1. Thus, we can rewrite Eq. (3-8) as\ninf\nm\ninf\nm\nK(x)2 =\nKjKm-j+1xjx m-j+1 =\nKjKm-j+1x m+1 .\n(3-9)\nm=1 j=1\nm=1 j=1\nBy using Eqs. (3-7) and (3-9) in Eq. (3-5), we obtain a quadratic equation for K(x).\nK(x)2 -K(x) + x = 0,\nor\nK(x) = 2 ±\n√\n1 -4x.\n(3-10)\nNotice from Eq. (3-6) that K(0) = 0, which means we must take the negative sign for\nthe square root.\n1 -√1 -4x\nK(x) =\n.\n(3-11)\n\nX\nX\n18.104 Term Paper\nIt is clear that the square root has a converging power series around x = 0 for |x| < 1\n4 ,\nand hence the infinite series defining K(x) has a radius of convergence of 1\n4 .\nUse the binomial formula to obtain the power series expansion of √1 -4x.\ninf\n√\n1 -4x = 1 +\nX ( 1\n2 )( 1\n2 -1)( 1\n2 -2) · · · ( 1\n2 -n + 1) (-4x)n\nn!\nn=1\ninf\nn\n= 1 -2x +\nX\n(-1)n-1 1 · 3 · 5 · · · (2n -3) (-1)n2n x\nn!\nn=2\ninf\n= 1 -2x -2\nX 1 · 2 · 3 · · · (2n -2) x n\nn!(n -1)!\nn=2\ninf\n\nn\n= 1 -2\nn\n2(\nn\nn\n-\n-\n1) x .\n(3-12)\nn=1\nNow we substitute Eq. (3-12) into Eq. (3-11) to find\ninf\n\nX\nKnx n =\nX 1\n2(n -1) x n ,\n(3-13)\nn\nn -1\nn=1\nn=1\nor\n\n2m\nKm+1 =\n,\nm ≥0.\n(3-14)\nm + 1\nm\nThis problem was solved by Catalan in 1838 [6, p. 259-260], and the Catalan numbers\nare conventionally defined as Cn = Kn+1, for n ≥0.\n\nThere are many other applications of generating functions in combinatorial problems\nthat cannot be covered here. A wide variety of examples are discussed in [5, Chapter\n3].\n4. Legendre Polynomials.\nSo far, we have only discussed generating functions of sequences of numbers. However,\nin Section 1, I mentioned that generating function methods can also be used to analyze\nsequences of functions. One interesting example is the generating function of Legendre\npolynomials. As we shall see, the generating function provides a physical insight, with\na deep connection to electromagnetism.\nThere are several ways to define the Legendre polynomials Pn(t).\nFor example,\nthey can be defined as solutions to a differential equation [2, p. 96]. For our purposes,\nhowever, it is more convenient to define Pn(t) as follows.\ninf\nDefinition 4-1. The Legendre polynomials {Pn(t)}n=0 are defined in the interval\n-1 ≤t ≤1. [2, p. 100] They satisfy the recurrence relation\n(n + 1)Pn+1(t) = (2n + 1)tPn(t) -nPn-1(t)\nfor n ≥1,\n(4-1)\nwith P0(t) = 1 and P1(t) = t.\nFrom this definition, it is easy to prove by induction that Pn(t) is a polynomial in t\nof degree n. We now want to find the generating function of Pn(t). In order to avoid\nconfusion in the notation, we denote the generating function of Pn(t) for fixed t as\ninf\nQt(x) =\nPn(t)x n .\n(4-2)\nn=0\n\nX\nX\nX\n\nX\nX\nX\nX\nX\nX\np\nGenerating Functions and Their Applications\nTaking the derivative gives\ninf\nxQ ′\nt(x) =\nnPn(t)x n .\n(4-3)\nn=1\nLet's multiply Eq. (4-1) by xn+1 and sum from n = 1 to n = inf.\ninf\ninf\n(n + 1)Pn+1(t)x n+1 =\n(2n + 1)tPn(t) -nPn-1(t) x n+1\nn=1\nn=1\ninf\ninf\ninf\nmPm(t)x m = 2tx\nnPn(t)x n + x 2\n\ntPk+1(t)x k -(k + 1)Pk(t)x k\n.\n(4-4)\nm=2\nn=1\nk=0\nThe last step follows from the substitutions m = n + 1 and k = n -1.\nWe note from Eqs. (4-2) and (4-3) that\ninf\nmPm(t)x m = xQ ′ (x) -P1(t)x = xQ ′ (x) -tx,\n(4-5)\nt\nt\nm=2\ninf\n(k + 1)Pk(t)x k = xQ ′\nt(x) + Qt(x),\n(4-6)\nk=0\nwhile Proposition2-2 implies\ninf\nx\nPk+1(t)x k = Qt(x) -P0(t) = Qt(x) -1.\n(4-7)\nk=0\nThus, Eq. (4-4) simplifies to\nxQ ′\nt(x) -tx = 2tx2Q ′\nt(x) + tx\n\nQt(x) -1\n\n-x 2\nxQ ′\nt(x) + Qt(x)\n\n,\nQ ′ (x) =\nx -t\nQt(x).\n(4-8)\nt\n-1 -2tx + x2\nBy integrating Eq. (4-8) and imposing the initial condition Qt(0) = P0(t) = 1, we obtain\ninf\nX\nQt(x) =\nPn(t)x n =\n.\n(4-9)\n√\n1 -2tx + x2\nn=0\nTo find the radius of convergence of the power series of Qt(x), we need to find the\nlocation zs (in complex plane) of the singularity nearest to the origin. Qt(zs) is singular\nif\n1 -2tzs + z 2 = 0,\n(4-10)\ns\nzs = t ± i\n1 -t2 ,\n|zs| = 1.\n(4-11)\nTherefore, Q(z) is analytic in the region z < 1, and its power series converges absolutely\n| |\nin this region.\nIn electrostatics, the potential along the z axis due to an azimuthally symmetric\nvolume charge distribution ρ(r, θ) is given by [2, p. 35] (we set 4πoo = 1)\nZ inf\nZ π\n′ 2\nρ(r ′ , θ′ )\nV (z) = 2π\ndr ′ r\ndθ ′ sin θ ′ p\n.\n(4-12)\nz2 -2zr ′ cos θ′ + r ′ 2\n\nZ\nX\nX\n18.104 Term Paper\nIf ρ(r ′ , θ′ ) is bounded, ρ(r ′ , θ′ ) = 0 for r ′ > a, and we are only interested in V (z) for\nz > a, then\nZ a\n′ 2 Z π\nr\nρ(r ′ , θ ′ )\nV (z) = 2π\ndr ′\ndθ ′ sin θ ′\n,\n(4-13)\nz\n√\n1 -2x cos θ′ + x2\nwhere x = r ′ /z < 1.\nNow we may use Eq. (4-9) with t = cos θ′ because |cos θ′ | ≤1. Eq. (4-13) can then\nbe written as\ninf\nX\nqn\nV (z) =\nzn+1 ,\n(4-14)\nn=0\nwith\nZ a\nZ π\nqn = 2π\ndr ′ (r ′ )n+2\ndθ ′ sin θ ′ ρ(r ′ , θ ′ )Pn(cos θ ′ )\nqn =\nd3 r ′ r ′ nρ(r ′ , θ ′ )Pn(cos θ ′ ).\n(4-15)\nThe numbers qn are called the multipole moments. In particular, q0 is the monopole\nmoment (or total charge), and q1 is the dipole moment [2, p. 146]. Since the n-th moment\nterm in the potential falls off as 1/zn+1, the first nonzero moment qn characterizes the\nbehavior of V (z) as z/a →inf.\nWe can see that the generating function of Pn(t) appears naturally in electromag\nnetism. This technique of expanding the potential as a series of \"moments\" is very\nuseful, and is called \"multipole expansion\".\nAn important property of Pn(t) can be shown directly from the generating function\nby considering Qt(-x).\ninf\nX\nQt(-x) =\nPn(t)(-x)n = √\n1 + 2tx + x2 .\n(4-16)\nn=0\nNotice that the right hand side is also equivalent to Q-t(x).\ninf\ninf\n(-1)nPn(t)x n =\nPn(-t)x n .\n(4-17)\nn=0\nn=0\nFrom the uniqueness of the power series of Qt(-x), we obtain Pn(-t) = (-1)nPn(t).\nTherefore Pn(t) is an odd (even) polynomial if and only if n is odd (even).\nAnother property can be obtained by setting t = 1 in Eq. (4-9).\ninf\ninf\nX\nX\nPn(1)x n =\n=\n=\nx n .\n(4-18)\nx2\nx\n√\n1 -2x +\n1 -\nn=0\nn=0\nThus, Pn(1) = 1 for all n. Since Pn(t) is odd for odd n, we also obtain Pn(-1) = (-1)n .\n5. Useful Trick to Find a Generating Function.\nIn Section 2, we saw that we can easily find the generating function of a sequence if\nthat sequence is defined through a linear recurrence. However, in some cases, we may\nnot have a linear recurrence, such as in the Catalan's problem in Section 3. For some\nsequences without a linear recurrence, it is possible to obtain the generating function\nusing a convolution property. In fact, we have actually used this property to solve the\nCatalan's problem.\n\nX\nX\nX\nX X\nX\n\nGenerating Functions and Their Applications\nDefinition 5-1. A convolution of two sequences {fn}inf\nand {gn}inf\nis another se\nn=0\nn=0\nquence denoted by {(f ∗g)n}inf , with\nn=0\nn\n(f ∗g)n =\nfkgn-k.\n(5-1)\nk=0\nTheorem 5-2. Let {un}inf\nand {vn}inf\nbe two sequences with generating func\nn=0\nn=0\ntions u(x) and v(x) respectively. If wn = (u ∗v)n and w(x) is the generating function\nof {wn}inf , then\nn=0\nw(x) = u(x)v(x).\n(5-2)\nThe radius of convergence of w(x) is the minimum of the radii of convergence of u(x)\nand v(x).\nProof: Let r > 0 and s > 0 be the radii of convergence of u(x) and v(x). Denote\nt = min(r, s). Consider the product\ninf\ninf\nu(x)v(x) =\nuix i\nvjxj ,\n(5-3)\ni=0\nj=0\nfor x < t. Since both series converge absolutely, we may rearrange the terms in the\n| |\ndouble summation. Suppose we want to group the same powers of x. We can do this by\nwriting j = n -i, with n ≥0. For each n, i goes from 0 to n, because j is nonnegative.\ninf\nn\ninf\nu(x)v(x) =\nuix i vn-ix n-i =\nwnx n .\n(5-4)\nn=0 i=0\nn=0\nThis is precisely the definition of w(x), and the series on the right hand side converges\nabsolutely for x < t.\n\n| |\nAs we shall see later in this section, it is sometimes more convenient to find a\ngenerating function for {an/n!} instead of {an}. This is the motivation to define the\nexponential generating function.\nDefinition 5-3. The exponential generating function F(x) of a sequence {fn}inf\nis\nn=0\ndefined as\ninf\nX\nxn\nF(x) =\nfn\n.\n(5-5)\nn!\nn=0\nThe exponential generating functions have the following property.\nLemma 5-4. If F(x) is the exponential generating function of a sequence {fn}inf\nn=0,\nthen\ninf\nX\nxn\nfn+1\n= F ′ (x).\n(5-6)\nn!\nn=0\nProof: Differentiate both sides of the definition of F(x) in Eq. (5-5).\ninf\nm-1\ninf\nm-1\ninf\nn\nX\nx\nX\nx\nX\nx\nF ′ (x) =\nmfm\n=\nfm\n=\nfn+1\n.\n(5-7)\nm!\n(m -1)!\nn!\nm=1\nm=1\nn=0\n\n18.104 Term Paper\nWe can state a theorem analogous to Theorem 5-2 for exponential generating func\ntions.\nTheorem 5-5. Let {fn}inf\nand {gn}inf\nbe two sequences with exponential gener\nn=0\nn=0\nating functions F(x) and G(x) respectively. If\nn\nX\nn\nhn =\nfkgn-k,\n(5-8)\nk\nk=0\nand H(x) is the exponential generating function of {hn}inf , then\nn=0\nH(x) = F(x)G(x).\n(5-9)\nThe radius of convergence of H(x) is the minimum of the radii of convergence of F(x)\nand G(x).\nProof: The proof follows the same steps as the proof of Theorem 5-2, by substituting\nun = fn/n! and vn = gn/n!.\n\nWe shall now discuss an example to illustrate the convolution method in a problem\nwhere the exponential generating function is a more convenient choice.\nExample 5-6. (Bell numbers) [6, p. 167]. Denote by bn the number of ways to write\na set of n distinct elements as a union of disjoint subsets, with b0 = 1. For n = 2, there\nare two ways: {a1, a2}, and {a1} ∪{a2}. Find a formula for bn.\nSolution: First we need to find a recurrence relation for bn. Consider a set A of\n(n + 1) distinct elements, A = {a1, a2, . . . , an+1}. Suppose a1 is contained in the first\nsubset along with j other elements, where 0 ≤j ≤n. There are\nn\nways to form this\nj\nsubset, which is the number of ways to pick j elements from {a2, a3, · · · , an+1}. Once\nthe first subset is fixed, we are left with a set S containing (n -j) distinct elements.\nThere are bn-j ways of partitioning S into subsets, and therefore we may write\nn\nn\nX\nn\nX\nn\nbn+1 =\nbn-j =\nbk,\n(5-10)\nj\nk\nj=0\nk=0\nn\nn\ninf\nby using k = n -j and\nk\n=\nn-k . If we define a sequence {tn = 1}n=0, then its\nexponential generating function T (x) is given by\ninf\nX xn\nT (x) =\n= e x .\n(5-11)\nn!\nn=0\nEq. (5-10) can be written as\nn\nX\nn\nwn =\nbktn-k,\n(5-12)\nk\nk=0\nwith wn = bn+1.\nNotice the similarity between Eqs. (5-12) and (5-8). Applying Theorem5-5 on {bn}inf\nn=0\nand {tn}inf , we obtain\nn=0\nB(x)T (x) = W(x)\ninf\ninf\nX\nxn\nX\nxn\nB(x)e x =\nwn\n=\nbn+1\n,\n(5-13)\nn!\nn!\nn=0\nn=0\n\nX\n\nGenerating Functions and Their Applications\nwhere\ninf\nX\nxn\nB(x) =\nbn\n(5-14)\nn!\nn=0\nis the exponential generating function of {bn}inf\nn=0.\nAccording to Lemma 5-4, the right hand side of Eq. (5-13) can be written as B′ (x).\nThus,\nB ′ (x) = e xB(x).\n(5-15)\nWe can integrate Eq. (5-15) and use B(0) = b0 = 1 to find\nB(x) = e e x -1 =\ne e x .\n(5-16)\ne\nSince ex has a power series that converges everywhere, we conclude that B(x) has an\ninfinite radius of convergence. Let's write the power series expansion of B(x).\n\ninf\n\nx\nekx\nB(x) =\ne e =\n1 +\ne\ne\nk!\nk=1\n\ninf\n\ninf\ninf\ninf\ninf\nn\nn\nX 1\n1 X X kn x\n1 X X\nkn-1 x\n=\n1 +\n+\n= 1 +\n.\n(5-17)\ne\nk!\ne\nk! n!\ne\n(k -1)! n!\nk=1\nn=1 k=1\nn=1 k=1\nTherefore, for any natural number n,\ninf\n1 X\nkn-1\nbn =\n.\n(5-18)\ne\n(k -1)!\nk=1\nTo check our answer, take n = 2. One way to find b2 is to sum the infinite series in\nEq. (5-18). However, there is a simpler way if we notice that\ninf\nX\nn-2\nx\nB ′′ (x) =\nn(n -1)bn\n,\n(5-19)\nn!\nn=2\nand thus b2 = B′′ (0). We can find B′′ (x) by differentiating Eq. (5-16) twice.\nB ′ (x) = e x e e x -1 = e e x +x-1 ,\nB ′′ (x) = (1 + e x)e e x +x-1 .\n(5-20)\nTherefore, b2 = 2 as expected. Incidentally, by using Eq. (5-18) for n = 2, we have\nproven the following infinite series,\ninf\nX\nk\n= 2e.\n(5-21)\n(k -1)!\nk=1\nConclusions\nWe have discussed some basic applications of generating functions, as a method to solve\na linear recurrence or combinatorial problems. However, there are certainly many more\naspects in the subject that are not discussed here. Readers interested to learn more are\ninvited to read [5] for a very extensive treatment of generating functions.\n\n18.104 Term Paper\nReferences\n[1] A. P. French, \"Special Relativity,\" W.W. Norton & Company, Inc., 1968.\n[2] J. D. Jackson, \"Classical Electrodynamics,\" Third Edition, Wiley, 1998.\n[3] H. Jeffreys, B. Jeffreys, \"Methods of Mathematical Physics,\" Third Edition, Cam\nbridge University Press, 2000.\n[4] R. Resnick, \"Introduction to Special Relativity,\" Wiley, 1968.\n[5] H. S. Wilf, \"Generatingfunctionology,\" Second Edition, Academic Press, 1994.\n[6] R. M. Young, \"Excursions in Calculus: An Interplay of the Continuous and the\nDiscrete,\" The Mathematical Association of America, 1992."
        },
        {
          "category": "Resource",
          "title": "talk_berke.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/94e07c13c26ce736bd860cc194db4342_talk_berke.pdf",
          "content": "Twin Prime Conjecture\n-Introduction\n-Mertens' Theorems\n-Brun's Conjecture\n-Exercises\n\nIntroduction\n\"Twin Prime\" - Paul Stackel, 1880s\n{p, p+2} equivalently, {6n+1, 6n-1}:\n6x + 0 =/= prime = 6x\n6x +1 = prime\n6x + 2 =/= prime = 2(3x+1)\n6x + 3 =/= prime = 3(2x +1)\n6x + 4 =/= prime = 2(3x +2)\n6x + 5 = prime = 6y - 1\n\nThe Prime Counting Function and the\nTwin Prime Constant\nPrime Counting Function\n:=\nTwin Prime Constant\nTwin Prime Counting Function:\nFormulated by Mertens\nFormulated by Hardy and Littlewood\n\nMertens' Theorems\n\nMertens' Second Theorem\n\nMertens' Theorem 4:\nThis shows that the sum of reciprocals of primes diverge, whereas\nthe reciprocals of twin primes converge\n\nLet\nThen U(t) = 0 for t < 2 and g(t) = O(1) by our assumption\n\nNow we not only know that the reciprocals of\nprimes diverge, but that they diverge like the\nfunction ln(ln(x)).\n\n(This result, which we assumed in the last theorem, actually has an\ninvolved proof using the Brun Sieve technique)\nFun Exercise: How many primes are in an interval?\nWe can first evaluate this by using Euler's\nexpression for the prime counting function.\n\nThis does not mean that the number of primes in an interval\nof length n is equal to the number of primes in the sequential\ninterval of length n. Instead, it means that\n\nConclusion\nThe infinitude of twin primes has not been proven, but current\nwork by Dan Goldston and Cem Yilidrim is focused on a\nformula for the interval between two primes:"
        },
        {
          "category": "Resource",
          "title": "talk_chan.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/4ebddf907ab0739a756195b720f6d23f_talk_chan.pdf",
          "content": "The Sine Product Formula\nThe Sine Product Formula\nand the Gamma Function\nand the Gamma Function\nErica Chan\nErica Chan\n18.104 Presentation\n18.104 Presentation\nDecember 12, 2006\nDecember 12, 2006\n\nOutline\nOutline\nI\nI Introduction\nIntroduction\nI\nI Weierstrass\nWeierstrass'' Product Formula\nProduct Formula\nI\nI Multiplication Formula\nMultiplication Formula\nI\nI Sine and Gamma Functions\nSine and Gamma Functions\nI\nI Applications of Sine Product Formula\nApplications of Sine Product Formula\n\nIntroduction\nIntroduction\n)!\n(\n)\n(\n)\n(\n)1\n(\n)\n-\n=\nΓ\nΓ\n=\n+\nΓ\n=\n(\nΓ\n-\ninf\n-\n∫\nn\nn\nx\nx\nx\ndt\nt\ne\nx\nx\nt\nUseful Formulas:\nUseful Formulas:\n\nIntroduction\nIntroduction\n...\n!7\n!5\n!3\nsin\n+\n-\n+\n-\n=\nx\nx\nx\nx\nx\nMore Useful Formulas:\nMore Useful Formulas:\n\nWeierstrass\nWeierstrass'' Product Formula\nProduct Formula\n)\n(\n)1\n(\n!\nlim\n)\n(\nn\nx\nx\nx\nn\nn\nx\nx\nn\n+\n+\n=\nΓ\ninf\n→\nL\nTheorem (Gauss):\nTheorem (Gauss):\n\nWeierstrass\nWeierstrass'' Product Formula\nProduct Formula\n)\nlog\n(\nlim\n\nwhere\n,\n/\n)\n(\n/\nn\nn\nC\ni\nx\ne\nx\ne\nx\nn\ni\ni\nx\nCx\n-\n+\n+\n+\n=\n+\n=\nΓ\ninf\n→\ninf\n=\n-\n∏\nL\nWeierstrass\nWeierstrass'' Product Formula\nProduct Formula\n\nMultiplication Formula\nMultiplication Formula\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n)\n(\n/\n/)\n(\n+\nΓ\nΓ\n=\nΓ\n-\n+\nΓ\n+\nΓ\nΓ\n=\nΓ\n-\n-\n-\nx\nx\nx\np\np\nx\np\nx\np\nx\nx\np\nx\nx\np\nπ\nπ\nL\nGauss\nGauss'' Multiplication Formula\nMultiplication Formula\nLegendre\nLegendre''s Relation, where\ns Relation, where pp=2\n=2\n\nSine and Gamma Functions\nSine and Gamma Functions\n).\n(\n)1\n(\nthen\n,\nsin\n)\n1(\n)\n(\n)\n(\n:\nDefine\nx\nx\nx\nx\nx\nx\nφ\nφ\nπ\nφ\n=\n+\n-\nΓ\nΓ\n=\n\nSine and Gamma Functions\nSine and Gamma Functions\nProof:\nProof:\n)\n(\nsin\n)1\n(\n)\n(\n)1\n(\n)\nsin\n(\n)1\n(\n)\n(\n)1\n(\n))\n(\nsin(\n)\n(\n)1\n(\n)1\n(\n)1\n(\n)\n(\n)\n(\n)1\n(\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nφ\nπ\nφ\nπ\nφ\nπ\nφ\n=\n+\n-\nΓ\nΓ\n=\n+\n-\n-\n+\n-\nΓ\nΓ\n=\n+\n+\n-\nΓ\n+\nΓ\n=\n+\n-\n+\n-\nΓ\n=\n-\nΓ\n-\nΓ\n-\n=\n+\n-\nΓ\n\nSine and Gamma Functions\nSine and Gamma Functions\n)\n1(\n)\n(\n)\n1(\n)\n(\n)\n(\n)\n(\nx\nx\nx\nb\nx\nx\nx\nb\nx\nx\n-\nΓ\n-\nΓ\n=\n-\nΓ\n+\nΓ\nΓ\n=\nΓ\n-\n-\n\nSine and Gamma Functions\nSine and Gamma Functions\n)\n(\nsin\n)\n1(\n)\n(\ncos\n)\n(\n)\n(\nsin\n)\n1(\n)\n(\n)\n(\n)\n(\nx\nb\nx\nx\nx\nb\nx\nx\nx\nx\nx\nx\nx\nx\nφ\nπ\nπ\nπ\nφ\nφ\n=\n-\nΓ\nΓ\n=\n-\nΓ\n+\nΓ\n-\nΓ\nΓ\n=\n+\n\nSine and Gamma Functions\nSine and Gamma Functions\nπ\nφ\nπ\nπ\nπ\nπ\nπ\nπ\nπ\nπ\nπ\nφ\n=\n+\n-\n+\n-\n-\nΓ\n+\nΓ\n=\n+\n-\n+\n-\n-\nΓ\n+\nΓ\n=\n-\nΓ\nΓ\n=\n)\n(\n)\n!7\n!5\n!3\n)(\n1(\n)\n1(\n)\n!7\n!5\n!3\n)(\n1(\n)\n1(\nsin\n)\n1(\n)\n(\n)\n(\nL\nL\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\nx\n\nSine and Gamma Functions\nSine and Gamma Functions\nDefine g(x) to be a periodic function, which\nDefine g(x) to be a periodic function, which\nis the second derivative of log(phi(x)).\nis the second derivative of log(phi(x)).\nIt is bounded and the bound of g(x) goes to 0,\nIt is bounded and the bound of g(x) goes to 0,\nso g(x)=0 and log(phi(x)) is linear.\nso g(x)=0 and log(phi(x)) is linear.\nSince log(phi(x)) is periodic, it must be\nSince log(phi(x)) is periodic, it must be\nconstant.\nconstant.\nTherefore phi(x) is constant and equals pi for\nTherefore phi(x) is constant and equals pi for\nall x.\nall x.\n\nSine and Gamma Functions\nSine and Gamma Functions\n)\n(\n)\n(\nsin\nsin\n)\n1(\n)\n(\n,\n)\n(\n\nSince\nx\nx\nx\nx\nx\nx\nx\nx\n-\nΓ\nΓ\n-\n=\n=\n-\nΓ\nΓ\n=\nπ\nπ\nπ\nπ\nπ\nφ\n\nSine and Gamma Functions\nSine and Gamma Functions\n)\n1(\nsin\n∏\ninf\n=\n-\n=\ni\ni\nx\nx\nx\nπ\nπ\nSine Product Formula\nSine Product Formula\n\nApplications\nApplications\n)\n(\n)\n(\n)\n(\nπ\nζ\nπ\nζ\nζ\n=\n=\n+\n+\n+\n+\n=\n= ∑\ninf\n=\nL\ns\ns\ns\nn\nsn\ns\nRiemann Zeta Function:\nRiemann Zeta Function:\n\nApplications\nApplications\n)\n(\n=\n+\n=\n=\n∑\n∑\n∑\n∑\n=\ninf\n=\ninf\n=\ninf\n=\nj\ni\ni\ni\ni\nj\ni\ni\ni\ni\nπ\nπ\nπ\nπ\nFrom zeta(2):\nFrom zeta(2):\n\nApplications\nApplications\n!5\n)\n)(\n)(\n1(\nsin\n=\n=\n-\n-\n-\n=\n∑\n=j\ni\nj\ni\nx\nx\nx\nx\nx\nπ\nπ\nπ\nπ\nL\nUsing the Sine Product Formula:\nUsing the Sine Product Formula:\nThe coefficient of x\nThe coefficient of x44::\n\nApplications\nApplications\n)\n(\n)\n(\n)\n(\nπ\nζ\nπ\nπ\nπ\nπ\n=\n=\n+\n=\n+\n=\n∑\n∑\n∑\n∑\n∑\ninf\n=\ninf\n=\n=\ninf\n=\ninf\n=\ni\ni\nj\ni\ni\ni\ni\ni\nj\ni\ni\ni"
        },
        {
          "category": "Resource",
          "title": "talk_gilbertson.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/601f9538677155d1a53e7482c41daac8_talk_gilbertson.pdf",
          "content": "Stirling's Formula: an\nApproximation of the Factorial\nEric Gilbertson\n\nOutline\n- Introduction of formula\n- Convex and log convex functions\n- The gamma function\n- Stirling's formula\n\nOutline\n- Introduction of formula\n- Convex and log convex functions\n- The gamma function\n- Stirling's formula\n\nIntroduction of Formula\nIn the early 18th century James Stirling proved the\nfollowing formula:\nFor some\nn\nn\nn\ne\nn\nn\n/\n/\n!\nθ\nπ\n+\n-\n+\n=\n<\n< θ\n\nIntroduction of Formula\nIn the early 18th century James Stirling proved the\nfollowing formula:\nFor some\nThis means that as\nn\nn\nn\ne\nn\nn\n/\n/\n!\nθ\nπ\n+\n-\n+\n=\n<\n< θ\nn\nn\ne\nn\nn\n-\n+\n/\n~\n!\nπ\ninf\n→\nn\n\nOutline\n- Introduction of formula\n- Convex and log convex functions\n- The gamma function\n- Stirling's formula\n\nConvex Functions\nA function f(x) is called convex on the interval (a,b)\nif the function\nIs a monotonically increasing function of on the\ninterval\n)\n(\n)\n(\n)\n,\n(\nx\nx\nx\nf\nx\nf\nx\nx\n-\n-\n=\nφ\n1x\n\n2x\nA function f(x) is called convex on the interval (a,b) if\nthe function\nIs a monotonically increasing function of on the\ninterval\nEx: is convex\nis not convex on the interval (-1,0)\nConvex Functions\n)\n(\n)\n(\n)\n,\n(\nx\nx\nx\nf\nx\nf\nx\nx\n-\n-\n=\nφ\n1x\n3x\n\nConvex Functions - Properties\nIf f(x) and g(x) are convex then f(x)+g(x) is\nalso convex\nIf f(x) is twice differentiable and the second\nderivative of f is positive on an interval,\nthen f(x) is convex on the interval\n\nLog Convex Functions\nA positive-valued function f(x) is called log convex on\nthe interval (a,b) if the function is convex\non the interval.\n)\n(\nln\nx\nf\n\nLog Convex Functions\nA positive-valued function f(x) is called log convex on\nthe interval (a,b) if the function is convex\non the interval.\nEx: is log convex\n)\n(\nln\nx\nf\nxe\n\nLog Convex Functions -\nProperties\nThe product of log convex functions is log\nconvex\n\nLog Convex Functions -\nProperties\nThe product of log convex functions is log convex\nIf f(t,x) is a log convex function twice differentiable in\nx, for t in the interval [a,b] and x in any interval then\nis a log convex function of x\n∫\nb\na\ndt\nx\nt\nf\n)\n,\n(\n\nLog Convex Functions -\nProperties\nThe product of log convex functions is log convex\nIf f(t,x) is a log convex function twice differentiable in\nx, for t in the interval [a,b] and x in any interval then\nis a log convex function of x\nEx: is log convex\ndt\nt\ne\nb\na\nx\nt\n∫\n-\n-\n∫\nb\na\ndt\nx\nt\nf\n)\n,\n(\n\nOutline\n- Introduction of formula\n- Convex and log convex functions\n- The gamma function\n- Stirling's formula\n\nThe Gamma Function\nAn extension of the factorial to all positive real\nnumbers is the gamma function where\n∫\ninf\n-\n-\n=\nΓ\n)\n(\ndt\nt\ne\nx\nx\nt\n\nThe Gamma Function\nAn extension of the factorial to all positive real\nnumbers is the gamma function where\nUsing integration by parts, for integer n\n∫\ninf\n-\n-\n=\nΓ\n)\n(\ndt\nt\ne\nx\nx\nt\n)!\n(\n)\n(\n-\n=\nΓ\nn\nn\n\nThe Gamma Function -\nProperties\n-\nThe Gamma function is log convex\nthe integrand is twice differentiable on\n)\n,0\n[\ninf\n\nThe Gamma Function -\nProperties\n-\nThe Gamma function is log convex\nthe integrand is twice differentiable on\n-\nAnd\n)\n,0\n[\ninf\n)1(\n=\n=\nΓ\n∫\ninf\n-dt\ne t\n\nThe Gamma Function -\nUniqueness\nTheorem: If a function f(x) satisfies the\nfollowing three conditions then it is\nidentical to the gamma function.\n\nThe Gamma Function -\nUniqueness\nTheorem: If a function f(x) satisfies the\nfollowing three conditions then it is\nidentical to the gamma function.\n(1) f(x+1) = f(x)\n\nThe Gamma Function -\nUniqueness\nTheorem: If a function f(x) satisfies the\nfollowing three conditions then it is\nidentical to the gamma function.\n(1) f(x+1) = f(x)\n(2) The domain of f contains all x>0 and f(x)\nis log convex\n\nThe Gamma Function -\nUniqueness\nTheorem: If a function f(x) satisfies the\nfollowing three conditions then it is\nidentical to the gamma function.\n(1) f(x+1) = f(x)\n(2) The domain of f contains all x>0 and f(x)\nis log convex\n(3) f(1)=1\n\nThe Gamma Function -\nUniqueness\nProof: Suppose f(x) satisfies the three\nproperties. Then since f(1)=1 and\nf(x+1)=xf(x), for integer n≥2,\nf(x+n)=(x+n-1)(x+n-2)...(x+1)xf(x)\nf(n) = (n-1)!\n\nThe Gamma Function -\nUniqueness\nProof: suppose f(x) satisfies the three\nproperties. Then since f(1)=1 and\nf(x+1)=xf(x), for integer n ≥2,\nf(x+n)=(x+n-1)(x+n-2)...(x+1)xf(x).\nf(n) = (n-1)!\nNow if we can show Γ(x) and f(x) agree on\n[0,1], then by these properties they agree\neverywhere.\n\nThe Gamma Function -\nUniqueness\nBy property (2) f(x) is log convex, so by definition of\nconvexity\nn\nn\nn\nf\nn\nf\nn\nn\nx\nn\nf\nn\nx\nf\nn\nn\nn\nf\nn\nf\n-\n+\n-\n+\n≤\n-\n+\n-\n+\n≤\n-\n+\n-\n-\n+\n-\n)\n1(\n)\n(\nln\n)\n1(\nln\n)\n(\n)\n(\nln\n)\n(\nln\n)\n(\n)\n(\nln\n)\n(\nln\n\nThe Gamma Function -\nUniqueness\nBy property (2) f(x) is log convex, so by definition of\nconvexity\nn\nn\nn\nf\nn\nf\nn\nn\nx\nn\nf\nn\nx\nf\nn\nn\nn\nf\nn\nf\n-\n+\n-\n+\n≤\n-\n+\n-\n+\n≤\n-\n+\n-\n-\n+\n-\n)\n1(\n)\n(\nln\n)\n1(\nln\n)\n(\n)\n(\nln\n)\n(\nln\n)\n(\n)\n(\nln\n)\n(\nln\nn\nx\nn\nn\nx\nf\nn\nln\n)!\nln(\n)\n(\nln\n)1\nln(\n≤\n-\n-\n+\n≤\n-\n\nThe Gamma Function -\nUniqueness\nBy property (2) f(x) is log convex, so by definition of\nconvexity\nn\nn\nn\nf\nn\nf\nn\nn\nx\nn\nf\nn\nx\nf\nn\nn\nn\nf\nn\nf\n-\n+\n-\n+\n≤\n-\n+\n-\n+\n≤\n-\n+\n-\n-\n+\n-\n)\n1(\n)\n(\nln\n)\n1(\nln\n)\n(\n)\n(\nln\n)\n(\nln\n)\n(\n)\n(\nln\n)\n(\nln\nn\nx\nn\nn\nx\nf\nn\nln\n)!\nln(\n)\n(\nln\n)1\nln(\n≤\n-\n-\n+\n≤\n-\n]\n)!\n(\nln[\n)\n(\nln\n]\n)!\n(\n)1\nln[(\n-\n≤\n+\n≤\n-\n-\nn\nn\nn\nx\nf\nn\nn\nx\nx\n\nThe Gamma Function -\nUniqueness\nBy property (2) f(x) is log convex, so by definition of\nconvexity\nThe logarithm is monotonic so\nn\nn\nn\nf\nn\nf\nn\nn\nx\nn\nf\nn\nx\nf\nn\nn\nn\nf\nn\nf\n-\n+\n-\n+\n≤\n-\n+\n-\n+\n≤\n-\n+\n-\n-\n+\n-\n)\n1(\n)\n(\nln\n)\n1(\nln\n)\n(\n)\n(\nln\n)\n(\nln\n)\n(\n)\n(\nln\n)\n(\nln\nn\nx\nn\nn\nx\nf\nn\nln\n)!\nln(\n)\n(\nln\n)1\nln(\n≤\n-\n-\n+\n≤\n-\n]\n)!\n(\nln[\n)\n(\nln\n]\n)!\n(\n)1\nln[(\n-\n≤\n+\n≤\n-\n-\nn\nn\nn\nx\nf\nn\nn\nx\nx\n)!\n(\n)\n(\n)!\n(\n)1\n(\n-\n≤\n+\n≤\n-\n-\nn\nn\nn\nx\nf\nn\nn\nx\nx\n\nThe Gamma Function -\nUniqueness\nSince f(x+n)=x(x+1)...(x+n-1)f(x) then\n)!\n(\n)\n(\n)1\n)...(\n(\n)!\n(\n)1\n(\n-\n≤\n-\n+\n+\n≤\n-\n-\nn\nn\nx\nf\nn\nx\nx\nx\nn\nn\nx\nx\n\nThe Gamma Function -\nUniqueness\nSince f(x+n)=x(x+1)...(x+n-1)f(x) then\n)!\n(\n)\n(\n)1\n)...(\n(\n)!\n(\n)1\n(\n-\n≤\n-\n+\n+\n≤\n-\n-\nn\nn\nx\nf\nn\nx\nx\nx\nn\nn\nx\nx\nn\nn\nx\nx\nn\nx\nn\nn\nn\nx\nx\nx\nn\nn\nx\nf\nx\nx\nn\nx\nx\nx\nn\nn\nx\n)\n...(\n)\n(!\n)1\n)...(\n(\n)!\n(\n)\n(\n)\n)...(\n(\n)!\n(\n)\n(\n+\n+\n=\n-\n+\n+\n-\n≤\n≤\n-\n+\n+\n-\n-\n\nThe Gamma Function -\nUniqueness\nSince f(x+n)=x(x+1)...(x+n-1)f(x) then\nSince this holds for n ≥2, we can replace n by n+1 on\nthe right. Then\n)!\n(\n)\n(\n)1\n)...(\n(\n)!\n(\n)1\n(\n-\n≤\n-\n+\n+\n≤\n-\n-\nn\nn\nx\nf\nn\nx\nx\nx\nn\nn\nx\nx\nn\nn\nx\nx\nn\nx\nn\nn\nn\nx\nx\nx\nn\nn\nx\nf\nx\nx\nn\nx\nx\nx\nn\nn\nx\n)\n...(\n)\n(!\n)1\n)...(\n(\n)!\n(\n)\n(\n)\n)...(\n(\n)!\n(\n)\n(\n+\n+\n=\n-\n+\n+\n-\n≤\n≤\n-\n+\n+\n-\n-\nn\nn\nx\nx\nx\nn\nx\nn\nn\nx\nf\nn\nx\nx\nx\nn\nn\nx\nx\n)\n)...(\n(\n)\n(!\n)\n(\n)\n)...(\n(\n!\n+\n+\n+\n≤\n≤\n+\n+\n\nThe Gamma Function -\nUniqueness\nThis simplifies to\n)\n(\n)\n)...(\n(\n!\n)\n(\nx\nf\nn\nx\nx\nx\nn\nn\nn\nx\nn\nx\nf\nx\n≤\n+\n+\n≤\n+\n\nThe Gamma Function -\nUniqueness\nThis simplifies to\nAs n goes to infinity,\n)\n(\n)\n)...(\n(\n!\n)\n(\nx\nf\nn\nx\nx\nx\nn\nn\nn\nx\nn\nx\nf\nx\n≤\n+\n+\n≤\n+\n)\n)...(\n(\n!\n)\n(\nlim\nn\nx\nx\nx\nn\nn\nx\nf\nx\nn\n+\n+\n=\ninf\n→\n\nThe Gamma Function -\nUniqueness\nThis simplifies to\nAs n goes to infinity,\nSince Gamma(x) satisfies the 3 properties of the\ntheorem, then it satisfies this limit also.\nThus Γ(x) = f(x)\n)\n)...(\n(\n!\n)\n(\n)\n(\n)\n)...(\n(\n!\n)\n(\nlim\nn\nx\nx\nx\nn\nn\nx\nf\nx\nf\nn\nx\nx\nx\nn\nn\nn\nx\nn\nx\nf\nx\nn\nx\n+\n+\n=\n≤\n+\n+\n≤\n+\ninf\n→\n)\n(\n)\n)...(\n(\n!\n)\n(\nx\nf\nn\nx\nx\nx\nn\nn\nn\nx\nn\nx\nf\nx\n≤\n+\n+\n≤\n+\n)\n)...(\n(\n!\n)\n(\nlim\nn\nx\nx\nx\nn\nn\nx\nf\nx\nn\n+\n+\n=\ninf\n→\n\nOutline\n- Introduction of formula\n- Convex and log convex functions\n- The gamma function\n- Stirling's formula\n\nStirling's Formula\nGoal: Find upper and lower bounds for Γ(x)\n\nStirling's Formulas\nGoal: Find upper and lower bounds for Γ(x)\nFrom the definition of e, for k=1,2,...,(n-1)\n)\n/\n1(\n)\n/\n1(\n+\n+\n≤\n≤\n+\nk\nk\nk\ne\nk\n\nStirling's Formulas\nGoal: Find upper and lower bounds for Gamma(x)\nFrom the definition of e, for k=1,2,...,(n-1)\nMultiply all of these together to get\n)\n/\n1(\n)\n/\n1(\n+\n+\n≤\n≤\n+\nk\nk\nk\ne\nk\n...\n)\n(\n)\n(\n...\n)\n(\n)\n(\n-\n-\n-\n-\n-\n-\n-\n≤\n≤\n-\n-\n-\nn\nn\nn\nn\nn\nn\nn\nn\nn\ne\nn\nn\nn\nn\n\nStirling's Formula\nSimplifying,\nn\nn\nn\nn\ne\nen\nn\ne\nen\n-\n+\n-\n≤\n≤\n!\n\nStirling's Formula\nSimplifying,\nn\nn\nn\nn\ne\nen\nn\ne\nen\n-\n+\n-\n≤\n+\nΓ\n≤\n⇒\n)1\n(\nn\nn\nn\nn\ne\nen\nn\ne\nen\n-\n+\n-\n≤\n≤\n!\n\nStirling's Formula\nSimplifying,\nGuess a function to approximate Γ(x)\nn\nn\nn\nn\ne\nen\nn\ne\nen\n-\n+\n-\n≤\n+\nΓ\n≤\n⇒\n)1\n(\n)\n(\n/\n)\n(\nx\nx\nx\ne\ne\nx\nx\nf\nμ\n-\n-\n=\nn\nn\nn\nn\ne\nen\nn\ne\nen\n-\n+\n-\n≤\n≤\n!\n\nStirling's Formula\nμ(x) must be chosen so that\n(1) f(x+1)=xf(x)\n(2) f(x) is log convex\n(3) f(1)=1\n\nStirling's Formula\nμ(x) must be chosen so that\n(1) f(x+1)=xf(x)\n(2) f(x) is log convex\n(3) f(1)=1\n(1) Is equivalent to f(x+1)/f(x)=x\n\nStirling's Formulas\nSimplifying f(x+1)/f(x) gives\nx\ne\ne\nx\nx\nx\nx\n=\n+\n-\n+\n-\n+\n)\n(\n)\n(\n/\n)\n/\n1(\nμ\nμ\n\nStirling's Formulas\nSimplifying f(x+1)/f(x) gives\n)\n(\n)\n/\nln(\n)\n/\n(\n)\n(\n)1\n(\nx\ng\nx\nx\nx\nx\n≡\n-\n+\n+\n=\n-\n+\n⇒\nμ\nμ\nx\ne\ne\nx\nx\nx\nx\n=\n+\n-\n+\n-\n+\n)\n(\n)\n(\n/\n)\n/\n1(\nμ\nμ\n\nStirling's Formulas\nSimplifying f(x+1)/f(x) gives\nThen satisfies the equation\n)\n(\n)\n/\nln(\n)\n/\n(\n)\n(\n)1\n(\nx\ng\nx\nx\nx\nx\n≡\n-\n+\n+\n=\n-\n+\n⇒\nμ\nμ\n∑\ninf\n=\n+\n=\n)\n(\n)\n(\nn\nn\nx\ng\nx\nμ\n)\n(\n)\n(\n)1\n(\nx\ng\nn\nx\ng\nn\nx\ng\nn\nn\n=\n+\n-\n+\n+\n∑\n∑\ninf\n=\ninf\n=\nx\ne\ne\nx\nx\nx\nx\n=\n+\n-\n+\n-\n+\n)\n(\n)\n(\n/\n)\n/\n1(\nμ\nμ\n\nStirling's Formula\nConsider the Taylor expansion\nwith\n...\n]\nln[\n/\n+\n+\n+\n=\n-\n+\ny\ny\ny\ny\ny\n+\n=\nx\ny\n\nStirling's Formula\nConsider the Taylor expansion\nwith\n...\n]\nln[\n/\n+\n+\n+\n=\n-\n+\ny\ny\ny\ny\ny\n+\n=\nx\ny\n...\n)1\n(3\n)\n/\nln(\n/\n3 +\n+\n+\n+\n=\n+\n⇒\nx\nx\nx\n\nStirling's Formula\nConsider the Taylor expansion\nwith\n...\n]\nln[\n/\n+\n+\n+\n=\n-\n+\ny\ny\ny\ny\ny\n+\n=\nx\ny\n...\n)1\n(3\n)\n/\nln(\n/\n3 +\n+\n+\n+\n=\n+\n⇒\nx\nx\nx\n...\n)1\n(\n)1\n(3\n)\n(\n)\n/\nln(\n)\n/\n(\n+\n+\n+\n+\n=\n=\n-\n+\n+\n⇒\nx\nx\nx\ng\nx\nx\n\nStirling's Formula\nConsider the Taylor expansion\nwith\nReplacing the denominators with all 3's gives\n...\n]\nln[\n/\n+\n+\n+\n=\n-\n+\ny\ny\ny\ny\ny\n+\n=\nx\ny\n...\n)1\n(3\n)\n/\nln(\n/\n3 +\n+\n+\n+\n=\n+\n⇒\nx\nx\nx\n...\n)1\n(\n)1\n(3\n)\n(\n)\n/\nln(\n)\n/\n(\n+\n+\n+\n+\n=\n=\n-\n+\n+\n⇒\nx\nx\nx\ng\nx\nx\n...\n)1\n(3\n)1\n(3\n)\n(\n+\n+\n+\n+\n≤\nx\nx\nx\ng\n\nStirling's Formula\ng(x) is a geometric series, thus\n)1\n(\n)1\n(\n)1\n(3\n)\n(\n+\n-\n=\n+\n-\n+\n≤\nx\nx\nx\nx\nx\ng\n\nStirling's Formula\ng(x) is a geometric series, thus\n))1\n(\n)\n(\n(\n)\n(\n)\n(\n+\n+\n-\n+\n≤\n+\n=\n∑\n∑\ninf\n=\ninf\n=\nn\nx\nn\nx\nn\nx\ng\nx\nn\nn\nμ\n)1\n(\n)1\n(\n)1\n(3\n)\n(\n+\n-\n=\n+\n-\n+\n≤\nx\nx\nx\nx\nx\ng\n\nStirling's Formula\ng(x) is a geometric series, thus\n))1\n(\n)\n(\n(\n)\n(\n)\n(\n+\n+\n-\n+\n≤\n+\n=\n∑\n∑\ninf\n=\ninf\n=\nn\nx\nn\nx\nn\nx\ng\nx\nn\nn\nμ\n)1\n(\n)1\n(\n)1\n(3\n)\n(\n+\n-\n=\n+\n-\n+\n≤\nx\nx\nx\nx\nx\ng\nx\n=\n\nStirling's Formula\nSince g(x) is positive then\nx\nx\n)\n(\n<\n< μ\n\nStirling's Formula\nSince g(x) is positive then\nfor some\nx\nx\n)\n(\n<\n< μ\n⇒\nx\nx\n)\n(\nθ\nμ\n=\n<\n< θ\n\nStirling's Formula\nSince g(x) is positive then\nfor some\nFor property (2) we need to show f(x) is log convex\nx\nx\n)\n(\n<\n< μ\n⇒\nx\nx\n)\n(\nθ\nμ\n=\n<\n< θ\n\nStirling's Formula\nSince g(x) is positive then\nfor some\nFor property (2) we need to show f(x) is log convex\nThis means\nmust be convex.\nx\nx\n)\n(\n<\n< μ\n⇒\nx\nx\n)\n(\nθ\nμ\n=\n<\n< θ\n)\n(\nln\n)\n(\n)\n(\nln\nx\nx\nx\nx\nx\nf\nμ\n+\n-\n-\n=\n\nStirling's Formula\nis convex because g(x) is the sum of convex\nfunctions\n)\n(x\nμ\n\nStirling's Formula\nis convex because g(x) is the sum of convex\nfunctions\nis convex since its second\nderivative is positive for positive x.\n)\n(x\nμ\nx\nx\nx\n-\n-\nln\n)\n/\n(\nx\nx +\n\nStirling's Formula\nis convex because g(x) is the sum of convex\nfunctions\nis convex since its second\nderivative is positive for positive x.\nThus f(x) is log convex.\n)\n(x\nμ\nx\nx\nx\n-\n-\nln\n)\n/\n(\nx\nx +\n\nStirling's Formulas\nFor property (3) we need f(1)=1 so that\nx\nx\nx\ne\nax\nx\n/\n/\n)\n(\nθ\n+\n-\n-\n=\nΓ\n\nStirling's Formulas\nFor property (3) we need f(1)=1 so that\nConsider the function\n)\n(\n)\n/\n(\n)\n(\n+\nΓ\nΓ\n=\nx\nx\nx\nh\nx\nx\nx\nx\ne\nax\nx\n/\n/\n)\n(\nθ\n+\n-\n-\n=\nΓ\n\nStirling's Formulas\nFor property (3) we need f(1)=1 so that\nConsider the function\nh(x) is log convex since the second derivative of\nis nonnegative and the gamma function is log convex.\n)\n(\n)\n/\n(\n)\n(\n+\nΓ\nΓ\n=\nx\nx\nx\nh\nx\nx\nx\nx\ne\nax\nx\n/\n/\n)\n(\nθ\n+\n-\n-\n=\nΓ\nx\nln\n\nStirling's Formula\n)1\n/\n(\n)\n(\n)1\n(\n+\nΓ\n+\nΓ\n=\n+\n+\nx\nx\nx\nh\nx\n\nStirling's Formula\n)1\n/\n(\n)\n(\n)1\n(\n+\nΓ\n+\nΓ\n=\n+\n+\nx\nx\nx\nh\nx\n)\n(\n)\n/\n(\n+\nΓ\nΓ\n=\nx\nx\nx\nx\n\nStirling's Formula\n)1\n/\n(\n)\n(\n)1\n(\n+\nΓ\n+\nΓ\n=\n+\n+\nx\nx\nx\nh\nx\n)\n(\n)\n/\n(\n+\nΓ\nΓ\n=\nx\nx\nx\nx\n)\n(\n)1\n(\nx\nxh\nx\nh\n=\n+\n⇒\n\nStirling's Formula\n)1\n/\n(\n)\n(\n)1\n(\n+\nΓ\n+\nΓ\n=\n+\n+\nx\nx\nx\nh\nx\n)\n(\n)\n/\n(\n+\nΓ\nΓ\n=\nx\nx\nx\nx\n)\n(\n)1\n(\nx\nxh\nx\nh\n=\n+\n⇒\nThis means h(x) satisfies properties (1) and (2) of\nthe uniqueness theorem, thus\n)\n(\n)\n(\nx\na\nx\nh\nΓ\n=\nfor some constant\n2a\n\nStirling's Formula\nSetting x=1 gives\n)1(\n)1(\n)\n/\n1(\n)1(\n2Γ\n=\nΓ\nΓ\n=\na\nh\n\nStirling's Formula\nSetting x=1 gives\n)1(\n)1(\n)\n/\n1(\n)1(\n2Γ\n=\nΓ\nΓ\n=\na\nh\n)1(\n)\n/\n1(\nΓ\nΓ\n=\n⇒a\n\nStirling's Formula\nSetting x=1 gives\nUsing the limit equation for the gamma function\n)1(\n)1(\n)\n/\n1(\n)1(\n2Γ\n=\nΓ\nΓ\n=\na\nh\n)1(\n)\n/\n1(\nΓ\nΓ\n=\n⇒a\n)!\n(\n!\nlim\n/\n)1(\n)\n/\n1(\n+\n+\n=\nΓ\nΓ\ninf\n→\nn\nn\nn\nn\nn\n\nStirling's Formula\n/\n)!\n(\n)!\n(\nlim\nn\nn\nn\na\nn\nn\ninf\n→\n=\n\nStirling's Formula\n/\n)!\n(\n)!\n(\nlim\nn\nn\nn\na\nn\nn\ninf\n→\n=\n/\n/\n/\n/\n]\n)\n(\n[\n]\n[\nlim\nn\ne\ne\nn\na\ne\ne\nn\na\nn\nn\nn\nn\nn\nn\nn\nn\nθ\nθ\n-\n+\n-\n+\ninf\n→\n=\n\nStirling's Formula\n/\n)!\n(\n)!\n(\nlim\nn\nn\nn\na\nn\nn\ninf\n→\n=\n/\n/\n/\n/\n]\n)\n(\n[\n]\n[\nlim\nn\ne\ne\nn\na\ne\ne\nn\na\nn\nn\nn\nn\nn\nn\nn\nn\nθ\nθ\n-\n+\n-\n+\ninf\n→\n=\nn\nn\nn\ne\na\n/\n/\nlim\nθ\nθ\n-\ninf\n→\n=\n\nStirling's Formula\n/\n)!\n(\n)!\n(\nlim\nn\nn\nn\na\nn\nn\ninf\n→\n=\n/\n/\n/\n/\n]\n)\n(\n[\n]\n[\nlim\nn\ne\ne\nn\na\ne\ne\nn\na\nn\nn\nn\nn\nn\nn\nn\nn\nθ\nθ\n-\n+\n-\n+\ninf\n→\n=\nn\nn\nn\ne\na\n/\n/\nlim\nθ\nθ\n-\ninf\n→\n=\n)\n/\n1(\na\na\n=\nΓ\n=\n⇒\n\nStirling's Formula\n/\n)!\n(\n)!\n(\nlim\nn\nn\nn\na\nn\nn\ninf\n→\n=\n/\n/\n/\n/\n]\n)\n(\n[\n]\n[\nlim\nn\ne\ne\nn\na\ne\ne\nn\na\nn\nn\nn\nn\nn\nn\nn\nn\nθ\nθ\n-\n+\n-\n+\ninf\n→\n=\nn\nn\nn\ne\na\n/\n/\nlim\nθ\nθ\n-\ninf\n→\n=\n)\n/\n1(\na\na\n=\nΓ\n=\n⇒\nπ\n)\n/\n1(\n=\nΓ\n=\n⇒a\n\nStirling's Formula\nSubstituting a in the formula for f gives Stirling's\nfinal result:\nfor some\nn\nn\nn\ne\nn\nn\n/\n/\n!\nθ\nπ\n+\n-\n+\n=\n<\n< θ"
        },
        {
          "category": "Resource",
          "title": "talk_peter_s.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/6785d40d8b72a2019dd123b56c15908c_talk_peter_s.pdf",
          "content": "The Golden Ratio\nAgustinus Peter Sahanggamu\nNovember 30, 2006\n\nOutline\n3⁄4Geometric Definition\n3⁄4Relation with Fibonacci Numbers\n3⁄4Euclidean Geometric Construction\n3⁄4Continuous Fraction Representation\n\nGeometric Definition\n(Mean and Extreme Ratio)\n3⁄4\nsatisfies\n3⁄4Golden ratio = positive root = τ =\n3⁄4Negative root = 1 - τ = μ =\nx2 - x - 1 = 0\n1+\n√\n1-\n√\n1 - x\nx\n\nRelation with Fibonacci Numbers\n3⁄4Binet's Formula\n3⁄4\n3⁄4\nsince\nfn = (τ n - μn)/\n√\nτ = lim\nn→inf\nfn+1\nfn\nfn+1\nfn\n= τ n+1 - μn+1\nτ n - μn\n|τ /μ| > 1\n\n3⁄4Construct a right triangle with sides\n3⁄4\nand\n3⁄4Add the hypotenuse and shortest side\nGeometric Construction\n√\n√\n\nContinuous Fraction\nRepresentation\n3⁄4\nτ = 1 +\n1 +\n1+\n1+···\n3⁄4 τ = lim un, where un+1 = 1 +\nn→inf\nun\n3⁄4and u1 = 1\n3⁄4Let un = an+1/an with a1 = a2 = 1\n3⁄4Recursion of an is an+2 = an+1 + an\n3⁄4{an} = Fibonacci numbers\n\nInfinite Resistor Network\n3⁄4Each resistor has resistance\n3⁄4Total resistance =\n= ?\n3⁄4Recall\n1Ω\nA\nB\nr\na + b\nab\na+b\nTotal =\nTotal =\na\na\nb\nb\n\nInfinite Resistor Network\n(continued)\n3⁄4\nrn+1\nrn\nrn+1 = 1 +\n1 + 1\nrn\nr = 1 +\n1 +\n1+\n1+···\n= τ\n\nExercise on Continued Fractions\n(Young, Problem 9, page 156)\nb\n3⁄4Find p = 2a + 2a +\nb\n2a+···\nwith a, b positive integers\n3⁄4p = lim pn with p1 = 2a and\nn→inf\npn+1 = 2a + p\nb\nn\n3⁄4Define\npn = un+1/un, u1 = 1, u2 = 2a\n\nExercise on Continued Fractions\n(continued)\n3⁄4\nun+2 - 2aun+1 - bun = 0\n3⁄4Basis of solutions: un = λn\nλ2 - 2aλ - b = 0\n3⁄4α = a +\n√\na2 + b, β = a -\n√\na2 + b\n3⁄4Note |β| < a +\n√\na2 + b = α\n3⁄4General solution un = cαn + dβn\n3⁄4un = αn - βn\n(matching u1 and u2)\nα - β\n\nYoung, Problem 20, page 136\n3⁄4For any four consecutive Fibonacci\n3⁄4numbers fn-1, fn, fn+1, fn+2\n3⁄4show that fn-1fn+2 and 2fnfn+1\n3⁄4form two shortest sides of a\n3⁄4Pythagorean triangle.\n3⁄4Writefn = b and fn+1 = a, a > b\n3⁄4\na\n-\n-\nb2b, b, a, a + b\nx = a2\n, y = 2ab\n3⁄4x2 + y2 = z2, z = a2 + b2\n\nYoung, Problem 20, page 136\n(continued)\n3⁄4Hypotenuse z = fn\n2 + fn\n+1\n3⁄4From previous class,\nfn\n2 + fn\n+1 = f2n+1\n3⁄4How is the area related to the original\n3⁄4four numbers?\n3⁄4A = xy/2 = fn-1fnfn+1fn+2\n3⁄4Product of four consecutive Fibonacci\n3⁄4numbers is the area of a Pythagorean\n3⁄4triangle"
        },
        {
          "category": "Resource",
          "title": "talk_wong.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/4779a14f160a725f10c463b60d5e1d96_talk_wong.pdf",
          "content": "Average Values of\nArithmetic Functions\nRose Wong\nDecember 5, 2006\n18.104\n\nOutline\n1. Introduction\n2. Average number of representations of n\nas a sum of two squares\n3. Average number of representations of n\nas a sum of three squares\n4. Average number of primitive Pythagorean\ntriangles with hypotenuse n\n5. Average number of divisors of n\n\nIntroduction\n- Recall: Arithmetic Functions\n- Examples:\n» Number of primes less than or equal to x\n» Number of divisors of n\n- The average of an arithmetic function is defined to be:\n- In particular, we want to look at the average as nÆ\n.\nC\nn\nf\n→\nΝ\n:)\n(\n)\n(x\nΠ\n)\n(n\nd\n,...\n3,2,1\n,)\n(\n...\n)1(\n=\n+\n+\nn\nn\nn\nf\nf\ninf\nn\nn\nf\nf\nn\n)\n(\n...\n)1(\nlim\n+\n+\ninf\n→\n\nOutline\n1. Introduction\n2. Average number of representations of\nn as a sum of two squares\n3. Average number of representations of n\nas a sum of three squares\n4. Average number of primitive Pythagorean\ntriangles with hypotenuse n\n5. Average number of divisors of n\n\nAverage number of representations of n as a\nsum of two squares\n- Let r(n) be the number of representations\nsuch that\n- Theorem: The average number of\nrepresentations of a natural number as a\nsum of two squares is . That is\ny\nx\nn\n+\n=\nπ\nπ\n=\n+\n+\ninf\n→\nn\nn\nr\nr\nn\n)\n(\n...\n)1(\nlim\n\nAverage number of representations of n as a\nsum of two squares (continued)\nProof\n-\nLet\nand r(n) be the number of\nlattice points with integer\ncoordinates on the circle\n-\nHence, 1+R(n) is the number\nof lattice points inside the\ncircle\n-\nIdea- Place a unit square on\neach lattice point inside the\ncircle and count the number of\nsquares to find the area.\n)\n(\n...\n)1(\n)\n(\nn\nr\nr\nn\nR\n+\n+\n=\nn\ny\nx\n=\n+\nn\ny\nx\n≤\n+\n)\n(\n...\n)1(\n)\n(\nn\nr\nr\nn\nR\n+\n+\n=\n\nAverage number of representations of n as a\nsum of two squares (continued)\n-\nThe total area of unit squares is 1+R(n)\n-\nUpper bound of area is\n-\nLower bound of area is\n-\nSo,\n-\nUsing fact that and that\n-\nTaking limit as nÆ\n, we have\n⎟⎟\n⎠\n⎞\n⎜⎜\n⎝\n⎛\n+\nn\nπ\n⎟⎟\n⎠\n⎞\n⎜⎜\n⎝\n⎛\n-\nn\nπ\n)\n(\n⎟⎟\n⎠\n⎞\n⎜⎜\n⎝\n⎛\n+\n<\n+\n<\n⎟⎟\n⎠\n⎞\n⎜⎜\n⎝\n⎛\n-\nn\nn\nR\nn\nπ\nπ\n2 <\nπ\nn\n<\n-\n<\nπ\n...\n3,2,1\n=\nn\nn\nn\nn\nR\nn\nn\n)\n(\n+\n<\n<\n-\nπ\nπ\nn\nn\nn\nR\n)\n(\n<\n-π\nn\nn\nn\nR\n)\n(\n<\n-π\ninf\nπ\n→\nn\nn\nR\n)\n(\n\nOutline\n1. Introduction\n2. Average number of representations of n\nas a sum of two squares\n3. Average number of representations of\nn as a sum of three squares\n4. Average number of primitive Pythagorean\ntriangles with hypotenuse n\n5. Average number of divisors of n\n\nAverage number of representations of n as a\nsum of three squares\n- Let f(n) be the number of representations\nsuch that\n- Theorem: The average number of\nrepresentations of a natural number as a\nsum of three squares is .That is\nz\ny\nx\nn\n+\n+\n=\nπ\nπ\n)\n(\n...\n)1(\nlim\n=\n+\n+\ninf\n→\nn\nn\nf\nf\nn\n\nAverage number of representations of n as a\nsum of three squares (continued)\nProof\n-\nLet\n-\nLet f(n) be the number of lattice points with integer coordinates on\nthe sphere\n-\nHence, 1+F(n) is the number of lattice points inside the sphere\n-\nIdea- Place a unit cube on each lattice point inside the sphere and\ncount the number of cubes to find the volume. To estimate volume,\nwe find volume of an outer sphere that encloses all cubes and an\ninner sphere that is inscribed inside the cubes.\n)\n(\n...\n)1(\n)\n(\nn\nf\nf\nn\nF\n+\n+\n=\nn\nz\ny\nx\n=\n+\n+\nn\nz\ny\nx\n≤\n+\n+\n\nAverage number of representations of n as a\nsum of three squares (continued)\n-\nThe total area of unit cubes is\n-\nRadius of outer sphere is\n-\nRadius of inner sphere is\n-\nComparing volumes, we get\n-\nUsing fact that and\nthat , and rearranging,\nwe get\n-\nDividing by\n-\nTaking limit as nÆ\n, we\nhave\n+\nn\n-\nn\n)\n(\nn\nF\n+\n)\n(\n⎟⎟\n⎠\n⎞\n⎜⎜\n⎝\n⎛\n+\n<\n+\n<\n⎟⎟\n⎠\n⎞\n⎜⎜\n⎝\n⎛\n-\nn\nn\nR\nn\nπ\nπ\n<\nπ\n<\nπ\n)\n(\n/\n-\n<\n+\n-\n-\nn\nn\nn\nn\nR\nπ\nπ\n/\n/\n/\n)\n(\nn\nn\nn\nn\nn\nn\nR\n-\n<\n+\n-\n-\nπ\nπ\n/\nn\nπ\n)\n(\n/\n→\nn\nn\nR\ninf\n\nOutline\n1. Introduction\n2. Average number of representations of n\nas a sum of two squares\n3. Average number of representations of n\nas a sum of three squares\n4. Average number of primitive\nPythagorean triangles with\nhypotenuse n\n5. Average number of divisors of n\n\nAverage number of primitive Pythagorean\ntriangles with hypotenuse n\n- Let P(n) be the number of primitive Pythagorean\ntriangles with hypotenuse equal to n.\nPrimitive\nNot Primitive\n- Example: P(5)=1 and P(65)=2\n- We want to look at:\n=\n+\n=\n+\n=\n+\nK\nn\nn\nP\nP\nn\n=\n+\n+\ninf\n→\n)\n(\n...\n)1(\nlim\n\nAverage number of primitive Pythagorean\ntriangles with hypotenuse n (continued)\n-\nWe want to examine the average of P(n) for large values of n.\n-\nApproach (computer): For each value of n from 1 to 1000, we generate\nthe average values and plot them on a graph. We want to look at the\nbehavior as n grows large.\nPlot of vs n\nn\nn\nP\nP\n)\n(\n...\n)1(\n+\n+\n\nAverage number of primitive Pythagorean\ntriangles with hypotenuse n (continued)\n- From the figure, we see that the average values\n(measured on the vertical axis) oscillate, but that the\naverage levels level off quickly.\n- We can conjecture that\n.0\n)\n(\n...\n)1(\nlim\n≈\n+\n+\ninf\n→\nn\nn\nP\nP\nn\n\nOutline\n1. Introduction\n2. Average number of representations of n\nas a sum of two squares\n3. Average number of representations of n\nas a sum of three squares\n4. Average number of primitive Pythagorean\ntriangles with hypotenuse n\n5. Average number of divisors of n\n\nAverage number of divisors\n-\nLet d(n) be the number of divisors of the natural number n.\n- Example: d(8)=4\n- Theorem: The average value of the number of divisors\nof natural numbers grows like log n.\nn\nn\nn\nd\nd\nlog\n~\n)\n(\n...\n)1(\n+\n+\n\nAverage number of divisors (continued)\nProof\n-\nLet k be a fixed integer. Listing multiples of k less than or equal to n,\nwe have\n-\nThere are multiples, where [ ] denotes the floor function. Each\nof these multiples contributes 1 to the sum\n-\nExamining multiples of all integers\nk\nk\nn\nk\nk\nk\n⎥⎦\n⎤\n⎢⎣\n⎡\n,...\n3,\n2,\n⎥⎦\n⎤\n⎢⎣\n⎡\nk\nn\nn\nk ≤\n)\n(\n...\n)1(\nn\nd\nd\nk\nn\nn\nk\n+\n+\n=\n⎥⎦\n⎤\n⎢⎣\n⎡\n∑\n=\n)\n(\n...\n)1(\nn\nd\nd\n+\n+\n\nAverage number of divisors (continued)\n-\nNow, we want to prove that\n-\nFirst we establish the relationship:\n-\nSumming over k gives:\n-\nFactoring out n gives\nThe first and last term are\n-\nrewritten in integral form\nk\nn\nk\nn\nk\nn\n<\n⎥⎦\n⎤\n⎢⎣\n⎡\n<\n-1\n∑\n∑\n∑\n=\n=\n=\n<\n⎥⎦\n⎤\n⎢⎣\n⎡\n<\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n-\nn\nk\nn\nk\nn\nk\nk\nn\nk\nn\nk\nn\n∑\n∑\n∑\n=\n=\n=\n<\n⎥⎦\n⎤\n⎢⎣\n⎡\n<\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n-\nn\nk\nn\nk\nn\nk\nk\nn\nk\nn\nn\nk\nn\n∫\n∑\n∫\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n<\n⎥⎦\n⎤\n⎢⎣\n⎡\n<\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n-\n=\nn\nn\nk\nn\ndk\nk\nn\nk\nn\ndk\nn\nk\nn\nlog\nlim\nlog\n)\n(\n...\n)1(\nlim\n=\n⎥⎦\n⎤\n⎢⎣\n⎡\n=\n+\n+\n∑\n=\ninf\n→\ninf\n→\nn\nn\nk\nn\nn\nn\nn\nd\nd\nn\nk\nn\nn\n\nAverage number of divisors (continued)\n-\nIntegrating gives\n-\nSo taking nÆ\n, we have\n-\nAnd so,\nn\nn\nn\nd\nd\nlog\n~\n)\n(\n...\n)1(\n+\n+\ninf\nn\nn\nk\nn\nn\nn\nn\nn\nk\nlog\nlog\n<\n⎥⎦\n⎤\n⎢⎣\n⎡\n<\n+\n-\n∑\n=\nlog\nlim\n=\n⎥⎦\n⎤\n⎢⎣\n⎡\n∑\n=\ninf\n→\nn\nn\nk\nn\nn\nk\nn"
        },
        {
          "category": "Resource",
          "title": "wong.pdf",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/2f53884e4a1c94334a0d8309e91d66bc_wong.pdf",
          "content": "AVERAGE VALUES OF ARITHMETIC FUNCTIONS\nROSE WONG\nAbstract. In this paper, we will present problems involving av\nerage values of arithmetic functions. The arithmetic functions we\ndiscuss are: (1)the number of representations of natural numbers\nas a sum of two squares and (2)as a sum of three squares, (3)the\nnumber of decompositions of natural numbers into sums of con\nsecutive primes, (4)the number of primitive pythagorean triangles\nwith a hypotenuse of a given length, and (5)the number of divisors\nof natural numbers.\n1. Introduction\nAn arithmetic function is defined to be a function f(n), defined for\nn ∈N, which maps to a complex number such that f: N\nC. Exam\n→\nples of arithmetic functions include: the number of primes less than a\ngiven number n, the number of divisors of n, and the number of ways\nn can be represented as a sum of two squares. While the behavior of\nvalues of arithmetic functions are hard to predict, it is easier to analyze\nthe behavior of the averages of arithmetic functions which we define\nas:\nf(1) + ... + f(n)\nlim\n= L\nn→inf\nn\nwhere L the average value of f(n). In this paper, we will examine\naverages of several different arithmetic functions.\n2. Average number of representations of a natural\nnumber as a sum of two squares\nLet r(n) be the number of representations of n as a sum of two\nsquares\nn = x + y .\nTheorem 2.1. The average number of representations of a natural\nnumber as a sum of two squares is π. That is,\nDate: November 30, 2006.\n\nROSE WONG\nr(1) + r(2) + ... + r(n)\nlim\n= π.\nn→inf\nn\nProof. For a fixed n, let\nR(n) = r(1) + ... + r(n)\nWe interpret r(n) to be the number of lattice points (points with\ninteger coordinates) on the circle x2 + y2 = n. R(n) is the number of\nlattice points in the disc x2 + y2\nn except for the origin.\n≤\nIf we place a unit square (centered on the lattice point with sides\nparallel to the coordinate axes) on each lattice point covered by the\ndisk, the total area of these squares is 1+R(n), because R(n) does not\ninclude the origin.\nThis is not exactly the area of the disk, as some squares go beyond\nthe disk's boundary and some areas of the circle are not covered by\nany square. However, we can find a circle that circumscribes all the\nsquares and a circle that is inscribed by all the squares. These will be\nthe bounding outer and inner circles, respectively.\nThe radius of the outer circle is √n + √\n2 , because the greatest dis\ntance from the origin (0,0) to an outermost lattice point is √n and the\ngreatest distance from that lattice point to the edge of its corresponding\nunit square is √1\n2 .\nHence, the area of the outter circle is larger than the area of the\nsquares.\n1 + R(n) < π(√n + √1\n2)2 .\nBy a similar argument, the area of the inner circle is less than the\narea of the squares.\n1 + R(n) > π(√n -√1\n2)2\nWe can simplify the inequalities using the fact that π\n√\n2 < 5 and\n0 < π\n2 -1 < √n (n = 1, 2, 3, ...). Therefore,\nR(n) < π(√n + √1\n)2 -1 = πn + π\n√\n2√n + π\n2 -1 < πn + 6√n\nand\n)2\nR(n) > π(√n -√1\n-1 = πn -π\n√\n2√n + π\n2 -1 > πn -6√n.\nIt follows that\nπn -6√n < R(n) < πn + 6√n,\n\nq\nAVERAGE VALUES OF ARITHMETIC FUNCTIONS\nwhich is the same as\n|R(n) -πn| < 6√n.\nIf we divide by n, we get\nR(n)\n|\nn\n-π| < √n.\nTaking the limit as n →inf, we have R(n)/n\nπ and the proof is\n→\ncomplete.\n3. Average Number of Representations of a Natural\nNumber as a sum of three squares\nWe will now extend the previous problem, that is, we will find the\naverage number of representations of a natural number as a sum of\nthree squares. Again, we will use geometry with integer lattice points\nto approach this problem.\nLet f(n) denote the number of integral solutions of\nx + y + z = n.\nTheorem 3.1. The average number of representations of a natural\nnumber as a sum of three squares is 4\n3 π.\nf(1) + f(2) + ... + f(n)\nlim\n=\nπ\nn→inf\nn3/2\nProof. For a fixed n, let\nF(n) = f(1) + ... + f(n).\nWe interpret f(n) to be the number of lattice points on the sphere\nx2 + y2 + z2 = n, so F(n) to be the number of lattice points in the\nsphere x2 + y2 + z2\nn except for the origin. If we place a unit cube\n≤\non each lattice point covered in the sphere, the total volume of these\ncubes is 1 + F(n).\nWe want to find a sphere that encloses all of the cubes and a sphere\nthat is inscribed inside the cubes. The radius of the outer sphere is\n√n +\n√\n3 . Since the greatest distance between outermost lattice point\nand the edge of its corresponding cube is\n( 1\n2 )2 + ( 1\n2 )2 + ( 1\n2 )2 =\n√\n3 .\nSimilarly, the radius of the inner sphere is √n -\n√\n3 .\nComparing the volumes, we have\n)3\n1 + R(n) < 4 π(√n +\n√\n\nROSE WONG\nand\n1 + R(n) > 4 π(√n -\n√\n3)3 .\nExpanding the inequalities out, we have\nR(n) < (4 πn3/2 + 3π√n -1) + (2\n√\n3πn +\n√\n3 π)\nand\nR(n) > (4 πn3/2 + 3π√n -1) -(2\n√\n3πn +\n√\n3 π).\nUsing the fact that 2\n√\n3π < 11 and\n√\n3 π < 3, we have\n|R(n) -3\n4 πn3/2 -3π√n + 1)| < 11n -3.\nDividing by n3/2 gives\nR(n)\n3π\n| n3/2 -3 π - n + n3/2 )| < √n -n3/2 .\nTaking the limit as n →inf, we have R(n)\n4 π and the proof is\nn3/2 → 3\ncomplete.\n4. Average value of number of decompositions of a\nnatural number n into a sum of consecutive primes\nLet f(n) be the number of decompositions of a natural number n\ninto a sum of one or more consecutive prime numbers. For example,\nf(395) = 2 because\n395 = 127 + 131 + 137 = 71 + 73 + 79 + 83 + 89.\nTheorem 4.1. The average number of decompositions of a natural\nnumber into a sum of one or more consecutive prime numbers is log 2.\n[1]\nf(1) + ... + f(n)\nlim\n= log 2\nn→inf\nn\nProof. Let P be a sequence of consecutive primes: p1 < p2 < p3 < ...\nEvery set of consecutive primes whose sum is less than or equal to x\ncontributes 1 to the sum f(1)+f(2)+... +f(n). The number of sets of\nr primes that satisfy the condition is at most π( x\nr ) and at least π( x\nr )-r.\nTherefore, we have\nk\nk\nX\nx\nX\nx\n(π( ) -r) ≤f(1) + f(2) + ... + f(x) ≤\n(π( )),\nr\nr\nr=1\nr=1\n\nr\np\n\nAVERAGE VALUES OF ARITHMETIC FUNCTIONS\nwhere k is determined by\np1 + p2 + ... + pk ≤x < p1 + p2 + ... + pk+1\nUsing the above inequality and pr ≍r log r we get\nx\nk ≍\nlog x\nThis implies that Pk\nr = O(x). If we can show that Pk\n(π( x)) ∼\nr=1\nr=1\nr\nx log 2, it will follow that f(1) + f(2) + ... + f(x) ∼x log 2\nUsing the above relationship and the prime number theorem, we have\nk\nk\nZ k\nX\nx\nX\nx\nx\nr=1\n(π( r )) ∼\nr=1\n( r log x/r) ∼\n1 r log x/r dr = [-x log(log x/r)]1\nk\nx\n= x(log log x -log log\n) ∼x(log log x -log log\nx log x) ∼x log 2.\nk\nNotation explanation: Let f and g be functions of x. The notation\nf ≍g denotes that f(x)/g(x) is bounded above and below by posi\ntive numbers for large values of x. The notation f = O(g) denotes\nthat ∃c such that f(x)\ncg(x). The notation f ∼g denotes that\nf(x)\n|\n| ≤\nlimx→inf, g(x) = 1\n5. Average value of primitive pythagorean triangles\nwith hypotenuse equal to n\nLet P(n) be the number of primitive Pythagorean triangles with\nhypotenuse equal to n. For example, P(5)=1 and and P(65)=2 because\n32 + 42 = 52 ,\n332 + 562 = 632 + 162 = 652\nWe would like to determine whether the average value of P(n) con\nverges, and if so, to what value. That is,\nP(1) + ... + P(n)\nlim\n= K\nn→inf\nn\nRather than approaching this problem through mathematical argu\nments, we will use computer computations to infer a value. For each\nvalue of n from 1 to 1000, we generate and graph the average values\nup to and including n. Looking at Figure 1, we see that the average\nvalues (measured on the vertical axis) oscillate, but as n →inf, the\n\nROSE WONG\n0.2\n0.18\n0.16\n0.14\n0.12\n0.1\n0.08\n0.06\n0.04\n0.02\nFigure 1. Average P(n) for n=[1,1000]\naverage values level off. We can conjecture that the limit as\nof\nn →inf\nthe average value is:\nP(1) + ... + P(n)\nlim\n= 0.159.\nn→inf\nn\n6. Average value of divisors of a natural numbers\nLet d(n) be the number of divisors of the natural number n. We will\nshow that\nTheorem 6.1. The average value of the number of divisors of natural\nnumbers grows like log n.\nd(1) + d(2) + ... + d(n) ∼log n\nn\nProof. Let k be a fixed integer. If we list the multiples of k less than\nor equal to n:\nn\nk, 2k, 3k, ...[ ]k,\nk\nwe find that there are [n\nk] multiples, where [ ] denotes the floor function.\nEach of those multiples contributes 1 to the sum d(1) + ... + d(n).\nIf we examine multiples of all integers k ≤n, it follows that summing\nover k gives\nn\nX n\n[ ] = d(1) + ... + d(n)\nk\nk=1\n\nP\nP\n\nAVERAGE VALUES OF ARITHMETIC FUNCTIONS\nNow, we want to prove that\nn\nk=1[n]\nlim\nk = 1\nn→inf n log n\nFirst, we establish the relationship:\nn\nn\nn\nk -1 < [k ] ≤k .\nSumming over k gives:\nn\nn\nn\nX n\nX n\nX n\n(k -1) <\n[ ] ≤\n.\nk\nk\nk=1\nk=1\nk=1\nWe then factor out n to get:\nn\nX 1\nn\nX n\nn\nX 1\nn\n(k -n ) <\n[k ] ≤n\nk\nk=1\nk=1\nk=1\nThe first and last term in the above inequality can be rewritten as\nthe integrals n\nR\nn( k\n1 -n\n1 ) dk and n\nR\nn\nk\n1 dk.\nIntegrating gives\nn\nX n\nn log n -n + 1 <\n[ ] ≤n log n.\nk\nk=1\nSo taking n →inf, we have\nn\n[n]\nlim\nk=1 k = 1.\nn→inf n log n\nAnd so,\nd(1) + d(2) + ... + d(n) ∼log n.\nn\nReferences\n[1] Moser, L., \"On the Sum of Consecutive Primes,\" Notes on Number Theory III,\nCanadian Mathematical Bulletin 6, p 159-161, 1963.\n[2] Young, Robert M., \"Excursions in Calculus,\" The Mathematical Association of\nAmerica, USA, 1992."
        }
      ],
      "source_url": "https://ocw.mit.edu/courses/18-104-seminar-in-analysis-applications-to-number-theory-fall-2006/",
      "course_info": "18.104 | Undergraduate",
      "subject": "General"
    },
    {
      "course_name": "Combinatorial Analysis",
      "course_description": "No description found.",
      "topics": [
        "Mathematics",
        "Applied Mathematics",
        "Discrete Mathematics",
        "Mathematics",
        "Applied Mathematics",
        "Discrete Mathematics"
      ],
      "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nPrerequisites\n\n18.02 Multivariable Calculus\nand\n18.06 Linear Algebra\nor\n18.700 Linear Algebra\n\nDescription\n\nThis course analyzes combinatorial problems and methods for their solution. Prior experience with abstraction and proofs is helpful. Topics include:\n\nEnumeration\n\nGenerating functions\n\nRecurrence relations\n\nConstruction of Bijections\n\nIntroduction to Graph Theory\n\nNetwork Algorithms\n\nExtremal Combinatorics\n\nTextbooks\n\nBona, Miklos.\nA Walk Through Combinatorics: An Introduction to Enumeration and Graph Theory\n. World Scientific Publishing Company, 2011. ISBN: 9789814335232. [Preview with\nGoogle Books\n]\n\nWe will cover Chapters 1 through 13. Additional lecture notes to be provided on the Matrix-Tree Theorem and Eulerian cycles.\n\nProblem Sets\n\nProblem sets are due at the end of lecture each Monday (or the first day of classes of that week if Monday is a holiday) for assignments of the previous week. Since solutions to problem sets will be made available shortly after the deadline for handing them in, in general late problem sets will not be accepted without a valid reason such as illness. A problem set that is one or two days late with a reason like \"more time needed\" might be accepted with a penalty, such as 15% off, if permission is granted before the problem set is due. (In this case the posting of solutions will be delayed.)\n\n\"Reasonable\" collaboration is permitted on problem sets, but you should not just copy someone else's work or look up the solution from an outside source. On each problem set please write the names of those students with whom you have collaborated.\n\nExams\n\nThere will be two one-hour exams and a final exam. There is no curve, but I suspect the average grade will be B.\n\nGrading\n\nGrades will be determined by a weighted average of\n\nACTIVITIES\n\nPERCENTAGES\n\nProblem Sets\n\n20%\n\nTwo Midterm Exams @ 20% each\n\n40%\n\nFinal Exam\n\n40%",
      "files": [
        {
          "category": "Exam",
          "title": "Combinatorial Analysis, Practice Final Exam",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-314-combinatorial-analysis-fall-2014/58d35ce43e702ce9d8710f7d33c5ff33_MIT18_314F14_pracexam.pdf",
          "content": "18.314 PRACTICE FINAL EXAM\n(for Final Exam of December 15, 2014)\nClosed book, calculators, computers, iPods, cell phones, etc., but you\nmay bring in one sheet of paper, no larger than 8 1′′ × 11′′\n, written on both\nsides. Do all eight problems. There are a total of 80 points. Be sure to show\nyour work on each problem.\n1.\n(a) (5 points) Solve the recurrence f(0) = 2, f(1) = 4, and\nf(n + 2) = 4f(n + 1) -2f(n),\nn ≥0.\n(Give a simple explicit formula for f(n). It is o.k. to use irrational\nnumbers.)\n(b) (5 points) Show that ⌊(2 +\n√\n2)n⌋is odd for all integers n ≥0.\n2. (10 points) Let f(n) be the number of ways to choose a permutation\nπ of 1, 2, . . . , n and color each cycle of π of even length either red or\nblue. For instance, f(1) = 1, f(2) = 3, f(3) = 9, and f(4) = 45. Set\nf(0) = 1. Find the generating function\nxn\nF(x) =\nX\nf(n)\nn≥0\n.\nn!\nTo get full credit, your answer should not involve any infinite sums,\ninfinite products, or the functions exp (the exponential function) and\nlog.\n3.\n(a) (5 points) Let f(n) be the number of ways to tile a 2×n rectangle\nwith 1 × 1 squares and 2 × k rectangles for any integer k ≥1,\nwhere the 2 × 1 rectangle must be vertical. (Set f(0) = 1.) For\ninstance, f(2) = 5, given by\nA larger example of such a tiling is given by\n\nFind a simple expression for the generating function\nF(x) =\nX\nf(n)xn.\nn≥0\n(b) (5 points; difficult) Let g(n) be the number of ways to tile a 2 × n\nrectangle with a × b rectangles for any integers a, b ≥1.\n(Set\ng(0) = 1.) For instance, g(2) = 8, given by\nA larger example of such a tiling is given by\nFind a simple expression for the generating function\nG(x) =\nX\ng(n)xn.\nn≥0\n4. (10 points) Let m, n ≥3. Let G be the graph obtained by identifying\nan edge of an m-cycle with the edge of an n-cycle. Thus G has m+n-2\nvertices and m + n -1 edges. Find the number κ(G) of spanning trees\nof G. (It is easiest to use \"naive\" reasoning and not to use the Matrix-\nTree Theorem.) The figure below shows the case m = 5 and n = 4.\n\n5. (10 points) Let G be a regular bipartite graph of degree d ≥2, i.e.,\nevery vertex of G has the same degree d ≥2.\nShow that G has a\nspanning subgraph (that is, a subgraph using every vertex of G) that\nis a disjoint union of cycles. (No two cycles should have a vertex in\ncommon.)\n6. (10 points) Compute the chromatic polynomial of the following graph\nG with eight vertices:\n7.\n(a) (5 points) Does there exist a planarly embedded graph with no\nisthmus (i.e., no edge e for which the same face lies on both sides\nof e) and with exactly one face with k vertices, 3 ≤k ≤8, and\nwith no other faces? Thus G has six faces in all.\n(b) (5 points) Same question, but for 3 ≤k ≤9.\n8. (10 points) Find the least positive integer n with the following property:\nif the edges of Kn are colored red and blue, then there must exist a\nmonochromatic non-closed path of length three, i.e., a path of length\nthree (not a triangle, so with four vertices and three edges) such that\nall edges in the path have the same color. (Five points for showing that\nsome value of n has this property. Full credit for finding the least such\nvalue and showing that it is indeed least.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.314 Combinatorial Analysis\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Exam",
          "title": "Combinatorial Analysis, Solutions to Practice Final Exam",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-314-combinatorial-analysis-fall-2014/818aac71b9f4cbf1c5349119fbae7dd5_MIT18_314F14_pracexamsol.pdf",
          "content": "18.314 SOLUTIONS TO PRACTICE FINAL EXAM\n(for Final Exam of December 15, 2014)\nP\n1.\n(a) (5 points) Let F(x) =\nn≥0 f(n)xn. Multiply the recurrence by\nxn+2 and sum on n ≥0 to get\nF(x) -2 -4x = 4x(F(x) -2) -2x2F(x),\nso\nF(x)\n=\n-4x\n1 -4x + 2x2\n=\n1 -(2 +\n√\n2)x +\n1 -(2 -\n√\n2)x.\nThus\nf(n) = (2 +\n√\n2)n + (2 -\n√\n2)n.\n(b) (5 points) We have 2\n√\n-\n2 = 0.5857 · · ·, so 0 < (2 -\n√\n2)n < 1 for\nall n ≥1. It follows that\n⌊(2 +\n√\n2)n⌋= f(n) -1.\nNow f(1) is even and f(n + 2) = 2(2f√(n + 1) -f(n)) for n ≥0,\nso f(n) is even for n ≥1. Thus ⌊(2 +\n2)n⌋is odd for n ≥1. We\ncan also see that ⌊(2 +\n√\n2)0⌋= 1, which is also odd.\n2. This is a situation for the exponential formula. Partition the set [n]\ninto blocks. On each block of odd size k place a cycle in (k -1)! ways.\nIn each of even size place a cycle and then color red or blue in 2(k -1)!\nways. By the exponential formula,\nX\nxk\nF(x)\n=\nexp\n(k\nk odd\n-1)! k! + 2\nX\nk even\n(k -1)!xk\nk!\n!\n=\nexp\nX\nk≥1\nxk\nk +\nX\nk≥1\nx2k !\n2k\n\n=\nexp\n-log(1 -x) -2 log(1 -x2)\n\n=\n(1 -x)\n√\n1 -x2.\n\n3.\n(a) Each tiling is a sequence of the following \"primes\": a 2×1 rectan-\ngle divided into two 1×1 squares, and a 2×k rectangle for k ≥1.\nThere are two primes of length one, and one prime of each length\nk ≥2. Hence\nF(x)\n=\n1 -(2x + x2 + x3 + x4 + · · · )\n=\n1 -x -\nx\n1-x\n=\n-x\n.\n1 -3x + x2\nNote, One can easily deduce from this generating function that\nf(n) = F2n+1 (a Fibonacci number), but this was not part of the\nproblem.\n(b) First consider those tilings that consist only of 2×k rectangles, k ≥\n1. The sequence of lengths of these rectangles form a composition\nof n. Thus the number a(n) of such tilings a(n) of a 2\nn\n×n rectangle\nis 2 -1 (n ≥1), the number of compositions of n. Therefore\nX\nA(x)\n:=\na(n)xn\nX\nn≥1\n=\n2n-1xn\nn≥1\nx\n=\n.\n1 -2x\nNow consider those tilings that contain no 2 × k rectangle. They\nhave a horizontal line down the middle. Above and below the line\nare rectangles whose lengths form a composition of n. There are\n(2n-1)2 such pairs of compositions. Hence if b(n) is the number of\nsuch tilings of a 2 × n rectangle, then\nX\nB(x)\n:=\nb(n)xn\nX\nn≥1\n=\n(2n-1)2xn\nn≥1\nx\n=\n.\n1 -4x\nAn arbitrary tiling of a 2 × n rectangle consists of a sequence of\ntilings beginning with those counted by a(n) (but which may be\n\nempty at this first step), then those counted by b(n), then by a(n),\netc., some finite number of times. Therefore\nG(x)\n=\n(1 + A(x))(B(x) + B(x)A(x) + B(x)A(x)B(x) + · · · )\nX\n=\n(1 + A(x))\n(B(x)A(x))j(1 + B(x))\nj≥0\n(1 + A(x))(1 + B(x))\n=\n.\n1 -A(x)B(x)\nSubstituting A(x) = x/(1 -2x), B(x) = x/(1 -4x), and simpli-\nfying gives\n(1\nG(x) =\n-x)(1 -3x).\n1 -6x + 7x2\n4. If a spanning tree T does not contain the identified edge e, then there\nare m+n-2 choices, i.e., remove any of the m+n-2 remaining edges.\nIf T does contain e, then we can remove any of the remaining m -1\nedges of the m-cycle and any of the n -1 remaining n -1 edges of the\nn-cycle, so (m -1)(n -1) choices in all. Hence\nκ(G) = m + n -2 + (m -1)(n -1) = mn -1.\nA somewhat more direct argument is to remove any edge of the m-\ncycle and any edge of the n-cycle in mn ways. This gives a spanning\ntree except when we choose the identified edge e both times, so we get\nmn -1 trees in all.\n5. We know (Exercise 11.12 on page 266, done in class) that G has a\ncomplete matching M. When we remove M from G we still have a\nregular bipartite graph (of degree d -1 ≥1), so we have another\nmatching M′ disjoint from M. The union of M and M′ is a disjoint\nunion of cycles [why?].\n6. The chromatic polynomial of a 4-cycle C4 was computed in class and\nis easy to do in several different ways. We get\nχ\nC4(n) = n -4n + 6n -3n.\nFor each of the other four vertices we have n-2 choices of colors. Hence\nχG(n) = (n4 -4n3 + 6n2 -3n)(n -2)4.\n\n7.\n(a) If a planar ePmbedding without isthmuses has fi faces with i sides,\nthen 2E =\nifi. (See equation (12.2) on page 280.) Hence\n2E = 3 + 4 + 5 + 6 + 7 + 8 = 33,\ncontradicting that E is an integer.\n(b) Now we get 2E = 3 + 4 + 5 + 6 + 7 + 8 + 9 = 42, so E = 21. Since\nF = 7 we get from V -E+F = 2 that V = 16. To show that such\na graph actually exists, we have to construct it. For instance, we\ncould put the 9-sided face f on the outside and the 7-sided face\ncompletely inside f. This leads to\nThis is by no means the only graph meeting the conditions of the\nproblem.\n8. We claim that n = 5.\nWe can easily two-color the edges of K4 so\nthat there is no monochromatic path of length three: color the edges\nof a triangle red and the remaining three edges blue. Hence n ≥5.\nConsider now K5 with vertices 1,2,3,4,5. The four cycle with edges 12,\n23, 34, 14 must have two red and two blue edges; otherwise it already\nhas a monochromatic path of length three. If the two red edges don't\nhave a common vertex then one of the paths {12, 34, 13} or {23, 14, 13}\nis monochromatic. Thus we can assume that the 4-cycle has two red\nedges with a common vertex and two blue edges with a common vertex.\nSuppose that the red edges are 12,23 and the blue edges are 34,14. Then\none of the paths {12, 23, 35} and {34, 14, 35} is monochromatic. (I'm\nsure there must be many other arguments.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.314 Combinatorial Analysis\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Combinatorial Analysis, Practice Midterm 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-314-combinatorial-analysis-fall-2014/4fedee5366e5075d4cd4a4fb3b25d808_MIT18_314F14_pracq1.pdf",
          "content": "18.314: PRACTICE HOUR EXAM #1\n(for hour exam of October 10, 2014)\nClosed book, notes, calculators, computers, cell phones, etc. Do all four\nproblems. Show your reasoning. There are a total of 50 points. However,\nthe actual hour exam will have four problems and a total of 40 points.\n1. (10 points) Fix an integer n ≥1.\nLet S be the set of all n-tuples\n(a1, . . . , an) whose entries ai are either 1, 2, or -3. Thus #S = 3n.\nFind the least number f(n) of elements of S we can pick so that we are\nguaranteed to have a nonempty subset T of these elements satisfying\nX\nv = (0, 0, . . . , 0).\nv∈T\nFor instance, f(2) > 3, since no nonempty subset of the set\n{(1, 2), (1, -3), (-3, 1)}\nhas elements summing to (0, 0).\nNote that you have to prove that\nyour value of f(n) has the stated property, and that this value is best\npossible, i.e., the result is false for f(n) -1.\n2. (10 points) Let f(n) be the number of self-conjugate partitions of n, all\nof whose parts are even. An example of such a partition is (4, 4, 2, 2).\nExpress f(4n) in terms of c(n), the total number of self-conjugate par-\ntitions of n. (Show your reasoning. A detailed proof is not necessary.\nJust state the basic idea.)\n3. (10 points) Fix n ≥1. Let f(n) be the number of permutations π of\n1, 2, . . . , 2n with the following property: π has exactly n cycles, and the\nlargest elements of the n cycles are the numbers 2, 4, 6, . . ., 2n. Find a\nsimple formula for f(n). You may write your answer either as a simple\nproduct or in terms of factorials and powers.\n4. (10 points) How many partitions of the set [9] have all their blocks\nof size 2 or 3? You may leave your answer expressed in terms of func-\ntions discussed in class (such as binomial coefficients, factorials, Stirling\nnumbers, etc.). You don't need to give a numerical answer.\n\n5. (10 points) For n ≥1, let f(n) be the number of n × n matrices of 0's\nand 1's such that every row and every column has at least one 1. For\ninstance f(1) = 1 and f(2) = 7. Use the sieve method (Principle of\nInclusion-Exclusion) to give a formula for f(n) as a single sum. (In my\nopinion this is the trickiest problem on the practice test, but I could\nbe wrong.)\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.314 Combinatorial Analysis\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Combinatorial Analysis, Solutions to Practice Midterm 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-314-combinatorial-analysis-fall-2014/cb9c07793999ec2530d2d5c5b6361784_MIT18_314F14_pracq1sol.pdf",
          "content": "18.314: SOLUTIONS TO\nPRACTICE HOUR EXAM #1\n(for hour exam of October 10, 2014)\n1. We can partition S into 3n-1 three-element blocks such that the sum of\nthe elements in each block is (0, 0, . . . , 0). To do this define π(1) =\n2, π(2) = -3, π(-3) = 1.\n(We are just cyclically permuting the\nnumbers 1, 2, -3.) Let the block containing (a1, a2, . . . , an) also contain\n(π(a1), π(a2), . . . , π(an)) and (π(π((a1)), π(π(a2)), . . . , π(π(an))).\nFor\ninstance, when n = 4 one of the blocks is\n{(1, 2, -3, 2), (2, -3, 1, -3), (-3, 1, 2, 1)}.\nIf we choose 2 · 3n-1 + 1 elements of S, then some three of them must\nbe in the same block of the partition and therefore sum to (0, 0, . . . , 0).\nThus f(n) ≤2 · 3n-1 + 1. If we choose all elements of S whose first\ncoordinate is either 1 or 2, then the sum of any nonempty subset of the\nchosen elements has positive first coordinate and therefore cannot be\n(0, 0, . . . , 0). Since there are 2 · 3n vectors (a1, . . . , an) with a1 = 1 or\n2, we see that f(n) > 2 · 3n-1. Hence f(n) = 2 · 3n-1 + 1.\n2. The Young diagram of a self-conjugate partition of 4n with even parts\ncan be divided into n 2 × 2 squares. If we replace each of these 2 × 2\nsquares with a single square, then we get the Young diagram of a self-\nconjugate partition of n. Conversely, given the Young diagram of a\nself-conjugate partition of n, replace each square with a 2 × 2 square\nto get the Young diagram of a self-conjugate partition of 4n with even\nparts. Hence f(4n) = c(n).\n3. Insert the numbers 2, 4, 6, . . . , 2n, followed by 2n-1, 2n-3, . . . , 3, 1, in\nthat order, into the cycle notation for π. We start with (2 ∗)(4 ∗) · · ·(2n ∗).\nWe always write the cycles so that 2, 4, . . . , 2n are the first (leftmost)\nelements. Then insert 2n -1. There is only one choice: it must be\nplaced after 2n. Then insert 2n -3. There are three choices: after\n2n -2, 2n -1, 2n. Then insert 2n -5. There are five choices: after\n2n -4, 2n -3, 2n -2, 2n -1, 2n. Continuing in this way, we see that\nf(n) = 1 · 3 · 5 · · ·(2n -1).\nAnother way to write this answer is (2n)!/2nn!.\n\n4. The possible block sizes are (3, 3, 3) and (3, 2, 2, 2).\nIn class it was\nproved that the number of partitions of [n] with ai blocks of size i is\nn!\n.\n1!a12!a2 · · ·a1! a2! · · ·\nHence the number of partitions of [9] with all blocks of size 2 or 3 is\nequal to\n9!\n3!3 · 3! +\n9!\n.\n2!3 · 3!1 · 1! · 3!\nThis turns out to be equal to 1540.\n5. For each subset S of {1, . . . , n}, let g(S) be the number of n×n matrices\nof 0's and 1's such that every row contains a 1, and if i ∈S then column\ni does not contain a 1. Each row then has n-i available positions where\nwe can place the 1's. Thus if #S = k then there are 2n-k-1 possibilities\nfor each row. Hence g(S) = (2n-k -1)n. By the sieve method,\nf(n)\n=\ng(∅) -\n#\nX\ng(S) +\nX\ng(S) -· · · + (-1)n X\ng(S)\nS=1\n#S=2\n#S=n\nn\n=\nX\n(-1)k\nk=0\nn\nk\n\n(2n-k -1)n.\n(The last term is 0 and can be omitted.) This problem can also be\ndone by writing f(n) as a double sum and using the binomial theorem\nto reduce it to a single sum. Full credit for doing it correctly this way,\nthough the solution above is simpler.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.314 Combinatorial Analysis\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Combinatorial Analysis, Practice Midterm 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-314-combinatorial-analysis-fall-2014/21d3bbafc454070e911c5bac96b12747_MIT18_314F14_pracq2.pdf",
          "content": "18.314: PRACTICE HOUR EXAM #2\n(for hour exam of November 14, 2014)\nClosed book, notes, calculators, computers, cell phones, etc. Do all four\nproblems. Show your reasoning. There are a total of 40 points\n1.\n(a) (7 points) Let f(n) be the number of partitions of n into powers of\n2, such that each power occurs at most three times. For instance,\nf(9) = 5, corresponding to the five partitions 81, 441, 4221, 42111,\n2221. (Set f(0) = 1.) Find a subset S of the positive integers such\nthat f(n) is equal to the number of partitions of n whose parts\ncome from S, for all n ≥0. (Show your reasoning.)\n(b) (3 points) Give a simple formula for f(n) in terms of the floor\nfunction ⌊a⌋, the greatest integer m ≤a.\n2. (10 points) For n ≥0 define an by a0 = 0, a1 = 1, and\nan+2 = 6an+1 -8an,\nn ≥0.\nFor what n is an a triangular number, i.e., a number of the form k(k +\n1)/2 for some integer k? (Show your reasoning.)\n3. (10 points) Let f(n) be the number of ways to color n pencils using the\nseven colors red, blue, green, yellow, white, black, and Halaya` u be, such\nthat an odd number of pencils are either red, blue, green, or yellow.\n(That is, the number of red pencils plus the number of blue pencils\nplus the number of green pencils plus the number of yellow pencils is\nodd.) Find a simple formula for f(n), n ≥0.\n4.\n(a) (7 points) Let n be odd, n ≥3.\nShow that the edges of the\ncomplete graph Kn cannot be partitioned into blocks such that\neach block consists of the edges of a Hamiltonian path.\n(b) (3 points) (difficult) Show that if n is even, then such a partition\nalways exists. The figure below shows a partition for n = 4 (solid\nlines are one block and dashed lines the other).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.314 Combinatorial Analysis\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Combinatorial Analysis, Solutions to Practice Midterm 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-314-combinatorial-analysis-fall-2014/0c8f173950572e7820b5d51c0bc0a6b5_MIT18_314F14_pracq2sol.pdf",
          "content": "18.314: SOLUTIONS TO\nPRACTICE HOUR EXAM #2\n(for hour exam of November 14, 2014)\n1.\n(a) We have\nX\nf(n)xn\n=\nY\n1 + x2k + x2·2k + x3·2k\nn≥0\nk≥0\nk\n=\nk\nY 1 -x4·2\n≥0\n.\n1 -x2k\nThe numerator factors cancel all the denominator factors except\nthe first two, i.e., 1 -x and 1 -x2, so\nX\nf(n)xn =\nn≥0\n.\n(1 -x)(1 -x2)\nHence f(n) is equal to the number of partitions of n with parts 1\nand 2, so S = {1, 2}.\n(b) We want to count partitions of n into parts 1 and 2. The number\nof 2's in the partition can range from 0 to ⌊n/2⌋, and the remaining\nparts must equal 1. Hence the number of choices is 1 + ⌊n/2⌋.\n2. Multiply the recurrence by xn+2 and sum on n ≥0.\nSet F(x) =\nP\nn≥0 a\nn\nnx . We get\nF(x) -x = 6xF(x) -8F(x),\nso\nx\nF(x)\n=\n1 -6x + 8x2\nx\n=\n(1 -2x)(1 -4x)\n1/2\n=\n1 -4x -\n1/2\n1 -2x.\nHence f(n) = 1\n2(4n-2n), Since 1\n2(4n-2n) = 1\nn\n22 (2n-1), f(n) is always\na triangular number.\n\n3. We are choosing an ordered pair (S, T) of subsets of the pencils such\nthat #S is odd and then coloring each pencil in S either red, blue,\ngreen, or yellow, and coloring each pencil in T either white, black, or\nHalaya` u be. If S has k elements where k is odd, then the number of\ncolorings of S is 4k. If T has k elements then the number of colorings\nof T is 3k.\nThe exponential generating function for the number of\ncolorings of S is\nk\nF(x\n=\nk\nX\n4k x\n)\nodd\nk!\n=\nsinh(4x)\n=\n(e4x -e-4x).\nThe exponential generating function for the number of colorings of T\nis\nk\nG(x =\nX\n3k x\n)\nk≥0\n= e3x.\nk!\nHence by Theorem 8.21 on page 168, we have\nn\nX\nx\nf(n)\nn≥0\nn!\n=\n1(e4x -e-4x)e3x\n=\n2(e7x -e-x)\n=\nxn\n(\nX\n7n -(-1)n)\nn≥0\nn! ,\nso f(n) = 1\nn\n2(7 -(-1)n).\nNote. Halaya` ub e is a kind of purple color named after a Phillipines\ndessert made from boiled and grated purple yams. See\nhttp://en.wikipedia.org/wiki/List of colors.\n4.\n(a) Each Hamiltonian path has n -1 edges.\nThe total number of\nedges of Kn is\nn\n\n= n(n -1)/2. Hence the number of paths is\nn/2, which fails to be an integer when n is odd.\n(b) Let the vertices be 0, 1, . . . , n -1. Let one of the paths P have\nvertices (in their order along P)\nn\n0, n -1, 1, n -2, 2, n -3, . . . ,\n.\n\nLet the other paths be obtained from P by adding i to each coor-\ndinate for i = 1, 2, . . . , n\n2 -1, and taking the sum modulo n (i.e.,\nif the sum exceeds n -1 then subtract n from it). For instance, if\nn = 8 then the four paths are\n7.\nWe leave the verification that this works as an exercise.\nAnother way to describe the same solution (suggested by Y. Hu) is\nto put the vertices 0, 1, . . . , n-1 in clockwise order on a circle. Let\nP be the zigzag path whose vertices (in order) are 0, n -1, 1, n -\n2, 2, n-3, 3, . . . , 1\n2n. Rotate the circle around the center by 2jπ/n\nradians for 0 ≤j ≤n\n2 -1. Each such rotation gives one of the\npaths in the partition into Hamiltonian paths.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.314 Combinatorial Analysis\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "Eulerian Digraphs and Oriented Trees",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-314-combinatorial-analysis-fall-2014/99d707393fabef4e95c3d2bd1690af91_MIT18_314F14_mt2.pdf",
          "content": "Eulerian digraphs and oriented trees.\nA famous problem which goes back to Euler asks for what graphs G is there\na closed walk which uses every edge exactly once. (There is also a version for\nnon-closed walks.) Such a walk is called an Eulerian tour (also known as an\nEulerian cycle). A graph which has an Eulerian tour is called an Eulerian\ngraph. Euler's famous theorem (the first real theorem of graph theory) states\nthat G is Eulerian if and only if it is connected and every vertex has even\ndegree. Here we will be concerned with the analogous theorem for directed\ngraphs.\nWe want to know not just whether an Eulerian tour exists, but\nhow many there are. We will prove an elegant determinantal formula for\nthis number closely related to the Matrix-Tree Theorem. For the case of\nundirected graphs no analogous formula is known, explaining why we consider\nonly the directed case.\nA (finite) directed graph or digraph D consists of a vertex set V\n=\n{v1, . . . , vp} and edge set E = {e1, . . . , eq}, together with a function φ : E →\nV × V (the set of ordered pairs (u, v) of elements of V ). If φ(e) = (u, v),\nthen we think of e as an arrow from u to v. We then call u the initial ver-\ntex and v the final vertex of e. (These concepts arose in the definition of\nan orientation in Definition 8.5.) A tour in D is a sequence e1, e2, . . . , er of\ndistinct edges such that the final vertex of ei is the initial vertex of ei+1 for\nall 1 ≤i ≤r -1, and the final vertex of er is the initial vertex of e1. A\ntour is Eulerian if every edge of D occurs at least once (and hence exactly\nonce). A digraph which has no isolated vertices and contains an Eulerian\ntour is called an Eulerian digraph. Clearly an Eulerian digraph is connected.\nThe outdegree of a vertex v, denoted outdeg(v), is the number of edges of\nG with initial vertex v. Similarly the indegree of v, denoted indeg(v), is the\nnumber of edges of D with final vertex v. A loop (edge of the form (v, v))\ncontributes one to both the indegree and outdegree. A digraph is balanced if\nindeg(v) = outdeg(v) for all vertices v.\n2.1 Theorem.\nA digraph D is Eulerian if and only if it is connected\nand balanced.\nProof. Assume D is Eulerian, and let e1, . . ., eq be an Eulerian tour.\nAs we move along the tour, whenever we enter a vertex v we must exit it,\n\nexcept at the very end we enter the final vertex v of eq without exiting it.\nHowever, at the beginning we exited v without having entered it. Hence\nevery vertex is entered as often as it is exited and so must have the same\noutdegree as indegree. Therefore D is balanced, and as noted above D is\nclearly connected.\nNow assume that D is balanced and connected. We may assume that D\nhas at least one edge. We first claim that for any edge e of D, D has a tour\nfor which e = e1. If e1 is a loop we are done. Otherwise we have entered\nthe vertex fin(e1) for the first time, so since D is balanced there is some exit\nedge e2. Either fin(e2) = init(e1) and we are done, or else we have entered\nthe vertex fin(e2) once more than we have exited it. Since D is balanced\nthere is new edge e3 with fin(e2) = init(e3). Continuing in this way, either\nwe complete a tour or else we have entered the current vertex once more than\nwe have exited it, in which case we can exit along a new edge. Since D has\nfinitely many edges, eventually we must complete a tour. Thus D does have\na tour which uses e1.\nNow let e1, . . . , er be a tour C of maximum length. We must show that\nr = q, the number of edges of D. Assume to the contrary that r < q. Since in\nmoving along C every vertex is entered as often as it is exited (with init(e1)\nexited at the beginning and entered at the end), when we remove the edges\nof C from D we obtain a digraph H which is still balanced, though it need\nnot be connected. However, since D is connected, at least one connected\ncomponent H1 of H contains at least one edge and has a vertex v in common\nwith C [why?]. Since H1 is balanced, there is an edge e of H1 with initial\nvertex v. The argument of the previous paragraph shows that H1 has a tour\nC′ of positive length beginning with the edge e. But then when moving along\nC, when we reach v we can take the \"detour\" C′ before continuing with C.\nThis gives a tour of length longer than r, a contradiction. Hence r = q, and\nthe theorem is proved. 2\nOur primary goal is to count the number of Eulerian tours of a connected\nbalanced digraph. A key concept in doing so is that of an oriented tree.\nAn oriented tree with root v is a (finite) digraph T with v as one of its\nvertices, such that there is a unique directed path from any vertex u to v.\nIn other words, there is a unique sequence of edges e1, . . . , er such that (a)\ninit(e1) = u, (b) fin(er) = v, and (c) fin(ei) = init(ei+1) for 1 ≤i ≤r -1.\n\nIt's easy to see that this means that the underlying undirected graph (i.e.,\n\"erase\" all the arrows from the edges of T) is a tree, and that all arrows\nin T \"point toward\" v. There is a surprising connection between Eulerian\ntours and oriented trees, given by the next result (due to de Bruijn and van\nAardenne-Ehrenfest). This result is sometimes called the BEST Theorem,\nafter de Bruijn, van Aardenne-Ehrenfest, Smith, and Tutte. However, Smith\nand Tutte were not involved in the original discovery.\n2.2 Theorem.\nLet D be a connected balanced digraph with vertex set\nV . Fix an edge e of D, and let v = init(e). Let τ(D, v) denote the number\nof oriented (spanning) subtrees of D with root v, and let o(D, e) denote the\nnumber of Eulerian tours of D starting with the edge e. Then\no(D, e) = τ(D, v)\nY\n(outdeg(u)\n.\n∈V\n-1)!\n(6)\nu\nProof. Let e = e1, e2, . . . , eq be an Eulerian tour E in D. For each vertex\nu = v, let e(u) be the \"last exit\" from u in the tour, i.e., let e(u) = ej where\ninit(e(u)) = u and init(ek) = u for any k > j.\nClaim #1. The vertices of D, together with the edges e(u) for all vertices\nu = v, form an oriented subtree of D with root v.\nProof of Claim #1. This is a straightforward verification. Let T be the\nspanning subgraph of D with edges e(u), u = v. Thus if |V | = p, then T has\np vertices and p -1 edges [why?]. There are three items to check to insure\nthat T is an oriented tree with root v:\n(a) T does not have two edges f and f ′ satisfying init(f) = init(f ′). This\nis clear since both f and f ′ can't be last exits from the same vertex.\n(b) T does not have an edge f with init(f) = v. This is clear since by\ndefinition the edges of T consist only of last exits from vertices other\nthan v, so no edge of T can exit from v.\n(c) T does not have a (directed) cycle C. For suppose C were such a cycle.\nLet f be that edge of C which occurs after all the other edges of C in\n\nthe Eulerian tour E. Let f ′ be the edge of C satisfying fin(f) = init(f ′)\n(= u, say). We can't have u = v by (b). Thus when we enter u via\nf, we must exit u. We can't exit u via f ′ since f occurs after f ′ in E.\nHence f ′ is not the last exit from u, contradicting the definition of T.\nIt's easy to see that conditions (a)-(c) imply that T is an oriented tree with\nroot v, proving the claim.\nClaim #2. We claim that the following converse to Claim #1 is true.\nGiven a connected balanced digraph D and a vertex v, let T be an oriented\n(spanning) subtree of D with root v. Then we can construct an Eulerian tour\nE as follows. Choose an edge e1 with init(e1) = v. Then continue to choose\nany edge possible to continue the tour, except we never choose an edge f\nof E unless we have to, i.e., unless it's the only remaining edge exiting the\nvertex at which we stand. Then we never get stuck until all edges are used,\nso we have constructed an Eulerian tour E. Moreover, the set of last exits\nof E from vertices u = v of D coincides with the set of edges of the oriented\ntree T.\nProof of Claim #2. Since D is balanced, the only way to get stuck is to\nend up at v with no further exits available, but with an edge still unused.\nSuppose this is the case. At least one unused edge must be a last exit edge,\ni.e., an edge of T [why?]. Let u be a vertex of T closest to v in T such that\nthe unique edge f of T with init(f) = u is not in the tour. Let y = fin(f).\nSuppose y = v. Since we enter y as often as we leave it, we don't use the\nlast exit from y. Thus y = v. But then we can leave v, a contradiction. This\nproves Claim #2.\nWe have shown that every Eulerian tour E beginning with the edge e\nhas associated with it a \"last exit\" oriented subtree T = T(E) with root\nv = init(e). Conversely, given an oriented subtree T with root v, we can\nobtain all Eulerian tours E beginning with e and satisfying T = T(E) by\nchoosing for each vertex u = v the order in which the edges from u, except\nthe edge of T, appear in E; as well as choosing the order in which all the\nedges from v except for e appear in E.\nThus for each vertex u we have\n(outdeg(u) -1)! choices, so for each T we have Q\nu(outdeg(u) -1)! choices.\nSince there are τ(G, v) choices for T, the proof is complete. 2\n\n2.3 Corollary.\nLet D be a connected balanced digraph, and let v be\na vertex of D. Then the number τ(D, v) of oriented subtrees with root v is\nindependent of v.\nProof.\nLet e be an edge with initial vertex v.\nBy equation (6), we\nneed to show that the number o(G, e) of Eulerian tours beginning with\ne is independent of e.\nBut e1e2 · · ·eq is an Eulerian tour if and only if\neiei+1 · · · eqe1e2 · · · ei-1 is also an Eulerian tour, and the proof follows [why?].\nWhat we obviously need to do next is find a formula for τ(G, v). Such a\nformula is due to W. Tutte in 1948. This result is very similar to the Matrix-\nTree Theorem, and indeed we will show (Example 2.6) that the Matrix-Tree\nTheorem is a simple corollary to Theorem 2.4.\n2.4 Theorem.\nLet D be a loopless connected digraph with vertex set\nV = {v1, . . . , vp}. Let L(D) be the p × p matrix defined by\n\n-mij,\nif i = j and there are m\nd\nij =\n\nij e ges with\nL\ninitial vertex vi and final vertex vj\noutdeg(vi),\nif i = j.\n(Thus L is the directed analogue of the laplacian matrix of an undirected\ngraph.) Let L0 denote L with the last row and column deleted. Then\ndet L0 = τ(D, vp).\n(7)\nNote. If we remove the ith row and column from L instead of the last row\nand column, then equation (7) still holds with vp replaced with vi.\nProof (sketch). Induction on q, the number of edges of D. The fewest\nnumber of edges which D can have is p -1 (since D is connected). Suppose\nthen that D has p -1 edges, so that as an undirected graph D is a tree. If\nD is not an oriented tree with root vp, then some vertex vi = vp of D has\noutdegree 0 [why?]. Then L0 has a zero row, so det L0 = 0 = τ(D, vp). If\non the other hand D is an oriented tree with root vp, then an argument like\nthat used to prove Lemma 1.7 (in the case when S is the set of edges of a\nspanning tree) shows that det L0 = 1 = τ(D, vp).\n\nNow assume that D has q > p -1 edges, and assume the theorem for\ndigraphs with at most q -1 edges. We may assume that no edge f of D\nhas initial vertex v, since such an edge belongs to no oriented tree with root\nv and also makes no contribution to L0.\nIt then follows, since D has at\nleast p edges, that there exists a vertex u = v of D of outdegree at least\ntwo.\nLet e be an edge with init(e) = u.\nLet D1 be D with the edge e\nremoved. Let D2 be D with all edges e′ removed such that init(e) = init(e′)\nand e′ = e. (Note that D2 is strictly smaller than D since outdeg(u) ≥2.)\nBy induction, we have det L0(D1) = τ(D1, vp) and det L0(D2) = τ(D2, vp).\nClearly τ(D, vp) = τ(D1, vp) + τ(D2, vp), since in an oriented tree T with\nroot vp, there is exactly one edge whose initial vertex coincides with that of\ne. On the other hand, it follows immediately from the multilinearity of the\ndeterminant [why?] that\ndet L0(D) = det L0(D1) + det L0(D2).\nFrom this the proof follows by induction. 2\n2.5 Corollary.\nLet D be a connected balanced digraph with vertex set\nV = {v1, . . . , vp}. Let e be an edge of D. Then the number o(D, e) of Eulerian\ntours of D with first edge e is given by\no(D, e) = (det L0(D))\nu\nY\n(outdeg(u)\n∈V\n-1)!.\nEquivalently (using Lemma 1.9), if L(D) has eigenvalues μ1, . . ., μp with μp =\n0, then\no(D, e) =\nμ1\nμp\n(outdeg(u)\n1)!.\np\n· · ·\n-\nu\nY\n∈V\n-\nProof. Combine Theorems 2.2 and 2.4. 2\n2.6 Example.\n(the Matrix-Tree Theorem revisited) Let G be a con-\nˆ\nnected loopless undirected graph. Let G be the digraph obtained from G by\nreplacing each edge e = uv of G with a pair of directed edges u →v and\nv →u\nGˆ\n. Clearly\nis balanced and connected. Choose a vertex v of G. There\nis an obvious one-to-one correspondence between spanning trees T of G and\nˆ\nˆ\noriented spanning trees T of G with root v, namely, direct each edge of T\n\nv\nL G\nL Gˆ\ntoward\n. Moreover,\n( ) =\n( ) [why?]. Hence the Matrix-Tree Theorem\nis an immediate consequence of the Theorem 2.4.\n\n2.7 Example. (the efficient mail carrier) A mail carrier2 has an itinerary\nof city blocks to which he (or she) must deliver mail. He wants to accomplish\nthis by walking along each block twice, once in each direction, thus passing\nalong houses on each side of the street. The blocks form the edges of a graph\nG, whose vertices are the intersections. The mail carrier wants simply to walk\nalong an Eulerian tour in the digraph Gˆ of the previous example. Making the\nplausible assumption that the graph is connected, not only does an Eulerian\ntour always exist, but we can tell the mail carrier how many there are. Thus\nhe will know how many different routes he can take to\navoid boredom. For instance, suppose G is the 3 × 3 grid illustrated below.\nr\nr\nr\nr\nr\nr\nr\nr\nr\nThis graph has 128 spanning trees. Hence the number of mail carrier\nroutes beginning with a fixed edge (in a given direction) is 128 · 1!42!43! =\n12288. The total number of routes is thus 12288 times twice the number of\nedges [why?], viz., 12288×24 = 294912. Assuming the mail carrier delivered\nmail 250 days a year, it would be 1179 years before he would have to repeat\na route!\n2.8 Example.\n(binary de Bruijn sequences) A binary sequence is just\na sequence of 0's and 1's.\nA binary de Bruijn sequence of degree n is a\nbinary sequence A = a1a2 · · · a2n such that every binary sequence b1 · · ·bn of\nlength n occurs exactly once as a \"circular factor\" of A, i.e., as a sequence\naiai+1 · · · ai+n-1, where the subscripts are taken modulo n if necessary. For\ninstance, some circular factors of the sequence abcdefg are a, bcde, fgab,\nand defga.\nNote that there are exactly 2n binary sequences of length n,\nso the only possible length of a binary de Bruijn sequence of degree n is 2n\n[why?]. Clearly any cyclic shift aiai+1 · · · a2na1a2 · · · ai-1 of a binary de Bruijn\nsequence a1a2 · · ·a2n is also a binary de Bruijn sequence, and we call two such\n2postperson?\n\nsequences equivalent. This relation of equivalence is obviously an equivalence\nrelation, and every equivalence class contains exactly one sequence beginning\nwith n 0's [why?]. Up to equivalence, there is one binary de Bruijn sequence\nof degree two, namely, 0011. It's easy to check that there are two inequivalent\nbinary de Bruijn sequences of degree three, namely, 00010111 and 00011101.\nHowever, it's not clear at this point whether binary de Bruijn sequences exist\nfor all n. By a clever application of Theorems 2.2 and 2.4, we will not only\nshow that such sequences exist for all positive integers n, but we will also\ncount the number of them. It turns out that there are lots of them. For\ninstance, the number of inequivalent binary de Bruijn sequences of degree\neight is equal to\n1329227995784915872903807060280344576.\nThe reader with some extra time on his or her hands is invited to write down\nthese sequences. De Bruijn sequences are named after Nicolaas Govert de\nBruijn, who published his work on this subject in 1946. However, it was\ndiscovered in 1975 that de Bruijn sequences had been earlier created and\nenumerated by C. Flye Sainte-Marie in 1894. De Bruijn sequences have a\nnumber of interesting applications to the design of switching networks and\nrelated topics.\nOur method of enumerating binary de Bruijn sequence will be to set up a\ncorrespondence between them and Eulerian tours in a certain directed graph\nDn, the de Bruijn graph of degree n. The graph Dn has 2n-1 vertices, which\nwe will take to consist of the 2n-1 binary sequences of length n -1.\nA\npair (a1a2 · · · an-1, b1b2 · · · bn-1) of vertices forms an edge of Dn if and only if\na2a3 · · · an-1 = b1b2 · · ·bn-2, i.e., e is an edge if the last n -2 terms of init(e)\nagree with the first n-2 terms of fin(e). Thus every vertex has indegree two\nand outdegree two [why?], so Dn is balanced. The number of edges of Dn is\n2n. Moreover, it's easy to see that Dn is connected (see Lemma 2.9). The\ngraphs D3 and D4 look as follows:\n\nSuppose that E = e1e2 · · ·e2n is an Eulerian tour in Dn. If fin(ei) is the\nbinary sequence ai1ai2 · · · ai,n-1, then replace ei in E by the last bit ai,n-1. It\nis easy to see that the resulting sequence β(E) = a1,n-1a2,n-1 · · · a2n,n-1 is a\nbinary de Bruijn sequence, and conversely every binary de Bruijn sequence\narises in this way. In particular, since Dn is balanced and connected there\nexists at least one binary de Bruijn sequence. In order to count the total\nnumber of such sequences, we need to compute det L(Dn).\nOne way to\ndo this is by a clever but messy sequence of elementary row and column\noperations which transforms the determinant into triangular form. We will\ngive instead an elegant computation of the eigenvalues of L(Dn) based on\nthe following simple lemma.\n2.9 Lemma.\nLet u and v be any two vertices of Dn. Then there is a\nunique (directed) walk from u to v of length n -1.\nProof. Suppose u = a1a2 · · · an-1 and v = b1b2 · · ·bn-1. Then the unique\npath of length n -1 from u to v has vertices\na1a2 · · · an-1, a2a3 · · · an-1b1, a3a4 · · · an-1b1b\n2, . . . , an-1b1 · · ·bn-2, b1b2 · · · bn-1.\n2.10 Theorem.\nThe eigenvalues of L(Dn) are 0 (with multiplicity one)\nand 2 (with multiplicity 2n-1 -1).\n\nProof. Let A(Dn) denote the directed adjacency matrix of Dn, i.e., the\nrows and columns are indexed by the vertices, with\nAuv =\n\n1,\nif (u, v) is an edge\n0,\notherwise.\nNow Lemma 2.9 is equivalent to the assertion that An-1 = J, the 2n-1×2n-1\nmatrix of all 1's [why?]. If the eigenvalues of A are λ1, . . . λ2n-1, then the\neigenvalues of J = An-1 are λn-1\nn\n, . . ., λ -1\n2n-1. By Lemma 1.4, the eigenvalues\nof J are 2n-1 (once) and 0 (2n-1 -1 times). Hence the eigenvalues of A are\n2ζ (once, where ζ is an (n -1)-st root of unity to be determined), and 0\n(2n-1 -1 times). Since the trace of A is 2, it follows that ζ = 1, and we have\nfound all the eigenvalues of A.\nNow L(Dn) = 2I -A(Dn) [why?]. Hence the eigenvalues of L are 2 -\nλ1, . . . , 2 -λ2n-1, and the proof follows from the above determination of\nλ1, . . . , λ2n-1. 2\n2.11 Corollary.\nThe number B0(n) of binary de Bruijn sequences of\ndegree n beginning with n 0's is equal to 22n-1-n. The total number B(n) of\nbinary de Bruijn sequences of degree n is equal to 22n-1.\nProof. By the above discussion, B0(n) is the number of Eulerian tours\nin Dn whose first edge the loop at vertex 00 · · ·0. Moreover, the outdegree\nof every vertex of Dn is two. Hence by Corollary 2.5 and Theorem 2.10 we\nhave\nB0(n) =\n22n-1-1\nn\nn-1\n= 22 -1-n.\nFinally, B(n) is obtained from B0(n) by multiplying by the number 2n of\nedges, and the proof follows. 2\nNote that the total number of binary sequences of length 2n is N = 22n.\nBy the√previous corollary, the number of these which are de Bruijn sequences\nis just\nN. This suggests the following unsolved problem. Let An be the set\nof all binary sequences of length 2n. Let Bn be the set of binary de Bruijn\nsequences of degree n. Find an explicit bijection φ : Bn × Bn →An, thereby\ngiving a combinatorial proof of Corollary 2.11.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.314 Combinatorial Analysis\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Resource",
          "title": "The Matrix Tree Theorem",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-314-combinatorial-analysis-fall-2014/2724112ea36679f82dc04f0b2f4f355e_MIT18_314F14_mt.pdf",
          "content": "THE MATRIX-TREE THEOREM\nThe Matrix-Tree Theorem.\nThe Matrix-Tree Theorem is a formula for the number of spanning trees of\na graph in terms of the determinant of a certain matrix. We begin with the\nnecessary graph-theoretical background. Let G be a finite graph, allowing\nmultiple edges but not loops. (Loops could be allowed, but they turn out to\nbe completely irrelevant.) We say that G is connected if there exists a walk\nbetween any two vertices of G. A cycle is a closed walk with no repeated\nvertices or edges, except for the the first and last vertex. A tree is a connected\ngraph with no cycles. In particular, a tree cannot have multiple edges, since\na double edge is equivalent to a cycle of length two. The three nonisomorphic\ntrees with five vertices are given by:\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\n@\n@\n@@\n@\n@\n@\n@\n@\nA basic theorem of graph theory (whose easy proof we leave as an exercise)\nis the following.\n1.1 Proposition.\nLet G be a graph with p vertices. The following\nconditions are equivalent.\n(a) G is a tree.\n(b) G is connected and has p -1 edges.\n(c) G is has no cycles and has p -1 edges.\n(d) There is a unique path (= walk with no repeated vertices) between any\ntwo vertices.\n\nA spanning subgraph of a graph G is a graph H with the same vertex set\nas G, and such that every edge of H is an edge of G. If G has q edges, then\nthe number of spanning subgraphs of G is equal to 2q, since we can choose\nany subset of the edges of G to be the set of edges of H. (Note that multiple\nedges between the same two vertices are regarded as distinguishable.)\nA\nspanning subgraph which is a tree is called a spanning tree. Clearly G has a\nspanning tree if and only if it is connected [why?]. An important invariant\nof a graph G is its number of spanning trees, called the complexity of G and\ndenoted κ(G).\n1.2 Example.\nLet G be the graph illustrated below, with edges a, b,\nc, d, e.\nr\nr\nr\nr\n@\n@\n@\n@\n@\n@\nb\ne\na\nc\nd\nThen G has eight spanning trees, namely, abc, abd, acd, bcd, abe, ace, bde, and\ncde (where, e.g., abc denotes the spanning subgraph with edge set {a, b, c}).\n1.3 Example.\nLet G = K5, the complete graph on five vertices. A\nsimple counting argument shows that K5 has 60 spanning trees isomorphic\nto the first tree in the above illustration of all nonisomorphic trees with five\nvertices, 60 isomorphic to the second tree, and 5 isomorphic to the third tree.\nHence κ(K5) = 125. It is even easier to verify that κ(K1) = 1, κ(K2) = 1,\nκ(K3) = 3, and κ(K4) = 16. Can the reader make a conjecture about the\nvalue of κ(Kp) for any p ≥1?\nOur object is to obtain a \"determinantal formula\" for κ(G). For this we\nneed an important result from matrix theory which is often omitted from\na beginning linear algebra course. This result, known as the Binet-Cauchy\ntheorem (or sometimes as the Cauchy-Binet theorem), is a generalization\nof the familiar fact that if A and B are n × n matrices, then det(AB) =\n\ndet(A) det(B) (where det denotes determinant)1. We want to extend this\nformula to the case where A and B are rectangular matrices whose product\nis a square matrix (so that det(AB) is defined). In other words, A will be an\nm × n matrix and B an n × m matrix, for some m, n ≥ 1.\nWe will use the following notation involving submatrices. Suppose A =\n(aij) is an m × n matrix, with 1 ≤i ≤m, 1 ≤j ≤n, and m ≤n. Given an\nm-element subset S of {1, 2, . . ., n}, let A[S] denote the m×m submatrix of A\nobtained by taking the columns indexed by the elements of S. In other words,\nif the elements of S are given by j1 < j2 < · · · < jm, then A[S] = (ai,jk),\nwhere 1 ≤i ≤m and 1 ≤k ≤m. For instance, if\n\nA =\n\nand S = {2, 3, 5}, then\nA[S] =\n\n.\nSimilarly, let B = (bij) be an n × m matrix with 1 ≤i ≤n, 1 ≤j ≤m and\nm ≤n. Let S be an m-element subset of {1, 2, . . ., n} as above. Then B[S]\ndenotes the m × m matrix obtained by taking the rows of B indexed by S.\nNote that At[S] = A[S]t, where t denotes transpose.\n1.4 Theorem.\n(the Binet-Cauchy Theorem) Let A = (aij) be an m×n\nmatrix, with 1 ≤i ≤m and 1 ≤j ≤n. Let B = (bij) be an n × m matrix\nwith 1 ≤i ≤n and 1 ≤j ≤m. (Thus AB is an m × m matrix.) If m > n,\nthen det(AB) = 0. If m ≤n, then\ndet(AB) =\nX\n(det A[S])(det B[S]),\nS\n1In the \"additional material\" handout (Theorem 2.4) there is a more general determi-\nnantal formula without the use of the Binet-Cauchy theorem. However, the use of the\nBinet-Cauchy theorem does afford some additional algebraic insight.\n\nwhere S ranges over all m-element subsets of {1, 2, . . ., n}.\nBefore proceeding to the proof, let us give an example. We write |aij| for\nthe determinant of the matrix (aij). Suppose\nc1\nd1\na\na\na\nA =\nb\n\nb2\nb3\n\n,\nB = c2\nd2\nc3\nd3\n.\nThen\na\na\nc\ndet(\n) =\n\n·\n\nd1\nAB\nb1\nb2\nc2\nd2\n\na1\na3\nc1\nd1\na2\na3\n\nc2\nd2\n\n+\n·\n+\n·\n.\n\nb1\nb3 c3\nd3 b2\nb3 c3\nd3\nProof of Theorem 1.4 (sketch).\nFirst suppose m > n. Since from\nlinear algebra we know that rank(AB) ≤rank(A) and that the rank of an\nm × n matrix cannot exceed n (or m), we have that rank(AB) ≤n < m.\nBut AB is an m × m matrix, so det(AB) = 0, as claimed.\nNow assume m ≤n. We use notation such as Mrs to denote an r × s\nmatrix M. It is an immediate consequence of the definition of matrix multi-\nplication (which the reader should check) that\nRmm\nSmn\nVmn\nWmm\nRV + SX\nRW + SY\n=\n.\n(1)\nTnm\nUnn\nXnn\nYnm\n\nTV + UX\nTW + UY\n\nIn other words, we can multiply \"block\" matrices of suitable dimensions as\nif their entries were numbers. Note that the entries of the right-hand side\nof (1) all have well-defined dimensions (sizes), e.g., RV + SX is an m × n\nmatrix since both RV and SX are m × n matrices.\nNow in equation (1) let R = Im (the m × m identity matrix), S = A,\nT = Onm (the n × m matrix of 0's), U = In, V = A, W = Omm, X = -In,\nand Y = B. We get\n\nIm\nA\n\nA\nOmm\nB\nOnm\nIn\n-I\nB\n\n=\n\nOmn\nA\n.\n(2)\nn\n-In\nB\n\nTake the determinant of both sides of (2). The first matrix on the left-hand\nside is an upper triangular matrix with 1's on the main diagonal. Hence its\n\ndeterminant is one. Since the determinant of a product of square matrices is\nthe product of the determinants of the factors, we get\n\nA\nOmm\nB\n\n=\n-In\n\nB\n\nOmn\nAB\n\n(\n\n-In\n.\n)\n\nIt is easy to see [why?] that the determinant on the right-hand side of\n(3) is equal to ± det(AB). So consider the left-hand side. A nonzero term in\nthe expansion of the determinant on the left-hand side is obtained by taking\nthe product (with a certain sign) of m + n nonzero entries, no two in the\nsame row and column (so one in each row and each column). In particular,\nwe must choose m entries from the last m columns. These entries belong to\nm of the bottom n rows [why?], say rows m + s1, m + s2, . . . , m + sm. Let\nS = {s1, s2, . . ., sm} ⊆{1, 2, . . ., n}. We must choose n -m further entries\nfrom the last n rows, and we have no choice but to choose the -1's in those\nrows m+i for which i ∈S. Thus every term in the expansion of the left-hand\nside of (3) uses exactly n -m of the -1's in the bottom left block -In.\nWhat is the contribution to the expansion of the left-hand side of (3)\nfrom those terms which use exactly the -1's from rows m + i where i ∈S?\nWe obtain this contribution by deleting all rows and columns to which these\n-1's belong (in other words, delete row m + i and column i whenever i ∈\n{1, 2, . . ., n} -S), taking the determinant of the 2m × 2m matrix MS that\nremains, and multiplying by an appropriate sign [why?]. But the matrix MS\nis in block-diagonal form, with the first block just the matrix A[S] and the\nsecond block just B[S]. Hence det MS = (det A[S])(det B[S]) [why?]. Taking\nall possible subsets S gives\ndet AB =\nX\n±(det A[S])(det B[S]).\nS⊆{1,2,...,n}\n|S|=m\nIt is straightforward but somewhat tedious to verify that all the signs are +;\nwe omit the details. This completes the proof. 2\nLet G be a graph with vertices v1, . . . , vp. The adjacency matrix of G\nis the p × p matrix A = A(G), over the field of complex numbers, whose\n(i, j)-entry aij is equal to the number of edges incident to vi and vj. Thus\n\nA is a real symmetric matrix (and hence has real eigenvalues) whose trace\nis the number of loops in G.\nWe now define two matrices related to A(G). Assume for simplicity that\nG has no loops. (This assumption is harmless since loops have no effect on\nκ(G).)\n1.5 Definition.\nLet G be as above. Give G an orientation o, i.e, for\nevery edge e with vertices u, v, choose one of the ordered pairs (u, v) or (v, u).\n(If we choose (u, v), say, then we think of putting an arrow on e pointing from\nu to v; and we say that e is directed from u to v, that u is the initial vertex\nand v the final vertex of e, etc.)\n(a) The incidence matrix M(G) of G (with respect to the orientation o)\nis the p × q matrix whose (i, j)-entry M ij is given by\n\n1,\nif the edge ej has initial vertex vi\nM ij =\n\n-1,\nif the edge ej has final vertex vi\n0,\notherwise.\n(b) The laplacian matrix L(G) of G is the p×p matrix whose (i, j)-entry\nLij is given by\n\n-mij,\nif i = j and there are mij edges between vi and v\nLij =\nj\ndeg(vi),\nif i = j,\nwhere deg(vi) is the number of edges incident to v\nL\ni. (Thus\n(G) is symmetric\nand does not depend on the orientation o.)\nNote that every column of M(G) contains one 1, one -1, and q -2\n0's; and hence the sum of the entries in each column is 0.\nThus all the\nrows sum to the 0 vector, a linear dependence relation which shows that\nrank(M(G)) < p. Two further properties of M(G) and L(G) are given by\nthe following lemma.\n1.6 Lemma.\n(a) We have\nt\nMM = L.\n(b) If G is regular of degree d (i.e., every vertex of G has degree d),\nthen L(G) = dI -A(G), where A(G) denotes the adjacency matrix of G.\n\nHence if G (or A(G)) has eigenvalues λ1, . . . , λp, then L(G) has eigenvalues\nd -λ1, . . ., d -λp.\nProof. (a) This is immediate from the definition of matrix multiplication.\nSpecifically, for vi, vj ∈V (G) we have\n(\nt\nMM )ij =\ne\nX\nM M\nik\njk.\n∈\nk E(G)\nIf i = j, then in order for M ikMjk = 0, we must have that the edge ek\nconnects the vertices vi and vj. If this is the case, then one of M ik and Mjk\nwill be 1 and the other -1 [why?], so their product is always -1. Hence\n(\nt\nMM )ij = -mij, as claimed.\nThere remains the case i = j. Then M M\nik\nik will be 1 if ek is an edge\nwith vi as one of its vertices and will be 0 otherwise [why?]. So now we get\n(\nt\nMM )ii = deg(vi), as claimed. This proves (a).\n(b) Clear by (a), since the diagonal elements of\nt\nMM are all equal to d.\nNow assume that G is connected, and let M 0(G) be M(G) with its last\nrow removed. Thus M 0(G) has p -1 rows and q columns. Note that the\nnumber of rows is equal to the number of edges in a spanning tree of G. We\ncall M 0(G) the reduced incidence matrix of G. The next result tells us the\ndeterminants (up to sign) of all (p-1)×(p-1) submatrices N of M 0. Such\nsubmatrices are obtained by choosing a set S of p -1 edges of G, and taking\nall columns of M 0 indexed by the edges in S. Thus this submatrix is just\nM 0[S].\n1.7 Lemma.\nLet S be a set of p -1 edges of G. If S does not form the\nset of edges of a spanning tree, then det M 0[S] = 0. If, on the other hand,\nS is the set of edges of a spanning tree of G, then det M 0[S] = ±1.\nProof. If S is not the set of edges of a spanning tree, then some subset\nR of S forms the edges of a cycle C in G. Consider the submatrix M 0[R]\nof M 0[S] obtained by taking the columns indexed by edges in R. Suppose\nthat the cycle C defined by R has edges f1, . . ., fs in that order. Multiply\nthe column of M 0[R] indexed by fj by 1 if in going around C we traverse\n\nfi in the direction of its arrow; otherwise multiply the column by -1. These\ncolumn multiplications will multiply the determinant of M 0[R] by ±1. It is\neasy to see (check a few small examples to convince yourself) that every row\nof this modified M 0[R] has the sum of its elements equal to 0. Hence the\nsum of all the columns is 0. Thus in M 0[S] we have a set of columns for\nwhich a linear combination with coefficients ±1 is 0 (the column vector of all\n0's). Hence the columns of M 0[S] are linearly dependent, so det M 0[S] = 0,\nas claimed.\nNow suppose that S is the set of edges of a spanning tree T. Let e be an\nedge of T which is connected to vp (the vertex which indexed the bottom row\nof M, i.e., the row removed to get M 0). The column of M 0[S] indexed by e\ncontains exactly one nonzero entry [why?], which is ±1. Remove from M 0[S]\nthe row and column containing the nonzero entry of column e, obtaining a\n(p-2)×(p-2) matrix M ′\n0. Note that det(M 0[S]) = ± det(M ′\n0) [why?]. Let\nT ′ be the tree obtained from T by contracting the edge e to a single vertex\n(so that vp and the remaining vertex of e are merged into a single vertex u).\nThen M ′\n0 is just the matrix obtained from the incidence matrix M(T ′) by\nremoving the row indexed by u [why?]. Hence by induction on the number\np of vertices (the case p = 1 being trivial), we have det(M ′\n0) = ±1. Thus\ndet(M 0[S]) = ±1, and the proof follows. 2\nWe have now assembled all the ingredients for the main result of this\nsection (due originally to Borchardt). Recall that κ(G) denotes the number\nof spanning trees of G.\n1.8 Theorem.\n(the Matrix-Tree Theorem) Let G be a finite connected\ngraph without loops, with laplacian matrix L = L(G). Let L0 denote L with\nthe last row and column removed (or with the ith row and column removed\nfor any i). Then\ndet(L0) = κ(G).\nProof.\nSince L =\nt\nMM\n(Lemma 1.6(a)), it follows immediately that\nt\nL = M M\n0. Hence by the Binet-Cauchy theorem (Theorem 1.4), we have\ndet(L0) =\nX\n(det M 0[S])(det\nt\nM 0[S]),\n(4)\nS\nwhere S ranges over all (p-1)-element subsets of {1, 2 . . ., q} (or equivalently,\nover all (p -1)-element subsets of the set of edges of G). Since in general\n\nAt[S] = A[S]t, equation (4) becomes\ndet(\n) =\nX\n(det\n[S])2\nL\nM\n.\n(5)\nS\nAccording to Lemma 1.7, det(M 0[S]) is ±1 if S forms the set of edges of a\nspanning tree of G, and is 0 otherwise. Therefore the term indexed by S in\nthe sum on the right-hand side of (5) is 1 if S forms the set of edges of a\nspanning tree of G, and is 0 otherwise. Hence the sum is equal to κ(G), as\ndesired. 2\nThe operation of removing a row and column from L(G) may seem some-\nwhat contrived. We would prefer a description of κ(G) directly in terms of\nL(G). Such a description will follow from the next lemma.\n1.9 Lemma.\nLet M be a p×p matrix with real entries such that the sum\nof the entries in every row and column is 0. Let M0 be the matrix obtained\nfrom M by removing the last row and last column (or more generally, any row\nand any column). Then the coefficient of x in the characteristic polynomial\ndet(M -xI) of M is equal to -p · det(M0). (Moreover, the constant term of\ndet(M -xI) is 0.)\nProof. The constant term of det(M -xI) is det(M), which is 0 since\nthe rows of M sum to 0.\nFor simplicity we prove the rest of the lemma only for removing the last\nrow and column, though the proof works just as well for any row and column.\nAdd all the rows of M -xI except the last row to the last row. This doesn't\neffect the determinant, and will change the entries of the last row all to -x\n(since the rows of M sum to 0). Factor out -x from the last row, yielding a\nmatrix N(x) satisfying det(M -xI) = -x det(N(x)). Hence the coefficient of\nx in det(M -xI) is given by -det(N(0)). Now add all the columns of N(0)\nexcept the last column to the last column. This does not effect det(N(0)).\nBecause the columns of M sum to 0, the last column of N(0) becomes the\ncolumn vector [0, 0, . . ., 0, p]t. Expanding the determinant by the last column\nshows that det(N(0)) = p · det(M0), and the proof follows. 2\n1.10 Corollary.\n(a) Let G be a connected (loopless) graph with p\nvertices. Suppose that the eigenvalues of L(G) are μ1, . . . , μp-1, μp, with μp =\n\n0. Then\nκ(G) =\nμ1μ2 · · · μp\np\n-1.\n(b) Suppose that G is also regular of degree d, and that the eigenvalues of\nA(G) are λ1, . . . , λp-1, λp, with λp = d. Then\nκ(G) =\n(d -λ1)(d -λ2) · · · (d -λp\n1).\np\n-\nProof. (a) We have\ndet(L -xI)\n=\n(μ1 -x) · · · (μp-1 -x)(μp -x)\n=\n-(μ1 -x)(μ2 -x) · · · (μp-1 -x)x.\nHence the coefficient of x is -μ1μ2 · · · μp-1. By Lemma 1.9, we get\n-μ1μ2 · · ·μp-1 = p · det(L0).\nBy Theorem 1.8 we have det(L0) = κ(G), and the proof follows.\n(b) Immediate from (a) and Lemma 1.6(b). 2\nLet us look at a couple of examples of the use of the Matrix-Tree Theorem.\n1.11 Example.\nLet G = Kp, the complete graph on p vertices. Now\nKp is regular of degree d = p -1, and\nA(Kp) + I = J,\nthe p×p matrix of all 1's. Note that rank(J) = 1 [why?], so p-1 eigenvalues\nof J are equal to 0.\nSince trace(J) = p and the sum of the eigenvalues\nequals the trace, the remaining eigenvalue of J is p. Thus the eigenvalues of\nA(Kp) = J-I are -1 (p-1 times) and p-1 (once).Hence from Corollary 1.10\nthere follows\nκ(Kp) =\n((p -1) -(-1))p-1 = pp-2.\np\nThis surprising result is often attributed to Cayley, who stated it without\nproof in 1889 (and even cited Borchardt explicitly). However, it was in fact\nstated by Sylvester in 1857, while a proof was published by Borchardt in\n1860.\nIt is clear that Cayley and Sylvester could have produced a proof\n\nif asked to do so. There are many other proofs known, including elegant\ncombinatorial arguments due to Pru fer, Joyal, Pitman, and others.\n1.12 Example.\nThe n-cube Cn is the graph with vertex set Zn\n2 (the set\nof all n-tuples of 0's and 1's), and two vertices u and v are connected by an\nedge if they differ in exactly one component. Now Cn is regular of degree n,\nand it can be shown that its eigenvalues are n -2i with multiplicity\nn\ni\nfor\n0 ≤i ≤n. (See the solution to Exercise 10.18(d) of the text.) Henc\ne f\n\nrom\nCorollary 1.10(b) there follows the amazing result\nκ(Cn)\n=\nn\n)\n2n\nY\n(n\n(2i\ni)\ni=1\nn\n=\n22n\nn\n-n-1 Y (i\ni).\ni=1\nTo my knowledge a direct combinatorial proof is not known.\n\n18.314 (Fall 2011)\nProblems on the Matrix-Tree Theorem\n1. The complete bipartite graph Krs has vertex set A ∪B, where #A = r,\n#B = s, and A ∩B = Ø. There is an edge between every vertex of\nA and every vertex of B, so rs edges in all. Let L = L(Krs) be the\nlaplacian matrix of Krs.\n(a) Find a simple upper bound on rank(L -rI).\nDeduce a lower\nbound on the number of eigenvalues of L equal to r.\n(b) Assume r = s, and do the same as (a) for s instead of r.\n(c) Find the remaining eigenvalues of L. (Hint. Use the fact that\nthe rows of L sum to 0, and compute the trace of L.)\n(d) Use (a)-(c) to compute κ(Krs), the number of spanning trees of\nKrs.\n(e) (optional) Give a combinatorial proof of the formula for κ(Krs).\n2. Let V be the subset of Z×Z on or inside some simple closed polygonal\ncurve whose vertices belong to Z×Z, such that every line segment that\nmakes up the curve is parallel to either the x-axis or y-axis. Draw an\nedge e between any two points of V at distance one apart, provided e\nlies on or inside the boundary curve. We obtain a planar graph G, an\nexample being\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nLet G′ be the dual graph G∗with the \"outside\" vertex deleted. (The\nvertices of G∗are the regions of G. For each edge e of G, say with\nregions R and R′ on the two sides of e, there is an edge of G∗between\nR and R′.) For the above example, G′ is given by\n\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nr\nLet λ1, . . . , λp denote the eigenvalues of G′ (i.e., of the adjacency matrix\nA(G′)). Show that\np\nκ(G) =\nY\n(4 -λi).\ni=1\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.314 Combinatorial Analysis\nFall 2014\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        }
      ],
      "source_url": "https://ocw.mit.edu/courses/18-314-combinatorial-analysis-fall-2014/",
      "course_info": "18.314 | Undergraduate",
      "subject": "General"
    },
    {
      "course_name": "Seminar in Topology",
      "course_description": "No description found.",
      "topics": [
        "Mathematics",
        "Topology and Geometry",
        "Mathematics",
        "Topology and Geometry"
      ],
      "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nPrerequisites\n\n18.901 Introduction to Topology\n\nDescriptions\n\nThis course is a seminar in topology. The main mathematical goal is to learn about the fundamental group, homology and cohomology. The main non-mathematical goal is to obtain experience giving math talks. Lectures will be delivered by the students, with two students speaking at each class. There are no exams. There will be some homework assignments and a final paper.\n\nTextbooks\n\nHatcher, Allen.\nAlgebraic Topology\n. Cambridge University Press, 2001. ISBN: 9780521795401. [Preview with\nGoogle Books\n]\n\nThis book is also available for free online at\nAllen Hatcher's webpage\n.\n\nMassey, William S.\nA Basic Course in Algebraic Topology\n. Springer-Verlag, 1991. ISBN: 9783540974307.\n\nGrading\n\nThe final grade is determined as follows:\n\nACTIVITIES\n\nPERCENTAGES\n\nLectures and participation\n\n60%\n\nFinal paper\n\n30%\n\nProblem sets\n\n10%\n\nAttendance is mandatory. Every three missed classes will result in the drop of a letter grade; thus one can miss up to two classes with no effect on the grade.\n\nLectures and Participation\n\nEach class two students will give lectures. Each lecture should be about 25 minutes long. Individual lectures will not be graded, but lectures make up a good portion of the final grade. In evaluating your lectures, I will look at their clarity, organization and preparedness. I will also consider how your lectures improve over the course of the semester.\n\nYou will give a practice lecture to a small audience before your first lecture. This group will consisting of the course instructor, the mathematics writing instructor working with this class, and the other student lecturing in the same class as you.\n\nEach lecturer will give one or two exercises relevant to the material being presented. These exercises, and their solutions, should be e-mailed to the course instructor as a Latex file. The exercises can be stated during lecture, though this is not necessary. It's ok if the exercises come from a book (although it'd be preferable if they did not, or at least if they were slightly modified), but be sure to give proper attribution.\n\nAs a member of the audience, I'd like you to write a few comments on each lecture you observe. I'm not asking for any kind of lengthy analysis; it would be enough to point out that the lecturer is writing too small. However, make sure the comments are useful--don't just say \"that proof was good,\" say why. I will collect these comments at the end of class and e-mail them to the lecturer so that they can have some feedback. (The lecturer will not know who made which comments.)\n\nHomework\n\nThere will be approximately four problem sets. These will count towards the final grade. Solutions are to be written in Latex. You may work together on the problem sets, but everyone must write up their own solutions.\n\nThere will also be exercise sets, mainly composed of exercises given by lecturers. These are optional and do not have to be turned in. If you are interested in learning the material, it is probably a good idea to do at least some of the exercises.\n\nFinal Paper\n\nThe final paper is an exposition of a topic in algebraic topology that we will not cover in the seminar. It must be at least 10 pages long and written in Latex. Topics will be selected for the papers by session 15. A first draft is due in session 27, and a final draft two weeks later. In the final five meetings of class, students will give talks on their final papers.\n\nCalendar\n\nSES #\n\nTOPICS\n\nKEY DATES\n\n1-14\n\nFundamental Group, Covering Spaces\n\nProblem set 1 due in Session 9\n\n15-25\n\nHomology\n\nTopic for final paper due in Session 15; Problem set 2 due in Session 20\n\n26-34\n\nCohomology, Poincare Duality\n\nFirst draft of paper due in Session 27; Final draft of paper due is Session 32\n\n35-39\n\nStudent Presentations of their final papers\n\nProblem set 3 due in Session 38",
      "files": [
        {
          "category": "Assignment",
          "title": "Problem Set 1",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-904-seminar-in-topology-spring-2011/714c3a7774fbf1cbb7bda4fbc9c4e932_MIT18_904S11_pset1.pdf",
          "content": "Problem Set 1\n18.904 Spring 2011\nInstructions. Write-up solutions in Latex, print them out and hand them in at the beginning of\nclass on Tuesday, February 22nd. See the website for additional instructions.\nProblem 1. Let n ≥ 1 be an integer. Let CPn denote the set of all lines in Cn+1 passing through\nthe origin. There is a natural map π : Cn+1 \\ {0} → CPn taking a point to the line it spans. We\ngive CPn the quotient topology, so that a set U in CPn is open if and only if π-1(U) is open in\nCn+1 . Let Ui ⊂ CPn denote the set of points of the form π(x0, . . . , xn) where xi = 0.\n\n(a) Show that the Ui form an open cover of CPn .\n(b) Show that an intersection of k + 1 distinct elements of {U0, . . . , Un} is homeomorphic to\n(C×)k × Cn-k, for 0 ≤ k ≤ n. (In particular, each Ui is homeomorphic to Cn.)\n(c) Prove the following lemma. Let X be a topological space and let U be a finite open cover of\nX. Suppose that each element of U is simply connected and any intersection of elements of\nU is non-empty and path-connected. Then X is simply connected. [Hint: use van Kampen's\ntheorem.]\n(d) Conclude that CPn is simply connected.\nRemark. The space CPn is called complex projective space. It is a very important space that shows\nup in all areas of mathematics. The space CP1 is called the Riemann sphere; it is homeomorphic\nto S2 (convince yourself of this!).\nProblem 2. Let X be a topological space and let x1 and x2 be two points in X. Given a path h\nbetween x1 and x2, we have seen that there is a canonical isomorphism\nih : π1(X, x1) → π1(X, x2).\nWrite C(G) for the set of conjugacy classes in a group G, and let\nih : C(π1(X, x1)) → C(π1(X, x2))\ndenote the map induced by ih.\n(a) Give an example (i.e., specify X, x1, x2, h and hi) where ih = ih' , with proof.\n(b) Show that ih = ih' for any two paths h and hi.\n(c) Assume π1(X, x1) is abelian. Show that ih = ih' for any h and hi.\nRemark. Note the contrast between (a) and (b) -- given two choices of basepoints x1 and x2, the\nsets C(π1(X, x1)) and C(π1(X, x2)) are in canonical bijection (assuming X is path connected),\nwhile the groups π1(X, x1) and π1(X, x2) are not.\nRemark. The fact that ih and ih' are not necessarily equal is why π1 is only a functor for basepoint\npreserving maps.\nProblem 3. Let X be a metric (and thus topological) space. Fix a basepoint x0 in X; the word\n\"loop\" will mean \"loop based at x0\" in this problem. Let ΩX denote the set of all loops in X, i.e.,\nthe set of all continuous functions p : [0, 1] → X with p(0) = p(1) = x0. Define a distance function\non ΩX by d(p1, p2) = maxx∈[0,1] d(p1(x), p2(x)).\n(a) Show that concatentation of loops defines a continuous map ΩX × ΩX → ΩX. Conclude\nthat there is a natural map of sets π0(ΩX) × π0(ΩX) → π0(ΩX). [Here π0 denotes the set\nof path components.]\n(b) Show that two loops in X are homotopic if and only if the corresponding points of ΩX are\nin the same path component.\n\n(c) Construct a canonical bijection of sets π0(ΩX) → π1(X, x0). Show that this map is a\nhomorphism, in the sense that it respects the multiplications on the two sets (the one on\nπ0(ΩX) constructed in (a) and the usual group operation on π1(X, x0)).\nRemark. In fact, the bijection from (c) is just the first in a sequence: there are natural group\nisomorphisms πn-1(ΩX, x0) → πn(X, x0) for all n ≥ 1. [Here πn denotes the nth homotopy group.]\nRemark. The above theory does not at all require X to be a metric space, it just simplifies the\ndefinition of the topology on ΩX. When X is a general topological space, the appropriate topology\non ΩX is the \"compact open topology.\"\nProblem 4. In this problem, we will show that every finitely presented group occurs as a funda\nmental group.\n(a) Let G be a group, let a be an element of G and let N be the normal closure of the subgroup\ngenerated by a. [Explicitly, N is the subgroup of G generated by all conjugates of a.] Let\nZ → G be the map defined by 1 → a. Show that the amalgamated free product G ∗ Z 1 is\nisomorphic to G/N. [Here 1 denotes the trivial group.]\n(b) Let X be a topological space with base point x0 and let i : S1 → X be a loop based at x0.\nLet Xi be the topological space obtained by attaching a 2-disc to X via i; that is, Xi is the\nquotient of X I D2 where an element x ∈ S1 = ∂D2 is identified with i(x) ∈ X. Show that\nπ1(Xi, x0) is the quotient of π1(X, x0) by the normal subgroup generated by the class of i.\n[Hint: use van Kampen's theorem.]\n(c) Show that every finitely presented group occurs as a fundamental groups. [Hint: let G be a\nfinitely presented group. Pick a presentation. Start with a bouquet of circles, one for each\ngenerator. Attach a 2-disc for each relation and apply (b).]\nRemark. The requirement that the group be finitely generated is completely unnecessary. The\ngeneral case can be established by the same means.\nRemark. There can be many very different homotopy types that have isomorphic fundamental\ngroups; for instance, both S1 and S1 ∨ S2 have fundamental group Z. However, given a group G\nthere is a unique homotopy type with fundamental group G and contractible universal cover (or\nequivalently, with all other homotopy groups vanishing).\nProblem 5. Let G be a topological group; thus G is simulateneously a group and a topological\nspace, and the multiplication map G × G → G and inversion map G → G are continuous.\n(a) Show that there is a unique group structure on π0(G) such that the natural map G → π0(G)\nis a group homomorphism.\n(b) Show that π1(G, 1) is a commutative group. [Hint: if c is a loop in G based at 1 and g is\nan element of G then t → gc(t) is a loop in G based at g. Using this you can slide one loop\nalong another to show that they commute in π1.]\nProblem 6. Let G = SL(2, R), the group of 2 × 2 real matrices with determinant 1. We can natu\nrally regard G as a closed subset of R4, and thus (after a few simple verifications) as a topological\ngroup. Let B* ⊂ G be the subgroup of matrices which are upper-triangular with positive entries\non the diagonal. Let K ⊂ G be the subgroup of rotations matrices. [An element of G belongs to\nK if and only if its two columns form an orthonormal basis of R2.]\n(a) Show that B* is homeomorphic to R2, and is thus contractible.\n(b) Show that K is homeomorphic to S1 .\n(c) Show that the map B* × K → G sending (b, k) to bk is a homeomorphism.\n(d) Conclude that G is homotopy equivalent to S1, and thus has fundamental group Z.\nRemark. The group SL(2, R) is better than just a topological group: as a topological space it is\nactually a smooth manifold, and the group operations are smooth maps. Such topological groups\nare called Lie groups. They are among the most important objects in all of mathematics.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.904 Seminar in Topology\nSpring 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 1 Solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-904-seminar-in-topology-spring-2011/3dd8c163570186e39e6ab1ae640038c9_MIT18_904S11_soln1.pdf",
          "content": "Solutions to Problem Set 1\n18.904 Spring 2011\nProblem 1\nStatement. Let n ≥1 be an integer. Let CPn denote the set of all lines in Cn+1 passing through\nthe origin. There is a natural map π : Cn+1 \\ {0} →CPn taking a point to the line it spans. We\ngive CPn the quotient topology, so that a set U in CPn is open if and only if π-1(U) is open in\nCn+1. Let Ui ⊂CPn denote the set of points of the form π(x0, . . . , xn) where xi = 0.\n(a) Show that the U form an open cover of CPn\ni\n.\n(b) Show that an intersection of k + 1 distinct elements of {U0, . . . , Un} is homeomorphic to\n(C×)k × Cn-k, for 0 ≤k ≤n. (In particular, each Ui is homeomorphic to Cn.)\n(c) Prove the following lemma. Let X be a topological space and let U be a finite open cover of\nX. Suppose that each element of U is simply connected and any intersection of elements of\nU is non-empty and path-connected. Then X is simply connected. [Hint: use van Kampen's\ntheorem.]\n(d) Conclude that CPn is simply connected.\nSolution. (a) The set π-1(Ui) consists of those points (x0, . . . , xn) of Cn+1 with xi = 0. This is\nopen, and so Ui is open in CPn+1. Every point of CPn is of the form π(x0, . . . , xn) where at least\none of x0, . . . , xn is not zero. If xi is non-zero, then the point belongs to Ui. This shows that the\nUi cover.\n(b) The situation is symmetrical, so to ease notation we only consider the intersection U =\nU0 ∩· · · ∩Uk. Let V be the subspace of Cn+1 consisting of elements of the form (1, x1, . . . , xn)\nwhere xi = 0 for 1\ni\nk. We give V the subspace topology, with which it is clearly homeomorphic\nto (C×)k ×Cn\nk\n≤\n≤\n-. It is clear that π maps V into U. We now define a map in the opposite direction:\ni : U →V,\ni(π(x\nx1\nxn\n0, . . . , xn)) =\n1,\n, . . . ,\n.\nx0\nx0\nIt is easy to see that i is well-defined: first, if π(x) belongs\n\nto U then x0\n\n= 0 and so we can divide\nby x0, and second, if π(x) = π(y) then x = λy for some λ ∈C×, and so xi = yi for all i\nx0\ny\n. It is\nalso easy to see that π and i are mutual inverses. Indeed, if x ∈V then x0 = 1 and so i(π(x)) = x.\nSimilarly, if x ∈U then we can write x = π(y) for some y\n∈Cn+1 \\{0}. We then have i(x) = y\n-y,\nand so π(i(x)) = π(y0\n-y) = π(y) = π(x), since y0\n-y and y span the same line. It remains to show\ncontinuity of each map. The map π : V →U is continuous since it is just the restriction of the\nmap Cn+1 \\ {0} →CPn, which is continuous by definition.\nWe now show that i is continuous. Let W be an open set of V . We must show that i-1(W)\nis an open subset of U. By the definition of the topology on U, this set is open if and only if\nπ-1(i-1(W)) is an open subset of Cn+1. Now, π-1(i-1(W)) is easily seen to be C×W, i.e., it\nconsists of all non-zero multiples of elements of W. Let x be an element of W. We can then find\nan open neighborhood W of 1 in C×, such that W = W -1\n1 , and an open neighborhood W2 of x\nin W such that W1W2 ⊂W. It follows then that C×W contains W1\nk\n× W2 (where here we regard\nW2 as a subset of (C×) × Cn-k). This is an open set of Cn+1 \\ {0}, which shows that x belongs\nto the interior of C×W. Now, if x is an arbitrary element of C×W, then we can find λ ∈C× such\nthat λx ∈W. If W ′ is an open neighborhood of λx in Cn+1 \\ {0} contained in C×V , then λ-1W ′\nis such a neighborhood of x. It follows that all points of C×W belong to its interior, i.e., it is open.\n(c) [When writing this problem, I forgot that we were doing a general version of van Kampen's\ntheorem allowing for covers with more than two open sets. The following inductive proof establishes\n(c) using van Kampen's theorem for only two element covers. Part (c) is fairly trivial to derive\nfrom the general van Kampen theorem.]\n\nWe proceed by induction on the cardinality of U .\nIt is clear if #U = 1.\nNow let U =\n{U1, . . . , Un} be given. Put Y = U2∪U3∪· · ·∪Un. By the inductive hypothesis applied to Y and the\ncover {U2, . . . , Un}, we conclude that Y is simply connected. Now, U1∩Y = (U1∩U2)∪· · ·∪(U1∩Un).\nWe claim that this set is path connected. Thus let x and y be points in it. Then x belongs to\nU1 ∩Ui and y belongs to U1 ∩Uj, for some i and j. The sets U1 ∩Ui and U1 ∩Uj are non-empty,\npath-connected, contained in Y and each contain the non-empty U1 ∩Ui ∩Uj. It follows that we\ncan find a path in U1 ∩Ui from x to some point in U1 ∩Ui ∩Uj, and then a path in U1 ∩Uj from\nthis point to y. The composite path provides a path from x to y contained entirely in U1 ∩Y . This\nproves that U1 ∩Y is path connected. Now, U1 and Y are simply connected and their intersection\nis path-connected; van Kampen's theorem now shows that X = U1 ∪Y is simply connected. This\ncompletes the proof.\n(d) The open cover {U\nn\n0, . . . , Un} of CP\nsatisfies the hypotheses of the lemma from (c), and so\nCPn is simply connected.\nProblem 2\nStatement. Let X be a topological space and let x1 and x2 be two points in X. Given a path h\nbetween x1 and x2, we have seen that there is a canonical isomorphism\nih : π1(X, x1) →π1(X, x2).\nWrite C(G) for the set of conjugacy classes in a group G, and let\nih : C(π1(X, x1)) →C(π1(X, x2))\ndenote the map induced by ih.\n(a) Give an example (i.e., specify X, x1, x2, h and h′) where ih = ih′, with proof.\n(b) Show that ih = ih′ for any two paths h and h′.\n(c) Assume π1(X, x1) is abelian. Show that ih = ih′ for any h and h′.\nSolution. (a) Take X to be S1 ∨S1, take x1 = x2 to be the point where the two circles meet,\ntake h to be the trival path from x1 to itself and take h′ to be the path going around one of\nthe circles. Then ih is the identity map from π1(X, x1) to itself, while ih′ is given by conjugation\nby [h′], regarded as an element of π1(X, x1). Since the center of π1(X, x1) is trivial (we know\nthis fundamental group is the free group on two letters) and [h′] is non-trivial (it is one of the\ngenerators), conjugation by [h′] is not the identity map on π1(X, x1). Thus ih = ih′.\n(b) If g is an element of π1(X, x1) then i\nh(g) is by definition h-gh, where h-\nis the reverse\npath from x2 to x1, and juxtaposition denotes concatenation of paths. We thus have\nih′(g) = (h′)-1gh′ = (h′)-1hih(g)h-1h′ = aih(g)a-1,\nwhere a = (h′)-1h is an element of π1(X, x2). This shows that ih′(g) and ih(g) are conjugate in\nπ1(X, x2). Thus ih′ = ih.\n(c) In an abelian group, two elements are conjugate if and only if they are equal. Thus ih = ih′\nimplies ih = ih′.\nProblem 3\nStatement. Let X be a metric (and thus topological) space. Fix a basepoint x0 in X; the word\n\"loop\" will mean \"loop based at x0\" in this problem. Let ΩX denote the set of all loops in X, i.e.,\nthe set of all continuous functions p : [0, 1] →X with p(0) = p(1) = x0. Define a distance function\non ΩX by d(p1, p2) = maxx [0,1] d(p1(x), p\n∈\n2(x)).\n(a) Show that concatentation of loops defines a continuous map ΩX × ΩX →ΩX. Conclude\nthat there is a natural map of sets π0(ΩX) × π0(ΩX) →π0(ΩX). [Here π0 denotes the set\nof path components.]\n\n(b) Show that two loops in X are homotopic if and only if the corresponding points of ΩX are\nin the same path component.\n(c) Construct a canonical bijection of sets π0(ΩX) →π1(X, x0).\nShow that this map is a\nhomorphism, in the sense that it respects the multiplications on the two sets (the one on\nπ0(ΩX) constructed in (a) and the usual group operation on π1(X, x0)).\nSolution. (a) Let (p1, p2) be an element of ΩX × ΩX and let ε > 0 be given. If (p′\n1, p′\n2) is another\npoint of ΩX × ΩX such that d(p1, p′\n1) < ε and d(p2, p′\n2) < ε, then d(p1p2, p′\n1p′\n2) < ε as well; this\nis immediate from the definitions. By elementary properties of metric spaces, this implies that\nconcatenation of loops is continuous. We now have maps\nπ0(ΩX) × π0(ΩX) →π0(ΩX × ΩX) →π0(ΩX),\nwhere the first comes from basic point-set topology, and the second is the one induced from the\nconcatenation map.\n(b) Let p0 and p1 be two loops in X. Suppose first that they are homotopic. Let pt be a homotopy\nbetween them. Then t 7→pt provides a path between p0 and p1 in ΩX, provided it is continuous.\nWe now show that it is continuous. Let t ∈[0, 1] and ε > 0 be given. Since (t, x) 7→pt(x) is\ncontinuous, for each x ∈[0, 1] we can find an open rectangle Ux in [0, 1]2 containing (t, x) with\nthe property that d(pt1(x1), pt2(x2)) < ε for all (t1, x1) and (t2, x2) in Ux. By compactness of the\ninterval, we can find x1, . . . , xn such that Ux1, . . . , Uxn covers t × [0, 1]. The union of these open\nsets contains a rectangle of the form V × [0, 1], where V is an open interval containing t. Thus for\nany t′ ∈V we have d(pt, pt′) < ε. This shows that t 7→pt is continuous.\nNow suppose that p0 and p1 belong to the same path component of ΩX. Let P : [0, 1] →ΩX be a\npath connecting them, i.e., a continuous map with P(0) = p0 and P(1) = p1. Let e : [0, 1]×ΩX\n→X\nbe the evaluation map (x, p) 7→p(x). Define a map [0, 1] →X by (t, x) 7→e(x, P(t)). This is a\nhomotopy between p0 and p1, provided it is continuous. To show that it is continuous, it suffices\nto show that e is continuous.\nWe now do this. Let (x, p) ∈[0, 1] × ΩX and ε > 0 be given. Let J be an open neighborhood of\nx such that d(p(x1), p(x2)) < ε for all x1, x2 ∈J. Let U be the open ball in ΩX centered at p and\nof radius ε. If (x1, p1) belongs to J × ΩX then\nd(p(x), p1(x1)) ≤d(p(x), p(x1)) + d(p(x1), p1(x1)) ≤2ε.\nThis shows that e is continuous.\n(c) Define a map i : π0(ΩX) →π1(X, x0) as follows. Let C be a path component of ΩX and let\np be a point on C. Then i(C) is the class of p in π1(X, x0). This is well-defined by (b): if p′ is a\ndifferent point on C, then there is a path between p and p′ in ΩX and thus a homotopy between\np and p′ in X, and so p and p′ represent the same class in π1(X, x0). It is also injective by (b). It\nis obviously surjective. Furthermore, it is obviously compatible with the two product operations,\nsince they're both defined by concatenation.\nProblem 4\nStatement. In this problem, we will show that every finitely presented group occurs as a funda-\nmental group.\n(a) Let G be a group, let a be an element of G and let N be the normal closure of the subgroup\ngenerated by a. [Explicitly, N is the subgroup of G generated by all conjugates of a.] Let\nZ →G be the map defined by 1 7→a. Show that the amalgamated free product G ∗Z 1 is\nisomorphic to G/N. [Here 1 denotes the trivial group.]\n(b) Let X be a topological space with base point x\n0 and let i : S →X be a loop based at x0.\nLet X′ be the topological space obtained by attaching a 2-disc to X via i; that is, X′ is the\nquotient of X ⨿D2 where an element x ∈S1 = ∂D2 is identified with i(x) ∈X. Show that\n\nπ1(X′, x0) is the quotient of π1(X, x0) by the normal subgroup generated by the class of i.\n[Hint: use van Kampen's theorem.]\n(c) Show that every finitely presented group occurs as a fundamental groups. [Hint: let G be a\nfinitely presented group. Pick a presentation. Start with a bouquet of circles, one for each\ngenerator. Attach a 2-disc for each relation and apply (b).]\nSolution. (a) It's easiest to prove this using the universal property of amalgamated free products.\nLet H be an arbitrary group. Giving a map G ∗Z 1 →H is the same as giving a map G →H that\nkills a, and this is the same as giving a map G/N →H. This shows that G/N satisfies the same\nuniversal property as G ∗Z 1, and so the two are isomorphic.\n(b) We first remark that it suffices to treat the case where X is path-connected. Indeed, let X1\nbe the path component to which x0 belongs and let X1\n′ be constructed in an analogous manner to\nX′. We have a diagram\nπ1(X, x0)\nπ1(X′, x0)\nπ1(X1, x0)\nπ1(X1\n′, x0)\nThe diagram obviously commutes. The vertical maps are easily seen to be isomorphisms, since π1\nonly depends on the path component that the basepoint lies in. We thus see that if the bottom\nmap is surjective with kernel the normal closure of [i], then the top map has the same property.\nThus we may as well replace X by X1 and assume that X is path-connected.\nLet 0 be a chosen point on D2 not on its boundary. Let U = X′ \\ {0} and let V be the open\nunit disc, regarded as a subset of X′. Let x1 be a point in U ∩V and let h be a path from x0 to\nx1 such that h(t) ∈U ∩V for t = 0. We have a commutative diagram\nπ1(U, x1)\nπ1(X′, x1)\nπ1(U, x0)\nπ1(X′, x0)\nwhere the horizontal maps are the natural ones and the vertical ones are ih. Furthermore, the\nnatural map π1(X, x0) →π1(U, x0) is an isomorphism, since U deformation retracts onto X. It\nfollows that π1(X, x0) →π1(X′, x0) is a surjection with kernel the normal closure of the subgroup\ngenerated by [i] if and only if π1(U, x1) →π1(X′, x1) is a surjection with kernel the normal closure\nof the subgroup generated by j = ih([i]). (Sorry for the two i's!)\nNow, U and V are path-connected open sets that cover X′ and their intersection is path connected\nand contains x1. In fact, their intersection is an annulus and j generates its fundamental group.\nBy van Kampen's theorem, π1(X′, x1) = π1(U, x1) ∗Z 1, where Z is really π1(U ∩V, x1) and 1 is\nreally π1(V, x1). Since the map Z →π1(U, x1) sends 1 to j, we see from part (a) that π1(X′, x1) is\nthe quotient of π1(U, x1) by the normal subgroup generated by j. This completes the proof of (b).\n(c) Let G be a finitely generated group. Let a1, . . . , an be generators for G and let b1, . . . , bm\nbe sufficient relations to present G. Let G0 be the free group on the ai and let Gi be the quotient\nof G0 by the normal subgroup generated by b1, . . . , bi. Note that Gi is the quotient of Gi-1 by\nthe normal subgroup generated by bi and that Gm = G. We now prove inductively that there are\nspaces\nX0 →X1 →· · · →Xm\nsuch that π1(Xi) = Gi, and the map π1(Xi) →π1(Xi+1) is the natural quotient map Gi →Gi+1. To\nobtain X0, simply take a bouquet of circles, one for each ai. Assume now that we have constructed\nXi\n1. Via the map G0 →Gi\n1, we can regard bi as an element of π1(Xi\n1). By (b), we can now\n-\n-\n-\nattach a 2-disc to Xi\n1 to obtain a space X with\n-\ni\nπ1(Xi) = Gi. This completes the proof.\n/\nO\n/\nO\n\n/\nO\n/\nO\n\nProblem 5\nStatement. Let G be a topological group; thus G is simulateneously a group and a topological\nspace, and the multiplication map G × G →G and inversion map G →G are continuous.\n(a) Show that there is a unique group structure on π0(G) such that the natural map G →π0(G)\nis a group homomorphism.\n(b) Show that π1(G, 1) is a commutative group. [Hint: if c is a loop in G based at 1 and g is\nan element of G then t 7→gc(t) is a loop in G based at g. Using this you can slide one loop\nalong another to show that they commute in π1.]\nSolution.\n(a) The map G →π0(G) is surjective, so there is at most one group structure on\nπ0(G) which makes this map a homomorphism. Let G*be the path component of G containing\nthe identity element. Then G*is a normal subgroup of G. Indeed, suppose that x and y belong to\nG*. Let p be a path from 1 to x and let q by a path from 1 to y. Then t 7→xq(t) is a path from\nx to xy. Concatenating this with p, we obtain a path from 1 to xy. This shows that G*is closed\nunder multiplication. It is clear that G*is closed under inversion; t 7→p(t)-1 provides a path from\n1 to x-1. This shows that G*is a group. Finally, if y is any element of G then t 7→yp(t)y-1 is a\npath from 1 to yxy-1, and so G*is normal.\nWe now claim that two elements x and y of G belong to the same path component if and only\nif xy-1 ∈G*. First suppose that xy-1 ∈G*. Let p be a path from 1 to xy-1. Then t 7→p(t)y is\na path from x to y, and so x and y lie in the same path component. Conversely, suppose that p is\na path from x to y. Then t 7→p(t)y-1 is a path from xy-1 to 1, and so xy-1 belongs to G*. This\nestablishes the claim.\nIt follows that the natural map G →π0(G) factors as G →G/G*followed by the bijection\nG/G*→π0(G). Since G*is normal, G/G*is a group, and the bijection of this with π0(G) gives a\ngroup structure on π0(G).\n(b) Let f and g be two loops based at the identity. Let Ft be the concatenation of the following\npaths: first, the path f|[0,t], from 1 to f(t); then, the loop s 7→f(t)g(s), based at f(t); and finally,\nthe map f|[t,1] from f(t) to 1. (In the second step, the juxtaposition denotes multiplication in the\ngroup.) One easily sees that F : [0, 1]2 →G is continuous. (The function F is defined piecewise\non three regions in [0, 1]2. It is clearly continuous on each region, and there is agreement at the\nboundaries. This implies it is continuous.) Now, F(0) is the concatenation gf, while F(1) is the\nconcatenation fg. Thus fg and gf are homotopic, and so π1(X, 1) is commutative.\nProblem 6\nStatement. Let G = SL(2, R), the group of 2 × 2 real matrices with determinant 1. We can natu-\nrally regard G as a closed subset of R4, and thus (after a few simple verifications) as a topological\ngroup. Let B*⊂G be the subgroup of matrices which are upper-triangular with positive entries\non the diagonal. Let K ⊂G be the subgroup of rotations matrices. [An element of G belongs to\nK if and only if its two columns form an orthonormal basis of R2.]\n(a) Show that B*is homeomorphic to R2, and is thus contractible.\n(b) Show that K is homeomorphic to S1.\n(c) Show that the map B*× K →G sending (b, k) to bk is a homeomorphism.\n(d) Conclude that G is homotopy equivalent to S1, and thus has fundamental group Z.\nSolution. (a) The group B*consists of matrices of the form\n\na\nb\na-1\n\nwith a > 0. We thus have evident bijections between B*and R\nR\n≥0 ×\nsending a point (a, b)\nin R≥0 × R to the above matrix, and the above matrix to (a, b) in R≥0 × R. These maps are\n\neach continuous since their components are. We thus find that B*is homeomorphic to R≥0 × R.\nSince R\nis\n≥0\nhomeomorphic to R (by the logarithm and exponential maps), we find that B*is\nhomeomorphic to R2, and thus contractible.\n(b) A rotation matrix necessarily has the form\n\nx\ny\n-y\nx\n\nwith x2+y2 = 1. We thus have evident bijections between K and S1 sending a point (x, y) on S1 to\nthe above matrix, and the above matrix to the point (x, y) on S1. These maps are each continuous\nsince their components are continuous functions.\n(c) Let ⟨, ⟩be the standard inner product on R2; it is given by\n⟨x, y⟩= x1y1 + x2y2.\nLet ∥x∥2 = ⟨x, x⟩be the associated norm. Let e1, e2 be the standard basis for R2. Let g be an\nelement of G. Then ge1 and ge2 is also a basis for R2. The Graham-Schmit process allows us to\ntake this basis and obtain an orthonormal basis. Precisely, put\nge\n=\nge\n=\n( )\n=\nge1, ge2\nge1 -ge1\nf1\nA g ge1,\nf2\n-⟨\n⟩∥\n∥\n= D(g)ge + B(g)ge\n∥ge1∥\n∥ge2 -⟨\n⟩∥\n∥-\n∥\nge1, ge2\nge\nge1\n(Here A(g), B(g) and D(g) are just real numbers; for instance, A(g) = ∥ge\n1∥-.) Then f1 and f2\nform an orthonormal basis for R2. Let β(g) be defined by\nA(g)\nB(g)\nβ(g)-1 =\n\nD(g)\n\nThen fi = gβ(g)-1ei. Since κ(g) = gβ(g)-1 takes the orthonormal basis (e1, e2) to the orthonormal\nbasis (f1, f2), it follows that κ(g) belongs to K. Thus β(g) has determinant 1, and is clearly upper\ntriangular, and so belongs to B*. Since the components of β(g)-1 (i.e., A, B and D) are clearly\ncontinuous functions of G, we find that β : G →B*is continuous. Since κ is defined from β and\nmatrix multiplication, κ : G →K is continuous.\nWe have thus constructed a continuous function\nG →K × B*,\ng 7→(κ(g), β(g))\nwhich is a one-sided inverse to the (obviously) continuous function\nK × B*→G,\n(b, k) 7→bk,\ni.e., the composite G →K ×B*→G is the identity. To finish the proof, it suffices to show that the\nmap K ×B*→G is injective, for then the two maps are forced to be mutual inverses. Thus assume\nthat bk = b′k′. Then (b′)-1b = k′k-1, and so k′k-1 belongs to K ∩B*. However, K ∩B*= 1 (easy\ncalculation), and so k = k′, from which it follows that b = b′. This completes the proof.\n[I just noticed that I did things backwards! The problem asked to show that B*× K →G is a\nhomeomorphism and I showed that K × B*→G is a homeomorphism. There are two ways to fix\nthis. First, one could change the above proof, using row operations instead of column operations.\nOr, one could observe that there is a commutative diagram\nφ\nG\nG\nψ\nK × B*\nB*× K\nwhere the vertical maps are multiplication maps, φ(g) = g-1 and ψ(k, b) = (b-1, k-1). Since φ, ψ\nand the left map are homeomorphisms, it follows that the right map is as well.]\n/\nO\n/\nO\n\n(d) From (a)-(c), we find that G is homeomorphic to R2 × S1, and thus homotopy equivalent to\nS1. Thus π1(G) = Z.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.904 Seminar in Topology\nSpring 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 2",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-904-seminar-in-topology-spring-2011/33c72da76b26921cbb1cbb13fb94dd23_MIT18_904S11_pset2.pdf",
          "content": "Problem Set 2\n18.904 Spring 2011\nInstructions. Same as last time. Due: Friday, March 18.\nProblem 1. Let Sn be the n-sphere and fix a base point 1 ∈ Sn . For a pointed topological space\n(X, x0) let πn(X, x0) denote the set of homotopy classes of maps (Sn , 1) → (X, x0).\n(a) Suppose X is contractible. Prove that πn(X, x0) is a one point set, for any n.\n(b) Suppose p : ( X, xX0) → (X, x0) is a covering space. Show that p∗ : πn( X, xX0) → πn(X, x0) is\na bijection for n ≥ 2. [Here p∗ is defined by p∗(f) = [p * f]; a standard argument shows this\nis well-defined.]\n(c) Let T be a torus (i.e., (S1)d) and t0 ∈ T a basepoint. Prove that πn(T, t0) is a one point set\nfor n ≥ 2. [Hint: what is the universal cover of T ?] This is not at all visually obvious!\nRemark. The set π0(X, x0) is in fact the set of path components of X (convince yourself of this!). This set\nhas no extra structure, such as that of a group. Of course, we know that π1(X, x0) is a group, and can\nbe any group. For n ≥ 2, the sets πn(X, x0) are in fact abelian groups in a natural way. These are the\nhigher homotopy groups. Part (b) above says that the higher homotopy groups don't change when passing\nto covers, in constrast to the fundamental group.\nProblem 2. As is well-known, there's no way to define a continuous square root function on the\nentire complex plane. More generally, one cannot always find a square root of a complex valued\nfunction on a given topological space. We'll show how covering spaces can be used to solve this\nproblem. (In what follows, \"function\" means \"continuous function.\")\n(a) Let X be a topological space and let f : X → C be a function which is never equal to 0.\nShow that there exists a natural degree two covering space p : X\n→ X such that p ∗f has a\nsquare root, i.e., there exists a function fX: X\n→ C such that fX(x)2 = f(p(x)).\n(b) Show that f has a square root if and only if p : X\n→ X is a trivial covering space, i.e.,\nisomorphic to the covering space X I X → X.\n(c) Establish analogues of (a) and (b) with logarithms taking the place of square roots.\nRemark. Notice that if X is simply connected then any non-vanishing complex valued function on X has a\nsquare root and logarithm, since any covering space is then trivial.\nProblem 3. Let C be a category and let A1 and A2 be two objects of C. A triple (B, p1, p2)\nconsisting of an object B of C and morphisms p1 : B → A1 and p2 : B → A2 is called a product of\nA1 and A2 if it satisfies the following condition: given any triple (T, f1, f2) consisting of an object\nT and morphisms f1 : T → A1 and f2 : T → A2, there exists a unique map f : T → B such that\nf1 = p1 * f and f2 = p2 * f. We say that \"C has products\" if for every A1 and A2 there is a product\n(B, p1, p2).\n;\n;\n(a) Suppose that (B, p1, p2) and (B;, p1, p ) are two products of A1 and A2. Show that there\n;\n;\nexists a unique isomorphism i : B → B; such that p1 = p1 * i and p2 = p2 * i.\n(b) For each category C in the following list, say whether C has products or not. If it has\nproducts, describe the product of two general objects (proof not required). If not, give\nan example of two specific objects which do not have a product (with a reason, but not\nnecessarily a formal proof).\n(i) The category of topological spaces.\n(ii) The category of pointed topological spaces.\n(iii) The category of groups.\n(iv) The category of covering spaces of a fixed space X.\n(v) The category whose objects are sets and whose morphisms are bijections of sets.\n\nAs with most notions in category theoy, the notion of a product has a dual notion, that of a\n\"coproduct,\" obtained by reversing all the arrows in the definition. Precisely, a coproduct of A1\nand A2 is a triple (C, i1, i2) consisting of an object C and morphisms i1 : A1 → C and i2 : A2 → C,\nwith a universal property similar to that of the product.\n(c) Carry out part (b) with coproducts in place of products.\nRemark. Due to part (a), products are essentially unique, and there is no harm in speaking of \"the\" product\nof two objects A1 and A2 (when it exists). This is usually denoted A1 × A2. Similar remarks apply to the\ncoproduct; the typical notation is A1 I A2. One of the wonders of category theory is that, after defining a\ncategory, all these things like product and coproduct come \"for free\" -- you don't need to give new definitions\nfor each category.\nProblem 4. In this problem, we'll examine covering spaces of topological groups.\n(a) Let G be a path-connected topological group and let p : GX → G be a covering map with\nGX path-connected. Let X1 be an element of GX mapping to 1 under p. Show that there is a\nunique group law on GX such that X1 is the identity, p is a homomorphism and multiplication\nand inversion are continuous. [Hint: use path lifting!]\n(b) Let G = SL(2, R). In the last problem set, we saw that π1(G, 1) = Z. By the Galois\ncorrespondence, we therefore have a unique connected degree two covering space GX → G\n(up to isomorphism), and by (a) we have a canonical group structure on GX after choosing\nX1. Give a description of GX, as a topological group. [Hint: look up \"metaplectic group\" on\nWikipedia.]\nPart (b) is really hard, don't feel bad if you cannot get it (but do try)!\nProblem 5. Let X be a topological space such that every point has a neighborhood basis of\ncontractible open sets. We'll show how the groups π1(X, x), for x varying, can be put together to\nform a covering space of X. The construction is similar to the that of the universal cover.\nLet Π(X) denote the set of all homotopy classes of loops in X, i.e., the set of all classes [γ] where\nγ : I → X satisfies γ(0) = γ(1). Given [γ] ∈ Π(X) and a contractible open neighborhood U of\nγ(0), let U[γ] consist of all loops of the form [ηγη-1] where η is a path in U with η(1) = γ(0). We\ntopologize Π(X) by taking the U[γ]'s to be a basis. Let p : Π(X) → X be defined by [γ] → γ(0).\n(a) Show that p is a covering space map.\n(b) Construct a canonical bijection fx : π1(X, x) → p-1(x) for any x ∈ X.\n(c) Let h be a path in X from x to y. Let ih : π1(X, x) → π1(X, y) be the usual isomorphism.\nDefine a map i; : π1(X, x) → π1(X, y) as follows. Given [γ] ∈ π1(X, x), regard [γ] as an\nh\nelement of p-1(x) via the isomorphism fx. Let Xh : I → Π(X) be a lift of h with Xh(0) = [γ].\nDefine i; ([γ]) to be Xh(1) ∈ p-1(x1), regarded as an element of π1(X, x1) via f-1 . Show that\nh\ny\nih\nh.\n= i;\n(d) Show that Π(X) is path-connected if and only if X is simply connected.\n(e) Suppose that X is path-connected and let x0 ∈ X be a basepoint. Let q : X ×π1(X, x0) → X\nbe the trivial covering map, given by q(x, [γ]) = x. Show that (Π1(X), p) is isomorphic to\n(X × π1(X, x0), q) as covering spaces if and only if π1(X, x0) is abelian. [Hint: use the\nresult from the first problem set that ih = ih' for any two paths h and h; if π1(X, x0) is\nabelian. You may also use its converse, without proof. It may also be useful to consider the\ncategorical form of the Galois correspondence.]\nRemark. The covering space Π(X) is in some ways more natural than the fundamental group, since it does\nnot depend on a base point. There is a natural multiplication map Π(X) ×X Π(X) → Π(X), where ×X\ndenotes the product in the category of covering spaces of X, which gives each fiber p-1(x) a group law in\nsuch a way that each fx is an isomorphism of groups. The covering space Π(X) is closely related to the\n\"fundamental groupoid.\"\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.904 Seminar in Topology\nSpring 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 2 Solutions",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-904-seminar-in-topology-spring-2011/11006f716c30435c4c024d3fa1c75458_MIT18_904S11_solns2.pdf",
          "content": "Problem Set 2 Solutions\n18.904 Spring 2011\nI don't have time at the moment to write up a complete solution set to this problem set. Everyone\nseemed to have most of the general ideas down, and most of the mistakes were technical. However,\nthere is one general fact -- the categorical form of the Galois correspondence -- that clarifies many\nof the problems, and that no one employed. Perhaps we didn't cover this well enough or completely\nenough, but hopefully the solutions that follow will show you how to work with it.\n1. Review of the Galois correspondence in categorical form\nLet G be a group. Recall that a G-set is a set S together with an action of G on S. If S and S′\nare two G-sets then a map f : S →S′ is simply a map of sets which commutes with the G-action,\ni.e., f(gx) = gf(x) for all g ∈G and x ∈X. In this way, we have a category of G-sets.\nNow let X be a path-connected topological space (with whatever other technical hypotheses are\nneeded, e.g., locally path connected). Let x0 be a point in X. Suppose that p : Y →X is a covering\nspace. We claim that the fiber p-1(x0) naturally has an action of the group π1(X, x0). To see this,\nsuppose that y is a point in the fiber and γ is a loop in X based at x0. By the unique lifting property,\nthere is a unique path γ in Y beginning at y which lifts γ. We define γ · y to be the endpoint of the\npath γe. Since γ is a loop, its endpoint is x0; thus the endpoint of γ is something which maps to x0\nunder p, i.e., an elemen\ne\nt of the fiber p-1(x0). Thus (after verifying details that this is well-defined\nand satisfies the necessary conditions), we have an action of π1(X, x0) on p-1(x0).\nNow suppose that p′ : Y ′ →X is another covering space and w\ne\ne have a map of covering spaces\nf : Y →Y ′. Recall that this means that f is a continuous map such that p′ *f = p. Due to this\ncondition, f maps the set p-1(x ) into (p′)-1\n(x0). One easily checks that this map commutes with\nthe action of π1(X, x0) on each set.\nThe above discussion shows that associating to a covering space Y →X its fiber p-1(x0) defines\na functor\n{covering spaces of X} →{π1(X, x0)-sets}.\n(Here we use the set notation to indicate a category.) The categorical form of the Galois correspon-\ndence states that this functor is an equivalence of categories. Thus, any purely categorical notion\nabout covering spaces can be determined by considering the category of π1(X, x0) sets.\n2. Problem 2\nThe categorical Galois correspondence can be used to give an elegant proof of 2(b), as follows.\nFirst, we introduce a term: if p : Y →X is a covering space, a section is a map of covering spaces\ns : X →Y , where X is regarded as the trivial covering space of itself. In other words, a section is\na map s : X →Y such that p *s = idX. It is clear in 2(b) that giving a square root of f is the\nsame as giving a section of Xe →X, for if we have a section s then we simply pull back f by s to\nget a square root, while if we have a square root f then defining s(x) = (x, f(x)) gives a section\n(where here we think of Xe as a subspace of X × C, as all of you did).\ne\nThus 2(b) is reduced to the following: if X →X\ne\nhas a section then X is isomorphic\ne\nto X ⨿X\nas a covering space. However, this is obvious from the Galois correspondence. We know that X\ncorresponds to a 2 element set S with some\ne\naction of π1(X, x0), while X\ne\ncorresponds to a 1 point\nset S0 with the trivial action of π1(X, x0). A section of X defines an inclusion S0 →S, which shows\ne\nthat one of the two elements of S is fixed under π1(X, x0). But since there are only two elements,\nthe other one must be fixed as well. Thus S is isomorphic\ne\nto S0 ⨿S0 as a π1(X, x0)-set, which\nshows that Xe is isomorphic to X ⨿X as a covering space.\n\nThis proof is perhaps not shorter than some of the ones that you all came up with, but I think its\neasier since you don't have to think about lifting paths. The same method does not apply verbatim\nto 2(c), but you can make it work. Exercise!\n3. Problem 3\nThe categorical Galois correspondence is the easiest way to handle parts (iv) of 3(b) and 3(c).\n(At least if we assume X is path-connected, which the problem didn't, but which you can easily\nreduce to.) Since these questions are about the structure of the category of covering spaces, they\ncan be answered by passing to an equivalent category, i.e., the category of π1(X, x0) sets. In this\ncontext, the answer is clear: the category of G-sets has products and coproducts for any group G.\nThe product of two G-sets S1 and S2 is the cartesian product S1 × S2 of sets, with the diagonal\nG-action, i.e., g(x, y) = (gx, gy). The coproduct of S1 and S2 is the disjoint union S1 ⨿S2 with G\nacting on each in the given manner.\nUnderstanding what these operations are in terms of covering spaces is not difficult, and a useful\nexercise. I'll tell you the answers but not why they're the answers. Suppose p1 : Y1 →X and\np2 : Y2 →X are two covering spaces. Then their coproduct is just the disjoint union of Y1 and Y2,\nwith the obvious map down to X. Their product (in the category of covering spaces) is the fiber\nproduct of Y1 and Y2. This is denoted Y1 ×X Y2 and is defined as the set of points (y1, y2) in Y1 ×Y2\nsuch that p1(y1) = p2(y2), with the subspace topology. Thus it is the union of the products of the\nfibers, which is why it is so named. (These descriptions of product and coproduct are valid even if\nX is not path-connected.)\nMany of you tried to use the (non-categorical) Galois correspondence to do this problem. That's\nthe right direction to think in, but doesn't work for two reaons: first, this bijection applies only to\npointed coverings; and second (and more importantly), it applies only to connected coverings. It\nis clear from the previous paragraph that the coproduct of covering spaces gives a non-connected\ncover. It's a worthwhile exercise for you to see how the product of connected covering spaces can\nend up being non-connected. For instance, if Y is a non-trivial covering space of X then its self-\nproduct Y ×X Y is always disconnected (hint: find a map of covering spaces from Y to Y ×X Y\nand use this to see that π1(X, x0) does not act transitively on the fiber of Y ×X Y ).\n4. Problem 5\nIn this problem the use of the categorical form of the Galois correspondence makes the solution\nvery elegant and provides much greater understanding of what's going on, at least for parts (d)\nand (e). Fix a point x0 in X. By part (a) we have that Π(X) is a covering space of X. By\npart (b) we know its fiber over x0 can be identified with π1(X, x0). This gives us the set that\nΠ(X) corresponds to under the categorical form of the correspondence. But what is the action of\nπ1(X, x0) on this set? The answer is provided in (c): π (X, x ) acts on the fiber p-1\n(x0) = π1(X, x0)\nby conjugation. (The definition of this action is given by i′\nh, and (c) shows that it equals ih, which\nis just conjugation by h.) We thus see that Π(X) corresponds to the π1(X, x0)-set π1(X, x0), with\naction being conjugation.\nParts (d) and (e) are now very easy, when combined with some further properties of the Galois\ncorrespondence. Let's do (d) first. A covering space of X is path-connected if and only if the\ncorresponding π1(X, x0)-set is transitive. The action of π1(X, x0) on itself by conjugation fixes\nthe identity element, and therefore is transitive if and only if π1(X, x0) = 1. Thus Π(X) is path-\nconnected if and only if X is simply connected.\nNow for part (e). A covering space of X is trivial if and only if the corresponding π1(X, x0)-set\nis trivial, i.e., every point is fixed by the action of π1(X, x0). The action of π1(X, x0) on itself by\nconjugation fixes all elements if and only if π1(X, x0) is abelian -- that is the definition of abelian!\nThis completes (e).\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.904 Seminar in Topology\nSpring 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        },
        {
          "category": "Assignment",
          "title": "Problem Set 3",
          "type": "PDF",
          "source_url": "https://ocw.mit.edu/courses/18-904-seminar-in-topology-spring-2011/6514e49e64c48fe32fc2f40d044e37bb_MIT18_904S11_pset3.pdf",
          "content": "Problem Set 3\n18.904 Spring 2011\nDue date. Monday, May 9, 2011.\nProblem 1 (5 points). Let T = S1 ×S1 be the two dimensional torus and let T / be the complement\nin T of a small disc. Let X be the space obtained by taking two copies of T / and connecting them\nby a cylinder meeting the boundaries of the removed discs. Compute Hi(X) for all i and describe\nthe cup product H1(X) × H1(X) → H2(X).\nProblem 2 (5 points). Let X be the topological space with four points a, b, x and y such that {x}\nand {y} are closed, while the closure of {a} is {a, x, y} and the closure of {b} is {b, x, y}. Compute\nHi(X) for all i ≥ 0.\nRemark. I believe it is true, generally, that for every compact CW complex X one can construct a\nfinite topological space X/ and a map X → X/ such that the induced map Hi(X) → Hi(X/) is an\nisomorphism. Thus finite spaces with non-Hausdorff topologies should not be ignored!\nProblem 3 (5 points). Let G be the cyclic group of order 3. Construct a topological space X on\nwhich G acts such that H2(X) is isomorphic to Z2 and the induced action of G is non-trivial.\nProblem 4 (15 points). Let c be a generator for H1(S1) ∼\nFor two topological spaces X and\n= Z.\nY , write [X, Y ] for the set of homotopy classes of maps between X and Y . Given a topological\nspace X, we have a natural map\nΦX : [X, S1] → H1(X),\nf → f ∗(c).\nObserve that Φ is a natural transformation of functors, that is, if X → Y is a map then there is a\ncommutative diagram\nH1(Y )\n- H1(X)\n\nΦY\nΦX\n[Y, S1]\n- [X, S1]\nThis will be a useful observation in what follows.\nThe purpose of this problem is to show that ΦX is an isomorphism whenever X is a CW complex\nwith finitely many cells. We'll break the proof into many steps.\n(a) Show that ΦX is an isomorphism if X is a one dimensional CW complex.\nWe now do some basic obstruction theory. Let me remind you of a simple fact: a map ∂Dn → X\nextends to Dn if and only if it is null-homotopic.\n(b) Let f and g be maps Dn → S1, with n ≥ 2, and let H be a homotopy between f|∂Dn and\ng|∂Dn . Show that H can be extended to a homotopy between f and g. [Hint: Interpret H\nas a map from a sphere and extend it to a disc!]\n(c) Let X be a topological space and let Y be obtained from X be attaching a single n-cell.\nShow that the natural map [Y, S1] → [X, S1] is injective if n ≥ 2.\n(d) Notation as in (c), show that [Y, S1] → [X, S1] is surjective if n ≥ 3.\nFor the next few steps, let X be a finite two dimensional CW complex and let Y be obtained from\nX be attaching a single 2-cell. Let i : S1 → X be the attaching map. We regard X as a subspace\nof Y .\n(e) Show that there is an exact sequence\n0 → H1(Y ) → H1(X) i∗\n→ H1(S1).\n\n(f) Let f : X → S1 be a given map. Show that f extends to Y if and only if f∗(c) belongs to\nH1(Y ). [Hint: Use part (e) to characterize H1(Y ) as a subset of H1(X) and the fact that\na map h : S1 → S1 is null-homotopic if and only if h∗(c) = 0.]\n(g) Suppose that ΦX is bijective. Show that ΦY is as well.\nBy part (f) and an easy induction argument, we find that ΦX is an isomorphism for all finite two\ndimensional CW complexes X. We now complete the argument. Let X be a finite CW complex\nand let Y be obtained from X be attaching a single n-cell, with n > 2.\n(h) Show that the natural map H1(Y ) → H1(X) is an isomorphism.\n(i) Suppose that ΦX is bijective. Show that ΦY is as well.\nWe now find that ΦX is bijective for all finite CW complexes X by induction!\nRemark. In fact, it is true that for any n ≥ 0 there is a CW complex Kn such that [X, Kn] = Hn(X)\nholds for CW complexes X. The above problem proves this for n = 1 and shows K1 = S1 . (At least\nwhen X is finite.) It is very easy to see that K0 = Z. It is much less easy to see that K2 = CPinf .\nFor n > 2, I do not know a nice description of Kn.\nProblem 5 (10 points). Let C be a category and let Set be the category of sets. There is a\ncategory F = Fun(Cop, Set) whose objects are functors Cop → Set and whose morphisms are\nnatural transformations of functors. (Recall that Cop is the opposite category to C, and that a\nfunctor Cop → Set is the same thing as a contravariant functor C → Set.)\n(a) Let X be an object of C. Show that hX (T ) = HomC (T, X) defines a functor hX : Cop → Set.\n(b) Show that X → hX defines a functor h : C →F.\n(c) Show that the functor h above is fully faithful, i.e., the natural map HomC(X, Y ) →\nHomF (hX , hY ) is a bijection.\n(d) In particular, show that if hX is isomorphic to hY in the category F then X is isomorphic\nto Y in the category C.\nRemark. The above result (specifically part (c)) is known as Yoneda's lemma. It can be very\nconfusing when you first see it, but it is essentially tautological! Yoneda's lemma is an extremely\nuseful organizational device: it says that an object in a category is fully determined by how other\nobjects of the category map to it.\nRemark. Let F : C → Set be a functor. We say that an object X of C represents F if F is\nisomorphic to hX . Such an object is unique up to isomorphism by Yoneda's lemma, but need not\nexist. In this language, Problem 4 is just the statement that S1 represents the functor H1 (on the\ncategory of finite CW complex with homotopy classes of maps). Yoneda's lemma tells us that this\nproperty uniquely characterizes S1: if another finite CW complex represents H1 then it must be\nhomotopy equivalent to S1 .\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\n18.904 Seminar in Topology\nSpring 2011\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
        }
      ],
      "source_url": "https://ocw.mit.edu/courses/18-904-seminar-in-topology-spring-2011/",
      "course_info": "18.904 | Undergraduate",
      "subject": "General"
    }
  ]
}