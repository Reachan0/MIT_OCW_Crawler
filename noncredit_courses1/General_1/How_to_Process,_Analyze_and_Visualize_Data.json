{
  "course_name": "How to Process, Analyze and Visualize Data",
  "course_description": "This course is an introduction to data cleaning, analysis and visualization. We will teach the basics of data analysis through concrete examples. You will learn how to take raw data, extract meaningful information, use statistical tools, and make visualizations.\nThis was offered as a non-credit course during the Independent Activities Period (IAP), which is a special 4-week term at MIT that runs from the first week of January until the end of the month.",
  "topics": [
    "Engineering",
    "Computer Science",
    "Data Mining",
    "Graphics and Visualization",
    "Engineering",
    "Computer Science",
    "Data Mining",
    "Graphics and Visualization"
  ],
  "syllabus_content": "Welcome! This class is an introduction to data cleaning, analysis and visualization. We will walk you through as we analyze real world datasets. Each day, we will spend the first 30 minutes introducing the day's concepts, and spend the rest of the class doing the lab exercises. We have written a daily walkthrough that you will read and program through in class, and we will be available to help.\n\nThis is our first time teaching this course, and we'll be learning as much as you. Don't hesitate to ask us to change something or improve on something. We'll be grateful.\n\nPrerequisites\n\nWe assume you have a working knowledge of Python (perhaps from\n6.01\n) and are willing to write code. Most of the code you interact with will come with an example that you can modify. Hopefully you won't need to write too much custom code--unless you're inspired to write more!\n\nWe expect you to install several development-related Python modules, and download the datasets we will be using in the class. Instructions can be found under Day 0, in the\nLectures and Labs\nsection.\n\nWhat We Will Teach\n\nWe will teach the basics of data analysis through concrete examples. All of your programming will be written in Python. The schedule is as follows:\n\nDay 0 (today): setup\n\nDay 1: An end-to-end example getting you from a dataset found online to several plots of campaign contributions.\n\nDay 2: Lots of visualization examples, and practice going from data to chart.\n\nDay 3: Statistics basics, including t-tests, linear regression, and statistical significance. We'll use campaign finance and per-county health rankings.\n\nDay 4: Text processing on a large text corpus (the Enron email dataset) using tf-idf and cosine similarity.\n\nDay 5: Scaling up to process large datasets using Hadoop/MapReduce on a larger copy of the Enron dataset.\n\nDay 6: You tell us! Get into groups or work on your own to analyze a dataset of your choosing, and tell us a story!\n\nWhat We Will Not Teach\n\nR. R is a wonderful data analysis, statistics, and plotting framework. We will not be using it because we can achieve all of our objectives in Python, and more MIT undergraduates know Python.\n\nVisualization using browser technology (canvas, svg, d3, etc) or in non-Python languages (\nProcessing\n). These tools are very interesting, and lots of visualizations on the web use these tools (such as the\nNew York Times visualizations\n), but they are out of the scope of this class. We'll teach you how to visualize data in static charts. If this is an area of interest for you, the next step will be to build interactive visualizations that the world can explore, and we can point you in the right direction with these.\n\nGitHub\n\nThe course materials can also be found\nin this GitHub repository\n, which may be updated more frequently than the OCW site.",
  "files": [
    {
      "category": "Lecture Notes",
      "title": "Day 0 Lecture Notes: Setup",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/42d3a02506e3a2bbb8c1c8eb120bfe3b_MITRES_6_009IAP12_lec0.pdf",
      "content": "How to Process, Visualize,\nand Analyze Data\nEugene Wu, Adam Marcus\n\nCourse Basics\n- 5 lab days: ~30min background, 2.5h lab\n- 1 presentation madness day\n- No grades\n- No homework, unless you don't finish lab\n\nDay 6 Madness\n- After lab 4, you will have all the skills you need\n- Find your own dataset + questions\n- Tell us a story: 2 slides, 1 minute\n\nWhy is this important?\n\n(c) The Economist, O'Reilly Media, and Nature Publishing Group. All rights reserved. This content is excluded\nfrom our Creative Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\n\"I keep saying that the sexy job in the\nnext 10 years will be statisticians\"\n\nHal Varian, Chief Economist Google\nStatisticians will never have a sexy job.\nData-powered storytellers will.\n\nSchedule\n\nDay 0\n- Setup\n- Optimistically, you've already done this\n\nToday\n\nDay 2: Visualizations\n\nDay 3: Statistics\n\nDay 4: Text Analysis with Kenneth Lay\nDay 5: Scaling up with\nHadoop/MapReduce\nImages of Kenneth Lay removed due to copyright restrictions.\n\nDay 6: Storytelling Madness!\nbiology\nstackoverflow\nsource code\nhealthcare\nfinance\nweb scraping\neconomics\nsociology\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Day 1 Lecture Notes: Let’s Play with Data!",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/5798f33c524d917abe52819adb68fc5f_MITRES_6_009IAP12_lec1.pdf",
      "content": "Day 1: Let's Play with Data!\n2008 Presidential Election Donations\n\n$1,644,712,232\n\n6X\nEarth's circumference!\n\n10%\nU.S. Dept of Energy's Budget!\n\nFederal Election Commission\n(FEC)\n\n2008 Donations Dataset\nC00430470,\"P80002801\",\n\"McCain, John S\",\n\"WAHLERS, STUART E. MR.\",\n\"A.P.O.\", \"AE\", \"091280013\",\n\"U.S. MILITARY\",\"SOLDIER\",\n250, 31‐JUL‐08,\n\"\",\"X\",\n\"TRANSFER FROM MCCAIN VICTORY 2008\",\n\"SA18\",377957\n\nThis content is in the public domain, from the Federal Election Commission.\n\nTotal donations\nTime\n\nNegative Donations\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Day 2 Lecture Notes: Visualizations",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/bcbf1667eaa64ca643e923fd884520bb_MITRES_6_009IAP12_lec2.pdf",
      "content": "Day 2: Introduction to\nVisualizations\nmatplotlib\n\nVisualization != Sexy Pictures\n\nThe Basics\nblog.okcupid.com\n\nBar Graph\nhttp://blog.okcupid.com\n(c) Humor Rainbow, Inc. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nScatter Plot\nhttp://blog.okcupid.com\n(c) Humor Rainbow, Inc. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nLine Graph\nhttp://blog.okcupid.com\n(c) Humor Rainbow, Inc. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nChoropleth Plots\nhttp://blog.okcupid.com\n(c) Humor Rainbow, Inc. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nBox Plots\n\nmatplotlib\nPython's version of Matlab plotting\n\nHow matplotlib Draws\n\nFigure object\n\nFigure\n\nSubplot object\nFigure\nDraw in here\nSubplot\nDraw in here\ne\ne\n\nfigure.add_subplot(2, 3, 1)\nrows\n3 columns\n\nfigure.add_subplot(2, 3, 1)\nrows\n3 columns\n\nfigure.add_subplot(2, 3, 1)\nrows\n3 columns\n\nfigure.add_subplot(2, 3, 1)\nrows\n3 columns\n\nCharting Library\n- subplot.bar()\n- subplot.plot()\n- subplot.scatter()\n- subplot.boxplot()\n\nLines\nFigure\nSubplot\n\nBoxes\nFigure\nSubplot\n\nPolygons\nFigure\nSubplot\n\nOverlaps\nFigure\nSubplot\n\nOverlaps\nFigure\nSubplot\n\nChoropleth Plots\nhttp://blog.okcupid.com\n(c) Humor Rainbow, Inc. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nNo Easy Way\n\nHow Maps are Drawn\n\nHow Maps are Drawn\n\nHelper Functions\nFIPS\ndraw_county(subplot, county_id, color)\n\ndraw_state(subplot, state, color)\nunty\nFIPS\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Day 3 Lecture Notes: Hypothesis Testing",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/19e0a9fac5ec147115bce81ba4daa68f_MITRES_6_009IAP12_lec3.pdf",
      "content": "How I learned to stop\nvisualizing and love statistics\n\nYou have a hunch\n\nVisualizations sanity check\nStatistics quantify the hunch\n(Visualizations storytelling)\n\nSomeone says:\n\"Obama got more small campaign\ncontributions than McCain\"\n\n???\n\n(c) Jhguch on Wikipedia. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\nMedian\n\n25% 75%\n\nInner Quartile Range\n\nWhiskers / Extremes\n\nOutliers\n\nBox-and-Whiskers Plot\n\n???\n\nAre they actually different?\nT-Test\n\nObama McCain\nObama McCain\nAssume\nReality\n\nObama McCain\nObama McCain\nHow likely is given ?\nHow likely is\n?\n\nObama\nMcCain\navg1\navg2\n\nObama\nMcCain\navg1\navg2\nEffect Size\n\nObama\nMcCain\navg1\navg2\nvariance 1\nvariance 2\n\nHow likely is given ?\nHow likely is\n?\navg1\navg2 2\na\nvariance 1\nvariance 2\n\nHow likely are they equal\ngiven avg/variance differences?\nProbablility p\np is low\np is high\nObama, McCain\nDon't trust\nare different\nthe difference\n(significant)\n(not significant)\nbabl\ni\n\nSignificance is binary\n- Pick a threshold: .01? .05?\n- Is p > threshold, or < threshold?\np < .05? significant\np > .05? don't trust the difference\n\navg1\navg2 2\na\nvariance 1\nvariance 2\nT-Test Signifiance\n# Samples\n\nObama: >1M\nMcCain: >1M\n+\n\nCorrelation, Linear Regression\n\nCounty Health Rankings\n- Every county in USA\n- Years of Potential Life Lost (YPLL): early morbidity\n- less is good\n- more is bad\n- Median income, % population w/ diabetes,\n% population under 18, ...\n\nWhat is correlated with early\ndeath in a community?\nBurgers\nSleep\nEducation\nExercise\n# Rappers\nYour theory here\n\ny = mx + b\nR2 (0 to 1)\np < .05?\nLine coefficients:\nCorrelation amount:\nSignificance:\n\nCorrelation != Causation\nCorrelation\nCausal Hunch\nRandomized Trial\nT-Test!\nal H\nmiz\nTes\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Day 5 Lecture Notes: Processing Large Datasets",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/4e9970d74fb095227b692be7fd134815_MITRES_6_009IAP12_lec5.pdf",
      "content": "Scaling Up with MapReduce,\nHadoop, and Amazon\n\nTerm Frequency\nKenneth Lay\n15 MB\nEnron\n1,300 MB\nGMail\n>1,000,000,000 MB\n\nParallelism\n\n[Google logo]\n\nMapReduce Paper\n[Hadoop logo]\n\nOpen Source Project\n\nCommon Pattern\n\ndoc\ndoc\ndoc\nthe dog\ni\nate\nfruit\nshe\nate\nfruit\nthe\ndog\ni\nate\nate\nshe\nfruit\nfruit\nthe: 1\ndog: 1\ni: 1\nate: 2\nshe: 1\nfruit: 2\n\nLoop\nGroup\nSummarize\n\nWord Count\nLoop\nwords in documents\nGroup\ninstances of a word\nSummarize\ncount instances\nof a word\n\nWord Count\nCandidates\nLoop\nwords in documents\nlines in csv\nGroup\ninstances of a word\ncandidate, day\nSummarize\ncount instances\nof a word\nsum of contributions\nby candidate, day\n\nMapReduce\nLoop\nMap\nGroup\nShuffle\nSummarize\nReduce\nYou Implement\nSh\n\nAmazon Provides Compute Power\nSimple Storage Service (S3): Files\nElastic MapReduce (EMR): Computers\n\nS3 stores files\n- Create bucket (unique name)\n- Files in bucket\n- Access via amazon web console\n- Access via programmatic API\n\nAmazon Charges Us Money\n- S3: 14 cents per GB per month\n- EMR: 10 cents per machine per hour\n- 1 minute = 1 hour\n- Ask us if you use more than 200 hours\n- Excess gets charged to our credit card\n\nSlower Than You Think\nScale, not Performance\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Lecture Notes",
      "title": "Day 6 Lecture Notes: Course Recap",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/9d352b3cac158c5ef71cdc7804544311_MITRES_6_009IAP12_lec6.pdf",
      "content": "Recap\nOverview\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\nImage of Schedule A-P, showing two contributions to Obama for America.\nData includes full name, date of contribution, and contribution amount.\n\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-\n07,\"\",\"\",\"\",\"SA17A\",288757C00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-\n07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\n\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-\n07,\"\",\"\",\"\",\"SA17A\",288757C00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-\n07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-MAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CINGEL, KEITH\",\"SEVERN\",\"AL\",\"20999\",\"SANTA CLAUS\",\"SNOWMAN\",50,17-MAY-07,\"\",\"\",\"\",\"SA17A\",305408\nC00420224,\"P80002983\",\"Cox, John H\",\"DUNAWAY, JONATHON\",\"DEATSVILLE\",\"AL\",\"36022\",\"CSC\",\"TECHNICAL MANAGER\",10,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"TERRY, R.S. MR. SR.\",\"SHEFFIELD\",\"AL\",\"35660\",\"RETIRED\",\"\",25,18-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"CANADY, DALE\",\"PHOENIX\",\"AZ\",\"85051\",\"RETIRED\",\"\",25,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"LORENZ, DWIGHT\",\"SUN CITY\",\"AZ\",\"85351\",\"NONE\",\"RETIRED\",20,12-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"STEWART, MICHAEL\",\"CHANDLER\",\"AZ\",\"85224\",\"DYNAMIC ENERGY\",\"TECHNICIAN\",5,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"ROSENTHAL, ARNOLD\",\"CAREFREE\",\"AZ\",\"85277\",\"RETIRED\",\"\",10,11-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"VADNAIS, DOROTHY\",\"SAN DIEGO\",\"CA\",\"92116\",\"\",\"RETIRED\",10,10-JAN-07,\"\",\"\",\"\",\"SA17A\",288757\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\n\"SANTA CLAUS\",\"SNOWMAN\",\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\n\nT-test\n\nCreate a model\n(linear regression)\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\n\nT-test\n\nCreate a model\n(linear regression)\nSignificance\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\n\nT-test\n\nCreate a model\n(linear regression)\nSignificance\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\n(c) New York Times, FlowingData. All rights reserved. This content is excluded from our\nCreative Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\nImages removed due to copyright restrictions: suggested\nmovies on Netflix, Facebook search, LinkedIn logo.\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\nContext\n\nYesterday and today, 3 companies kindly\ncame to talk about their technologies. I\npersonally found it awesome as well\nbecause it gives context to the stuff we've\nbeen teaching and learning.\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\nSimilar\n\nWhat struck me was how similar their\nprocesses are to what we've done in\nthis class, but on a different dataset,\nor different scale, etc.\n\nPipeline\n- Crazy raw data\n- Cleaned, structured data\n- Exploratory data analysis\n- Verify Hunches\n- Data Product (tm hammer@cloudera)\n\n- Different companies fit into different subsets of the\npipeline\n- locu is the first segment (100% accuracy)\n- visible measures is full pipeline, at huge scale\n- Hadapt makes exploratory and verifying faster\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\nhttp://locu.com/\n\n[logo removed due to copyright\nrestrictions]\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\nhttp://www.visiblemeasures\n.com/\n\nGoogle analytics.\nTakes structured apache logs (access logs) and\nanalyzes them to see how many people are viewing\na particular internet video ad.\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\nhttp://www.vertica.com/\n\n[logo removed due to copyright restrictions]\n\nRaw Data\nCleaned, Structured\nData\nExploratory Data\nAnalysis\nVerify Hunches (stats)\nData Product\nhttp://www.hadapt.com/\n\nHadapt doesn't actively perform data analysis etc.\nInstead, they create platforms that help other\ncompanies (like visiblemeasures) perform their\ndata analysis faster.\n\nYou'll find companies focused on every part of this\npipeline. It's what makes companies \"smarter\".\n\n- Visible Measures\n- Locu\n\n- Gave us context about what companies that\nare centered around data analytics are doing\n- A lot of them are very similar to what we did,\nat a huge scale.\n\nRaw Data\nClean\nData\nExplore\nVerify\nData\nProduct\n- Getting data\n- Visualization\n- Statistics\n- Machine Learning\n- Graph Analysis\n- Text Analysis\n- Databases\n- \"Big Data\"\n\nGetting Data\n- Surveys\n- Web Crawling/Scraping\n- https://scraperwiki.com\n- http://nutch.apache.org\n- Sensors\nRaw Data\nClean\nData\nExplore\nVerify\nData\nProduct\n\nVisualizations\n- Interactive Visualizations\n- HTML5/CSS/JavaScript\n- Tools\n- processingjs, d3, prefuse\n- Blogs\n- http://flowingdata.com\n- http://infosthetics.com\n\n- Harvard http://cs171.org\n- MIT 6.831\n\nRaw Data\nClean\nData\nExplore\nVerify\nData\nProduct\n\nStatistics\n- Are they different?\n- T-Tests, ANOVA\n- Bayesian Statistics\n- Correlation\n- Regressions\n- Linear\n- Non-Linear\n\n- 16.470j\n- http://statistics.mit.edu\nRaw Data\nClean\nData\nExplore\nVerify\nData\nProduct\n\nMachine Learning\n- Classification\n- Clustering\n\n- http://www.ml-class.org\n- MIT 6.867\n- Python scikit-learn (sklearn)\nRaw Data\nClean\nData\nExplore\nVerify\nData\nProduct\n\nGraph Analysis\nRaw Data\nClean\nData\nExplore\nVerify\nData\nProduct\n- Examples:\n- web pages, friend graph, twitter\n- Metrics\n- Centrality\n- Cohesion\n- \"Importance\" (page rank)\n\n- Social Network Analysis\n- Web data mining MIT Course\n- Sep Kamvar Fall 2012\n-\nhttp://www.stats.ox.ac.uk/~snijders/sna_course.htm\n\nText Analysis\nRaw Data\nClean\nData\nExplore\nVerify\nData\nProduct\n- Natural Language Processing\n- Parsing sentences\n- Extracting the grammar/structure\n\n- Similarity measures\n- Cosine Similarity\n- Jaccard\n\n- Identifying Entities\n- Opencalais\n\n- MIT 6.864/6.863J\n\nDatabases\nRaw Data\nClean\nData\nExplore\nVerify\nData\nProduct\n- SQL Implements a lot of what we did\n- Filtering\n- Joining\n- Grouping\n- Summarizing\n- Specialized system to do this\n- SQL databases, Hive, Pig\n\n- MIT 6.830\n- http://db-class.org\n\n\"Big Data\"\nRaw Data\nClean\nData\nExplore\nVerify\nData\nProduct\n- How to process on 1000+ machines?\n- Problems\n- Managing\n- Machines fail all the time\n- Network problems\n- Data out-of-sync (consistency)\n\n- Distributed Systems\n- MIT 6.824 (6.830 a bit)\n\nBerkeley Also Has a Class!\nhttp://datascienc.es\n\nThank You!\ngit pull\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Syllabus",
      "title": "Day 0 Syllabus and Setup",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/b34a71a318b78cae5720e5f6625a75b1_MITRES_6_009IAP12_setup.pdf",
      "content": "Syllabus and Setup\nWelcome!\nThis class is an introduction to data cleaning, analysis and visualization. We will walk you through as\nwe analyze real world datasets. Each day, we will spend the first 30 minutes introducing the day's\nconcepts, and the rest of the class will be exercises. We have written a daily walkthrough that you will\nread and program through in class, and we will be available to help.\nThis is our first time teaching this course, and we'll be learning as much as you. Don't hesitate to ask\nus to change something or improve on something. We'll be grateful.\nPrereqs\nWe assume you have a working knowledge of python (6.01) and are willing to write code. Most of the\ncode you interact with will come with an example that you can modify. Hopefully little specialized\ncode will be generated except for programs you're inspired to write!\nWhat we will teach\nWe will teach the basics of data analysis through concrete examples. All of your programming will be\nwritten in python. The schedule is as follows:\n● Day 0 (today): setup\n● Day 1: An end-to-end example getting you from a dataset found online to several plots of\ncampaign contributions.\n● Day 2: Lots of visualization examples, and practice going from data to chart.\n● Day 3: Statistics basics, including T-Tests, Linear Regression, and statistical significance.\nWe'll use campaign finance and per-county health rankings.\n● Day 4: Text processing on a large text corpus (the Enron email dataset) using tf-idf and cosine\nsimilarity.\n● Day 5: Scaling up to process large datasets using Hadoop/MapReduce on a larger copy of the\nEnron dataset.\n● Day 6: You tell us! Get into groups or work on your own to analyze a dataset of your choosing,\n\nand tell us a story!\nWhat we will not teach\n● R. R is a wonderful data analysis, statistics, and plotting framework. We will not be using it\nbecause we can achieve all of our objectives in Python, and more MIT undergraduates know\nPython.\n● Visualization using browser technology (canvas, svg, d3, etc) or in non python languages\n(Processing). These tools are very interesting, and lots of visualizations on the web use these\ntools (e.g., nytimes visualizations), however they are out of the scope of this class. We'll teach\nyou how to visualize data in static charts. If this is an area of interest for you, the next step will\nbe to build interactive visualizations that the world can explore, and we can point you in the\nright direction with these.\nProgramming Environment (Important!)\nBefore the class, please set up the environment. You will need to install some software, packages,\nand download some datasets to get started.\nWe assume that you are developing in a unix-like environment and are familiar with the common\ncommands (e.g., less, man). If you are a windows user, we assume you are using cygwin but are on\nyour own.\nTools and Libraries\nIn this class, you will need to install a number of tools. The major ones are:\n● python 2.7\nH Python is usually installed in Mac OSX and major unix distributions. Type python --\nversion to make sure it is the right version\n● easy_install\nH python package manager.\n● pip\nH Makes installing python packages really easy. Requires easy_install.\nH Either install it by typing sudo easy_install pip or download the tar.gz file at the\n\nlink above, untar it, go into the newly created directory, and type sudo python setup.\npy install.\n● git\nH git is a version control system. Using it, you can check out our code and examples.\nH If everything is working, check the dataiap sourcecode into a directory called dataiap\nusing git clone git://github.com/dataiap/dataiap.git dataiap\nH We'll be updating the repository periodically. To get the latest copy, go to the dataiap\ndirectory and type git pull.\nWe will also require a number of python modules:\n● numpy 1.6.x: numerical processing module.\nH PIP users can type sudo pip install numpy\n● scipy 0.10: scientific computing module.\nH Ubuntu users can type sudo apt-get install python-scipy\nH PIP users can type sudo pip install scipy\nH Even if PIP works, at least on MacOS you might have to install Fortran. We strongly\nrecommend reading and following the installation instructions.\nH Unfortunately, scipy installation might not work from PIP, and you may have to compile\nit from source (see \"Obtaining and Building NumPy and SciPy\"). Try something akin to\n■ git clone https://github.com/scipy/scipy.git\n■ cd scipy\n■ python setup.py build\n■ python setup.py install\n● matplotlib 1.1.0\nH PIP users can type sudo pip install matplotlib\nH Note: If compiling from source, matplot lib requires a number of other libraries: (libpng,\nfreetype 2)\nH Some MacOS users might run into issues and should just download the binary.\n● dateutil\nH PIP users can type sudo pip install python-dateutil\n● pyparsing\nH PIP users can type sudo pip install pyparsing\n● mrjob: This is a MapReduce package that we will use it in day 5.\n\nH PIP users can type sudo pip install mrjob\nH If compiling from source, it requires boto (try sudo pip install boto).\nFor convenience, Enthought provides numpy, scipy, matplotlib in a single installable package. Many\nstudents that had trouble installing these modules separately were able to install Enthought.\ndataiap/ Directory Structure\nThe repository contains the contents of the full course. We will be using\n● dayX/: files containing the lecture for day X\n● datasets/: the datasets we will be using should live here\n● resources/: contains python scripts that you will eventually run\nH util/: contains python modules we have written that you will use in this course.\nH inst/: instructor python files. Used to setup and test the labs. Please don't view during\nthe course.\nDatasets\nWe will be working with several datasets in this course. Most of them have been added to the git\nrepository.\nThe presidential contributions dataset is fairly large. We will use it on the first day, so please\ndownload it from ftp://ftp.fec.gov/FEC/Presidential_Map/2008/P00000001/P00000001-ALL.zip.\nThe datasets we will use are\n● 2008 Presidential Campaign Contributions\nH The linked file contains all of the 2008 campaign contributions to each presidential\ncandidate. You can look at the 2012 campaign for various primary candidates as well,\nbut we'll work with 2008 since it's complete.\nH unzip into dataiap/datasets/pres_campaign/\n● 2011 County Health Rankings\nH The dataset contains per-county health and morbidity statistics.\nH The necessary data should already be uncompressed in dataiap/datasets/\ncounty_health_rankings/additional_measures_cleaned.csv\ndataiap/datasets/county_health_rankings/ypll.csv\n\n● The Enron email dataset\nH This is the complete set of emails on the enron email server that was released during\nthe scandal. Don't download the dataset as it's huge. We have included subsets of the\ndatasets in the git repository.\n■ dataiap/datasets/emails/kenneth.zip contains a subset of Kenneth Lay's\nemails that you will analyze in day 4.\n■ dataiap/datasets/emails/kenneth_json.zip contains a JSON-encoded\nsubset of Kenneth Lay's emails that you will analyze in day 5.\nH We will upload a JSON encoded version of the full dataset to amazon's S3.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Day 1 Lab Handout",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/0cf5cabdadcec52777c5f5020341a6c6_MITRES_6_009IAP12_lab1.pdf",
      "content": "Day 1: Let's play with some data!\nToday we will analyze the presidential campaign contributions dataset. We will go through the full process of downloading\na new dataset, the initial steps of understanding the data, visualizing it, coming up with hypotheses, and exploring\nthe dataset. Hopefully, you'll learn something new about presidential elections.\nA lot of other organizations have analyzed the data:\n● The FEC is the organization that published the dataset, but also offers basic summaries of the data.\n● Pitch Interactive has some pretty sweet visualizations of the data.\n● More from Pitch Interactive\n● Donor Occupations\nThese visualizations are beautiful but high level overviews, which tend to hide interesting details. We have our\nown questions, and we'll answer some of them today. We'll provide the commands and code to initially explore the\ndata, and ask you to further analyze the data in the exercises.\nFirst Steps\nWe assume that you have already downloaded the dataset. We need to first unzip the file and rename it to\nsomething meaningful:\n> unzip P00000001-ALL.zip\n> mv P00000001-ALL.txt donations.txt\nLets see how much data we are dealing with. The word count (wc) command will tell us the number of lines in this file:\n> wc -l donations.txt\nLet's take a quick look at the file. head prints the first N lines in a file.\n> head -n3 donations.txt cmte_id,cand_id,cand_nm,contbr_nm,contbr_city,contbr_st,\ncontbr_zip,contbr_employer,contbr_occupation,contb_receipt_amt,contb_receipt_dt,receipt_desc,\nmemo_cd,memo_text,form_tp,file_num\nC00420224,\"P80002983\",\"Cox, John H\",\"BROWN, CHARLENE\",\"EAGLE RIVER\",\"AK\",\"99577\",\"\",\"STUDENT\",25,01-\nMAR-07,\"\",\"\",\"\",\"SA17A\",288757\nC00420224,\"P80002983\",\"Cox, John H\",\"KELLY, RAY\",\"HUNTSVILLE\",\"AL\",\"35801\",\"ARKTECH\",\"RETIRED\",25,25-\nJAN-07,\"\",\"\",\"\",\"SA17A\",288757\nOn line 1 we see the names of each field in the file, and the data starts from line 2. It's in a format called CSV, or\ncomma-separated values, where each row contains a new set of field values separated by commas.\nIf we take a look at the file format description on the fec.gov website, it specifies that\nThe text file is comma delimited and uses double-quotation marks as the text qualifier.\n\nThe file contains information about the candidate, the donor's city, state, zip code, employer and occupation information,\nas well as the amount donated. In addition it contains the date of the donation,\nLet's write a script to read and print each donation's date, amount and candidate. Python comes with a csv module\nthat helps read CSV files.\nimport csv,sys,datetime\nreader = csv.DictReader(open(sys.argv[1], 'r'))\nfor row in reader:\nname = row['cand_nm']\ndatestr = row['contb_receipt_dt']\namount = row['contb_receipt_amt']\nprint ','.join([name, datestr, amount])\nDictReader assumes that the first line are the names of the fields, and creates a dictionary for each row of\nfieldname->value pairs. Copy the above code into a file (say exercise1.py) in the same directory as donations.\ntxt, and run python exercise1.py donations.txt. This will make donations.txt be the first argument to\nthe program (sys.argv[1]), which will be read and printed line by line. We'll only print the name, date, and amount of\nthe contribution for now.\nIntroducing matplotlib\nWe will be using matplotlib in the rest of the course, and work with it extensively in day 2. The following code is a\ncrash course on how to graph a line in matplotlib.\n# pyplot is the plotting module\nimport matplotlib.pyplot as plt\nimport random\n# generate the data\nxs = range(10) # 0...9\nys1 = range(10) # 0...9\nys2 = [random.randint(0, 20) for i in range(10)] # 10 random numbers from 0-19\n# create a 10-inch x 5-inch figure\nfig = plt.figure(figsize=(10,5))\n# draw a line graph\nplt.plot(xs, ys1, label='line 1')\nplt.plot(xs, ys2, label='line 2')\n# create the legend\nplt.legend(loc='upper center', ncol = 4)\n# finally, render and store the figure in an image\nplt.savefig('twolines.png', format='png')\n\nplt.plot() takes a list of x and a list of y values, and draws a line between every pair of (x,y) points. The line is drawn\non the most recently created figure.\nplt.legend() draws the legend in the figure. There are a bunch of other common chart objects like x-axis labels\nthat matplotlib supports.\nIn the final line, plt.savefig() saves the figure to a file called twolines.png in the directory we ran the script. Try it\nout! You should see something like\nSampling The Data\nThe dataset is quite large, and processing the full dataset can be pretty slow. It is often useful to sample the dataset and\ntry things out on the sample before doing a complete analysis of all of the data. The following is a script that samples\nthe donations dataset. It will print 1 out of every 1000 donations (or roughly 5000 total donations):\nimport sys\nwith file(sys.argv[1], 'r') as f:\ni = 0\nfor line in f:\nif i % 1000 == 0:\nprint line[:-1]\ni += 1\nThe line print line[:-1] prints the entire line except its last character to the screen. Why skip the last\ncharacter? Because each line ends in a carriage return, and print will add one for us. We don't want a space\nbetween each line!\nThis script will print every thousandth line of whatever file you pass in as an argument to the screen. To create a new\nfile, use the > standard output rediretor.\npython exercise3.py donations.txt > donations_sampled.txt\nWe will be analyzing Obama vs McCain data, so you can modify this code to create a file that only contains donations\nfor McCain and Obama. That way later analysis will run faster.\nPlotting The Data\n\nWe learned how to iterate and extract data from the dataset, and how to plot lines, so we will now combine the two to\nplot Obama's campaign contributions by date. We will compute the total amount of donations for each day, and\nuse matplotlib to create the charts.\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport csv, sys, datetime\nreader = csv.DictReader(open(sys.argv[1], 'r'))\nobamadonations = defaultdict(lambda:0)\nfor row in reader:\nname = row['cand_nm']\ndatestr = row['contb_receipt_dt']\namount = float(row['contb_receipt_amt'])\ndate = datetime.datetime.strptime(datestr, '%d-%b-%y')\nif 'Obama' in name:\nobamadonations[date] += amount\n# dictionaries\nsorted_by_date = sorted(obamadonations.items(), key=lambda (key,val): key)\nxs,ys = zip(*sorted_by_date)\nplt.plot(xs, ys, label='line 1')\nplt.legend(loc='upper center', ncol = 4)\nplt.savefig('/tmp/test.png', format='png')\nA few notes about the code\n● defaultdict is a convenience dictionary. When we use a regular dictionary, it throws an error when we access a key\nthat doesn't exist:\n>>> d = {}\n>>> d['foo']\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nKeyError: 'foo'\nWe provide defaultdict a function to call and return if the key doesn't exist. This is nice because we can assume\na default value. It is otherwise used as a normal python dictionary:\n>>> d = collections.defaultdict(lambda:0)\n>>> d['foo']\n>>> d['bar'] += 1\n● We parse the dates using the datetime module's strptime function. The string %d-%b-%y is called a date format string.\n● In the loop that reads the data, we record the total donation amount for each date.\n\n●Finally, we need to sort the data in obamadonations by the date (the key). sorted(l, key=f) returns a sorted copy of\nl and calls f to extract the key to use for comparison.\n●zip(*pairs) then unzips the list of pairs into two lists.\nYou should see something like this:\nGreat! It's interesting to see a spike in donations on August 2008 -- does it relate to the democratic party\nnomination speech he gave on August 28th? At this point a reporter may try to understand some of the spikes in the graph.\nBut wait! There's a really weird dip in his donations in the lower right corner. How does someone give\nnegative donations? The next part will investigate this further.\nThe Case of the Negative Donation\nThe first thing we should do is look at some of the data where the donation amount is negative and see if there's\nanything interesting. We can modify our existing code.\nimport csv,sys,datetime\nreader = csv.DictReader(open(sys.argv[1], 'r'))\nfor row in reader:\nname = row['cand_nm']\ndatestr = row['contb_receipt_dt']\namount = float(row['contb_receipt_amt'])\nif amount < 0:\nline = '\\t'.join(row.values())\nprint line\nNote that we cast amount into a float. The CSV module returns strings, so its our job to cast the data into the proper type.\nIf you scan through the output, you'll see data such as:\nC00430470 DARIEN RETIRED McCain, John S SA17A P80002801 068202003 VAN MUNCHING, LEO MR.\n\nJR. 02-AUG-07 CT X REATTRIBUTION TO SPOUSE 315387 REATTRIBUTION TO SPOUSE -2300\nC00430470 LOS ANGELES EXECUTIVE McCain, John S SA17A P80002801 900492125 A.E.G.\nLEIWEKE, TIMOTHY J. MR. 30-APR-08 CA X REFUND; REDESIGNATION REQUESTED 364146 REFUND;\nREDESIGNATION REQUESTED -2300\nLots of text, but \"REDESIGNATION TO GENERAL\" and \"REATTRIBUTION TO SPOUSE\" pop out as pretty strange.\nIt turns out that \"redesignations\" and \"reattributions\" are perfectly normal. If a donation by person A is excessive, the\npart that exceeds the limits can be \"reattributed\" to person B, meaning that person B donated the rest to the\ncampaign. Alternatively, the excess amount can be redesignated to another campaign in the same party. So a donation\nto Obama could be redesignated to a poor democrat in Nebraska.\nWhat's fishy is \"REATTRIBUTION TO SPOUSE.\" A quick google search gives a potential theory: that this is a tactic to\nhide campaign contributions from CEOs. A CEO will donate money, which will be reattributed (refunded) to the\nCEO's spouse. Then the humble spouse will turn around and donate the money to the candidate. In this way, it's hard for\na casual browser to notice that the candidate is backed by a company's CEOs.\nExercise 1: Plot Obama vs. McCain\nSo far we have only plotted Obama's campaign donations. Modify the script to also plot McCain's donations on the\nsame chart. It should look something like:\nWhoa whoa whoa, what was McCain up to March 2008? That's a whole lot of negative donations! We'll deal with that in\na few exercises.\nExercise 2: Cumulative Graphs\nWord on the street says that Obama's donations eclipsed McCain's donations. Let's see if that's true. Plot the\ncumulative donations (for a given date, plot the total donations up to that date). It should look something like:\n\nExercise 3: Understand \"Reattribution to Spouse\"\nLet's now filter the contributions to only see the cumulative \"reattribution to spouse\" donations. Which candidate do\nthe dark, hooded CEOs prefer?\nYou will need to find the name of the field that contains the \"reattribution\" text, and filter on that field. Depending on\nhow you filter it, you may get different results. Try out a few ways to see what happens.\nExercise 4: Pause and Think\nIt's time for a reality check. If you saw the graph in the previous exercise, you would think \"That's a lot of\nnegative donations! This candidate is really sneaky.\" Don't believe that just yet. Re-plot the ratio between\neach candidate's cumulative \"reattribution to spouse\" donations and that candidate's cumulative overall donations.\nThat changes our offenders quite a bit.\nKey Lesson: don't automatically trust charts in the wild. It's easy to make a chart say\nwhatever you want by selectively leaving out data!\nDone!\nCongrats! You are now a data sleuth. To recap the process we just went through we:\n1. Took a quick look at the data using head to get a sense of what we're dealing with. We also figured out the format of\nthe data. This is usually important, because the fields are otherwise somewhat non-sensical!\n2. Create a quick, initial visualization of some of the data fields and see if there are interesting trends.\n3. Listen to your hunch, and form a hypothesis around it\n4. Figure out why the trend exists\n5. Filter the dataset to the \"interesting portion\" and go to step 2\nTomorrow, we will dive deeper into matplotlib's visualization facilities, and further analyze the data using\ndifferent visualizations.\n\nRelated Datasets\nIf you are interested more campaign finance data, you can also download the campaign expense data from the\nsame website, if you click the \"expenditures\" tab in the right table.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Day 2 Lab Handout",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/30d5bbd6d59d6edac96d214ff773f274_MITRES_6_009IAP12_lab2.pdf",
      "content": "Overview\nToday, we will do more with\n.\nbar graphs\nline graphs\nbox plots (will be useful tomorrow)\nscatter plots\nchoropleth plots (map plots)\nWe will also learn to\ncreate figures with multiple sub-figures (called subplots)\ncustomize labels, colors, error bars etc.\nIn the exercises, we will use this to further visualize and analyze the campaign donations data.\nis quite powerful, and is intended to emulate matlab 's visualization facilities. We will give you a basic understanding of how plotting works, which\nshould be enough for a majority of the charts that you will want to create.\nPlotting Large Datasets\nThe dataset that we are working with is fairly large for a single computer, and it can take a long time to process the whole dataset, especially if you will process\nit repeatedly during the labs.\nUse the sampling technique we discussed in yesterday' s lab! You can change the sampling frequency (\nyesterday) to change the size of the sample. Use\nas a starting point, but realize the graphs we show are with sampling set to (all rows are included).\nIntroduction\nVisualizations are used to succinctly and visually describe different parts or different interpretations of your data. They give the reader, who is not necessarily an\nexpert in the dataset, an intuition of trends or relationships.\nI typically use visualizations for two purposes:\n1. Exploring: Quickly viewing the dataset to spot outliers and trends and form hypotheses.\n2. Storytelling: Illustrating a piece of data that I 've cleaned, and processed in order to make a point.\nFigure and Subplots\nThe package we will be using is\n. It provides a lot of shorthands and defaults (we used some of them yesterday when making line charts),\nbut today we will do things the \"right way\".\nA\nis the area that we will draw in. We can specify that it is 50 inches wide and 30 inches tall.\nimport matplotlib.pyplot as p\nlt\nfig = plt.figure(figsize=(50, 30))\nIt is common to create multiple subfigures or\nin a single figure.\ntells the figure to treat the area as a nrows\nx ncols grid, and return an Axes object that represents the i' th grid cell. You will then create your chart by calling methods on object. It helps me to think of an\nAxes object as a subplot.\nsubplot1 = fig.add_subplot(2, 3\n, 1)\nsubplot2 = fig.add_subplot(2, 3\n, 2)\nFor example, the above code creates a figure with the following layout. The black box is the area of the figure, while each blue box is a subplot in a 2x3 grid.\nThe number in a blue box is the subplot 's index.\n\nIt 's important to mention that\ndoes not actually create a grid, it just finds the area in an imaginary grid where a\nshould be and return an\nobject that represents the area. Thus it is possible to do draw on overlapping\n. Be careful!\nfig.add_subplot(2, 3, 1)\nfig.add_subplot(2, 1, 1)\nWhen you read matplotlib code on the internet, you will often see a shorthand for creating subplots when the subplot index, number of rows, and number of\ncolumns are all less than 10.\nreturns the z' th subplot in a x by y grid. So\nreturns the first subplot in a 2x3\ngrid.\nHow Drawing Works\nThe functions that we will be using to create charts are simply convenience functions that draw and scale points, lines, and polygons at x,y coordinates.\nWhenever we call a plotting method, it will return a set of objects that have been added to the subplot. For example, when we use the\nmethod to create\na bar graph,\nwill draw rectangles for each bar, and return a list of\nobjects so that we can manipulate them later (e.g., in an\nanimation).\nAs you use\n, keep in mind that:\nMany of the plotting functions will ask you to specify things similar to x,y coordinates / offsets.\nWhen you call a drawing function, it won' t rearrange the layout of what was drawn before. It simply draws the pixels on top of what has been drawn\nbefore.\nYou are ultimately adding points, lines and polygons on top of one another.\nLet 's get to drawing graphs! By the end of this tutorial, you will have experience creating bar charts, line charts, box plots, scatter plots, and choropleths (map\nplots). We will walk you through how to create a figure similar to\n\nThis figure creates subplots in a 3x2 grid, so let 's first setup the figure and generate two sets of data. Both sets have the same x values, but different y values.\nThink \"obama\" and \"mccain\" from yesterday :)\nimport matplotlib.pyplot as p\nlt\nimport random\nrandom.seed(0)\nfig = plt.figure(figsize=(50, 30))\nN = 100\nxs = range(N)\ny1 = [random.randint(0, 50) for i in xs]\ny2 = range(N)\nBar Plots documentation\nBar plots are typically used when you have categories of data that you want to compare. The\nfunction is:\nThe bar plot function either takes a single left and height value, which will be used to draw a rectangle whose left edge is at\n, and is\ntall:\nsubplot.bar(10, 30) # left edge at 10, and the height is 30.\nor you can pass a list of lefts values and a list of height values, and it will draw bars for each pair of left,height value. For example, the following will create three\nbars at the x coordinates 10, 20 and 30. The bars will be 5, 8, 2 units tall.\nsubplot.bar([10, 20, 30], [5, 8, 2])\nwill automatically scale the x and y axes so that the figure looks good. While the numbers along the x and y axes depend on the values of\nand\n, the sizes of the bars just depend on their relative values. That 's why we' ve used the word \"unit\" instead of \"pixel\".\nThe\nkeyword argument sets width of the bars It can be a single value, which sets the width of all of the bars, or a list that specifies the list of each bar.\nSet it relative to the differences of the\nvalues. For example, the code above sets each bar 10 units apart (\n), so I would set\n.\nThe\nkeyword argument specifies the bottom edge of the bars.\nsubplot.bar([10, 20, 30], [\n5, 8\n, 2], width=[5,5,5], b\nottom=[5, 10, 15])\n\nWhat if you want to draw 2 sets of bars? We simply call\nmultiple times. However, we would need to set the\nargument appropriately. If\nwe used the same\nlist for all the calls, then the bars would be drawn on top of each other.\nWhat if we want to shift the second set of bars by\nunits? One way to do this is to turn\ninto a\nlist. Numpy arrays let us perform math\noperations (e.g.,\n) on every element in the array, and\nmethods also accept\nlists. So\nadds\nto every\nelement in\n, which serves to shift the second set of bars to the right by\nunits. The following code should reproduce the first subplot in the figure.\nimport numpy as n\np\nleft = np.arange(len(xs))\nwidth = 0.25\nsubplot = fig.add_subplot(3,2,1)\nsubplot.bar(left, ys, width=width)\nsubplot.bar(left+width, ys2, w\nidth=width, bottom=ys)\nYou can further customize your bar charts using the following popular keyword arguments:\n: set color to a hex value (\"#ffffff\"), a common color name (\"green\"), or a shade of grey (\"0.8\").\n: the width of the bar 's border. set it to to remove the border.\n: set the color of the bar 's border.\n: give a set of bars a name. Later, we will teach you how to create legends that will display the labels.\nFor example, the following would draw a set of red bars:\nsubplot.bar(left, ys, color='red')\nLine Plots documentation\nLine plots are typically used when graphing data with two continuous dimensions. In addition, drawing the line implies that we can extrapolate values between\nadjacent points on the line. This is a key difference between line plots and scatter plots.\nThe\ncommand draws a line graph. It takes a list of x values and y values, and draws a line between every adjacent pair of points. Try it out using the data\nin the previous section.\nis a convenient shorthand for:\nsubplot.plot(xs1, ys1)\nsubplot.plot(xs2, ys2)\n...\nTo reproduce the line graph, we can simply write\nsubplot = fig.add_subplot(322)\nsubplot.plot(xs, y\ns1, xs, ys2)\nYou can also customize the lines using the keyword arguments:\n: same as\nfor bar graphs'\n: specify the marker to draw at each point. I commonly use\nor '\n. Set it to\nto not draw markers. The\ndocumentation has a full list of the available markers.\n: give a line a name. I usually call\nfor each line if I want to give each one a name.\nBox Plots documentation\nBoxplots are used to summarize and compare groups of numerical data. Each box summarizes a set of numbers and depicts 5 parameters:\n\nNOTE The words we are about to use might seem foreign to you. We will teach boxplots in depth tomorrow. We are just introducing how to draw box plots\ntoday, and will use them a whole lot tomorrow.\nThe smallest number\nThe lower quartile\nThe median\nThe upper quartile\nThe largest observation\nwill automatically compute these values, and also ignore numbers that it thinks are outliers. Don' t worry about when and why they are used\n-- we will discuss that tomorrow. Just know that one box summarizes a set of numbers.\nUnlike the other charts, you can't draw each box individually. The\nvariable is either a list of numbers, in which case it will compute and draw a single box:\nsubplot.boxplot(range(10))\nor\ncan be a list of lists. In which case, it will compute and draw a box for each list. The following code reproduces the box plot shown earlier. We create\nsets of data (\n,\n,\n) and create a box for each set.\nsubplot = fig.add_subplot(323)\nboxdata1 = [random.randint(0, 20) for i in xrange(10)]\nboxdata2 = [random.randint(20,40) for i in xrange(10)]\nboxdata3 = [random.randint(40,60) for i in xrange(10)]\ndata = [boxdata1, boxdata2, boxdata3]\nsubplot.boxplot(data)\nYou can customize your box plots with the following keyword arguments\n: By default, the boxes are drawn vertically. You can draw them horizontally by setting\n: Like\nin bar charts, this sets the width of each box\nScatter Plots documentation\nScatter plots are used to graph data along two continuous dimensions. In contrast to line graphs, each point is independent.\nThis method will draw a single point if you give it a single x,y pair\nsubplot.scatter(10, 10)\nor you can give it a list of x and a list of y values\nsubplot.scatter([0, 1, 2], [9, 3, 10])\nI 've included the commonly used keyword arguments\n: sets the size of each point to 20 pixels.\n: sets the color of each point to blue\n: each point will be drawn as a circle. The documentation lists large number of other markers\n: the alpha (transparency) value of the points. Between and .\n: sets the width of the line around the point to 4 pixels. I usually set it to .\nChoropleths/Maps\nCartography is a very involved process and there is an enormous number of ways to draw the 3D world in 2D. Unfortunately, we couldn' t find any native\nfacilities to easily draw US states and counties. It 's a bit of a pain to get it working, so we 've written some wrappers that you can call to draw\ncolored state and counties in a\n. We' ll describe the api, and briefly explain how we went about the process at the end.\nThe API is defined in\n. You can import the methods using the following code:\nimport sys\n# this a\ndds the resources/util/ folder i\nnto your p\nython path\n# you may need t\no edit t\nhis so t\nhat the path i\ns correct\nsys.path.append('resources/util/')\nfrom map_util import *\n: draws the county with the specified\ncounty code. Most datasets that contain per-county data\nwill include the fips code. If you don' t include a\n, we will randomly pick a nice shade of blue for you.\n\n: draws the state with the full state name as specified by the official USPS state names. If you\ndon' t include\n, we will pick a shade of blue for you.\n: retrieve the full state name from its abbreviation. The method is case insensitive, so\nis the same as\n.\nWe also included a list of all fips county codes and state names in\nand\n. We will use\nthem to reproduce the map charts.\nimport json\n# Map of C\nounties\n#\nsubplot = fig.add_subplot(325)\n# data i\ns a list o\nf strings that c\nontain f\nips values\ndata = json.load(file('../datasets/geo/id-counties.json'))\nfor fips i\nn data:\ndraw_county(subplot, fips)\n# Map of S\ntates\n#\nsubplot = fig.add_subplot(326)\ndata = json.load(file('../datasets/geo/id-states.json'))\nfor state in d\nata:\ndraw_state(subplot, state)\nThe files are in JSON format, so we call\n, which parses the files into python lists. The rest of the code simply iterates through the fips ' and\nstates, and draws them.\nThe gritty details (advanced)\nThe process of drawing maps yourself requires a number of steps:\n1. Download shape files. A shape file specifies the lat, lon positions that describe the border of an area (e.g., county, zip code, state).\n2. Parse the shape files. They come in all types of formats. We downloaded the shape files that D3 uses. It comes in the GeoJS format, which is nice\nbecause\ncan parse it for us.\nRemember how we mentioned that there lots of ways to draw the world in 2D. Our shape files use the Albers Equal Area Projection, which is also\nused by the US Census Bureau.\n3. Once we have the shapes, we need to draw polygons in the subplot. Thankfully\nfills in the region defined by the list of x,y\npoints.\n4. Our methods actually take any keyword argument that\naccepts. So take a look at its documentation if you want to further customize\nyour maps.\nCustomizing Subplots\nWe only touched a small part of what\ncan do. Here are some additional ways that you can customize subplots.\nKeyword Arguments\n: list of floats that specify x-axis error bars.\n: list of floats that specify y-axis error bars\nAdditional Charting Methods\n: plots a log-log line graph\n: plots the x-axis in log scale\n: plots the y-axis in log scale\n: draws a filled polygon with vertices at\n.\n: write\nat coordinates\n.\nSubplot Customization\n: clear everything that has been drawn on the subplot.\n- add a legend. You can specify where to place it using\n, and the number of columns in the\nlegend.\n: Set the subplot title.\n: Set the x-axis label\n: Draws x-axis tick marks at the points specified by\n. Otherwise\nwill draw reasonable tick marks.\n: Draw x-axis tick labels using the\nlist.\n: Sets the x-axis scaling.\nis\nor\n.\n\nFor Obama, that 's donations between\n. For McCain, that' s between\nExercise 2: More line graphs\n: Set the x-axis limits\n: Set the y-axis limits\nColor\nThis document provides a good summary of what to think about when choosing colors.\nColor Brewer 2 is a fantastic tool for picking colors for a map. We used it to pick the default colors for the choropleth library.\nExercise 1: Histograms\nWe will use yesterday 's Obama vs McCain dataset and visualize it using different chart types.\nMany people say that Obama was able to attract votes from \"the common man\", and had far more smaller contributions that his competitors. Let 's plot a\nhistogram of each candidate' s contribution amounts in $100 increments to see if this is the case. A histogram breaks a data range (donation amounts, in this\ncase) into fixed size buckets (100 in this case), and counts the number of items that fall into each bucket. Plot both candidates on the same graph. You' ll want to\nuse a bar chart.\nYou'll find that it 's difficult to read the previous chart because the donation amounts vary from -$1 Million to $8 Million, while the majority of the donations are\nless than 2000. One method is to ignore the outliers and focus on the majority of the data points. Let 's only print histogram buckets within 3 standard deviations\nof the overall average donation.\nNow create a cumulative line graph of Obama and McCain' s donations. The x-axis should be the donation amount, and the y-axis should be the cumulative\ndonations up to that amount.\nWe can see that even though Obama and McCain have some very large contributions, the vast majority of their total donations were from small contributors.\nAlso, not only did Obama get more donations, he also received larger donations.\nOnly after we' ve verified that the small donations were the major contributors, is it safe to zoom in on the graph! Use the ranges in the previous exercise.\n\nExercise 3: Scatter plots\nScatter plot of re-attribution by spouses for all candidates. Find all re-attribution by spouses data points for each candidate and plot them on a scatter plot. The\nx-axis is the donation date and the y-axis is the donation amount.\nIt seems to be concentrated in a small group of Republican candidates.\nAt this point, we' ve only scratched the surface of one dimension (reattributions) of this interesting dataset. You could continue our investigation by correlating\nprofessions with candidates, visualize donations by geography, or see if there are any more suspicious and interesting data points.\nFor example, which professions and companies are using this \"re-attribution to spouse\" trick?\nAlso, the 2012 campaign contributions are also available on the website, so you could use your analysis on the current election!\nExercise 4\nNow create a figure where each subgraph plots the total amount of per-state donations to a candidate. Thus, if there are 5 candidates (for example), there\nwould be 5 subplots.\nThe tricky part is mapping the donation amount to a color. Here' s some sample code to pick a shade of blue depending on the value of a donation between 0\nand MAXDONATION. The bigger index means a darker shade.\n# this c\nreates a\nn array of g\nrey colors f\nrom white to b\nlack\ncolors = ['0','1','2','3','4','5','6','7','8','9','a', 'b', '\nc', 'd', '\ne', 'f']\ncolors = map(lambda s\n: '#%s' % (s*6), c\nolors)\ncolors.sort(reverse=True)\n# a\nssume MAXDONATION was defined\n# a\nssume curdonation is t\nhe d\nonation to p\nick a color for\nratio = (curdonation/float(MAXDONATION))\ncolor_idx = int( r\natio * (\nlen(colors) - 1) )\n\ncolors[color_idx]\nUsing this, you should be able to create something like the following:\n\nYou'll notice that if you plot more candidates, they are very difficult to see because their donations are eclipsed by Obama' s. One way is to use a log scale\ninstead of a linear scale when mapping donations\nHere 's some sample code for computing a log\n\nimport math\nmath.log(100) # log of 100\nNow you have hands on experience with the most popular python plotting library!\nis far more power than what we have covered. To cover the\ngeneral process of using visualizations:\n\nAlways start by looking at your data with the simplest visualizations possible. For most datasets, a scatter plot or line graph is sufficient.\nFirst view a summary of the whole dataset so that you know which subsets are worth visualizing in more detail, and how significant the details really are.\nPlot your interesting data along a bunch of different dimensions.\nStare at your data, try to identify trends, outliers and other interesting regions and form hypotheses.\nUse statistics to see if your hypotheses were correct (tomorrow 's lecture)\nRepeat\nDomain specific visualizations\nWe only covered a small number of core visualizations in this lab. There are lots of other types of visualizations specialized for different domains. A few of them\nare listed below.\nGene Expression Matrix\nGene expression matrixes can be used to show correlations between genes and properties of patients. Here is one:\nNetwork Graphs\nPlotting graphs of social networks is a topic unto itself, and isn't well supported in\n. There are other libraries for drawing them, but we unfortunately\ndon' t have the time to talk about it in this class. If you' re interested, one useful network graphing library is NetworkX. Here' s an example of a network graph\noverlayed on a map:\n(c) NetworkX Developers. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n(c) Source unknown. All rights reserved. This content is excluded from our Creative\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\nTreeMaps\nA Treemap helps summarize relative proportions of a whole. Here' s a treemap of financial markets. You can make treemaps in matplotlib.\n\nImage from Wikipedia, in the public domain.\nOther visualization tools\nSome other visualization tools. A few are in python, and many are in other languages like javascript or java.\nhttp://orange.biolab.si/features.html: visualization and machine learning package for python\nProcessing: a fantastic java-based visualization language.\nProcessingJs: Processing ported to javascript\nd3: A javascript based visualization library that makes drawing on\nmuch much easier.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Day 3 Lab Part A Handout",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/5a85b1b2482129e6633ca47a06cc59d5_MITRES_6_009IAP12_lab3a.pdf",
      "content": "So far, weʹve plotted and visualized data in various ways. Today, weʹll see how to statistically back up some of the\nobservations weʹve made in looking at our data. Statistics is a tool that helps separate newsmaking data‐backed stories from\none‐off anecdotes. Usually, both kinds of stories start with a hunch, and statistics helps us quantify the evidence backing\nthat hunch.\nWhenever you have a hunch (a hypothesis in statistician‐speak), the first thing to do is to look at some summary statistics\n(e.g., averages), and explore the data graphically as we did yesterday. If the visualizations seem to support your hunch, you\nwill move into hypothesis‐testing mode.\nTwo Running Examples\nFor our first set of tests, weʹre going to use two running examples: campaign spending and a fun comparison of two townsʹ\ncitizensʹ heights. Here are the two scenarios:\nOne thing thatʹs been claimed about the 2008 election is that President Obama raised smaller quantities from a larger\ngroup of donors than Senator McCain, who raised a smaller number of large contributions. Statistical techniques will\nhelp us determine how true this statement is.\nImagine two towns that only differ in that one of the towns had ʺsomething in the waterʺ the year a bunch of kids\nwere born. Did that something in the water affect the height of these kids? (Note: This situation is unrealistic. Itʹs\nnever the case that the only difference between two communities is the one you want to measure, but itʹs a nice goal!)\nWeʹll use statistics to determine whether the two communities have meaninfully different heights.\nComparing Averages\nLetʹs start by comparing a simple statistic, to see if in the data we observe thereʹs any difference. Weʹll start by comparing the\naverage heights of the two towns. (As an aside: it would help if you wrote and ran your code in dataiap/day3/ today, since\nseveral modules like ols.py are available in that directory).\nimport numpy\ntown1_heights = [5, 6, 7, 6, 7.1, 6, 4]\ntown2_heights = [5.5, 6.5, 7, 6, 7.1, 6]\ntown1_mean = numpy.mean(town1_heights)\ntown2_mean = numpy.mean(town2_heights)\nprint \"Town 1 avg. height\", town1_mean\nprint \"Town 2 avg. height\", town2_mean\nprint \"Effect size: \", abs(town1_mean ‐ town2_mean)\nIt looks like town 2ʹs average height (6.35 feet) is higher than town 1 (5.87 feet) by a difference of .479 feet. This difference is\ncalled the effect size . Town 2 certainly looks taller than Town 1!\nExercise Compute the average campaign contribution for the Obama and McCain campaigns from the dataset in day 1.\nWhatʹs the effect size? We have an average contribution of $423 for McCain and $192 for Obama, for an effect size of $231.\nMcCain appears, on average, to have more giving donors.\nBefore we fire up the presses on either of these stories, letʹs look at the data in more depth.\nGraph The Data\nIf you finished yesterdayʹs histogram exercise, then feel free to skip down to the box plot section\nThe effect size in both of our examples seems large. It would be nice to more than just compare averages. Letʹs try to look at\n\na histogram of the distributions. We created a histogram of the two campaigns contributions, binned by $100 increments.\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nincrement = 1\nwidth= .25\ntown1_bucketted = map(lambda ammt: ammt ‐ ammt%increment, town1_heights)\ntown2_bucketted = map(lambda ammt: ammt ‐ ammt%increment + width, town2_heights)\ntown1_hist = Counter(town1_bucketted)\ntown2_hist = Counter(town2_bucketted)\nminamount = min(min(town1_heights), min(town2_heights))\nmaxamount = max(max(town1_heights), max(town2_heights))\nbuckets = range(int(minamount), int(maxamount)+1, increment)\nfig = plt.figure()\nsub = fig.add_subplot(111)\nsub.bar(town1_hist.keys(), town1_hist.values(), color='b', width=width, label=\"town 1\")\nsub.bar(town2_hist.keys(), town2_hist.values(), color='r', width=width, label=\"town 2\")\nsub.legend()\nplt.savefig('figures/town_histograms.png', format='png')\nThis results in a histogram that looks like this:\nNot bad! The buckets are all exactly the same size except for one person of height between 4 and 5 feet in town 1.\nExercise Build a histogram for the Obama and McCain campaigns. This is challenging, because there are a large number of\noutliers that make the histograms difficult to compare. Add the line\n\nsub.set_xlim((‐20000, 20000))\nbefore displaying the plot in order to set the x‐values of the histogram to cut off donations larger than $20,000 or smaller\nthan ‐$20,000 (refunds). With bar widths of 50 and increments of $100, your histogram will look something like this:\nOuch! I canʹt make heads or tails of that. It seems like Obama has a larger number of small donations, but there isnʹt a lot of\ngranularity at that scale. For large datasets, a histogram might have too much information on it to be helpful. Luckily,\ndescriptive statisticians have a more concise visualization. Itʹs called a box‐and‐whisker plot! The code for it is quite simple\nas well:\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nsub = fig.add_subplot(111)\nsub.boxplot([town1_heights, town2_heights], whis=1)\nsub.set_xticklabels((\"Town 1\", \"Town 2\"))\nsub.set_title(\"Town 1 vs. Town 2 Heights\")\nplt.savefig('figures/town_boxplots.png', format='png')\nHereʹs what we see:\n\nLetʹs interpret this plot. We show town 1 on the left and town 2 on the right. Each town is represented by a box with a red\nline and whiskers.\nThe red line in the box represents the median , or 50th percentile value of the distribution. If we sort the dataset, 50%\nof the values will be below this line, and 50% will be above it.\nThe bottom edge of the box represents the 25th percentile (the value larger than 25% of your dataset), and the top\nedge represents the 75th percentile (the value larger than 75% of your dataset). The difference between the 75th and\n25th percentile is called the inner quartile range (IQR) .\nThe whiskers represent the ʺextremesʺ of our dataset: the largest value weʹre willing to consider in our dataset before\ncalling it an outlier. In our case, we set whis=1 , requesting that we show whiskers the most extreme value at a\ndistance of at most 1x the IQR from the bottom and top edges of the box plot.\nIf normal distributions are your thing, this image might help you interpret the box‐and‐whiskers plot.\nLike in the histogram, we see that the townsʹ height distributions donʹt look all that different from one‐another. Generally, if\nthe boxes of each distribution overlap, and you havenʹt taken something on the order of a buttload (metric units) of\nmeasurements, you should doubt the differenes in distribution averages. It looks like a single height measurement for town\n1 is pretty far away from the others, and you should investigate such measurements as potential outliers.\nExercise Build a box‐and‐whiskers plot of the McCain and Obama campaign contributions. Again, outliers make this a\ndifficult task. With whis=1 , and by setting the y range of the plots like so\nsub.set_ylim((‐250, 1250))\nwe got the following plot\n\nObama is on the left, and McCain on the right. Real data sure is more confusing than fake data! Obamaʹs box plot is a lot\ntighter than McCains, who has a larger spread of donation sizes. Both of Obamaʹs whiskers are visible on this chart, whereas\nonly the top whisker of McCainʹs plot is visible. Another feature we havenʹt seen before is the stream of blue dots after each\nof the whiskers on each of Obama and McCainʹs plots. These represent potential outliers , or values that are extreme and\ndo not represent the majority of the dataset.\nIt was easy to say that the histograms and box plots for the town heights overlapped heavily. So while the effect size for\ntown heights was pretty large, the distributions donʹt actually look all that different from one‐another.\nThe campaign plots are a bit harder to discern. The histogram told us virtually nothing. The box plot showed us that\nObamaʹs donations seemed more concentrated on the smaller end, whereas McCains seemed to span a larger range. There\nwas overlap between the boxes in the plot, but we donʹt really have a sense for just how much overlap or similarity there is\nbetween these distributions. In the next section, weʹll quantify the difference using statistics!\nRun a Statistical Test\nWe have two population height averages. We know that they are different, but charts show that overall the two towns look\nsimilar. We have two campaign contribution averages that are also different, but with a murkier story after looking at our\nbox‐and‐whisker plots. How will we definitively say whether the differences we observe are meaningful?\nIn statistics, what we are asking is whether differences we observed are reliable indicators of some trend, or just happened\nby lucky chance. For example, we might simply have measured particularly short members of town 1 and tall members of\ntown 2. Statistical significance is a measure of the probability that, for whatever reason, we stumbled upon the results we\ndid by chance.\nThere are several tests for statistical significance, each applying to a different question. Our question is: ʺIs the difference\nbetween the average height of people in town 1 and town 2 statistically significant?ʺ We ask a similar question about the\ndifference in average campaign contributions. The test that answers this question is the T‐Test. There are several flavors of\nT‐Test and we will discuss these soon, but for now weʹll focus on Welchʹs T‐Test.\n\nimport welchttest\nprint \"Welch's T‐Test p‐value:\", welchttest.ttest(town1_heights, town2_heights)\nThe Welchʹs T‐Test emitted a p‐value of .349 . A p‐value is the probability that the effect size of .479 feet between town 1 and\ntown 2 happened by chance. In this case, thereʹs 34.9% chance that weʹve arrived at our effect size by chance.\nWhatʹs a good cutoff for p‐values to know whether we should trust the effect size weʹre seeing? Two popular values are .05\nor .01: if there is less than a 5% or 1% chance that we arrived at our answer by chance, weʹre willing to say that we have a\nstatistically significant result.\nSo in our case, our result is not significant. Had we taken more measurements, or if the differences in heights were farther\napart, we might have reached significance. But, given our current results, letʹs not jump to conclusions. After all, it was just\nfood coloring in the water!\nExercise Run Welchʹs T‐test on the campaign data. Is the effect size between McCain and Obama significant? By our\nmeasurements, the p‐value reported is within rounding error of 0. Thatʹs significant by anyoneʹs measure: thereʹs a\nnear‐nonexistant chance weʹre seeing this difference between the candidates by some random fluke in the universe. Time to\nwrite an article!\nCan You Have a Very Significant Result?\nNo. There is no such thing as ʺveryʺ or ʺalmostʺ significant. Remember: the effect size is the interesting observation, and itʹs\nup to you what makes for an impressive effect size depending on the situation. You can have small effects, large effects, and\neverything in between. Significance testing tells us whether to believe that the observations we made happened by anything\nmore than random chance. While people disagree about whether a p‐value of .05 or .01 is required, they all agree that\nsignificance is a binary value.\nStrictly speaking, youʹve learned about T‐Tests at this point. If you are pressed for time, read Putting it all Together below\nand move on to the next section. For the overachievers in our midst, thereʹs lots of important information to follow, and you\ncan instead keep reading until the end.\nTypes of T‐Test\nThe T‐Test has two major flavors: paired and unpaired.\nSometimes your datasets are paired (also called dependent ). For example, you may be measuring the performance of the\nsame set of students on an exam before and after teaching them the course content. To use a paired T‐Test, you have to be\nable to measure an item twice, usually before and after some treatment. This is the ideal condition: by having before and\nafter measurements of a treatment, you control for other potential differences in the items you mentioned, like performance\nbetween students.\nOther times, you are measuring the difference between two sets of measured data, but the individual measurements in each\ndataset are unpaired (sometimes called independent ). This was the case in our tests: different people contributed to each\ncampaign, and different people live in town 1 and 2. With unpaired datasets, we lose the ability to control for differences\nbetween individuals, so weʹll likely need more data to achieve statistical significance.\nUnpaired datasets come in all flavors. Depending on whether the sizes of the sets are equal or unequal, and depending on\nwhether the variances of both sets are equal, you will run different versionf of an unpaired T‐Test. In our case, we made no\nassumptions about the sizes of our datasets, and no assumptions on their variances, either. So we went with an unpaired,\nunequal size, unequal variance test. Thatʹs Welchʹs T‐Test.\nAs with all life decisions, if you want more details, check out the Wikipedia article on T‐Tests. There are implementations of\npaired T‐Tests and unpaired ones in scipy. The unequal variance case is not available in scipy, which is why we included\nwelchsttest.py. Enjoy it!\n\nT‐Test Assumptions we Broke:(\nWeʹve managed to sound like smartypantses that do all the right things until this moment, but now we have to admit we\nbroke a few rules. The math behind T‐Tests makes assumptions about the datasets that makes it easier to achieve statistical\nsignificance if those assumptions are true. The big assumption is that the data we used came from a normal distribution.\nThe first thing we should have done is check whether or not our data is actually normal. Luckily, the fine scipy folks have\nimplemented the Shapiro‐Wilk test test for normality. This test calculates a p‐value, that, if low enough (usually < 0.05), tells\nus there is a low chance the distribution is normal.\nimport scipy.stats\nprint \"Town 1 Shapiro‐Wilks p‐value\", scipy.stats.shapiro(town1_heights)[1]\nWith a p‐value of .380, we donʹt have enough evidence that our town heights are not normally distributed, so itʹs probably\nfine to run Welchʹs T‐Test\nExercise Test the campaign contribution datasets for normality. We found them to not be normal (p = .003 for Obama and\n.014 for McCain), which means we likely broke the normality assumption of Welchʹs T‐Test. The statistics police are going to\nbe paying us a visit.\nThis turns out to be OK for two reasons: T‐Tests are resilient to breaking of the normality assumption, and, if youʹre really\nserious about your statistics, there are nonparametric equivalents that donʹt make normality assumptions. They are more\nconservative since they canʹt make assumptions about the data, and thus likely require a larger sample size to reach\nsignificance. If youʹre alright with that, feel free to run the Mann‐Whitney U nonparametric version of the T‐Test, which has\na wonderful name.\nimport scipy.stats\nprint \"Mann‐Whitney U p‐value\", scipy.stats.mannwhitneyu(town1_heights, town2_heights)[1]\nRemember: we donʹt need to run the Mann‐Whitney U test on our town data, since it didnʹt exhibit non‐normalcy. And\nbesides, the p‐value is .254. Thatʹs still not significant. This makes sense: our less conservative Welchʹs test was unable to give\nus significance, so we donʹt expect a more conservative test to magically find significance.\nExercise since we shouldnʹt be using Welchʹs T‐Test on the campaign contribution data, run the Mann‐Whitney U test on\nthe data. Is the difference between the Obama and McCain contributions still significant?\nWe got a p‐value of about 0, so you will still find the result to be statistically significant. A+ for you!\nPutting it All Together\nSo far, weʹve learned the steps to test a hypothesis:\nCompute summary statistics, like averages or medians, and see if these numbers match your intuition.\nLook at the distribution histograms or summary visualizations like box plots to understand whether your hypothesis\nappears to be backed up by the data\nIf itʹs not immediately clear your hypothesis was wrong, test it using the appropriate statistical test to 1) quantify the\neffect size, and 2) ensure the data you observed couldnʹt have happened by chance.\nThereʹs a lot more to statistics than T‐Tests, which compare two datasetsʹ averages. Next, weʹll cover correlation between\ntwo datasets using linear regression.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    },
    {
      "category": "Resource",
      "title": "Day 3 Lab Part B Handout",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/res-6-009-how-to-process-analyze-and-visualize-data-january-iap-2012/7efe00f12b11fefc7374e366957f63b0_MITRES_6_009IAP12_lab3b.pdf",
      "content": "We hear about correlations every day. Various health outcomes are correlated with socioeconomic status. Iodine\nsupplementation in infants is correlated with higher IQ. Models are everywhere as well. An object falling for t seconds\nmoves .5gt^2 meters. You can calculate correlation and build approximate models using several techniques, but the simplest\nand most popular technique by far is linear regression . Letʹs see how it works!\nCounty Health Rankings\nFor our examples, weʹll use the County Health Rankings. Specifically, weʹll be looking at two datasets in this example: Years\nof Potential Life Lost and Additional Measures.\nYears of potential life lost (YPLL) is an early mortality measure. It measures, across 100,000 people, the total number of\nyears below the age of 75 that a 100,000‐person group loses. For example, if a person dies at age 73, they contribute 2 years\nto this sum. If they die at age 77, they contribute 0 years to the sum. The YPLL for each 100,000 people, averaged across\ncounties in the United States is between 8000 and 9000 depending on the year. The file ypll.csv contains per‐county YPLLs\nfor the United States in 2011.\nThe additional measures (found in additional_measures_cleaned.csv ) contains all sorts of fun measures per county, ranging\nfrom the percentage of people in the county with Diabetes to the population of the county.\nWeʹre going to see which of the additional measures correlate strongly with our mortality measure, and build predictive\nmodels for county mortality rates given these additional measures.\nLoading the Rankings\nThe two .csv files weʹve given you (ypll.csv and additional_measures_cleaned.csv) went through quote a bit of scrubbing\nalready. You can read our notes on the process if youʹre interested.\nWe need to perform some data cleaning and filtering when loading the data. There is a column called ʺUnreliableʺ that will\nbe marked if we shouldnʹt trust the YPLL data. We want to ignore those. Also, some of the rows wonʹt contain data for\nsome of the additional measures. For example, Yakutat, Alaska doesnʹt have a value for % child illiteracy. We want to skip\nthose rows. Finally, there is a row per state that summarizes the stateʹs statistics. It has an empty value for the ʺcountyʺ\ncolumn and we want to ignore those rows since we are doing a county‐by‐county analysis. Hereʹs a function, read_csv , that\nwill read the desired columns from one of the csv files.\nimport csv\ndef read_csv(file_name, cols, check_reliable):\nreader = csv.DictReader(open(file_name, 'rU'))\nrows = {} # map \"statename__countyname\" to the column names in cols\nfor row in reader:\nif check_reliable and row['Unreliable'] == \"x\": # discard unreliable data\ncontinue\nif row['County'] == \"\": # ignore the first entry for each state\ncontinue\nrname = \"%s__%s\" % (row['State'], row['County'])\ntry: # if a row[col] is empty, float(row[col]) throws an exception\nrows[rname] = [float(row[col]) for col in cols]\nexcept:\npass\nreturn rows\nThe function takes as input the csv filename, an array of column names to extract, and whether or not it should check and\ndiscard unreliable data. It returns a dictionary mapping each state/county to the values of the columns specified in cols . It\nhandles all of the dirty data: data marked unreliable, state‐only data, and missing columns.\nWhen we call read_csv multiple times with different csv files, a row that is dropped in one csv file may be kept in another.\n\nWe need to do what database folks call a join between the dict objects returned from read_csv so that only the counties\npresent in both dictionaries will be considered.\nWe wrote a function called get_arrs that retrieves data from the YPLL and Additional Measures datasets. It takes the\narguments dependent_cols , which is a list of column names to extract from ypll.csv , and independent_cols , which is a list of\ncolumn names to extract from additional_measures_cleaned.csv . This function performs the join for you.\nimport numpy\ndef get_arrs(dependent_cols, independent_cols):\nypll = read_csv(\"../datasets/county_health_rankings/ypll.csv\", dependent_cols, True)\nmeasures = read_csv(\"../datasets/county_health_rankings/additional_measures_cleaned.csv\", independent_cols, False)\nypll_arr = []\nmeasures_arr = []\nfor key, value in ypll.iteritems():\nif key in measures: # join ypll and measures if county is in both\nypll_arr.append(value[0])\nmeasures_arr.append(measures[key])\nreturn (numpy.array(ypll_arr), numpy.array(measures_arr))\nWe return numpy arrays (matrices) with rows corresponding to counties and columns corresponding to the columns we\nread from the spreadsheet. We can finally call the get_arrs function to load the desired columns from each file.\ndependent_cols = [\"YPLL Rate\"]\nindependent_cols = [\"Population\", \"< 18\", \"65 and over\", \"African American\",\n\"Female\", \"Rural\", \"%Diabetes\" , \"HIV rate\",\n\"Physical Inactivity\" , \"mental health provider rate\",\n\"median household income\", \"% high housing costs\",\n\"% Free lunch\", \"% child Illiteracy\", \"% Drive Alone\"]\nypll_arr, measures_arr = get_arrs(dependent_cols, independent_cols)\nprint ypll_arr.shape\nprint measures_arr[:,6].shape\nexit()\nPhew. That sucked. Letʹs look at the data!\nLook at a Scatterplot\nLike we did during hypothesis testing, our first step is to look at the data to identify correlations. The best visualization to\nidentify correlations is a scatterplot, since that shows us the relationship between two potentially related variables like ypll\nand % diabetes.\nLetʹs start by looking at scatterplots of ypll versus three potentially correlated variables: % of a community that has diabetes,\n% of the community under the age of 18, and median income.\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(6, 8))\nsubplot = fig.add_subplot(311)\nsubplot.scatter(measures_arr[:,6], ypll_arr, color=\"#1f77b4\") # :,6 means all rows of \"diabetes\"\nsubplot.set_title(\"ypll vs. % of population with diabetes\")\nsubplot = fig.add_subplot(312)\nsubplot.scatter(measures_arr[:,1], ypll_arr, color=\"#1f77b4\") # 1 = age\nsubplot.set_title(\"ypll vs. % population less than 18 years of age\")\nsubplot = fig.add_subplot(313)\nsubplot.scatter(measures_arr[:,10], ypll_arr, color=\"#1f77b4\") # 10 = income\nsubplot.set_title(\"ypll vs. median household income\")\n\nplt.savefig('figures/three‐scatters.png', format='png')\nwhatʹs measures_arr[:,6] ? Thatʹs a numpy supported syntax to extract a subset of a matrix. The first argument specifies\nwhich rows to extract. It can be a number (like 3), a python slice ( :3 means the rows from 0 to 3, while 3:5 means 3 to 5), or\n: , which means all of the rows. The second argument specifies which columns to extract. In this case it is 6 , which is the\n7ʹth column (remember, itʹs 0 indexed).\nYour plots should look something like this:\nWe picked these three examples because they show visual evidence of three forms of correlation:\nIn the first plot, we can see that when the percentage of people in a county with diabetes is higher, so is the mortality\nrate (YPLL)‐‐‐evidence of a positive correlation.\nThe second plot looks like a blob. Itʹs hard to see a relationship between mortality and the fraction of people under the\nage of 18 in a community.\nThe final plot shows evidence of negative correlation. Counties with higher median incomes appear to have lower\nmortality rates.\nExercise Look at scatter plots of other variables vs. YPLL. We found the percent of children eligible for school lunch to be\n\nalarmingly correlated with YPLL!\nYour First Regression\nItʹs time we turn the intuition from our scatterplots into math! Weʹll do this using the ols module, which stands for ordinary\nleast squares regression. Letʹs run a regression for YPLL vs. % Diabetes.\nimport ols\nmodel = ols.ols(ypll_arr, measures_arr[:,6], \"YPLL Rate\", [\"% Diabetes\"]) # 6 = diabetes\nmodel.summary()\nthe ols script in dataiap/day3/ implement a method called ols() that takes four arguments:\n1. a 1‐dimensional numpy array containing the values of the dependent variable (e.g., YPLL)\n2. a 2‐dimensional numpy array where each row contains the values of an independent variable. In this case the only\nindependent variable is ʺ% Diabetesʺ, so the matrix has the same shape as ypll_arr.\n3. The label for the first argument\n4. A list of labels for each row in the second argument\nAs you can see, running the regression is simple, but interpreting the output is tougher. Hereʹs the output of model.summary()\nfor the YPLL vs. % Diabetes regression:\n======================================================================\nDependent Variable: YPLL Rate\nMethod: Least Squares\nDate:\nFri, 23 Dec 2011\nTime:\n13:48:11\n# obs:\n# variables:\n======================================================================\nvariable\ncoefficient\nstd. Error\nt‐statistic\nprob.\n======================================================================\nconst\n585.126403\n169.746288\n3.447064\n0.000577\n%Diabetes\n782.976320\n16.290678\n48.062846\n0.000000\n======================================================================\nModels stats\nResidual stats\n======================================================================\nR‐squared\n0.511405\nDurbin‐Watson stat\n1.951279\nAdjusted R‐squared\n0.511184\nOmnibus stat\n271.354997\nF‐statistic\n2310.037134\nProb(Omnibus stat)\n0.000000\nProb (F‐statistic)\n0.000000\nJB stat\n559.729657\nLog likelihood\n‐19502.794993\nProb(JB)\n0.000000\nAIC criterion\n17.659389\nSkew\n0.752881\nBIC criterion\n17.664550\nKurtosis\n4.952933\n======================================================================\nLetʹs interpret this:\nFirst, letʹs verify the statistical significance, to make sure nothing happened by chance, and that the regression is\nmeaningful. In this case, Prob (F‐statistic) , which is under Models stats , is something very close to 0, which is less\nthan .05 or .01. That is: we have statistical significance, and we an safely interpret the rest of the data.\nThe coefficients (called betas ) help us understand what line best fits the data, in case we want to build a predictive\nmodel. In this case const is 585.13, and %Diabetes has a coefficient of 782.98. Thus, the line (y = mx + b) that best\npredicts YPLL from %Diabetes is: YPLL = (782.98 * %Diabetes) + 585.13.\nTo understand how well the line/model weʹve built from the data helps predict the data, we look at R‐squared . This\nvalue ranges from 0 (none of the change in YPLL is predicted by the above equation) to 1 (100% of the change in\nYPLL is predicted by the above equation). In our case, 51% of the changes YPLL can be predicted by a linear equation\non %Diabetes. Thatʹs a reasonably strong correlation.\n\nPutting this all together, weʹve just discovered that, without knowing the YPLL of a community, we can take data on the\npercentage of people affected by diabetes, and roughly reconstruct 51% of the YPLLʹs characteristics.\nIf you want to use the information in your regression to do more than print a large table, you can access the data\nindividually\nprint \"p‐value\", model.Fpv\nprint \"coefficients\", model.b\nprint \"R‐squared and adjusted R‐squared:\", model.R2, model.R2adj\nTo better visualize the model weʹve built, we can also plot the line weʹve calculated through the scatterplot we built before\nfig = plt.figure(figsize=(6, 4))\nsubplot = fig.add_subplot(111)\nsubplot.scatter(measures_arr[:,6], ypll_arr, color=\"#1f77b4\") # 6 = diabetes\nsubplot.set_title(\"ypll vs. % of population with diabetes\")\ndef best_fit(m, b, x): # calculates y = mx + b\nreturn m*x + b\nline_ys = [best_fit(model.b[1], model.b[0], x) for x in measures_arr[:,6]]\nsubplot.plot(measures_arr[:, 6], line_ys, color=\"#ff7f0e\")\nplt.savefig('figures/scatter‐line.png', format='png')\nThat should result in a plot that looks something like\nWe can see that our line slopes upward (the beta coefficient in front of the %Diabetes term is positive) indicating a positive\ncorrelation.\nExercise Run the correlations for percentage of population under 18 years of age and median household income.\nWe got statistically significant results for all of these tests. Median household income is negatively correlated (the slope beta\nis ‐.13), and explains a good portion of YPLL (R‐squared is .48). Remember that we saw a blob in the scatterplot for\npercentage of population under 18. The regression backs this up: the R‐squared of .005 suggests little predictive power of\nYPLL.\n\nExercise Plot the lines calculated from the regression for each of these independent variables. Do they fit the models?\nExercise Run the correlation for % of children eligible for school lunches. Is it significant? Positively or negatively\ncorrelated? How does this R‐squared value compare to the ones we just calculated?\nExplaining R‐squared\nR‐squared roughly tells us how well the linear model (the line) we get from a linear regression explains the independent\nvariable.\nR‐squared values have several interpretations, but one of them is as the square of a value called the Pearson Correlation\nCoefficient. That last link has a useful picture of the correlation coefficient that shows you the value of R for different kinds\nof data.\nSquaring R makes it always positive and changes its asymptotic properties, but the same trends (being near 0 or near 1) still\napply.\nRunning Multiple Variables\nSo far, weʹve been able to explain about 50% of the variance in YPLL using our additional measures data. Can we do better?\nWhat if we combine information from multiple measures? Thatʹs called a multiple regression, and we already have all the\ntools we need to do it! Letʹs combine household income, %Diabetes, and percentage of the population under 18 into one\nregression.\ndependent_cols = [\"YPLL Rate\"]\nindependent_cols = [\"< 18\", \"%Diabetes\" , \"median household income\"]\nypll_arr, measures_arr = get_arrs(dependent_cols, independent_cols)\nmodel = ols.ols(ypll_arr, measures_arr, \"YPLL Rate\", independent_cols)\nprint \"p‐value\", model.Fpv\nprint \"coefficients\", model.b\nprint \"R‐squared and adjusted R‐squared:\", model.R2, model.R2adj\nWe got the following output:\np‐value 1.11022302463e‐16\ncoefficients [\n4.11471809e+03\n1.30775027e+02\n5.16355557e+02 ‐ 8.76770577e‐02]\nR‐squared and adjusted R‐squared: 0.583249144589 0.582809842914\nSo weʹre still significant, and can read the rest of the output. A read of the beta coefficients suggests the best linear\ncombination of all of these variables is YPLL = 4115 + 131(% under 18) + 516(% Diabetes) ‐ 877*(median household income)\n.\nBecause there are multiple independent variables in this regression, we should look at the adjusted R‐squared value, which\nis .583. This value penalizes you for needlessly adding variables to the regression that donʹt give you more information about\nYPLL. Anyway, check out that R‐squared‐‐‐nice! Thatʹs larger than the R‐squared value for any one of the regressions we\nran on their own! We can explain more of YPLL with these variables.\nExercise Try combining other variables. Whatʹs the largest adjusted R‐squared you can achieve? We can reach .715 by an\nexcessive use of variables. Can you replicate that?\nEliminate Free Lunches, Save the Planet\nAt some point in performing a regression and testing for a correlation, you will be tempted to come up with solutions to\nproblems the regression has not identified. For example, we noticed that the percentage of children eligible for free lunch is\npretty strongly correlated with the morbidity rate in a community. How can we use this knowledge to lower the morbidity\nrate?\n\nALERT, ALERT, ALERT!!! The question at the end of the last paragraph jumped from a question of correlation to a\nquestion of causation.\nIt would be far‐fetched to think that increasing or decreasing the number of children eligible for school lunches would\nincrease or decrease the morbidity rate in any significant way. What the correlation likely means is that there is a third\nvariable, such as available healthcare, nutrition options, or overall prosperity of a community that is correlated with both\nschool lunch eligibility and the morbidity rate. Thatʹs a variable policymakers might have control over, and if we somehow\nimproved outcomes on that third variable, weʹd see both school lunch eligibility and the morbidity rate go down.\nRemember: correlation means two variables move together, not that one moves the other.\nWeʹve hit the point that if youʹre stressed for time, you can jump ahead to the closing remarks. Realize, however, that thereʹs\nstill mind‐blowing stuff ahead, and if you have time you should read it!\nNonlinearity\nIs finding a bunch of independent variables and performing linear regression against some dependent variable the best we\ncan do to model our data? Nope! Linear regression gives us the best line to fit through the data, but there are cases where\nthe interaction between two variables is nonlinear. In these cases, the scatterplots we built before matter quite a bit!\nTake gravity for example. Say we measured the distance an object fell in a certain amount of time, and had a bit of noise to\nour measurement. Below, weʹll simulate that activity by generating the time‐distance relationship that we learned in high\nschool (displacement = .5gt^2). Imagine we record the displacement of a ball as we drop it, storing the time and\ndisplacement measurements in timings and displacements .\ntimings = range(1, 100)\ndisplacements = [4.9*t*t for t in timings]\nA scatterplot of the data looks like a parabola, which wonʹt fit lines very well! We can transform this data by squaring the\ntime values.\nsq_timings = [t*t for t in timings]\nfig = plt.figure()\nsubplot = fig.add_subplot(211)\nsubplot.scatter(timings, displacements, color=\"#1f77b4\")\nsubplot.set_title(\"original measurements (parabola)\")\nsubplot = fig.add_subplot(212)\nsubplot.scatter(sq_timings, displacements, color=\"#1f77b4\")\nsubplot.set_title(\"squared time measurements (line)\")\nplt.savefig('figures/parabola‐linearized.png', format='png')\nHere are scatterplots of the original and transformed datasets. You can see that squaring the time values turned the plot\ninto a more linear one.\n\nExercise Perform a linear regression on the original and transformed data. Are they all significant? Whatʹs the R‐squared\nvalue of each? Which model would you prefer? Does the coefficient of the transformed value mean anything to you?\nFor those keeping score at home, we got R‐squared of .939 and 1.00 for the unadjusted and adjusted timings, which means\nwe were able to perfectly match the data after transformation. Note that in the case of the squared timings, the equation we\nend up with is displacement = 4.9 * time^2 (the coefficient was 4.9), which is the exact formula we had for gravity.\nAwesome!\nExercise Can you improve the R‐squared values by transformation in the county health rankings? Try taking the log of the\npopulation, a common technique for making data that is bunched up spread out more. To understand what the log\ntransform did, take a look at a scatterplot.\nLog‐transforming population got us from R‐squared = .026 to R‐squared = .097.\nLinear regression, scatterplots, and variable transformation can get you a long way. But sometimes, you just canʹt figure out\nthe right transformation to perform even though thereʹs a visible relationship in the data. In those cases, more complex\ntechnques like nonlinear least squares can fit all sorts of nonlinear functions to the data.\nWhere to go from here\nToday youʹve swallowed quite a bit. You learned about significance testing to support or reject high‐likelihood meaningful\nhypotheses. You learned about the T‐Test to help you compare two communities on whom youʹve measured data. You then\nlearned about regression and correlation, for identifying variables that change together. From here, there are several\ndirections to grow.\nA more general form of the T‐Test is an ANOVA, where you can identify differences among more than two groups,\nand control for known differences between items in each dataset.\n\nThe T‐Test is one of many tests of statistical significance.\nThe concept of statistical significance testing comes from a frequentist view of the world. Another view is the Bayesian\napproach, if mathematical controversy is your thing.\nLogistic regression, and more generally classification, can take a bunch of independent variables and map them onto\nbinary values. For example, you could take all of the additional measures for an individual and predict whether they\nwill die before the age of 75.\nMachine learning and data mining are fields that assume statistical significance (you collect boatloads of data) and\ndevelop algorithms to classify, cluster, and otherwise find patterns in the underlying datasets.\n\nMIT OpenCourseWare\nhttp://ocw.mit.edu\nResource: How to Process, Analyze and Visualize Data\nAdam Marcus and Eugene Wu\nThe following may not correspond to a particular course on MIT OpenCourseWare, but has been\nprovided by the author as an individual learning resource.\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
    }
  ]
}