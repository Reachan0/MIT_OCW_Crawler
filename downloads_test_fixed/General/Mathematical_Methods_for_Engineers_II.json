{
  "course_name": "Mathematical Methods for Engineers II",
  "course_description": "No description found.",
  "topics": [
    "Engineering",
    "Systems Engineering",
    "Computational Science and Engineering",
    "Mathematics",
    "Applied Mathematics",
    "Differential Equations",
    "Linear Algebra",
    "Engineering",
    "Systems Engineering",
    "Computational Science and Engineering",
    "Mathematics",
    "Applied Mathematics",
    "Differential Equations",
    "Linear Algebra"
  ],
  "syllabus_content": "Course Meeting Times\n\nLectures: 3 sessions / week, 1 hour / session\n\nPrerequisites\n\nCalculus (18.02), Differential Equations (\n18.03\n) or Honors Differential Equations (\n18.034\n).\n\nTextbooks\n\nThis course as taught during the Spring 2006 term on the MIT campus used the following text:\n\nStrang, Gilbert.\nIntroduction to Applied Mathematics.\nWellesley, MA:\nWellesley-Cambridge Press\n, 1986. ISBN: 9780961408800. (\nTable of Contents\n)\n\nSince that time, Professor Strang has published a new textbook that is being used for this course as it is currently taught on the MIT campus, as well as for Mathematical Methods for Engineers I (18.085). Information about the new book can be found at the\nWellesley-Cambridge Press\nWeb site, along with a link to Prof. Strang's new \"Computational Science and Engineering\" Web page developed as a resource for everyone learning and doing Computational Science and Engineering.\n\nStrang, Gilbert.\nComputational Science and Engineering\n. Wellesley, MA:\nWellesley-Cambridge Press\n, 2007. ISBN: 9780961408817.\n\nDescription\n\nThis course has two major topics:\n\nInitial Value Problems\n\nLinear: Wave Equation, Heat Equation, Convection Equation\n\nNonlinear: Conservation Laws, Navier-Stokes Equation\n\nFinite Difference Methods: Accuracy and Stability\n\nLax Equivalence Theorem: CFL and Von Neumann Conditions\n\nFourier Analysis: Diffusion, Dissipation, Dispersion\n\nSeparation of Variables and Spectral Methods\n\nSolution of Large Linear Systems\n\nFinite Differences, Finite Elements, Optimization\n\nDirect Methods: Reordering by Minimum Degree\n\nIterative Methods and Preconditioning\n\nSimple Iteration (Jacobi, Gauss-Seidel, Incomplete LU)\n\nKrylov Methods: Arnoldi Orthogonalization\n\nConjugate Gradients and GMRES\n\nMultigrid Methods\n\nInverse Problems and Regularization\n\nRequirements\n\nThere are no exams in 18.086. Two computational projects take their place, one on each of the major topics in the course. The projects are chosen by each student and they include a brief report.",
  "files": [
    {
      "category": "Resource",
      "title": "josephkovacpro.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/12c176f3c3db7c3d8f72c9dcad3e9390_josephkovacpro.pdf",
      "content": "Joseph Kovac\n18.086 Final Project\nSpring 2005\nProf. Gilbert Strang\n\nThe Fundamentals and Advantages of Multi-grid Techniques\n\nIntroduction\n\nThe finite difference method represents a highly straightforward and logical\napproach to approximating continuous problems using discrete methods. At its heart is a\nsimple idea: substitute finite, discrete differences for derivatives in some way appropriate\nfor a given problem, make the time and space steps the right size, run the difference\nmethod, and get an approximation of the answer.\n\nMany of these finite difference methods can ultimately be written in a matrix\nform, with a finite difference matrix multiplying a vector of unknowns to equal a known\nquantity or source term. In this paper, we will be examining the problem Au=f, where A\nrepresents a finite difference matrix operating on u, a vector of unknowns, and f\nrepresents a time-independent vector of source terms. While this is a general problem,\nwe will specifically examine the case where A is the finite difference approximation to\nthe centered second derivative. We will examine solutions arising when f is zero\n(Laplace's equation) and when it is nonzero (Poisson's equation).\n\nThe discussion would be quite straightforward if we wanted it to be; to find u, we\nwould simply need to multiply both sides of the equation by A-1, explicitly finding\nu= A-1f. While straightforward, this method becomes highly impractical as the mesh\nbecomes fine and A becomes large, requiring inversion of an impractically large matrix.\nThis is especially true for the 2D and 3D finite difference matrices, whose dimensions\ngrow as the square and cube of the length of one edge of the square grid.\n\nIt is for this reason that relaxation methods became both popular and necessary.\nMany times in engineering applications, getting the exact answer is not necessary; getting\nthe answer right to within a certain percentage of the actual answer is often good enough.\nTo this end, relaxation methods allow us to take steps toward the right answer. The\nadvantage here is that we can take a few iterations toward the answer, see if the answer is\ngood enough, and if it is not, iterate until it is. Oftentimes, using such an approach,\ngetting an answer \"good enough\" could be done with orders of magnitude less time and\ncomputational energy than with an exact method.\n\nHowever, relaxation methods are not without their tradeoffs. As will be shown,\nthe error between the actual answer and the last iteration's answer ultimately will decay\nto zero. However, not all frequency components of the error will get to zero at the same\nrate. Some error modes will get there faster than others. What we seek is to make all the\nerror components get to zero as fast as possible by compensating for this difference in\ndecay rates. This is the essence of multi-grid; multi-grid seeks to allow the error modes\nof the solution to decay as quickly as possible by changing the resolution of the grid to\nlet the error decay properties of the grid be an advantage rather than a liability.\n\nBasic Theory of the Jacobi Relaxation Method\n\nBefore going into the theory of the method, I first want to state that much of the\nfollowing closely comes from an explanation in A Multi-grid Tutorial by William Briggs\net al. This text explained the material as clearly and concisely as one could hope for. To\na large extent, much of the \"theory section\" following will be reiteration of their\nexplanation, but with emphasis on concepts which will be validated in the numerical\nexperiments later. In no way do I claim these derivations as my own. The following is a\nderivation of the Jacobi method in matrix form, which is the relaxation method which\nwill be used for the rest of the paper.\n\nWe can first express the matrix A as a sum of its diagonal component D and lower\nand upper triangular components L and U:\n\nU\nL\nD\n+\n+\n=\nA\n(1)\n\nso\n\nf\nu\nU\nL\nD\n=\n+\n+\n)\n(\n(2)\n\nWe can move the upper and lower triangular parts to the right side:\n\nf\nu\nU\nL\nDu\n+\n+\n-\n=\n)\n(\n(3)\n\nWe can then multiply both sides by D-1:\n\n)\n)\n(\n(\nf\nu\nU\nL\nD\nu\n+\n+\n-\n=\n-\n(4)\n\nWe can define\n\n)\n(\nU\nL\nD\nRJ\n+\n-\n=\n-\n(5)\n\nTherefore, we have defined the iteration in matrix form, and can write, in the notation of\nBriggs's chapter in Multi-grid Methods:\n\nf\nD\nu\nR\nu\nJ\n)\n(\n)\n(\n-\n+\n=\n(6)\n\nWeighed Jacobi takes a fraction of the previous iteration and adds it to a fraction of the\nprevious iteration with the Jacobi iteration applied:\n\nf\nD\nu\nR\nI\nu\nJ\n)\n(\n)\n1(\n]\n)\n[(\n-\n+\n+\n-\n=\nω\nω\nω\n(7)\n\nWe can rewrite the above as\n\nf\nD\nu\nR\nu\n)\n(\n)\n1(\n-\n+\n=\nω\nω\n(8)\n\nwhere\n]\n)\n[(\nJ\nR\nI\nR\nω\nω\nω\n+\n-\n=\n(9)\n\nThis iteration and its characteristics on the grid is the focus of this paper. Before\nattempting to implement this method, it is first good to predict the behavior we expect to\nsee theoretically. One way to do this is to look at the eigenvalues of the matrix Rω. The\nfollowing again stems from Briggs, but some of the following was not explicitly\nexplained and left as an \"exercise\" in the text.\n\nWe first note that, by the properties of eigenvalues and by eq. 9,\n\nRJ\nR\nωλ\nω\nλ ω\n+\n-\n=\n)\n1(\n(10)\n\nTherefore, we first need to find λRJ. We observe that:\n\nI\nA\nU\nL\n-\n=\n+\n(11)\n\nTherefore,\n\n-\n=\n+\nA\nU\nL\nλ\nλ\n(12)\n\nNoting that, for the 1D case,\n\nI\nD\n1 =\n-\n(13)\n\nSo, using eq. 5 and properties of eigenvalues,\n\n)\n(\n+\n-\n=\n-\n-\n=\nA\nA\nRJ\nλ\nλ\nλ\n(14)\n\nTherefore, remembering eq. 10,\n\nA\nR\nωλ\nλ ω\n-\n=\n(15)\n\nThe kth eigenvalue of the matrix A is:\n\n),\n(\nsin\n)\n(\n-\n≤\n≤\n=\nn\nk\nn\nk\nA\nk\nπ\nλ\n(16)\n\nSo, by eq. 15, the eigenvalues λω are:\n\n1,\nsin\n)\n(\n-\n≤\n≤\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n-\n=\nn\nk\nn\nk\nR\nk\nπ\nω\nλ\nω\n(17)\n\nAnd the jth component of the kth eigenvector is:\n\nn\nj\nn\nk\nn\njk\nj\nk\n≤\n≤\n-\n≤\n≤\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n=\n0,1\n1,\nsin\n,\nπ\nω\n(18)\n\nWe can make two quick observations here. First, for ω between 0 and 1, the\neigenvalues will always lie between -1 and 1, implying stability to the iteration. Second,\nwe remember that all vectors in the space of the matrix A can be represented as a\nweighed sum of the eigenvectors:\n\n∑\n-\n=\n=\n)\n(\nn\nk\nk\nkc\nu\nω (19)\n\nIn this case, since the eigenvectors are Fourier modes, there is an additional useful\ninterpretation of the weighting coefficients ck of the linear combination of eigenvectors;\nthese are analogous to the Fourier series coefficients in a periodic replication of the\nvector u. The other key point to see here is that varying the value of ω allows us to adjust\nhow the eigenvalues of A vary with the frequency of the Fourier modes. Plotted below is\nthe eigenvalue magnitude versus k, for n=32. We can easily see that varying ω\nsignificantly changes the relative eigenvalue magnitude at various frequencies.\n\nFigure 1: Distribution of eigenvalue magnitude as ω is varied\n\nThe implications of the graph above manifest themselves when we think of the\nhomogenous case of Au=0. If we were to use the Jacobi iteration in this case, and started\nfrom a vector as our \"guess\" at the final answer:\n\n⎟\n⎠\n⎞\n⎜\n⎝\n⎛\n=\nn\njk\nu\nπ\nsin\n)\n(\n(20)\n\nwhere k is between 1 and n-1, we would have an error made of only one mode, i.e. the\nerror would lie perfectly along a single eigenvector of the matrix A and ck would be zero\nfor all k except the k which the vector u lay along. A priori, in this simple case, we know\nthat the solution should converge to 0 with enough steps, as there are no source or sink\nterms.\n\nIn practical situations, we won't know the correct u, and the error will not be\nsimply along one eigenvector. Now, the importance of Figure 1 becomes clear: adjusting\nω allows us to decide which error frequencies on the grid we want to decay quickly\nrelative to others. Picking the correct ω is somewhat application specific. In Briggs'\nexample, he picks to have the eigenvalue magnitudes of the middle frequency and highest\nfrequency match, so that the grid we work on will be decidedly favored towards either\ndecay of high frequency modes or low frequency modes. The motive for this choice will\nbecome apparent later. For this condition, ω=2/3. This value of ω will be used\nthroughout the numerical experiments.\n\nBasic Theory of Multi-grid\n\nThere is an obvious disadvantage to the relaxation method so far: while high\nfrequency error components can quickly decay, the eigenvalues of lower frequency\ncomponents approach 1, meaning that these lower frequency error components take many\nmore iterations to be damped out than higher frequencies. The essence of multi-grid is to\nuse this feature to our advantage rather than to our detriment. What if we were to\nsomehow take a few iterations to first smooth out the high-frequency components on the\nfine grid, then downsample the problem onto a coarser grid where the lower frequency\ncomponents would decay faster, then somehow go back up to the fine grid?\n\nFirst, consider what happens to the kth mode when downsampled onto a grid half\nas fine as the original vector (i.e. downsampling by a factor of 2). The kth mode on the\nfine grid becomes the kth mode on the coarse grid. This also implies that the \"more\noscillatory\" modes on the fine grid become aliased on the coarse grid. A rigorous\nargument complete with Fourier series plots could be made here, but that is not the point.\nThe implication is that now the error that refused to decay quickly on the fine grid has\nbeen frequency-shifted so that it has become high-frequency error on the coarse grid and\nwill decay quickly.\n\nAll that is left to do is to define what it means to move from a fine grid to a coarse\ngrid and eventually come back again, and how to correctly state the problem so that the\nanswer achieved is accurate. First, a few basic relationships need to be established.\nAgain, this is not original thought, and closely follows the Briggs text. First, the\nalgebraic error of the current iteration is defined as\n\n)\n(\n)\n(\nn\nn\nu\nu\ne\n-\n=\n(21)\n\nwhere u is the correct value of u, and u(n) is the resulting approximation after n steps.\n\nThe residual is defined as the amount by which the current guess at u(n) fails to satisfy\nAu=f:\n\n)\n(\n)\n(\nn\nn\nu\nf\nr\nA\n-\n=\n(22)\n\nGiven these relationships, we can also state that\n\n)\n(\n)\n(\nn\nn\nr\ne\n=\nA\n(23)\n\nThis fact lets us make the following statement about relaxation, as quoted from Briggs:\n\n\"Relaxation on the original equation Au=f with an arbitrary initial guess v is equivalent to\nrelaxing on the residual equation Ae=r with the specific initial guess e=0.\"\n\nThis makes intuitive sense by eqs. 21-23: We don't know the error, but we know\nthat the error will be zero when the residual is zero. Therefore, we can either iterate to\nsolve Au=f or we can ask, what would the error vector have to be to yield the current\nresidual? If we know the error, we know the answer by simple rearrangement of eq. 21.\nIn more mathematical terms, what the above statements are saying is the\nfollowing: if we take a few iterations to get the current value of r, we could then\nreformulate the problem by taking that value of r, then solving the new problem Ae=r\nusing Jacobi iteration, and read off the value of e after a few iterations. This will give us\na guess at what the error was before the problem was restated. Rearrangement of eq. 21\nwould then imply that if we just added the calculated value of e to the u(n) we had before\nrestating the problem, we would get a refined guess at the true vector u.\n\nPutting this fact together with the idea of moving from grid to grid, we can\ncombine the overall idea into the following:\n\n1) Relax the problem for a few steps on the fine grid with Au=f\n2) Calculate the residual r=f-Au(n)\n3) Downsample the residual onto a coarser grid\n4) Relax on Ae=r for a number of steps, starting with a guess of e=0\n5) Upsample and interpolate the resulting e onto the fine grid\n6) Refine our guess at u by adding e on the fine grid to the original value of u(n)\n\nThe above method is the central theory of multi-grid and variations of it will show\nthat there are significant gains to be made by changing the grid.\n\nImplementing a Multi-grid Solver - 1D\n\nUp to this point, the paper has mostly been a reiteration and thorough explanation\nof the Briggs text, specifically highlighting points which will be of importance later. At\n\nthis point, however, the subtleties and difficulties of actually implementing a multi-grid\nsolver arise, and while a few of the upcoming points were explained in the Briggs text,\nmuch of the actual implementation required original struggling on my part. It was quite\ndifficult despite the clarity of the theoretical basis of multi-grid. I also consulted with\ntwo students in Prof. Jacob White's group to help me think about aspects of boundary\nconditions more clearly.\n\nIn the 1-D case, I sought to implement as simple of a solver as possible; I was far\nmore interested in developing a more feature-rich 2D solver. Therefore, in the 1-D case,\nI developed a solver which would solve Laplace's equation only, and with zero boundary\nconditions. In other words, I wanted to solve only the homogenous case to demonstrate\nthat the error decays faster on the fine grid for high frequencies versus low frequencies,\nand that an inter-grid transfer would make error decay faster.\n\nSince I only needed to deal with zero boundary conditions in this case, I was able\nto use the standard, second finite difference matrix with zero boundary conditions from\nclass. To demonstrate the multi-grid method, I designed one solver and its associated\nfinite difference matrix for a 16 point grid problem, and another which would operate on\nan 8 point grid. The finite difference method was the standard one from class.\n\nThe inter-grid transfers between the fine and coarse grids were the trickier parts.\nBriggs implements downsampling from the fine grid to the coarse grid by the following\n\"full weighting\" definition:\n\n(\n)\n-\n≤\n≤\n+\n+\n=\n+\n-\nn\nj\nv\nv\nv\nv\nh\nj\nh\nj\nh\nj\nh\nj\n(24)\n\nFor a vector 7 components long, this operation can be implemented by a\nmultiplication by the following matrix:\n\n1 2 1 0 0 0 0\n\n1⁄4 *\n0 0 1 2 1 0 0 (25)\n\n0 0 0 0 1 2 1\n\nSuch a matrix would move the vector from a grid of seven points to a grid of three\npoints. This takes care of the coarsening operation; a scaled transpose of this matrix\nperforms linear interpolation, and allows us to transfer data from the coarse grid to the\nfine grid. That fact is the primary motivation for using the full weighting method rather\nthan simply downsampling by taking every other point from the fine grid.\nUnfortunately, practical, non-ideal interpolators will also introduce error through\nthe interpolation; this error will need to be smoothed out by relaxing again on the fine\ngrid as it will likely have some higher-frequency components in the interpolation error\nbest smoothed by the finer grid. If one transposes the above matrix and scales it by 2, the\nlinear interpolation scheme would be realized.\nAs stated before, I only sought to confirm the idea that the higher frequency error\nwill decay faster on the fine grid than the low frequency error. In the graph below, I\ndefined the initial \"guess\" as the sum of a low frequency (discrete frequency π/8) and a\nhigher frequency (discrete frequency 15π/16). It is obvious that the high frequency\ncomponent decays much faster than the low frequency component.\n\nFigure 2: High-frequency component of error decays faster than low frequency component\n\nThe only other thing left to confirm in the 1D case was that a multi-grid approach\nshowed some promise of benefit. To demonstrate this, I used the same initial function\nand compared a relaxation of thirty steps on the fine grid with a relaxation of ten steps on\nthe fine grid, ten on the coarser grid, and ten more to smooth out interpolation error at the\nend on the fine grid, giving both approaches the same total number of steps. The results\nfor the single grid approach versus the multi-grid approach are shown below.\n\nFigure 3: The advantage of the grid transfer quickly becomes apparent\nI must qualify the above plot with the following information. There was a bit of a\ndiscrepancy with the definition of h in the finite difference method (i.e. the 1/h2 term in\nfront of the matrix K). Intuitively, as the grid coarsens, h should change. This change\nwas necessary and gave the best results in the 2D case. However, in the 1D case I had to\ntweak this factor a bit; I had to multiply the proper K on the coarse grid by 4 to get the\n\nexpected advantage working with the grid transfer. I couldn't find the source of the\ndiscrepancy, and it might be a subtlety that I missed somewhere. Nonetheless, even with\nthis mysterious \"gain factor,\" the above experiment proves that faster convergence to the\nzero error state can happen with a grid transfer rather than simply staying on the fine grid\nfor all steps.\n\nImplementing a Multi-grid Solver - 2D\n\nThe 2D case shares a number of similarities with the 1D case, but it carries a\nnumber of subtleties with it that make implementation of the method significantly more\ndifficult than the 1D case. The most difficult aspect to attack was getting the boundary\nconditions right. I decided that I would stick to Dirichlet boundary conditions for this\nproject, as their implementation was significant work, let alone think about Neumann\nconditions.\n\nThe 1D case was implemented minimally, only thoroughly enough to demonstrate\nthe relative rates at which the different modes of the error in the homogenous case\ndecayed and that grid transfers showed a hint of promise. In the 2D case, I wanted to\nimplement a more useful and practical solver. Specifically, I wanted to be able to specify\nDirichlet boundaries, source terms in the grid, and boundaries within the grid. In the\nelectrostatics case, this would be like saying that I wanted to be able to specify the\nboundary voltages of my simulation grid, any charge source in the medium, and the\nvoltages of any electrodes existing internal to the grid.\n\nSpecifying charge sources is very easy: just specify them in f. However,\nspecifying boundary conditions is more difficult. I decided to incorporate the boundary\nvalues by altering both the matrix A and the right-hand side f. As we learned, the 2D\nfinite difference matrix generally has the following form:\n\nFigure 4:The K matrices from Prof. Strang's new text\n\nIn order to properly implement the boundary condition, we must remember the\nequations underlying the K2D matrix: we are simply solving an N2 by N2 system of linear\nequations. Therefore, if we fix u(p)=b for some value p and constant b, this means that in\nour system of linear equations, whenever the value u(p) shows up in one of the\nsimultaneous equations, its value must be b. The way to accomplish this is simple; we\nmust alter Au=f to reflect this fact. If we simply set the pth row of A to zero, and then set\nthe pth column of that row to be 1 (i.e. set the pth diagonal entry to 1), the value at u(p)\nwill be forced to f(p). Therefore, assuming f was originally the zero vector, we must now\nsatisfy that the pth entry of f now be equal to u(p), so now f(p)=b. This has forced u(p)=b.\nOne might wonder if we should also set the pth column to zero. We should not, as\nthe columns allow the forced value of u(p) to propagate its information into other parts of\n\nthe grid. Physically, at least in electrostatics, there is no intuition of having a source at a\npoint where there is a boundary condition, because the boundary manifests itself as a\nsource in this implementation. Therefore, if there is a boundary at u(p), f(p) will be zero\nat that point before we put the constraint on the point.\n\nThe above method works excellently for interior boundary points. The actual grid\nboundaries, where Dirichlet conditions were sought, are not as straightforward. Some\nfinite difference methods deal with these points implicitly by never explicitly defining\ngrid points at the edges. Instead, I decided to explicitly define these points and alter the\nA matrix, creating a matrix format which deviated from that in the figure above.\n\nThe difficulties in the above implementation arise when the difference matrix of\n\"K2D\" \"looks\" outside the grid implicitly when it touches the edges of the grid. This is\neasier to see in the 1D K matrix. The first and last rows of K are missing -1's in that\nmatrix. Implicitly, this means that the finite difference operator looked \"off the grid\" and\nfound zero, unless a nonzero value shows up to make a non-zero entry in f. I decided to\nexplicitly define the boundary values instead of trying to keep up with these issues.\n\nFirst, the ordering definition of u and A must be defined. For my implementation,\nu(0) was the upper-left corner of the grid and u(N) was the lower-left corner. u(N+1) was\nthe point right of u(0), and u(2N) was the point to the right of u(N). u(N2-N+1) was the\nupper-right corner, and u(N2) was the lower-right corner.\n\nTherefore, to define explicit boundaries, I needed to set the first N values of u to\nthe Dirichlet conditions. Therefore, when constructing A, by the reasoning from the\ninterior boundary points described above, the upper-left corner of A was a block identity\nmatrix of size N, and f(1...N) was set to the boundary value. This construction dealt with\nthe left edge easily. I constructed the rest of A by using the traditional 2D stencil from\nclass. In order to account for the top and bottom edges, I made sure to set those\ncorresponding rows in A to zero, except with a 1 on the diagonal and the corresponding\nvalue of f to the boundary condition. When I reached the lower-right corner, I augmented\nA with another block identity matrix of size N, and set f to the boundary condition at\nthose points. A matrix density plot is shown below to illustrate this construction.\nApproaching the boundaries explicitly made them easier to track, but an algorithm to\nconstruct a general A for a given mesh size was quite difficult; that is the tradeoff.\n\nFigure 5: Sparsity pattern of my altered finite difference matrix which allows for explicit boundary\ndefinition. Notice the periodic gaps along the diagonal representing the top and bottom edges of the\ngrid.\n\nWith boundary conditions properly incorporated, the last topic to address was that\nof inter-grid transfers: what matrix downsamples the original data to a coarser grid?\nWhich matrix transfers from the coarse grid to the fine grid? The proper way to phrase\nthe question is this: what is the proper way to transfer data from one grid to another?\n\nIn going from a coarse grid to a fine grid, the central problem is interpolation.\nThe central ideas of interpolation and downsampling were discussed in the 1-D section.\nThe 2D implementation is highly similar, but with a little more complexity than the 1D\ncase due to slightly trickier boundaries on the edges. I decided that I would again seek to\ndo downsampling as a weighted averaging of neighboring points rather than by injection.\nAgain, the reason for this approach was so that simply transposing the downsampling\nmatrix would yield the linear interpolation matrix for upsampling and linear interpolation.\n\nSuch a downsampling matrix was rather straightforward to implement for the\ninterior points of the grid. Incorporating the edges would have been somewhat trickier,\nand the averaging scheme used, if simply allowed to include the edges, would have\nchanged the boundary values themselves, which is to be avoided at all costs. Therefore, I\ntook the following approach.\n\nDownsampling\n\n1) Calculate the residual\n2) Remove the edges from the fine grid residual data\n3) Design a downsampling matrix to transform the inner grid residual data from the\nfine grid to the twice-as-coarse grid\n4) Apply the downsampling matrix to the interior residual data\n\n5) Append zeros around downsampled residual data grid to represent the fact that the\nresiduals are by definition zero at the edges where we have defined the exterior\nboundaries.\n\nUpsampling\n\n1) Remove the zeros from the coarse grid's edges (this is after we have relaxed the\nresidual problem on the coarser grid)\n2) Apply the scaled, transposed downsampling matrix to the interior points to get the\ninterpolated guess at the error on the fine grid\n3) Pad the resulting upsampled interior points with zeros since there is no refinement\nin the error at the known boundaries\n4) Add the upsampled guess at the error to the original guess at u\n\nThe downsampling operator was defined explicitly in Briggs, though in an index form\nrather than matrix form. I implemented the operation as a matrix in order to speed up\ncomputation in MatLab. Briggs defines the downsampling operation as follows in 2D\n(v2h is the vector represented on the coarse grid, vh is the grid on the fine grid):\n\n(\n)\n,\n1,\n,\n,1\n,1\n,\n,\n,1\n,1\n,1\n,1\n-\n≤\n≤\n⎥\n⎥\n⎥\n⎥\n⎦\n⎤\n⎢\n⎢\n⎢\n⎢\n⎣\n⎡\n+\n+\n+\n+\n+\n+\n+\n+\n=\n+\n-\n+\n-\n+\n+\n-\n+\n+\n-\n-\n-\nn\nj\ni\nv\nv\nv\nv\nv\nv\nv\nv\nv\nv\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nj\ni\nh\nij\n(26)\n\nI implemented it instead as a matrix operator. In order to implement the above\noperation, the following stencil was used:\n\n))\n)(\n)\n((\n)\n)(\n)3\n)((\n((\nzeros\nM\nzeros\nM\n-\n-\n\n(27)\n\nM was the number of interior points in one edge of the fine grid from which the\nmapping was supposed to take place. As a final point, the stencil was not simply\nreplicated along the diagonal, rather to implement the correct operation it was\nimplemented in a staggered pattern (similar to 1D) as shown in the sparsity pattern\nbelow.\n\nFigure 6: Sparsity pattern for the downsampling matrix; stencil is replicated in the matrix in a\nstaggered fashion\n\nBriggs describes the indexed form of the 2-D linear interpolation operator, and it is\nsimply implemented by transposing the above matrix and scaling by 4.\n\nNumerical Experiments in 2-D\n\nFinally, with all the tools in place for 2-D, numerical experiments could be\nundertaken. A convincing example that the system was working properly would be\nsolution of a problem with a known solution. To this end, I decided to compare the\nmulti-grid solver's solution to the actual solution to Laplace's equation with the\nfollowing boundary conditions:\n\nFigure 7: Boundary conditions for the known solution to Laplace's equation\n\nThe solution to Laplace's equation in this case can be expressed as a sum of sines\nand hyperbolic sines. I will not go through the derivation here for that answer, but I\nwrote a loop in MatLab to compute a partial sum of the relevant terms to produce the\ncorrect answer so that it could be compared to the multi-grid solver's output. The two\nsolutions produced are very similar. The Gibbs phenomenon is apparent in the partial\nsum of sines. They are plotted below.\n\nFigure 8: Fourier expansion of actual solution. Right edge converges to 1; perspective of plot is\nmisleading.\n\nFigure 9: My solver's output after 1000 steps on the fine grid\n\nThe next obvious experiment is to see how quickly the relaxation error decays to\nzero. The decay of pure modes was examined for the 1-D case. Now however, the solver\nwas considerably more powerful, so examining more sophisticated problems would be\ninteresting. In general, we don't know the final solution; we only know the residual after\na step. So, from now on, instead of discussing error, we will examine how the norm of\nthe residual decays.\n\nAn interesting case to examine would be a unit spatial impulse. The Fourier\nexpansion of an impulse has equal weights on all frequencies, so examining how an\ninitial guess of an \"impulse\" decays to zero everywhere in homogenous conditions would\nbe insightful. The following plots show a unit impulse located at 67,67 on a 133 by 133\ngrid after various numbers of steps.\nFigure 10: Decay of a unit impulse. Notice that after 30 steps, the solution is much \"smoother\" than\nafter 10. This is because the higher-frequency modes have been filtered out by the fine grid.\n\nFigure 11: Stalling of residual decay on the fine grid\n\nWe can see that the residual decays very quickly initially, but the decay rate then\nstalls. This is because the error that is left is lower-frequency error which does not decay\nquickly on the fine grid. This is seen in the figure, as after twenty and thirty iterations,\nthe solution looks very smooth.\n\nThe question to ask now is, how much better could the answer be after a number\nof steps if we employ a multi-grid approach? In the following experiment, three grid\ncoarseness levels were available. Grid 1 was the fine grid. Grid 2 was the \"medium\"\ngrid, and was twice as coarse as Grid 1. Grid 3 was the \"coarse\" grid, and was twice as\ncoarse as grid 2.\n\nAn experiment similar to the one in Figure 10 was attempted with the unit\nimpulse. Three relaxations were performed, starting with homogenous conditions and a\nunit impulse initial condition.\n\nTrial 1: Relax with 2500 steps on Grid 1\n\nTrial 2:\na) Relax with 534 steps on Grid 1\nb) Move to Grid 2 and relax for 534 steps\nc) Move back to Grid 1, incorporate the refinement from (b), and relax\nfor 1432 steps for a total of 2500 steps\n\nTrial 3:\n\na) Relax with 300 steps on Grid 1\n\nb) Relax with 300 steps on Grid 2\n\nc) Relax with 300 steps on Grid 3\n\nd) Move back to Grid 2, incorporate refinement from (c) and relax for 800\n\nsteps\n\ne) Move back to Grid 1, incorporate refinement from (d) and relax for 800\n\nsteps for a total of 2500 steps\n\nThis scheme was chosen because it gave all methods the total number of steps.\nAdditionally, for trial 2 and trial 3, the ratio of forward relaxations (i.e. relaxation after\nmoving from fine to coarse) to backwards relaxation was constant at 3/8. The detail after\n2500 steps is shown below for all three cases.\n\nFigure 12: The three-grid scheme outperforms both the single and dual grid schemes.\n\nIt is clearly visible that given the same number of steps, the three-grid scheme\noutperforms the single grid scheme and the dual grid scheme. However, it is not a given\nthat this result will always be the case. If the error, for example, was known to be almost\npurely high-frequency, the advantage of the grid transfers might be outweighed by the\ncomputation power necessary to keep making the transfers and interpolations.\nThe case shown above for the unit impulse is a case where the frequencies are\nequally weighted in the initial conditions. As a second trial, I examined how the\nresiduals decayed for an initial condition with more low-frequency content. This case\nwas again homogenous with boundary conditions of zero, but the initial \"guess\" was 1\neverywhere except at the boundaries. I repeated the experiment with these initial\nconditions, and the results are shown below.\n\nFigure 13: Decay of residuals for the three schemes. The only fair comparison across methods is\nafter all three trials have made it back to the fine grid (i.e. after the last spike in residual on the green\nline which comes from the interpolation error). The three-grid method is the most accurate after\n2500 steps.\n\nFigure 14: Detail of the final residual values for the three methods. The three-grid method clearly\nwins out over the others. This is a more drastic out-performance than before since the initial\ncondition contained more low frequency error, which was better managed on the coarser grids.\nOnce again, the three-grid scheme wins. It is important to note that in the first\nfigure, the \"norm\" during the steps while the problem's residual resides in the coarser\ngrid is not comparable to the norm of vectors in other grids, as the norm is vector-size\ndependent. Therefore, the only truly fair points on the graph to compare the methods are\nwhen all methods are on the same grids, namely the very beginning and very end (shown\nin detail in Figure 14).\n\nThere are two ways to interpret the results. We can get a better answer with the\nsame number of steps by using the three-grid cycle. Alternatively, we could stop earlier\nwith the three-grid cycle and settle for the value that the other methods would have\nproduced with more steps. The tradeoff is completely problem dependent.\nI was suspicious as to how much difference the above residuals made in the\nappearance of the answer, especially given the much higher initial values of the residuals.\nThe difference after trials 1, 2 and 3 is stark and is shown below. Remember, with an\ninfinite number of steps, all three methods would converge to a value of zero everywhere.\n\nThe results are obviously different. Trial 3 yielded an answer visually very close\nto the correct answer of 0. It is clear that going beyond simply one coarsening operation\nyielded great benefits. The natural next step would be to try a fourth, coarser grid, and\ncontinue coarsening. One could coarsen the grid all the way to a single point. Also,\n\ntrying a multitude of different step distribution schemes in order to maximize efficiency\nof steps at each grid could be tried too.\nOne could easily write a book about these concerns, but going far down either of\nthese paths would step outside the scope of this introduction to multi-grid and its benefits.\nInstead, it would be more appropriate to confirm this limited-multi-grid system on other\nproblems.\nAs stated earlier, a key goal of my 2D implementation was the ability to impose\nboundary conditions within the grid. I designed my Jacobi relaxer, as described earlier,\nto support internal boundaries as well. I implemented the system so that I could simply\nuse Windows Paint (r) to draw whatever regions I wanted to specify as at a particular\n\"voltage.\" As an appropriate example, I decided to determine the potential distribution\nresulting from having an electrode in the shape of the letters \"MIT\" in the grid, with 0\nvolt boundary conditions on the edge of the grid. The bitmap used to create the boundary\nconditions is shown below. The black letters are defined to be 1 volt, the white area zero\nvolts.\n\nShown below is a plot of the relaxation solution (still staying all the time on the\nfine grid) of the solution to the problem.\n\nFigure 15: \"Electrodes\" in shape \"MIT\" relaxed on fine grid\n\nFinally, just to prove that the ability to add charge into the simulation was added,\nI added a line of charge the under the \"electrodes\" used to write \"MIT\" to underline the\nword.\n\nFigure 16: MIT electrodes with a line of charge underlining them\n\nPlacement of arbitrary charge within the medium with arbitrary boundary\nconditions was supported as well. The figure below shows the gains made with a 930\nstep double-grid method vs. a single grid method; the point is to show that the charge\nplacement was supported across multiple grids.\n\nFigure 17: Multi-grid support included for arbitrary charge distributions as well\n\nAs for multi-grid support of internal boundary conditions (i.e. if we wanted to\nrelax the MIT electrode problem with multi-grid), I did not quite get around to that. I\nthought I had it done, but I discovered too late that I had forgotten a subtle aspect. When\nrelaxing on Ae=r, I forgot to pin internal values of e at the boundaries to zero, as by\ndefinition there would never be error at one of the internal boundaries. Without doing\nthis, the compensated error approximation from the coarse grid will attempt to force the\nvalue at the boundary to deviate from the correct internal boundary condition.\nThis could be fixed by changing the matrix A by making the rows corresponding\nto these points zero, except for a 1 on the diagonal. Additionally, r at that point would\nneed to be 0, but I had already thought of and taken care of that and had implemented that\naspect. As simple as the fix sounds, I had an elaborate setup in the algorithm for the\ncurrent system, and making the change would have meant tearing the whole system down\nand building it back up, which was unrealistic as late as I found the problem. However, I\ndid determine the source of the problem and its likely fix.\n\nConclusion\n\nThe most convincing figure of the paper is replicated below.\n\nThis figure truly sums up the power of multi-grid. In the same number of steps,\nthe approach with the largest utilization of coarsening got closest to the right answer.\nOne can be more quantitative about the true efficiency of the method: what is the\ncomputational tradeoff between doing a few more iterations on the fine grid and moving\nto a coarse grid? Do the benefits of moving outweigh the computational costs of\ndownsampling and interpolation? What is the best way to design a multi-grid cycle\nscheme? How long should one spend on a coarse grid versus a fine one? These are all\nexcellent questions of multi-grid, and there is no definitive right answer.\n\nAs for the tradeoff between interpolation and downsampling versus spending time\non a fine grid, making an absolutely definitive answer is difficult. However, multiplying\nby the Jacobi matrix for an iteration and multiplying by an upsampling or downsampling\nmatrix consist of matrix multiplications of relatively the same size and density, making\nthe intergrid transfers relatively cheap and insignificant compared to large numbers of\n\nsteps of relaxation computation. It is more likely that the tradeoffs will come from\ndetermining the proper amount of time to spend at each grid. A possible way to do this\nwould be to look at the FFT of the residual, try to predict the spectral content of the error,\nand adaptively decide which grid to move to based on that result. Other ways would be\nto look for patterns in the particular class of data being examined. Such design issues\nwould make excellent projects in and of themselves.\n\nWhat is definite, however, is that multi-grid can yield astonishing results in the\nright circumstances and can give excellent answers in a fraction of the time that a single-\ngrid relaxation would need. If an exact solution is not necessary, and the grid is huge,\nmulti-grid is an excellent way to go.\n\nReferences\n\nBriggs, William, et al. A Multigrid Tutorial, Second Edition. (c) 2000 by Society for\nIndustrial and Applied Mathematics.\n\nJaydeep Bardhan and David Willis, two great advisors from Prof. Jacob White's group.\n\nProf. Gilbert Strang, for his draft of his new textbook."
    },
    {
      "category": "Resource",
      "title": "overview.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/4cd6d25812214df9a2883e6a96ab3229_overview.pdf",
      "content": "Overview of Multi-grid Project\n\nThis project example on this OCW site is a slight refinement of my final\nproject in Prof. Strang's 18.086 class. The project implements a 2D multi-grid solver\nfor Laplace's and Poisson's equation. Multi-grid support is included for Dirichlet\nboundary conditions, as well as for sources within the grid. Internally pinned\nDirichlet boundaries, while possible to specify, do not function properly if multi-grid\nschemes are used in my implementation. I wanted to add this feature, but ran out of\ntime at the end of the project.\nI have attempted to give an explanation of how to use the solver by including\nthe file \"ProjectOCW.m,\" a simple numerical experiment conducted with the solver.\nOnce all project files are placed in the same directory, one should simply be able to\nopen the mentioned file and run it in MatLab. The file will plot the error and the\nguess at the solution at various steps in a V-cycle approach to multi-grid.\nAdditionally, I have extensively commented this file to give the user enough\nknowledge to modify the file to conduct experiments of one's own. The solver tracks\nthe norm of the residual as the solver progresses, which is a key statistic to keep an\neye on. Upon altering the MatLab script, the user can decide parameters like how\nmany steps to stay on a grid, what Jacobi damping factor to use, or a number of other\nparameters.\nThe user can also import images to use as a two-dimensional source term in\nthe problem, or import other images to define the boundary conditions or initial guess.\nImages can be imported into MatLab. In order to be incorporated into the solver,\nhowever, the images must become appropriately sized matrices with values\ncorresponding to some interpretation in Laplace's equation. The simplest example is\nimporting a monochrome bitmap for source terms, where black pixels could map to\nzero entries in the source terms, and white to some determined value.\nI attempted to thoroughly explain \"ProjectOCW.m,\" but left explanation of the\nhelper functions more vague. This project was not intended originally to teach multi-\ngrid, but rather act as an example of implementation of a multi-grid solver.\nFurthermore, there are definitely more efficient and elegant ways to implement a\nnumber of the operations of the solver in MatLab. For these reasons, I found a\nthorough explanation of the inner workings of the solver to be outside the scope of\nthis posting, which is meant to show multi-grid in action, rather than thoroughly\nexplain every detail of design decision in implementing the system.\nI hope you enjoy running the pre-designed experiment and will alter the\nexample file to run experiments of your own. The system here is capable of a good\nnumber of experiments. I learned a great deal about the method by simply changing\nparameters easily alterable in this example.\nFinally, I'd like to thank Prof. Strang for all of his help and advice, Dave\nWillis and Jaydeep Bardhan for their help, and would like to acknowledge the\nimmense help of A Multigrid Tutorial by Briggs et al., a must-read for anyone\ninterested in learning multi-grid.\n\nJoseph Kovac, S.B. MIT '05\n\n9/12/05\n\nGraduate Student, MIT EECS"
    },
    {
      "category": "Resource",
      "title": "project1domnguez.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/4bfb14a306e4cc0a7f89e0d719754fdb_project1domnguez.pdf",
      "content": "MASSACHUSETTS INSTITUTE OF\nTECHNOLOGY\nDEPARTMENT OF MECHANICAL ENGINEERING\nCOURSE: 18.086\nMathematical Methods for Engineers II\nProject 1\nExperimental Analysis of the Two Dimensional Laplacian\nMatrix (K2D): Applications and Reordering Algorithms\n\nProject 1\nCourse: 18.086\nExperimental Analysis of the Two Dimensional Laplacian\nMatrix (K2D): Applications and Reordering Algorithms\nSection 1: Introduction\nThis project focuses on the analysis and experimentation of the two dimensional (2D)\nLaplacian matrix (K2D). Special emphasis is put on solving large systems, particularly\nfor problems related to digital image processing and optical imaging. We discuss a\ntypical application of the K2D matrix in finding the edges of an image. In the forward\n2 ⋅\nproblem, F = K D U , the input is the image we want to process by a Laplacian based\nedge detection algorithm. As will be discussed in the next section, the input image is\npreprocessed by a raster scanning algorithm to form the vector U. The output vector F is\nalso processed by the inverse raster scanning algorithm to form the 2D Laplacian of the\nimage. This new matrix is used in the edge detection algorithm to find all the edges.\n-1 ⋅\n=\nIn the inverse problem, U\nK D\nF\n\n, the Laplacian of the image is the input and we\ntry to recover the original object. This is the case for certain optical systems as discussed\nin Section 2. In order to solve this problem efficiently, we discuss the properties of K2D\nand experiment several ways to speed up the elimination and also reduce the storage\nduring computations.\nIn Section 3, we analyze three popular reordering algorithms: minimum degree, red-black\nand graph separator orderings. We developed several experiments to explore their\nbehavior under conditions such as variable matrix sizes. In addition, experiments are\ndeveloped to estimate their required computational time and efficiency under LU and\nCholesky decompositions.\n\nSection 2: Application of the Laplacian Matrix in Digital\nImage Processing\nIn this section, the implementation of the two dimensional (2D) Laplacian Matrix (K2D)\non a digital image processing problem is discussed. In this problem, the second derivative\nof the image (i.e. the Laplacian) is used to find discontinuities by implementing an edge\ndetection algorithm. Edge detection algorithms are widely used in applications such as\nobject detection and recognition, shape measurement and profilometry, image analysis,\ndigital holographic imaging and robotic vision.\nEdges in images appear as regions with strong intensity variations. In the case of images\nobtained with a conventional camera, edges typically represent the contour and/or\nmorphology of the object(s) contained in the field of view (FOV) of the imaging system.\nFrom an optics perspective, edges represent the high spatial frequency information of the\nscattered wave coming from an illuminated object. If an object contains a sharp\ndiscontinuity, the information for this region will be mapped in a region of high\nfrequency in the Fourier plane. Edge detecting an image is very important as it\nsignificantly reduces the amount of data and filters out useless information, while\npreserving structural properties of the image [1].\nThere are two main categories of edge detection algorithms: gradient and Laplacian based\nalgorithms. In a typical gradient based algorithm, the first derivative of the image is\ncomputed in both dimensions (row and column wise). In this new image, the edges\nbecome apparent and the maximum and minimum gradients of the image are compared to\na threshold. If there is a maximum with a value larger than the threshold, the spatial\ncoordinate of that maximum is considered an edge. An example of gradient based\nalgorithms is the Sobel edge detection algorithm. Figure 2.1 shows a 1D discontinuity\ncentered at x=0. Figure 2.2 is a plot of the first derivative of the intensity function of\nFigure 2.1. It is important to note that the maximum is also centered at x=0. If we set the\nthreshold of the edge detection algorithm equal to 0.2, an edge would be detected at x=0.\nFigure 2.1: Example of 1D edge in an image\n\nFigure 2.2: First derivative of the 1D edge of Figure 2.1.\nFor a Laplacian based algorithm, the second derivative of the original image is computed.\nThe zero crossings of the resulted matrix are used to find the edges. Figure 2.3 shows the\nsecond derivative computed for the 1D example discussed above. The Matlab code used\nto generate Figures 2.1-2.3 is included in Appendix A.1.\nFigure 2.3: Second derivative of the 1D edge of Figure 2.1.\nIn the 2D case, the Laplacian is computed in both dimensions (row and column wise).\nThis can be achieved by using the matrix K2D in the forward problem:\n⋅\nK D U = F ,\nwhere U is a vector formed after raster scanning the original image. The measurement\nvector F is the raster scanned version of the 2D Laplacian of the image. The matrix K2D\nis an N × N matrix formed by the addition of the Kronecker tensor products\n2 =\n(\n,\n( ,\nK D kron K I ) + kron I K ).\nHere, I is the N\nN identity matrix and K is also N\nN and tridiagonal of the form:\n×\n×\n\n-1\n\"0\nK =\n-1\n-1 \"0\n\n.\n#\n%\n%\n%\n\n\" -1\nTo exemplify the 2D forward problem, consider the 500 ×500 pixels image of Figure 2.4.\nTo form the vector U, we need to implement a raster scanning algorithm. In the raster\nscanning algorithm, the N\nN matrix (i.e. the image) is decomposed into a vector of size\n×\nN 2 . This is achieved by extracting each column of the original matrix and placing it at\nthe bottom of the vector. For example, a 3 3\npixels image would produce\n×\nu1\n\nu4\nu7\n\nu3\nu1\nu2\nraster\nu2\nl\n\nU = u4\nu5\nu6\n→ scanning → U = u .\n\nu7\n\nu8\nu9\nu8\n\nu3\nu6\nu9\n\nFigure 2.4: Photograph of Prof. Gilbert Strang (500 × 500 pixels ).\nThe forward problem is solved by multiplying the K2D matrix, which in this case is\n250000 × 250000 , with the vector U. Since both the vectors and the matrix are large, it is\nnecessary to implement the 'sparse' function available in Matlab. The 'sparse' function\nreduces the required storage by only considering the coordinates of the nonzero elements\nin the sparse matrix. The resultant matrix is obtained by the implementation of the\ninverse-raster scanning algorithm. Figure 2.5 shows the 2D Laplacian obtained after\n\nsolving the forward problem for the image of Figure 2.4. The code used to generate\nFigure 2.5 is included in Appendix A.2.\nFigure 2.5: 2D Laplacian of the image of Figure 2.4.\nFor a Laplacian based algorithm, the information contained in Figure 2.5 is used to find\nthe edges of the image. The result of this algorithm is a binary image such as the one\nshown in Figure 2.6. For more information about edge detection algorithms refer to [2].\nFigure 2.6: Result of applying the Laplacian edge detection algorithm on Figure 2.4.\nNow we discuss the inverse problem. In the inverse problem, we are given the Laplacian\nof the image and our task is to find the original image\nU\nK2D 1\n.\n=\n- F\n\nThis is a common problem in communications where we want to transmit as little\ninformation as possible. In an extreme case, we would like to transmit just the edges of\nthe image (a sparse matrix) and from this recover the original image. Another example is\nan optical imaging system that has a high-pass Fourier filter in the Fourier plane, such as\nthe one shown in Figure 2.7. In the optical configuration of Figure 2.7, an object is\nilluminated by a mutually-coherent monochromatic wave and the forward scatter light\nenters a 4f system. The 4f system is a combination of two positive lenses that are\n= 1+\nseparated by a total distance of d\nf\nf\n\n2 , where f1and f 2 are the focal lengths of\nboth lenses. The first lens functions like an optical Fourier Transform operator. A high-\npass Fourier filter is positioned in between both lenses (at the focal plane). This high-pass\nfilter is designed to block the low spatial frequency content of the optical field and only\nlet the high frequency information pass. The filtered spectrum is inverse Fourier\ntransformed by the second lens and forms an image on a CCD detector. If the cut-off\nfrequency is set relatively high (i.e. cutting a great part of the low frequency\ninformation), the edges of the object start being accentuated at the image plane.\nFigure 2.7: Optical realization of edge extraction\nIn the inverse problem, the input is an image similar to the one shown in Figure 2.5 and\nthe output is another image like the one shown in Figure 2.4. The algorithm to compute\nthis includes an elimination step (for the code included in Appendix A.3, the elimination\nis done using Matlab's '\\' function) and a reordering. Different types of reordering such\nas minimum degree, red-black and nested dissection orderings will be analyzed and\ncompared in Section 3. Choosing a suitable reordering algorithm is very important\nespecially for large systems (for images with a high space-bandwidth product), as it\nreduces the number of fill-ins during elimination.\n2.1 The K2D Matrix\nThe K2D matrix can be though of as the discrete Laplacian operator applied to vector U.\nAs mentioned before, K2D can be formed from the addition of the Kronecker products of\nK and I. K2D has 4's in the main diagonal and -1's for the off-diagonal terms. For\nexample, the 9 9 K2D matrix is given by\n×\n\n-1\n-1\n-1\n-1\n-1\n\n-1\n-1\n\n-1\n-1\n-1\n\nK D = 0\n-1\n-1\n-1\n-1\n\n-1\n-1\n-1\n-1\n-1\n\n-1\n-1\n-1\n-1\n-1\n\nThe main characteristics of K2D is that it is symmetric and positive definite. From\n2 =\nsymmetry we know that K D K DT and that all its eigenvalues are real and its\neigenvectors are orthogonal. From positive defines we know that all the eigenvalues of\nK2D are positive and larger than zero; therefore, its determinant is larger than zero. The\ndeterminant for the 9 9 matrix shown above is det\n×\nK D = 100352 . The Gershgorin\ncircle theorem applied to the matrix K2D indicates that all its eigenvalues should be\ncontained inside a circle centered at 4 and with a radius of 4. Figure 2.8 shows the\neigenvalues of the 9 9 K2D matrix in the complex plane. As predicted by the\n×\nGershgorin circle theorem, all the eigenvalues are between 0 and 8. The code to generate\nthe plot of Figure 2.8 is included in Appendix A.4. Figure 2.9 shows the eigenvalues for a\nmuch larger K2D matrix ( 4900 × 4900 ). These 4900 eigenvalues also remain between 0\nand 8.\nFigure 2.8: Eigenvalues of the 9 9K2D matrix\n×\n\nFigure 2.9: Eigenvalues of a 4900 ×4900 K2D matrix\nAnother interesting measure that describes the behavior of K2D is the condition number\nor in the context of information theory, the Rayleigh metric. The condition number is\ndefined as\nλmax\nc =\n,\nλmin\nand gives information about how close the matrix is to ill-posedness. If c = 1, the matrix\nis said to be well-posed, but if the condition number is large ( c →inf), the matrix gets\nclose to becoming singular. In other words, we cannot completely trust the results\nobtained from an ill-posed matrix. Figure 2.10 shows the condition number for K2D of\ndifferent sizes. The code used to generate this figure is included in Appendix A.5.\nFigure 2.10: Condition number of K2D as a function of size\n\nSection 3: Reordering Algorithms\nIn the previous section we described some of the properties of the K2D matrix along with\nrelated problems (forward and inverse problems) that are important for digital image\nprocessing. The main difficulty in solving such systems arises when the system is large.\nA large system would be required if we intend to process an image with a large number\nof pixels. If we do the elimination directly on K2D, several fill-ins would appear and the\ncomputational costs (memory and computational time) would increase. For this reason, in\nthis section we discuss different reordering algorithms that help reduce the number of fill-\nins during elimination.\n3.1: Minimum Degree Algorithm\nn\n(\n)\nThe minimum degree algorithms reorder K2D to form a new K D = P K\n2D PT by\neliminating the pivots that have the minimum degree. If we draw the graph corresponding\nto K2D, the minimum degree algorithm eliminates the edges (off-diagonal terms) from\nthe nodes that have the minimum degree (i.e. the least number of edges connected to it).\nTo describe the steps involved in this algorithm, we carry the reordering algorithm on the\n9 9K2D matrix described above. Figure 3.1 shows its corresponding graph. The steps of\n×\nthe algorithm are as follows:\nFigure 3.1: Graph corresponding to a 9 × 9 K2D matrix\n1. We start by choosing the first element in the matrix as the pivot (in reality, you\ncan choose any of the corner nodes of the graph, as all of them are degree 2).\nUsing this pivot, carry out elimination as usual. This is equivalent to eliminating\nthe edges connected to the pivot node. Two new fill-ins are generated in this step,\nand this is shown in the graph as a new edge as shown in Figure 3.2.\n2. Update the permutation vector: P = [1]. The permutation vector keeps track of the\norder of the nodes we choose for elimination.\n3. Go back to step one and choose one of the three remaining nodes that are degree\n2. For example, if the next node we choose is node 3, after elimination, the\npermutation vector becomes: P = [1 3].\nFigure 3.3 shows the graph sequence for the remaining steps of the minimum degree\nalgorithm. The final permutation vector is P = [1\n4]. Figure 3.4\n\nshows a comparison between the structures of the original and the reordered K2D\nmatrices. Figure 3.5 shows the lower triangular matrices produced after LU decomposing\nthe original and the reordered K2D matrices. As can be seen from this figure, reordering\nthe matrix produces fewer numbers of nonzeros during the LU decomposition. The\nMatlab code used to generate Figures 3.1-3.5 was obtained at [3].\nFigure 3.2: Graph of the 9 ×9 K2D matrix after step one of the minimum degree\nalgorithm\nFigure 3.3: Sequence of graphs for the remaining steps in the minimum degree algorithm\n\nFigure 3.4: Structural comparison between the original and the reordered K2D matrices\nFigure 3.5: Lower triangular matrices produced after LU decomposition of the original\n(#nonzeros = 29) and the reordered (#nonzeros = 26) K2D matrices\nSeveral minimum degree reordering algorithms are available. Each of these algorithms\nproduces a different permutation vector. We now compare five of such algorithms:\n1. Matlab's \"symamd\" algorithm: Symmetric approximate minimum degree\npermutation.\n2. Matlab's \"symmmd\" algorithm: Symmetric minimum degree permutation.\n3. Matlab's \"colamd\" algorithm: Column approximate minimum degree\npermutation.\n4. Matlab's \"colmmd\" algorithm: Column minimum degree permutation.\n5. Function \"realmmd\" algorithm [3]: Real minimum degree algorithm for\nsymmetric matrices.\nFigure 3.6 shows the structure of a 81× 81K2D matrix. Figure 3.7 shows a structural\ncomparison of the reordered matrices using the five minimum degree algorithms\nmentioned above. Figure 3.8 shows their respective lower triangular matrix produced\nafter LU decomposing the reordered K2D matrix. From this figure, it is evident that the\nreal minimum degree algorithm produces the least number of nonzero elements (469) in\n\nthe decomposed lower triangular matrix. The code used to generate Figures 3.6-3.8 is\nincluded in Appendix B.1.\nFigure 3.6: Structure of a 81 × 81K2D matrix\nFigure 3.7: Structural comparison of reordered matrices using the five algorithms\ndescribed above\n\nFigure 3.8: Structural comparison of the lower triangular matrices decomposed from the\nreordered matrices using the five algorithms.\nNow we compare how the five minimum degree algorithms behave as a function of\nmatrix size. In particular, we are interested to know the number of nonzeros in the lower\ntriangular matrix, L, after LU decomposing the reordered K2D matrices for different\nmatrix sizes. Figure 3.9 shows this comparison for matrices with sizes ranging from 4 4\n×\nto 3844 ×3844 entries. Figure 3.10 zooms into Figure 3.9 to show an interesting behavior\nof the 'realmmd' and the 'symamd' algorithms with relative small sized matrices. For\nsmall matrices, the 'realmmd' algorithm produces the least number of nonzeros in L;\nhowever, if the matrix size increases (especially for large matrices), the 'symamd'\nalgorithm is the winner. The code used to generate Figure 3.9-3.10 is included in\nAppendix B.2.\nIn our next experiment, we compare four of these minimum degree methods for different\nmatrix sizes after performing a Cholesky decomposition on the reordered matrix. The\n×\nsize of the matrices ranges from 4 4 to 120409 ×120409 in steps of 25. The results of\nthis experiment are plotted in Figure 3.11. As in the previous experiment, we can see that\nthe methods 'symamd' and 'symmmd' produce the least number of nonzeros entries in\nthe factorized matrix. However, the question now is: are these methods (symamd and\nsymmmd) fast to compute?\n\nFigure 3.9: Comparison of the nonzeros generated by the five algorithms as a function of\nmatrix size\nFigure 3.10: Zoomed in from Figure 3.9.\nFigure 3.11: Nonzeros generated by the Cholesky factorized matrix, reordered with four\ndifferent minimum degree algorithms\n\nTo answer the question stated above, we now turn to compare the required computational\ntime for reordering K2D by these four minimum degree algorithms. The results are\nshown in Figure 3.12. From this figure we can see that although it is slightly faster to\nreorder a large matrix with 'colamd' rather than using 'symamd', the number of nonzeros\ngenerated in the decomposed matrix is significantly less for a matrix reordered by the\n'sysmamd' algorithm. In conclusion, 'sysmamd' is our big winner.\nFigure 3.12: Computational time for reordering K2D of different sizes\nThe reason we didn't include the 'realmmd' algorithm in the comparison of Figure 3.13,\nis because this algorithm requires a long computational time (even for relatively small\nmatrices) as shown in Figure 3.13.\nFigure 3.13: Computational time required by 'realmmd' as a function of matrix size\nIf we use Matlab's functions 'chol' and 'lu' to produce the Cholesky and LU\ndecompositions respectively, it is fair to ask: what is the computational time required by\nboth functions? Is the computational time dependent on the input matrix (i.e. if the matrix\nwas generated by 'symamd', 'symmd', 'colamd' or 'colmmd')? How does the\ncomputational time behave as a function of the matrix size? To answer all of these\nquestions, we generated the plots shown in Figures 3.14 and 3.15. The Matlab code used\nto generate Figures 3.11-3.15 is included in Appendix B.3.\n\nFigure 3.14: Computational time required by Matlab's Cholesky decomposition function\n'chol'\nFigure 3.15: Computational time required by Matlab's LU decomposition function 'lu'\n3.2: Red-Black Ordering Algorithm\nThe red-black ordering algorithm is an alternative reordering technique in which it is\ndivided in a way similar to a checkerboard (red and black or odd and even squares). The\npermutation vector is simply generated by first selecting for elimination the odd nodes\nand then all the even nodes on the grid. The red-black permutation is given by\nP K )\nBT\n(\n2D PT =\n4Ired\n4I\nB\nred\n,\nwhere B is a matrix composed by -1s from the off-diagonal or black elements of K2D.\n\nFor example, Figure 3.16 shows a structural comparison between the original 9 9K2D\n×\nmatrix and the its equivalent reordered using the red-black ordering algorithm. The\npermutation vector in this example is: P = [1\n8] . The reordered\nK2D matrix is given by\n-1 -1\n-1\n-1\n\n-1 -1 -1 -1\n\n-1\n-1\n(\n)\nP K D PT = 0\n-1 -1 .\n\n-1 -1 -1\n-1\n-1 -1\n\n-1 -1\n-1\n-1 -1 -1\n\nOriginal 9x9 K2D\nK2D reordered by red-black algorithm\nnz = 33\nnz = 33\nFigure 3.16: Structural comparison between the original K2D (to the left) and the\nreordered matrix using the red-black algorithm (to the right)\nFigure 3.17 shows the sequence of eliminations that occur on the graph representation of\nthe matrix described above. From this figure, we can see that the algorithm starts by\neliminating all the edges of the odd/red nodes (remember that the graph is numbered row\nby row starting from the bottom left corner). After finishing with the odd nodes, the\nalgorithm continues to eliminate the even nodes until all the edges have been eliminated.\nA comparison between the lower triangular matrices produced after elimination of the\noriginal and reorder matrices is shown in Figure 3.18. The reordered matrix produced less\nnumber of nonzero entries in L (27 nonzeros instead of 29). The Matlab code used to\ngenerate Figures 3.16-3.18 is included in Appendices B.4 and B.5. This code generates a\nmovie of the sequence followed during elimination in the graph.\n\nFigure 3.17: Sequence of eliminations for the red-black ordering algorithm\nFigure 3.18: Comparison of lower triangular matrices produced after LU decomposing\nthe original and the reordered K2D matrices\n\nWe now try to study the behavior of the red-black ordering algorithm under matrices of\ndifferent sizes. Figure 3.19 shows a comparison between the original and the reordered\nmatrices for the number of nonzeros generated in the lower triangular matrix during LU\ndecomposition. Especially for large matrices, the red-black algorithm shows an important\nimprovement due to the significant reduction of the number of nonzeros in L. From this\nfigure we can see that the red-black algorithm performs worse than the minimum degree\nalgorithms discussed earlier. However, the main advantage of the red-black algorithm\nresides in its simplicity and easy implementation.\nFigure 3.19: Number of nonzeros generated in L after LU decomposing the original and\nthe reordered K2D matrices\nFigure 3.20 shows the computational time required by the red-black algorithm as a\nfunction of matrix size.\nFigure 3.20: Computational time required by the red-black ordering algorithm as a\nfunction of matrix size\n\nFinally, we want to compare the computational time required for the LU decomposition\nof the original and the reordered matrices as a function of matrix size. This is shown in\nFigure 3.21. As expected, the reordered matrix requires less time to produce the factors L\nand U. The Matlab code used to generate Figures 3.19-3.21 is included in Appendix B.6.\nFigure 3.21: Computational time required by the LU decomposition of the original and\nthe reordered matrices\n3.3: Graph Separators Algorithm and Nested Dissection\nIn this subsection we discuss an alternative reordering algorithm called graph separators.\nAgain, it is easier to understand the logic behind this algorithm by looking directly at the\ngraph. The key idea is that a separator, S, is introduced to divide the graph in two parts P\nand Q (subsequent divisions of the graph would lead to nested dissection algorithms).\nThe separator S is formed by a collection of nodes and it has typically a smaller or equal\nsize than P and Q. To illustrate this algorithm, we go back to our 9 9matrix. The graph\n×\nof this matrix is shown in Figure 3.22. As shown in this figure, the graph is divided in\ntwo parts by S. In this case, P, Q and S have the same size. The graph was initially\nnumbered row wise as before. In the graph separators algorithm, the graph is reordered\nstarting from all nodes in P, then all nodes in Q and finally all nodes in S as shown in the\nsame\nfigure.\nFor\nthis\nexample,\nthe\npermutation\nvector\nbecomes:\nP = [1\n8] . The elimination sequence is performed following\nthis new order.\nFigure 3.23 shows the structure of the reordered matrix and its corresponding lower\ntriangular factor. The graph separator permutation is given by\nKP\nKPS\nP K )\n\n(\n2D PT = 0\nKQ\nKQS\n\n.\n\nK\n\nSP\nKSQ\nKS\n\nFigure 3.22: Reordering occurred in the graph separators algorithm\nFigure 3.23: Structure of the reordered and the lower triangular factor of the 9 × 9K2D\nmatrices\nAs we mentioned before, the introduction of additional separators will produce a nested\ndissection algorithm. For the example described above, we can introduce a maximum of\ntwo additional separators as shown in Figure 3.24. With this ordering, the permutation\nvector becomes: P = [1\n8] . Figure 3.25 shows the new structure\nof the reordered matrix as well as the structure for L. The total number of nonzeros got\nreduced from 28 to 26 (remember that with out any reordering L has 29 nonzeros).\n\nFigure 3.24: Nested dissection algorithm\nFigure 3.25: Structure of the reordered matrix and its lower triangular factor\nWe now turn to experiment with more sophisticated nested dissection algorithms\navailable in Matlab Mesh Partitioning and Graph Separator Toolbox [4]. In\nparticular, we will compare two algorithms:\n1. \"specnd\": Spectral nested dissection ordering. This algorithm makes use of\ncertain properties of the Laplacian matrix to compute the proper separators. For\nmore information regarding spectral nested dissection algorithms refer to [7].\n2. \"gsnd\": Geometric spectral nested dissection ordering.\n\nFigure 3.26 compares the number of nonzero entries in L after LU decomposing the\nmatrices reordered by both algorithms and the original K2D. From this figure we can see\nthat the results produced by the spectral-based nested dissection algorithms get closer to\nthose obtained by the minimum degree algorithms; however, some of the minimum\ndegree algorithms such as the \"symamd\" algorithm perform much better.\nFigure 3.26: Number of nonzeros generated in L after LU decomposing the original and\nthe reordered K2D matrices (by \"gsnd\" and \"specnd\")\nA comparison of the computational time required by both algorithms is plotted in Figure\n3.27. Again, we confirm that Matlab's minimum degree algorithms perform much faster\non large matrices than the nested dissection algorithms.\nFigure 3.27: Computational time required by \"gsnd\" and \"specnd\" as a function of matrix\nsize\n\nFinally, we compare the computational time required by the LU factorization algorithm\nwhen the input matrix was reordered by both algorithms. The results are plotted in Figure\n3.28. The Matlab codes used to generate Figures 3.26-3.28 is included in Appendix B.6.\nFigure 3.28: Computational time required by the LU decomposition of the original and\nthe reordered matrices\nSection 4: References\n[1]: Internet access: http://www.pages.drexel.edu/~weg22/edge.html. Date accessed:\n04/02/2006.\n[2]: J. S. Lim, Two-Dimensional Signal and Image Processing, Prentice Hall, 1990.\n[3]: Internet access: http://www.cerfacs.fr/algor/Softs/MESHPART/, Matlab mesh\npartitioning and graph separator toolbox. Date accessed: 04/04/06.\n[4]: G. Strang, Introduction to applied mathematics, Wellesley, Cambridge Press.\n[5]: A. Pothen, H.D. Simon, L. Wang, Spectral Nested Dissection, 1992.\nSection 5: Appendices\nA.1: This Appendix contains the Matlab code used to generate Figures 2.1-2.3.\n%Computation of the first and second derivatives of an intensity function\n%I(x) to be used to detect discontinuities using and edge detection\n%algorithm\n%General Parameters\nm = 0:0.01:1;\nK = 5;\nx = -100:100;\n%1D discontinuity\nF1 = exp(-K*m);\nF2 = -F1+2;\nF = fliplr(F1);\n\nF(length(F2)+1:length(F2)*2-1)=F2(2:end);\n%Plot 1\nfigure;\nplot(x,F,zeros(length(x),1)',[0:0.01:2],'--r',x,zeros(length(x),1)','--r')\ntitle('Intensity variation at the discontinuity')\nxlabel('x')\nylabel('Intensity')\n%Gradient\ndelF = gradient(F);\n%Plot 2\nfigure;\nplot(x,delF,zeros(length(x),1)',[0: 2.4279e-004:0.0488],'--r',x,zeros(length(x),1)','--r')\ntitle('First derivative')\nxlabel('x')\nylabel('Intensity')\n%Laplacian\ndel2F = gradient(gradient(F));\n%Plot 2\nfigure;\nplot(x,del2F,zeros(length(x),1)',[-0.0023:2.3e-005:0.0023],'--r',x,zeros(length(x),1)','--\nr')\ntitle('Second derivative')\nxlabel('x')\nylabel('Intensity')\nA.2: Matlab code used to generate Figure 2.4.\n%Forward problem: given an image, find the Laplacian for edge detection\n%General parameters\nU = imread('g1','bmp');\nU = double(U);\nN = size(U,1);\nh = 0.5;\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\n%Raster Scanning the image\ncount = 1;\nfor m = 1:N\nUr(count:count+N-1,1) = U(:,m);\ncount = count+N;\nend\n%Forward problem\nF = K2D*Ur;\n%Inverse-raster scanning for F:\ncount = 1;\nfor m = 1:N\nFnew(:,m) = F(count:count+N-1,1);\ncount = count+N;\nend\nFnew = Fnew(2:499,2:499);\nFnew = -Fnew*h^2;\nfigure; imagesc(Fnew);\ncolorbar\ncolormap gray\naxis equal\n%Note: alternative solution is obtained using Matlab's del2 function:\n% Fnew = del2(U);\nA.3: Inverse Problem code\n%Inverse Problem: Given the Laplacian of an image, find original image\n%General parameters\n\nF = imread('g1','bmp');\nF = double(F);\nN = size(F,1);\nh = 0.5;\n%Computing the 2D Laplacian\nF = del2(F);\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\n%Raster Scanning Measurement:\ncount = 1;\nfor m = 1:N\nFr(count:count+N-1,1) = F(:,m);\ncount = count+N;\nend\n%Elimination\nU = K2D\\Fr;\n%Undo raster scanning for U:\ncount = 1;\nfor m = 1:N\nUnew(:,m) = U(count:count+N-1,1);\ncount = count+N;\nend\nUnew = -Unew/h^2;\nfigure; imagesc(Unew);\ncolorbar\ncolormap gray\naxis equal\nA.4: Plots the eigenvalues of a 9 9 K2D matrix\n×\n%Code to plot the eigenvalues of a 9X9 K2D matrix inside Gershgorin circle\n%General Parameters\nb=-1:0.0125:9;\nc = -4:0.01:4;\nN=3;\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\n%Eigenvalues of K2D\nE = eig(K2D);\n%Gershgorin Circle\nx = 0:0.01:8;\ny = sqrt(4^2-(x-4).^2);\n%Plot\nfigure;\nhold on\nplot(x,y,'--r',x,-y,'--r',b,zeros(801,1),'--r',zeros(801,1),c,'--r')\nplot(E,zeros(length(E),1),'X')\nhold off\naxis equal\nxlabel('Real')\nylabel('Imaginary')\ntitle('Eigenvalues of K2D')\nA.5: Plots the condition number as a function of size for K2D\n%Code to plot condition number as a function of size for matrix K2D\nT=2:70;\nfor m = 1:length(T)\nN = T(m);\n%1D difference matrix\nK = diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1);\nK = sparse(K);\n%Identity matrix\n\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear K; clear I; clear N;\n%Compute Eigenvalues and condition number\nE = eig(K2D);\ncondi(m) = max(E)/min(E);\nclear E;\nend\n%Plot\nplot(T.^2,condi)\nxlabel('size: N^2')\nylabel('Condition Number')\ntitle('Condition Number vs size of K2D')\nB.1: Generates plots that compare different minimum degree algorithms\n%Comparison of different minimum degree algorithms\nN = 9;\n%1D difference matrix\nK = sparse(diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1));\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear I K;\nfor m = 1:5\nif m ==1\np=symamd(K2D);\ns = 'symamd';\nend\nif m ==2\np=symmmd(K2D);\ns = 'symmmd';\nend\nif m ==3\np=colamd(K2D);\ns = 'colamd';\nend\nif m ==4\np=colmmd(K2D);\ns = 'colmmd';\nend\nif m ==5\np=realmmd(K2D);\ns = 'realmmd'\nend\nK2Dmod=K2D(p,p);\nfigure;\nspy(K2Dmod)\ntitle(['Matrix reordered using: ',s]);\n[L,U]=lu(K2Dmod);\nfigure;\nspy(L)\ntitle(['Lower triangular matrix from K2D reordered with: ',s]);\nend\nB.2: Comparison of different minimum degree algorithms as a function of matrix size\n%Comparison of different minimum degree algorithms\nT = 2:5:62;\nfor k = 1:length(T)\nN = T(k);\n%1D difference matrix\nK = sparse(diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1));\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear I K;\nfor m = 1:5\nif m ==1\np=symamd(K2D);\nend\nif m ==2\np=symmmd(K2D);\nend\nif m ==3\np=colamd(K2D);\n\nend\nif m ==4\np=colmmd(K2D);\nend\nif m ==5\np=realmmd(K2D);\nend\nK2Dmod=K2D(p,p);\n[L,U]=lu(K2Dmod);\nNoN(m,k) = nnz(L);\nclear L K2Dmod;\nend\nend\n%Plot\nfigure;\nplot(T.^2,NoN(1,:),T.^2,NoN(2,:),T.^2,NoN(3,:),T.^2,NoN(4,:),T.^2,NoN(5,:))\nlegend('symamd','symmmd','colamd','colmmd','realmmd')\ntitle('Comparison of minimum degree algorithms vs matrix size')\nylabel('Number of nonzeros in L');\nxlabel('Size: N^2')\nfigure;\nplot(T(4:6).^2,NoN(1,4:6),T(4:6).^2,NoN(2,4:6),T(4:6).^2,NoN(5,4:6))\nlegend('symamd','symmmd','realmmd')\ntitle('Comparison of minimum degree algorithms vs matrix size')\nylabel('Number of nonzeros in L');\nxlabel('Size: N^2')\nB.3: Comparison: nonzeros in Cholesky factor, computational time for LU and Cholesky\ndecompositions, computational time in reordering\n%Comparison of different minimum degree algorithms: Computational time,\n%Cholesky and LU decomposition and number of nonzero elements\nT = 2:5:347;\nfor k = 1:length(T)\nN = T(k);\n%1D difference matrix\nK = sparse(diag([2*ones(N,1)],0)+diag([-ones(N-1,1)],1)+diag([-ones(N-1,1)],-1));\n%Identity matrix\nI = speye(N);\n%2D Laplacian matrix\nK2D = kron(K,I)+kron(I,K);\nclear I K;\nfor m = 1:4\nif m ==1\ntic\np=symamd(K2D);\ntime = toc;\nK2Dmod=K2D(p,p);\nend\nif m ==2\ntic\np=symmmd(K2D);\ntime = toc;\nK2Dmod=K2D(p,p);\nend\nif m ==3\ntic\np=colamd(K2D);\ntime =toc;\nK2Dmod=K2D(p,p);\nend\nif m ==4\ntic\np=colmmd(K2D);\ntime=toc;\nK2Dmod=K2D(p,p);\nend\nif m ==5\ntic\np=realmmd(K2D);\ntime=toc;\nK2Dmod=K2D(p,p);\nend\ntic\n[L]=chol(K2Dmod);\ntime2 = toc;\ntic\n[L2,U2]=lu(K2Dmod);\ntime4 = toc;\nNoN1(m,k) = nnz(L);\n\nNoN2(m,k) = nnz(L2);\nTiMe(m,k)=time;\nTiMeChol(m,k)=time2;\nTiMeLU(m,k)=time4;\nclear L K2Dmod time conn time2 time3 time4 time5;\nend\nend\n%Plot\nfigure;\nplot(T.^2,NoN1(1,:),T.^2,NoN1(2,:),T.^2,NoN1(3,:),T.^2,NoN1(4,:))\nlegend('symamd','symmmd','colamd','colmmd')\ntitle('Comparison of minimum degree algorithms vs matrix size')\nylabel('Number of nonzeros in U');\nxlabel('Size: N^2')\nfigure;\nplot(T.^2,TiMeChol(1,:),T.^2,TiMeChol(2,:),T.^2,TiMeChol(3,:),T.^2,TiMeChol(4,:))\nlegend('Cholesky <= symamd','Cholesky <= symmmd','Cholesky <= colamd','Cholesky <=\ncolmmd')\ntitle('Computational time for Cholesky decomposing reordered matrices vs size')\nylabel('Time (sec)');\nxlabel('Size: N^2')\nfigure;\nplot(T.^2,TiMeLU(1,:),T.^2,TiMeLU(2,:),T.^2,TiMeLU(3,:),T.^2,TiMeLU(4,:))\nlegend('LU <= symamd','LU <= symmmd','LU <= colamd','LU <= colmmd')\ntitle('Computational time for LU decomposing reordered matrices vs size')\nylabel('Time (sec)');\nxlabel('Size: N^2')\nfigure;\nplot(T.^2,TiMe(1,:),T.^2,TiMe(2,:),T.^2,TiMe(3,:),T.^2,TiMe(4,:))\nlegend('symamd','symmmd','colamd','colmmd')\ntitle('Reordering computational time')\nylabel('Time (sec)');\nxlabel('Size: N^2')\nB.4: Movie for red-black ordering algorithm (based on code obtain at [3])\n%Movie for red-black ordering algorithm\nN=3;\nA=sparse(toeplitz([2,-1,zeros(1,N-2)]));\nI=speye(N,N);\nB=kron(A,I)+kron(I,A);\n[x,y]=ndgrid(1:N,1:N);\np=redblack(B);\nB=B(p,p);\nR=chol(B(p,p));\nxy=[x(p);y(p)]';\nn=size(B,1);\nL=zeros(n);\nlpars={'marker','.','linestyle','none','markersize',64};\ntpars={'fontname','helvetica','fontsize',16,'horiz','center','col','g'};\nfor i=1:n\nclf\ngplot(B,xy);\nline(xy(i:N^2,1),xy(i:N^2,2),'color','k',lpars{:});\nfor j=i:n\ndegree=length(find(B(:,j)))-1;\ntext(xy(j,1),xy(j,2),int2str(degree),tpars{:});\nend\naxis equal,axis off,axis([1,N,1,N])\npause(0.3);\nline(xy(i,1),xy(i,2),'color','r',lpars{:});\npause(0.3);\nL(i,i)=sqrt(B(i,i));\nL(i+1:n,i)=B(i+1:n,i)/L(i,i);\nB(i+1:n,i+1:n)=B(i+1:n,i+1:n)-L(i+1:n,i)*L(i+1:n,i)';\nB(i, i:n)=0;\nB(i:n, i)=0;\nend\nspy(L,32)\nB.5: Red-black ordering algorithm\nfunction p=redblack(A)\n%REDBLACK: Computes the permutation vector implementing the red-black\n%ordering algorithm\n\nn=size(A,1);\ntemp = 1:n;\ncount = 0;\nodd = 2;\nflag =1;\nfor m = 1:n\nif flag\np(m)=temp(m+count);\ncount = count+1;\nif p(m)==n|p(m)+2>n\nflag = 0;\nend\nelse\np(m)=temp(odd);\nodd = odd+2;\nend\nend\nB.5: Red-black algorithm: comparison code\n%Comparison code: red-black algorithm\nT=3:2:233;\nfor m = 1:length(T)\nN = T(m);\nA=sparse(toeplitz([2,-1,zeros(1,N-2)]));\nI=speye(N,N);\nB=kron(A,I)+kron(I,A);\nclear N I A;\ntic\np=redblack(B);\ntime = toc;\nC=B(p,p);\ntic\n[L,U]= lu(B);\ntime2 = toc;\ntic\n[L2,U2]= lu(C);\ntime3 = toc;\nNoN1(m)=nnz(L);\nNoN2(m)=nnz(L2);\nTiMe(m)=time;\nTiMeLUor(m)=time2;\nTiMeLUrb(m)=time3;\nclear L L2 U U2 time time2 time3 p B C m;\nend\nfigure;\nplot(T.^2,NoN1,T.^2,NoN2)\ntitle('Number of nonzeros in L as a function of matrix size')\nlegend('L<=Original K2D','L<=Reordered K2D');\nxlabel('Size: N^2')\nylabel('Number of nonzeros in L')\nfigure;\nplot(T.^2,TiMe)\ntitle('Computational time for red-black ordering algorithm')\nxlabel('Size: N^2')\nylabel('Time (sec)')\nfigure;\nplot(T.^2,TiMeLUor,T.^2,TiMeLUrb)\ntitle('Computational time of the LU decomposition algorithm')\nlegend('L<=Original K2D','L<=Reordered K2D');\nxlabel('Size: N^2')\nylabel('Time (sec)')\nB.6: Nested dissection ordering algorithm: comparison code\n%Comparison code: red-black algorithm\nT=3:4:100;\nfor m = 1:length(T)\nN = T(m);\nA=sparse(toeplitz([2,-1,zeros(1,N-2)]));\nI=speye(N,N);\nB=kron(A,I)+kron(I,A);\nclear N I A;\ntic\np = gsnd(B);\ntime = toc;\ntic\nd = specnd(B);\n\ntime4 = toc;\nC=B(p,p);\nD = B(d,d);\ntic\n[L,U]= lu(B);\ntime2 = toc;\ntic\n[L2,U2]= lu(C);\ntime3 = toc;\ntic\n[L3,U3]= lu(D);\ntime6 = toc;\nNoN1(m)=nnz(L);\nNoN2(m)=nnz(L2);\nNoN3(m)=nnz(L3);\nTiMe(m)=time;\nTiMe2(m)=time4;\nTiMeLUor(m)=time2;\nTiMeLUnd1(m)=time3;\nTiMeLUnd2(m)=time6;\nclear L L2 U U2 time time2 time3 p B C m time4 time6 D d;\nend\nfigure;\nplot(T.^2,NoN1,T.^2,NoN2,T.^2,NoN3)\ntitle('Number of nonzeros in L as a function of matrix size')\nlegend('L<=Original K2D','L<=Reordered by \"gsnd\"','L<=Reordered by \"specnd\"');\nxlabel('Size: N^2')\nylabel('Number of nonzeros in L')\nfigure;\nplot(T.^2,TiMe,T.^2,TiMe2)\ntitle('Computational times for nested dissection algorithms: \"gsnd\" and \"specnd\"')\nlegend('\"gsnd\"','\"specnd\"')\nxlabel('Size: N^2')\nylabel('Time (sec)')\nfigure;\nplot(T.^2,TiMeLUor,T.^2,TiMeLUnd1,T.^2,TiMeLUnd2)\ntitle('Computational time of the LU decomposition algorithm')\nlegend('L<=Original K2D','L<=Reordered by \"gsnd\"','L<=Reordered by \"specnd\"');\nxlabel('Size: N^2')\nylabel('Time (sec)')"
    },
    {
      "category": "Resource",
      "title": "am35.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/5f469576670a2c7a874999dbafc8c3cb_am35.pdf",
      "content": "c2006 Gilbert Strang\n3.5\nFinite Differences and Fast Poisson Solvers\nK\nIt is extremely unusual to use eigenvectors to solve a linear system KU = F. You\nneed to know all the eigenvectors of K, and (much more than that) the eigenvector\nmatrix S must be especially fast to work with. Both S and S-1 are required, because\n-1 = S-1S-1 . The eigenvalue matrices and -1 are diagonal and quick. When\nthe derivatives in Poisson's equation -uxx\nyy = f(x, y) are replaced by second\n- u\ndifferences, we do know the eigenvectors (discrete sines in the columns of S). Then\nthe Fourier transform quickly inverts and multiplies by S.\nOn a square mesh those differences have -1, 2, -1 in the x-direction and -1, 2, -1\nin the y-direction (divided by h2, where h = meshwidth). Figure 3.20 shows how\nthe second differences combine into a \"5-point molecule\" for the discrete Laplacian.\nBoundary values u = u0(x, y) are assumed to be given along the sides of a unit square.\nThe square mesh has N interior points in each direction (N = 5 in the figure).\nIn this case there are n = N 2 = 25 unknown mesh values Uij . When the molecule is\ncentered at position (i, j), the discrete Poisson equation gives a row of K2D U = F:\nK2D U = F\n4uij - ui, j-1 - ui-1, j - ui+1, j - ui, j+1 = h2f(ih, jh) = Fij .(1)\nThe inside rows of K2D have five nonzero entries 4, -1, -1, -1, -1. When\n(i, j) is next to a boundary point of the square, the known value of u0 at that neigh\nboring boundary point moves to the right side of equation (1). It becomes part of\nthe vector F, and a -1 drops out of the corresponding row of K. So K2D has five\nnonzeros on inside rows and fewer nonzeros on next-to-boundary rows.\n-1\n-1\n-1\n-1\n-1\n-1\n-1\nN\nN\nFigure 3.20: 5-point molecules at inside points (fewer -1's next to boundary).\nThis matrix K2D is sparse. Using blocks of size N, we can create the 2D matrix\nfrom the familiar N by N second difference matrix K. Number the nodes of the\nsquare a row at a time (this \"natural numbering\" is not necessarily best). Then the\n-1's for the neighbor above and the neighbor below are N positions away from the\n\n3.5. FINITE DIFFERENCES AND FAST POISSON SOLVERS c2006 Gilbert Strang\nmain diagonal of K2D. The 2D matrix is block tridiagonal with tridiagonal blocks:\n⎡\n⎡\n2 -1\nK + 2I\n-I\n⎢\n2 -1\n⎢\nK + 2I -I\nK = 6 -1\n⎢\nK2D = 6\n-I\n⎢\n(2)\n⎣\n⎣\n·\n·\n·\n·\n·\n·\n-I\nK + 2I\n-1\nN 2\nSize N\nSize n =\nBandwidth w = N\nTime N\nSpace nw = N 3\nTime nw2 = N 4\nThe matrix K2D has 4's down the main diagonal in equation (1). Its bandwidth\nw = N is the distance from the main diagonal to the nonzeros in -I. Many of the\nspaces in between are filled during elimination ! This is discussed in Section 6.1.\nKronecker product\nOne good way to create K2D from K and I is by the kron\ncommand. When A and B have size N by N, the matrix kron(A, B) is N 2 by N 2 .\nEach number aij is replaced by the block aij B.\nTo take second differences in all rows at the same time, kron(I, K) produces a\nblock diagonal matrix of K's. In the y-direction, kron(K, I) multiplies -1 and 2 and\n-1 by I (dealing with a column of meshpoints at a time). Add x and y differences:\n⎡\n⎡\nK\n2I\n-I ·\nK2D = kron(I, K) + kron(K, I) = 4\nK\n⎣ + 4 -I\n2I\n· ⎣\n(3)\n·\n·\n·\n·\nThis sum agrees with the 5-point matrix in (2). The computational question is how\nto work with K2D. We will propose three methods:\n1.\nElimination in a good order (not using the special structure of K2D)\n2.\nFast Poisson Solver (applying the FFT = Fast Fourier Transform)\n3.\nOdd-Even Reduction (since K2D is block tridiagonal).\nThe novelty is in the Fast Poisson Solver, which uses the known eigenvalues and\neigenvectors of K and K2D. It is strange to solve linear equations KU = F by\nexpanding F and U in eigenvectors, but here it is extremely successful.\nElimination and Fill-in\nFor most two-dimensional problems, elimination is the way to go. The matrix from\na partial differential equation is sparse (like K2D). It is banded but the bandwidth\nis not so small. (Meshpoints cannot be numbered so that all five neighbors in the\nmolecule receive nearby numbers.) Figure 3.21 has points 1, . . . , N along the first\nrow, then a row at a time going up the square. The neighbors above and below point\nj have numbers j - N and j + N.\n\nc\n2006 Gilbert Strang\nOrdering by rows produces the -1's in K2D that are N places away from the\ndiagonal. The matrix K2D has bandwidth N. The key point is that elimination\nfills in the zeros inside the band. We add row 1 (times 4 ) to row 2, to eliminate\nthe -1 in position (2, 1). But the last -1 in row 1 produces a new -1 in row 2. A\nzero inside the band has disappeared. As elimination continues from A to U, virtually\nthe whole band is filled in.\nIn the end, U has about 5 times 25 nonzeros (this is N 3, the space needed to store\nU). There will be about N nonzeros next to the pivot when we reach a typical row,\nand N nonzeros below the pivot. Row operations to remove those nonzeros will require\nup to N 2 multiplications, and there are N 2 pivots. So the count of multiplications is\nabout 25 times 25 (this is N 4, for elimination in 2D).\nFigure 3.21: Typical rows of K2D have 5 nonzeros. Elimination fills in the band.\nSection 6.1 will propose a different numbering of the meshpoints, to reduce the\nfill-in that we see in U. This reorders the rows of K2D by a permutation matrix\nP, and the columns by P T . The new matrix P(K2D)P T is still symmetric, but\nelimination (with fill-in) proceeds in a completely different order. The MATLAB\ncommand symamd(K) produces a nearly optimal choice of P.\nElimination is fast in two dimensions (but a Fast Poisson Solver is faster !). In\nthree dimensions the matrix size is N 3 and the bandwidth is N 2 . By numbering the\nnodes a plane at a time, vertical neighbors are N 2 nodes apart. The operation count\nfor elimination becomes N 7, which can be seriously large. Chapter 6 on Solving Large\nSystems will introduce badly needed alternatives to elimination in 3D.\nSolvers Using Eigenvalues\nOur matrices K and K2D are extremely special. We know the eigenvalues and eigen\nvectors of the second-difference matrix K. The eigenvalues have the special form\n= 2 -2 cos ∂, and the eigenvectors are discrete sines. There will be a similar pattern\nfor K2D, which is formed in a neat way from K (by Kronecker product). The Poisson\nSolver uses those eigenvalues and eigenvectors to solve (K2D)(U2D) = (F2D). On a\nsquare mesh it is much faster than elimination.\nHere is the idea, first in one dimension. The matrix K has eigenvalues 1, . . . , N\nand eigenvectors y1, . . . , yN . There are three steps to the solution of KU = F:\n1.\nExpand F\nF = a1y1 +\n+ aN yN\n2.\nak\nk\n3.\nU\na1/1) y1 +\naN /N ) yN .\nas a combination\n· · ·\nof the eigenvectors\nDivide each\nby\nRecombine eigenvectors into\n= (\n· · · + (\n\n3.5. FINITE DIFFERENCES AND FAST POISSON SOLVERS c2006 Gilbert Strang\nThe success of the method depends on the speed of steps 1 and 3. Step 2 is fast.\nTo see that U in step 3 is correct, multiply it by the matrix K. Every eigenvector\ngives Ky = y. That cancels the in each denominator. The result is that KU\nagrees with the vector F in step 1.\nNow look at the calculation required in each step, using matrices. Suppose S is\nthe eigenvector matrix, with the eigenvectors y1, . . . , yN of K in its columns. Then\nthe coefficients a1, . . . , aN come by solving Sa = F:\n⎡\n⎡\na1\n6 . ⎢\nStep 1\nSolve Sa = F\n4 y1 · · · yN ⎣4 .. ⎣ = a1y1 +\n+ aN yN = F . (4)\n· · ·\naN\nThus a = S-1F. Then step 2 divides the a's by the 's to find -1a = -1S-1F.\n(The eigenvalue matrix is just the diagonal matrix of 's.) Step 3 uses those\ncoefficients ak/k in recombining the eigenvectors into the solution vector U:\n⎡\n⎡\na1/1\n.\n⎢\nStep 3\nU = 4 y1\nyN ⎣4\n.\n⎣ = S-1 a = S-1S-1F .\n(5)\n.\n· · ·\naN /N\nYou see the matrix K-1 = S-1S-1 appearing in that last formula. We have K-1F\nbecause K itself is SS-1--the usual diagonalization of a matrix. The eigenvalue\nmethod is using this SS-1 factorization instead of K = LU from elimination.\nThe speed of steps 1 and 3 depends on multiplying quickly by S-1 and S. Those\nare full matrices, not sparse like K. Normally they both need N 2 operations in one\ndimension (where the matrix size is N). But the \"sine eigenvectors\" in S give the\nDiscrete Sine Transform, and the Fast Fourier Transform executes S and S-1 in\nN log2 N steps. In one dimension this is not as fast as cN from elimination, but in\ntwo dimensions N 2 log(N 2) easily wins.\nColumn k of the matrix S contains the eigenvector yk. The number Sjk = sin jk\nN+1\nis the jth component of that eigenvector. For our example with N = 5 and N +1 = 6,\nyou could list the sines of every multiple of /6. Here are those numbers:\np\np\n3 1\nSines ,\n, 1,\n,\n, 0, (repeat with minus signs) (repeat 12 numbers forever) .\nThe kth column of S (kth eigenvector yk) takes every kth number from that list:\n⎡\n⎡\n⎡\n⎡\n⎡\n1/2\np\n3/2\np\n3/2\n1/2\n6 p\n3/2⎢\n6 p\n3/2⎢\n⎢\n6 p\n3/2⎢\n6 p\n3/2⎢\n6 0\n⎢\n⎢\n⎢\n6 -\n⎢\n6 -\n⎢\n0 ⎢\n⎢\n0 ⎢\ny5 = 6\n1 ⎢\ny1 = 6\n1 ⎢\ny2 = 6\n-\np\n3/2\n⎢\ny3 = 6-1 ⎢\ny4 =\n⎢\n⎢\n⎢\n4 p\n3/2⎣\n⎣\n4 0⎣\n4 p\n3/2⎣\n⎣\n1/2\n-\np\n3/2\n-\np\n3/2\n-\np\n3/2\n1/2\nThose eigenvectors are orthogonal ! This is guaranteed by the symmetry of K. All\nthese eigenvectors have length 3 = (N + 1)/2. Dividing each column by\np\n3, we have\northonormal eigenvectors. S/\np\n3 is an orthogonal matrix Q, with QTQ = I.\n\nc2006 Gilbert Strang\nIn this special case, S and Q are also symmetric. So Q-1 = QT = Q.\nNotice that yk has k - 1 changes of sign. It comes from k loops of the sine curve.\nThe eigenvalues are increasing: = 2 -\np\n3, 2 - 1, 2 - 0, 2 + 1, 2 +\np\n3. Those\neigenvalues add to 10, which is the sum down the diagonal (the trace) of K5. The\nproduct of the 5 eigenvalues (easiest by pairs) confirms that det(K5) = 6.\nFast Poisson Solvers\nTo extend this eigenvalue method to two dimensions, we need the eigenvalues and\neigenvectors of K2D. The key point is that the N 2 eigenvectors of K2D are separable.\nEach eigenvector ykl separates into a product of sines:\ny\ny\nis sin N\nsin N+1 .\n(6)\nEigenvectors\nkl\nThe (i, j) component of\nkl\nik\n+1\njl\nWhen you multiply that eigenvector by K2D, you have second differences in the x-\ndirection and y-direction. The second differences of the first sine (x-direction) produce\na factor k = 2 - 2 cos k . This is the eigenvalue of K in 1D. The second differences\nN+1\nof the other sine (y-direction) produce a factor l = 2 - 2 cos l . So the eigenvalue\nN+1\nin two dimensions is the sum k + l of one-dimensional eigenvalues:\n(K\ny\n= y\n\n-\nN+1\n-\nN+1\n(7)\n2D)\nkl\nkl\nkl\nkl = (2\n2 cos k ) + (2\n2 cos l ) .\nNow the solution of K2D U = F comes by a two-dimensional sine transform:\n⎤⎤ akl\nik\njl\nFi,j = ⎤⎤ akl sin ik sin jl\nUi,j =\nsin N +1 sin\n(8)\nN +1\nN+1\nkl\nN+1\nAgain we find the a's, divide by the 's, and build U from the eigenvectors in S:\na = S-1F\n-1a\n-1S-1F\nU = S-1S-1F\nStep 1\nStep 2\n=\nStep 3\nSwartztrauber [SIAM Review 19 (1977) 490] gives the operation count 2N 2 log2 N.\nThis uses the Fast Sine Transform (based on the FFT) to multiply by S-1 and S.\nThe Fast Fourier Transform is explained in Section 4.3.\nS\nNote\nWe take this chance to notice the good properties of a Kronecker product\nC = kron(A, B). Suppose A and B have their eigenvectors in the columns of SA and\nB . The eigenvalues are in A and B . Then we know SC and C :\nThe eigenvectors of kron(A, B) are in kron(SA, SB ).\n(9)\nThe eigenvalues are in kron(A, B ).\nThe diagonal blocks in kron(A, B) are entries k(A) times the diagonal matrix B.\nSo the eigenvalues kl of C are just the products k(A)l(B).\n\n3.5. FINITE DIFFERENCES AND FAST POISSON SOLVERS c2006 Gilbert Strang\nIn our case A and B were I and K. The matrix K2D added the two products\nkron(I, K) and kron(K, I). Normally we cannot know the eigenvectors and eigen\nvalues of a matrix sum--except when the matrices commute. Since all our matrices\nare formed from K, these Kronecker products do commute. This gives the separable\neigenvectors and eigenvalues in (6) and (7).\nCyclic Odd-Even Reduction\nThere is an entirely different (and very simple) approach to KU = F. I will start in\none dimension, by writing down three rows of the usual second difference equation:\nRow i - 1\n-Ui-2 + 2Ui-1 - Ui\n= Fi-1\nRow i\n-Ui-1 + 2Ui - Ui+1\n= Fi\n(10)\nRow i + 1\n-Ui + 2Ui+1 - Ui+2 = Fi+1\nMultiply the middle equation by 2, and add. This eliminates Ui-1 and Ui+1:\n-Ui-2\nUi - Ui\n= Fi-1\nFi + Fi\n.\n(11)\nOdd-even reduction in 1D\n+ 2\n+2\n+ 2\n+1\nNow we have a half-size system, involving only half of the U's (with even indices).\nThe new system (11) has the same tridiagonal form as before. When we repeat, cyclic\nreduction produces a quarter-size system. Eventually we can reduce KU = F to a\nvery small problem, and then cyclic back-substitution produces the whole solution.\nHow does this look in two dimensions ? The big matrix K2D is block triangular:\n⎡\nA\n-I\n⎢\nA\n-I\nK2D = 6 -I\n⎢\nwith A = K + 2I\nfrom equation (4) .\n(12)\n⎣\n·\n·\n·\nA\n-I\nThe three equations in (10) become block equations for whole rows of N mesh values.\nWe are taking the unknowns Ui = (Ui1, . . . , UiN ) a row at a time. If we write three\nrows of (10), the block A replaces the number 2 in the scalar equation. The block\n-I replaces the number -1. To reduce (K2D)(U2D) = (F2D) to a half-size system,\nmultiply the middle equation (with i even) by A and add the three block equations:\nReduction in 2D\n-IUi-2 + (A2 - 2I)Ui - IUi+2 = Fi-1 + AFi + Fi+1 . (13)\nThe new half-size matrix is still block tridiagonal. The diagonal blocks that were\npreviously A in (12) are now A2 - 2I, with the same eigenvectors. The unknowns are\nthe 1 N 2 values Ui,j at meshpoints with even indices i.\nThe bad point is that each new diagonal block A2 - 2I has five diagonals, where\nthe original block A = K+2I had three diagonals. This bad point gets worse as cyclic\nreduction continues. At step r, the diagonal blocks become Ar = A2\nr-1 - 2I and their\nbandwidth doubles. We could find tridiagonal factors (A-\np\n2I)(A+\np\n2I) = A2 - 2I,\n\nc2006 Gilbert Strang\nbut the number of factors grows quickly. Storage and computation and roundoff error\nare increasing too rapidly with more reduction steps.\nStable variants of cyclic reduction were developed by Buneman and Hockney. The\nclear explanation by Buzbee, Golub, and Nielson [SIAM Journal of Numerical Anal\nysis 7 (1970) 627] allows other boundary conditions and other separable equations,\ncoming from polar coordinates or convection terms like C@u/@x + D@u/@y. After m\nsteps of cyclic reduction, Hockney went back to a Fast Poisson Solver.\nThis combination FACR(m) is widely used, and the optimal number of cyclic\nreduction steps (before the FFT takes over) is small. For N = 128 a frequent choice\nis m = 2. Asymptotically mopt grows like log log N and the operation count for\nFACR(mopt) is 3N 2 log log N. In practical scientific computing with N 2 unknowns\n(or with N 3 unknowns in three dimensions), a Fast Poisson Solver is a winner.\n****** Add code for Fast Poisson *******\nProblem Set 3.5\nProblems 1-\nare for readers who get enthusiastic about kron.\nWhy is the transpose of C = kron(A, B) equal to kron(AT, BT) ? Why is the\ninverse equal to C-1 = kron(A-1, B-1) ? You have to transpose each block aij B\nof the Kronecker product C, and then patiently multiply CC-1 by blocks.\nC is symmetric (or orthogonal) when A and B are symmetric (or orthogonal).\nWhy is the matrix C = kron(A, B) times the matrix D = kron(S, T ) equal to\nCD = kron(AS, BT ) ? This needs even more patience with block multiplica\ntion. The inverse CC-1 = kron(I, I) = I is a special case.\nNote\nSuppose S and T are eigenvector matrices for A and B. From AS =\nSA and BT = T B we have CD = kron(AS, BT ) = kron(SA, T B ). Then\nCD = D kron(A, B ) = DC . So D = kron(S, T ) is the eigenvector matrix\nfor C."
    },
    {
      "category": "Resource",
      "title": "am36.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/cb76484c6f1637c4ae205b66064c28c8_am36.pdf",
      "content": "CHAPTER 3. BOUNDARY VALUE PROBLEMS\n3.6\nSolving Large Linear Systems\nFinite elements and finite differences produce large linear systems KU = F. The\nmatrices K are extremely sparse. They have only a small number of nonzero entries in\na typical row. In \"physical space\" those nonzeros are clustered tightly together--they\ncome from neighboring nodes and meshpoints. But we cannot number N2 nodes in a\nplane in any way that keeps neighbors close together! So in 2-dimensional problems,\nand even more in 3-dimensional problems, we meet three questions right away:\n1. How best to number the nodes\n2. How to use the sparseness of K (when nonzeros can be widely separated)\n3. Whether to choose direct elimination or an iterative method.\nThat last point will split this section into two parts--elimination methods in 2D\n(where node order is important) and iterative methods in 3D (where preconditioning\nis crucial).\nTo fix ideas, we will create the n equations KU = F from Laplace's difference\nequation in an interval, a square, and a cube. With N unknowns in each direction,\nK has order n = N or N2 or N3 . There are 3 or 5 or 7 nonzeros in a typical row of\nthe matrix. Second differences in 1D, 2D, and 3D are shown in Figure 3.17.\n-1\n-1\nBlock\n-1\nTridiagonal K\nTridiagonal\nK\n-1\n-1\n-1\n-1\n-1\n-1\nN2 by N2\nN by N\n-1\nN3 by N3\n-1 -1\nFigure 3.17: 3, 5, 7 point difference molecules for -uxx, -uxx - uyy , -uxx - uyy - uzz .\nAlong a typical row of the matrix, the entries add to zero. In two dimensions this\nis 4 - 1 - 1 - 1 - 1 = 0. This \"zero sum\" remains true for finite elements (the element\nshapes decide the exact numerical entries). It reflects the fact that u = 1 solves\nLaplace's equation and Ui = 1 has differences equal to zero. The constant vector\nsolves KU = 0 except near the boundaries. When a neighbor is a boundary point\nwhere Ui is known, its value moves onto the right side of KU = F. Then that row of\nK is not zero sum. Otherwise K would be singular, if K ∗ ones(n, 1) = zeros(n, 1).\nUsing block matrix notation, we can create the 2D matrix K = K2D from the\nfamiliar N by N second difference matrix K. We number the nodes of the square a\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nrow at a time (this \"natural numbering\" is not necessarily best). Then the -1's for\nthe neighbor above and the neighbor below are N positions away from the main\ndiagonal of K2D. The 2D matrix is block tridiagonal with tridiagonal blocks:\n⎡\n⎤\n⎤\n⎡\n2 -1\nK + 2I\n-I\n⎢⎢⎣\n-1\n2 -1\n·\n·\n·\n⎥⎥⎦\nK2D =\n⎢⎢⎣\n-I\nK + 2I -I\n·\n·\n·\n⎥⎥⎦\nK =\n(1)\n-1\n\n-I\nK\n\n+ 2I\nSize N\nElimination in this order: K2D has size n = N2\nTime N\nBandwidth w = N, Space nw = N3 , Time nw2 = N4\nThe matrix K2D has 4's down the main diagonal. Its bandwidth w = N is the\ndistance from the diagonal to the nonzeros in -I. Many of the spaces in between are\nfilled during elimination! Then the storage space required for the factors in K = LU\nis of order nw = N3 . The time is proportional to nw2 = N4, when n rows each\ncontain w nonzeros, and w nonzeros below the pivot require elimination.\nThose counts are not impossibly large in many practical 2D problems (and we\nshow how they can be reduced). The horrifying large counts come for K3D in three\ndimensions. Suppose the 3D grid is numbered by square cross-sections in the natural\norder 1, . . . , N. Then K3D has blocks of order N2 from those squares. Each square\nis numbered as above to produce blocks coming from K2D and I = I2D:\n⎤\n⎡ K2D + 2I\n-I\nSize n = N3\nK3D =\n⎢⎢⎣\n-I\nK2D + 2I -I\n·\n·\n·\n⎥⎥⎦\nBandwidth w = N2\nElimination space nw = N5\n-I\nK2D + 2I\nElimination time ≈ nw2 = N7\nNow the main diagonal contains 6's, and \"inside rows\" have six -1's. Next to a point\nor edge or corner of the boundary cube, we lose one or two or three of those -1's.\nThe good way to create K2D from K and I (N by N) is to use the kron(A, B)\ncommand. This Kronecker product replaces each entry aij by the block aij B. To take\nsecond differences in all rows at the same time, and then all columns, use kron:\nK2D = kron(K, I) + kron(I, K) .\n(2)\nThe identity matrix in two dimensions is I2D = kron(I, I). This adjusts to allow\nrectangles, with I's of different sizes, and in three dimensions to allow boxes. For a\ncube we take second differences inside all planes and also in the z-direction:\nK3D = kron(K2D, I) + kron(I2D, K) .\nHaving set up these special matrices K2D and K3D, we have to say that there are\nspecial ways to work with them. The x, y, z directions are separable. The geometry\n(a box) is also separable. See Section 7.2 on Fast Poisson Solvers. Here the matrices\nK and K2D and K3D are serving as models of the type of matrices that we meet.\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nMinimum Degree Algorithm\nWe now describe (a little roughly) a useful reordering of the nodes and the equations\nin K2DU = F. The ordering achieves minimum degree at each step--the number\nof nonzeros below the pivot row is minimized. This is essentially the algorithm\nused in MATLAB's command U = K\\F , when K has been defined as a sparse matrix.\nWe list some of the functions from the sparfun directory:\nspeye (sparse identity I)\nnnz (number of nonzero entries)\nfind\n(find indices of nonzeros)\nspy\n(visualize sparsity pattern)\ncolamd and symamd\n(approximate minimum degree permutation of K)\nYou can test and use the minimum degree algorithms without a careful analysis. The\napproximations are faster than the exact minimum degree permutations colmmd and\nsymmmd. The speed (in two dimensions) and the roundoff errors are quite reasonable.\nIn the Laplace examples, the minimum degree ordering of nodes is irregular com\npared to \"a row at a time.\" The final bandwidth is probably not decreased. But the\nnonzero entries are postponed as long as possible! That is the key.\nThe difference is shown in the arrow matrix of Figure 3.18. On the left, minimum\ndegree (one nonzero off the diagonal) leads to large bandwidth. But there is no fill-in.\nElimination will only change its last row and column. The triangular factors L and\nU have all the same zeros as A. The space for storage stays at 3n, and elimination\nneeds only n divisions and multiplications and subtractions.\n⎤\n⎡\n⎤\n⎡ ∗\n∗\n∗\n∗\n⎢⎢⎢⎢⎢⎢⎢⎢⎣\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n⎥⎥⎥⎥⎥⎥⎥⎥⎦\nBandwidth\n6 and 3\nFill-in\n0 and 6\n⎢⎢⎢⎢⎢⎢⎢⎢⎣\n∗\n∗\n∗\n∗\n\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n∗\n∗\nF\nF\n\n∗\nF ∗ F\n⎥⎥⎥⎥⎥⎥⎥⎥⎦\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n∗\nF\nF\n∗\nFigure 3.18: Arrow matrix: Minimum degree (no F) against minimum bandwidth.\nThe second ordering reduces the bandwidth from 6 to 3. But when row 4 is\nreached as the pivot row, the entries indicated by F are filled in. That full lower\nquarter of A gives 1\n8n 2 nonzeros to both factors L and U . You see that the whole\n\"profile\" of the matrix decides the fill-in, not just the bandwidth.\nThe minimum degree algorithm chooses the (k + 1)st pivot column, after k\ncolumns have been eliminated as usual below the diagonal, by the following rule:\nIn the remaining matrix of size n -k, select the column with the fewest nonzeros.\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nThe component of U corresponding to that column is renumbered k + 1. So is the\nnode in the finite difference grid. Of course elimination in that column will normally\nproduce new nonzeros in the remaining columns! Some fill-in is unavoidable. So\nthe algorithm must keep track of the new positions of nonzeros, and also the actual\nentries. It is the positions that decide the ordering of unknowns. Then the entries\ndecide the numbers in L and U .\nExample\nFigure 3.19 shows a small example of the minimal degree ordering, for\nLaplace's 5-point scheme. The node connections produce nonzero entries (indicated by\n∗) in K. The problem has six unknowns. K has two 3 by 3 tridiagonal blocks from\nhorizontal links, and two 3 by 3 blocks with -I from vertical links.\nThe degree of a node is the number of connections to other nodes. This is the\nnumber of nonzeros in that column of K. The corner nodes 1, 3, 4, 6 all have degree\n2. Nodes 2 and 5 have degree 3. A larger region has inside nodes of degree 4, which\nwill not be eliminated first. The degrees change as elimination proceeds, because of\nfill-in.\nThe first elimination step chooses row 1 as pivot row, because node 1 has minimum\ndegree 2. (We had to break a tie! Any degree 2 node could come first, leading to different\nelimination orders.) The pivot is P, the other nonzeros in that row are boxed. When\nrow 1 operates on rows 2 and 4, it changes six entries below it. In particular, the two\nfill-in entries marked by F change to nonzeros. This fill-in of the (2, 4) and (4, 2) entries\ncorresponds to the dashed line connecting nodes 2 and 4 in the graph.\nF\nF\nF\n∗\n∗\nP\n∗\nF\n∗\nF\n∗\nP\nF\n∗\nF\n∗\n∗\n\n∗\n∗\n∗\n\n∗\n∗\nF\n∗\n∗\n∗\n∗\n\nP\nFill-in F\nfrom\nPivots\nZeros\nelimination\n∗\nF\n∗\n∗\n∗\n∗\n∗\n∗\n∗\n\n∗\n∗\n\n∗\n∗\n\nFigure 3.19: Minimum degree nodes 1 and 3. The pivots P are in rows 1 and 3; new\nedges 2-4 and 2-6 in the graph match the matrix entries F filled in by elimination.\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nNodes that were connected to the eliminated node are now connected to each other.\nElimination continues on the 5 by 5 matrix (and the graph with 5 nodes). Node 2 still has\ndegree 3, so it is not eliminated next. If we break the tie by choosing node 3, elimination\nusing the new pivot P will fill in the (2, 6) and (6, 2) positions. Node 2 becomes linked\nto node 6 because they were both linked to the eliminated node 3.\nThe problem is reduced to 4 by 4, for the unknown U 's at the remaining nodes\n2, 4, 5, 6. Problem\nasks you to take the next step--choose a minimum degree node\nand reduce the system to 3 by 3.\nStoring the Nonzero Structure = Sparsity Pattern\nA large system KU = F needs a fast and economical storage of the node connections\n(which match the positions of nonzeros in K). The connections and nonzeros change\nas elimination proceeds. The list of edges and nonzero positions corresponds to the\n\"adjacency matrix \" of the graph of nodes. The adjacency matrix has 1 or 0 to indicate\nnonzero or zero in K.\nFor each node i, we have a list adj(i) of the nodes connected to i. How to combine\nthese into one master list NZ for the whole graph and the whole matrix K? A simple\nway is to store the lists adj(i) sequentially in NZ (the nonzeros for i = 1 up to i = n).\nAn index array IND of pointers tells the starting position of the sublist adj(i) within\nthe master list NZ. It is useful to give IND an (n + 1)st entry to point to the final\nentry in NZ (or to the blank that follows, in Figure 3.20). MATLAB will store one\nmore array (the same length nnz(K) as NZ) to give the actual nonzero entries.\nNZ\n| 2\n\n↑\n| 1\n\n↑\n5 | 2\n\n↑\n| 1\n\n↑\n|\n↑\n4 6 |\n↑\n5 |\n↑\nIND\n\nNode\nFigure 3.20: Master list NZ of nonzeros (neighbors in Figure 3.19). Positions in\nIND.\nThe indices i are the \"original numbering\" of the nodes. If there is renumbering,\nthe new ordering can be stored as a permutation PERM. Then PERM(i) = k when\nthe new number i is assigned to the node with original number k. The text [GL] by\nGeorge and Liu is the classic reference for this entire section on ordering of the nodes.\nGraph Separators\nHere is another good ordering, different from minimum degree. Graphs or meshes are\noften separated into disjoint pieces by a cut. The cut goes through a small number\nof nodes or meshpoints (a separator). It is a good idea to number the nodes in the\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nseparator last. Elimination is relatively fast for the disjoint pieces P and Q. It only\nslows down at the end, for the (smaller) separator S.\nThe three groups P, Q, S of meshpoints have no direct connections between P and\nQ (they are both connected to the separator S). Numbered in that order, the \"block\narrow\" stiffness matrix and its K = LU factorization look like this:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nK\nKP\nKP S\nLP\nUP\nA\nK = ⎣\nKQ\nKQS ⎦\nL = ⎣ 0\nLQ\n⎦\nU = ⎣\nUQ B ⎦\nSP KSQ\nKS\nX\nY\nZ\nC\n(3)\nThe zero blocks in K give zero blocks in L and U. The submatrix KP comes first\nin elimination, to produce LP and UP . Then come the factors LQUQ of KQ, followed\nby the connections through the separator. The major cost is often that last step, the\nsolution of a fairly dense system of the size of the separator.\nS\nQ\nP\nP\nS\nQ\nArrow matrix\nSeparator comes last\nBlocks P, Q\n(Figure 3.18)\n(Figure 3.19)\nSeparator S\nFigure 3.21: A graph separator numbered last produces a block arrow matrix K.\nFigure 3.21 shows three examples, each with separators. The graph for a perfect\narrow matrix has a one-point separator (very unusual). The 6-node rectangle has a\ntwo-node separator in the middle. Every N by N grid can be cut by an N-point\nseparator (and N is much smaller than N2). If the meshpoints form a rectangle, the\nbest cut is down the middle in the shorter direction.\nYou could say that the numbering of P then Q then S is block minimum degree.\nBut one cut with one separator will not come close to an optimal numbering. It\nis natural to extend the idea to a nested sequence of cuts. P and Q have their\nown separators at the next level. This nested dissection continues until it is not\nproductive to cut further. It is a strategy of \"divide and conquer.\"\nFigure 3.22 illustrates three levels of nested dissection on a 7 by 7 grid. The first\ncut is down the middle. Then two cuts go across and four cuts go down. Numbering\nthe separators last within each stage, the matrix K of size 49 has arrows inside arrows\ninside arrows. The spy command will display the pattern of nonzeros.\nSeparators and nested dissection show how numbering strategies are based on the\ngraph of nodes and edges in the mesh. Those edges correspond to nonzeros in the\nmatrix K. The nonzeros created by elimination (filled entries in L and U) correspond\nto paths in the graph. In practice, there has to be a balance between simplicity and\noptimality in the numbering--in scientific computing simplicity is a very good thing!\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nzero\nzero\n3 × 18\n3 × 18\nzero\nzero\nzero\nzero\n∗0 ∗\n\n0 ∗∗ 5\n∗∗∗\n9×9\n1 to 9\n22 to 30\n19 to 21\n40 to 42\nK =\n10 to 18\n31 to 39\n7 × 42\n7×7\nFigure 3.22: Three levels of separators. Still\nnonzeros in K, only\nin L.\nA very reasonable compromise is the backslash command U = K\\F that uses a\nnearly minimum degree ordering in Sparse MATLAB.\nOperation Counts (page K)\nHere are the complexity estimates for the 5-point Laplacian with N 2 or N 3 nodes:\nMinimum Degree\nSpace (nonzeros from fill-in)\nTime (flops for elimination)\nn = N 2 in 2D\nn = N 3 in 3D\nX\nX\nX\nX\nNested Dissection\nSpace (nonzeros from fill-in)\nX\nX\nTime (flops for elimination)\nX\nX\nIn the last century, nested dissection lost out--it was slower on almost all applica\ntions. Now larger problems are appearing and the asymptotics eventually give nested\ndissection an edge. Algorithms for cutting graphs can produce short cuts into nearly\nequal pieces. Of course a new idea for ordering could still win.\nIterative versus Direct Methods\nThis section is a guide to solution methods for problems Ax = b that are too large\nand expensive for ordinary elimination. We are thinking of sparse matrices A, when\na multiplication Ax is relatively cheap. If A has at most p nonzeros in every row,\nthen Ax needs at most pn multiplications. Typical applications are to large finite\ndifference equations or finite element problems on unstructured grids. In the special\ncase of a square grid for Laplace's equation, a Fast Poisson Solver (Section 7.2) is\navailable.\nWe turn away from elimination to iterative methods and Krylov subspaces.\nPure iterative methods are easier to analyze, but the Krylov subspace methods are\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nmore powerful. So the older iterations of Jacobi and Gauss-Seidel and overrelaxation\nare less favored in scientific computing, compared to conjugate gradients and GM\nRES. When the growing Krylov subspaces reach the whole space Rn, these methods\n(in exact arithmetic) give the exact solution A-1b. But in reality we stop much ear\nlier, long before n steps are complete. The conjugate gradient method (for positive\ndefinite A, and with a good preconditioner ) has become truly important.\nThe next ten pages will introduce you to numerical linear algebra. This has\nbecome a central part of scientific computing, with a clear goal: Find a fast stable\nalgorithm that uses the special properties of the matrices. We meet matrices that\nare sparse or symmetric or triangular or orthogonal or tridiagonal or Hessenberg or\nGivens or Householder. Those matrices are at the core of so many computational\nproblems. The algorithm doesn't need details of the entries (which come from the\nspecific application). By using only their structure, numerical linear algebra offers\nmajor help.\nOverall, elimination with good numbering is the first choice until storage and CPU\ntime become excessive. This high cost often arises first in three dimensions. At that\npoint we turn to iterative methods, which require more expertise. You must choose\nthe method and the preconditioner. The next pages aim to help the reader at this\nfrontier of scientific computing.\nPure Iterations\nWe begin with old-style pure iteration (not obsolete). The letter K will be reserved\nfor \"Krylov\" so we leave behind the notation KU = F. The linear system becomes\nAx = b with a large sparse matrix A, not necessarily symmetric or positive definite:\nLinear system Ax = b\nResidual rk = b - Axk\nPreconditioner P ≈ A\nThe preconditioner P attempts to be \"close to A\" and at the same time much easier\nto work with. A diagonal P is one extreme (not very close). P = A is the other\nextreme (too close). Splitting the matrix A gives an equivalent form of Ax = b:\nSplitting\nPx = (P - A)x + b .\n(4)\nThis suggests an iteration, in which every vector xk leads to the next xk+1:\nIteration\nPxk+1 = (P - A)xk + b .\n(5)\nStarting from any x0, the first step finds x1 from Px1 = (P - A)x0 + b. The iteration\ncontinues to x2 with the same matrix P, so it often helps to know its triangular factors\nL and U. Sometimes P itself is triangular, or its factors L and U are approximations\nto the triangular factors of A. Two conditions on P make the iteration successful:\n1. The new xk+1 must be quickly computable. Equation (5) must be fast to solve.\n2. The errors ek = x - xk must converge quickly to zero.\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nSubtract equation (5) from (4) to find the error equation. It connects ek to ek+1:\nError\nPek+1 = (P - A)ek which means ek+1 = (I - P -1A)ek = Mek .\n(6)\nThe right side b disappears in this error equation. Each step multiplies the error\nvector by M = I - P -1A. The speed of convergence of xk to x (and of ek to zero)\ndepends entirely on M . The test for convergence is given by the eigenvalues of M :\nConvergence test\nEvery eigenvalue of M must have |λ(M )| < 1.\nThe largest eigenvalue (in absolute value) is the spectral radius ρ(M ) = max |λ(M )|.\nConvergence requires ρ(M ) < 1. The convergence rate is set by the largest eigen\nvalue. For a large problem, we are happy with ρ(M ) = .9 and even ρ(M ) = .99.\nSuppose that the initial error e0 happens to be an eigenvector of M . Then the\nnext error is e1 = Me0 = λe0. At every step the error is multiplied by λ, so we must\nhave |λ| < 1. Normally e0 is a combination of all the eigenvectors. When the iteration\nmultiplies by M , each eigenvector is multiplied by its own eigenvalue. After k steps\nthose multipliers are λk . We have convergence if all |λ| < 1.\nFor preconditioner we first propose two simple choices:\nJacobi iteration\nP = diagonal part of A\nGauss-Seidel iteration\nP = lower triangular part of A\nTypical examples have spectral radius ρ(M ) = 1 - cN -1 . This comes closer and\ncloser to 1 as the mesh is refined and the matrix grows. An improved preconditioner\nP can give ρ(M ) = 1 - cN -1/2. Then ρ is smaller and convergence is faster, as in\n\"overrelaxation.\" But a different approach has given more flexibility in constructing\na good P , from a quick incomplete LU factorization of the true matrix A:\nI ncomplete LU\nP = (approximation to L)(approximation to U ) .\nThe exact A = LU has fill-in, so zero entries in A become nonzero in L and U . The ap\nproximate L and U could ignore this fill-in (fairly dangerous). Or P = LapproxUapprox\ncan keep only the fill-in entries F above a fixed threshold. The variety of options,\nand the fact that the computer can decide automatically which entries to keep, has\nmade the ILU idea (incomplete LU ) a very popular starting point.\nExample\nThe -1, 2, -1 matrix A = K provides an excellent example. We choose\nthe preconditioner P = T , the same matrix with T11 = 1 instead of K11 = 2. The LU\nfactors of T are perfect first differences, with diagonals of +1 and -1. (Remember that\nall pivots of T equal 1, while the pivots of K are 2/1, 3/2, 4/3, . . .) We can compute the\nright side of T -1Kx = T -1b with only 2N additions and no multiplications (just back\nsubstitution using L and U ). Idea: This L and U are approximately correct for K.\nThe matrix P -1A = T -1K on the left side is triangular. More than that, T is a rank 1\nchange from K (the 1, 1 entry changes from 2 to 1). It follows that T -1K and K-1T\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nare rank 1 changes from the identity matrix I. A calculation in Problem\nshows that\nonly the first column of I is changed, by the \"linear vector\" l = (N, N - 1, . . . , 1):\nP -1A = T -1K = I + leT\nand\nK-1T = I - (leT\n1 )/(N + 1) .\n(7)\nT\nHere e1 =\n. . . 0\nso leT has first column l. This example finds x = K-1b by\na quick exact formula (K-1T)T -1b, needing only 2N additions for T -1 and N additions\nand multiplications for K-1T. In practice we wouldn't precondition this K (just solve).\nThe usual purpose of preconditioning is to speed up convergence for iterative methods,\nand that depends on the eigenvalues of P -1A. Here the eigenvalues of T -1K are its\ndiagonal entries N+1, 1, . . ., 1. This example will illustrate a special property of conjugate\ngradients, that with only two different eigenvalues it reaches the true solution x in two\nsteps.\nThe iteration Pxk+1 = (P - A)xk + b is too simple! It is choosing one particular\nvector in a \"Krylov subspace.\" With relatively little work we can make a much better\nchoice of xk . Krylov projections are the state of the art in today's iterative methods.\nKrylov Subspaces\nOur original equation is Ax = b. The preconditioned equation is P -1Ax = P -1b.\nWhen we write P -1, we never intend that an inverse would be explicitly computed\n(except in our example). The ordinary iteration is a correction to xk by the vector\nP -1rk:\nPxk+1 = (P - A)xk + b\nor\nPxk+1 = Pxk + rk\nor\nxk+1 = xk + P -1 rk . (8)\nHere rk = b - Axk is the residual. It is the error in Ax = b, not the error ek\nin x. The symbol P -1rk represents the change from xk to xk+1, but that step is\nnot computed by multiplying P -1 times rk . We might use incomplete LU, or a few\nsteps of a \"multigrid\" iteration, or \"domain decomposition.\" Or an entirely new\npreconditioner.\nIn describing Krylov subspaces, I should work with P -1A. For simplicity I will\nonly write A. I am assuming that P has been chosen and used, and the precondi\ntioned equation P -1Ax = P -1b is given the notation Ax = b. The preconditioner is\nnow P = I. Our new matrix A is probably better than the original matrix with that\nname.\nThe Krylov subspace Kk(A, b) contains all combinations of b, Ab, . . . , Ak-1b.\nThese are the vectors that we can compute quickly, multiplying by a sparse A. We\nlook in this space Kk for the approximation xk to the true solution of Ax = b. Notice\nthat the pure iteration xk = (I - A)xk-1 + b does produce a vector in Kk when xk-1\nis in Kk-1 . The Krylov subspace methods make other choices of xk . Here are four\ndifferent approaches to choosing a good xk in Kk --this is the important decision:\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\n1. The residual rk = b -Axk is orthogonal to Kk (Conjugate Gradients, . . . )\n2. The residual rk has minimum norm for xk in Kk (GMRES, MINRES, . . . )\n3. rk is orthogonal to a different space like Kk(AT) (BiConjugate Gradients, . . . )\n4. ek has minimum norm (SYMMLQ; for BiCGStab xk is in ATKk(AT); . . . )\nIn every case we hope to compute the new xk quickly and stably from the earlier x's.\nIf that recursion only involves xk-1 and xk-2 (short recurrence) it is especially fast.\nWe will see this happen for conjugate gradients and symmetric positive definite A.\nThe BiCG method in 3 is a natural extension of short recurrences to unsymmetric\nA--but stability and other questions open the door to the whole range of methods.\nTo compute xk we need a basis for Kk . The best basis q1, . . . , qk is orthonormal.\nEach new qk comes from orthogonalizing t = Aqk-1 to the basis vectors q1, . . . , qk-1\nthat are already chosen. This is the Gram-Schmidt idea (called modified Gram-\nSchmidt when we subtract projections of t onto the q's one at a time, for numerical\nstability). The iteration to compute the orthonormal q's is known as Arnoldi's\nmethod:\n1 q1 = b/∥b∥2;\n% Normalize to ∥q1∥ = 1\nfor j = 1, . . ., k -1\nt = Aqj ;\n% t is in the Krylov space Kj+1(A, b)\nfor i = 1, . . . , j\nhij = qi\nTt;\n% hij qi = projection of t onto qi\nt = t -hij qi; % Subtract component of t along qi\nq\nh\nend;\nj+1,j = ∥t∥2;\n% t is now orthogonal to q1, . . ., qj\nj+1 = t/hj+1,j ;\n% Normalize t to ∥qj+1∥ = 1\nend\n% q1, . . . , qk are orthonormal in Kk\nPut the column vectors q1, . . . , qk into an n by k matrix Qk . Multiplying rows of\nT\nQT by columns of Qk produces all the inner products qi qj , which are the 0's and 1's\nk\nin the identity matrix. The orthonormal property means that QT\nk Qk = Ik .\nArnoldi constructs each qj+1 from Aqj by subtracting projections hij qi.\nIf\nwe\n\nexpress the steps up to j = k-1 in matrix notation, they become AQk-1 = Qk Hk,k-1:\n⎡\n⎤\n⎡\n⎤\n⎡\n⎤\nArnoldi\nh11\nh12\n· h1,k-1\nAQk-1 =\n⎢⎢⎣ Aq1\n· · · Aqk-1\n⎥⎥⎦ =\n⎢⎢⎣ q1\n· · · qk\n⎢⎢⎣\n⎥⎥⎦\nh21\nh22\n· h2,k-1\nh23\n·\n·\n· hk,k-1\n⎥⎥⎦ .\n(9)\nn by k -1\nn by k\nk by k -1\nThat matrix Hk,k-1 is \"upper Hessenberg\" because it has only one nonzero diagonal\nbelow the main diagonal. We check that the first column of this matrix equation\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\n(multiplying by columns!) produces q2:\nAq1 - h11q1\nh\nAq1 = h11q1 + h21q2\nor\nq2 =\n.\n(10)\nThat subtraction is Step 4 in Arnoldi's algorithm. Division by h21 is Step 6.\nUnless more of the hij are zero, the cost is increasing at every iteration. We have\nk dot products to compute at step 3 and 5, and k vector updates in steps 4 and 6. A\nshort recurrence means that most of these hij are zero. That happens when A = AT .\nThe matrix H is tridiagonal when A is symmetric. This fact is the founda\ntion of conjugate gradients. For a matrix proof, multiply equation (9) by QT\n. The\nk-1\nright side becomes H without its last row, because (QT\nk-1Qk )Hk,k-1 = [ I 0 ] Hk,k-1.\nThe left side QT\nk-1AQk-1 is always symmetric when A is symmetric. So that H matrix\nhas to be symmetric, which makes it tridiagonal. There are only three nonzeros in\nthe rows and columns of H, and Gram-Schmidt to find qk+1 only involves qk and qk-1:\nArnoldi when A = AT\nAqk = hk+1,k qk+1 + hk,k qk + hk-1,k qk-1 .\n(11)\nThis is the Lanczos iteration. Each new qk+1 = (Aqk - hk,kqk - hk-1,kqk-1)/hk+1,k\ninvolves one multiplication Aqk , two dot products for new h's, and two vector updates.\nThe QR Method for Eigenvalues\nAllow me an important comment on the eigenvalue problem Ax = λx. We have seen\nthat Hk-1 = QT\nk-1AQk-1 is tridiagonal if A = AT. When k - 1 reaches n and Qn is\nsquare, the matrix H = QTAQn = Q-1AQn has the same eigenvalues as A:\nn\nn\nSame λ\nHy = Q-1AQny = λy\ngives\nAx = λx with x = Qny .\n(12)\nn\nIt is much easier to find the eigenvalues λ for a tridiagonal H than the for original A.\nThe famous \"QR method\" for the eigenvalue problem starts with T1 = H, factors\nit into T1 = Q1R1 (this is Gram-Schmidt on the short columns of T1), and reverses\norder to produce T2 = R1Q1. The matrix T2 is again tridiagonal, and its off-diagonal\nentries are normally smaller than for T1. The next step is Gram-Schmidt on T2,\northogonalizing its columns in Q2 by the combinations in the upper triangular R2:\nQR Method Factor T2 into Q2R2 . Reverse order to T3 = R2Q2 = Q-1T2Q2 (13)\n.\nBy the reasoning in (12), any Q-1TQ has the same eigenvalues as T . So the matrices\nT2, T3, . . . all have the same eigenvalues as T1 = H and A. (These square Qk from\nGram-Schmidt are entirely different from the rectangular Qk in Arnoldi.) We can\neven shift T before Gram-Schmidt, and we should, provided we remember to shift\nback:\nShifted QR\nFactor Tk - skI = Qk Rk . Reverse to Tk+1 = Rk Qk + bk I .\n(14)\nT\nWhen the shift sk is chosen to be the n, n entry of Tk , the last off-diagonal entry of\nk+1 becomes very small. The n, n entry of Tk+1 moves close to an eigenvalue. Shifted\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nQR is one of the great algorithms of numerical linear algebra. It solves moderate-size\neigenvalue problems with great efficiency. This is the core of MATLAB's eig(A).\nFor a large symmetric matrix, we often stop the Arnoldi-Lanczos iteration at a\ntridiagonal Hk with k < n. The full n-step process to reach Hn is too expensive, and\noften we don't need all n eigenvalues. So we compute (by the same QR method) the k\neigenvalues of Hk instead of the n eigenvalues of Hn. These computed λ1k, λ2k, . . . , λkk\ncan provide good approximations to the first k eigenvalues of A. And we have an\nexcellent start on the eigenvalue problem for Hk+1, if we decide to take a further step.\nThis Lanczos method will find, approximately and iteratively and quickly, the\nleading eigenvalues of a large symmetric matrix.\nThe Conjugate Gradient Method\nWe return to iterative methods for Ax = b. The Arnoldi algorithm produced or\nthonormal basis vectors q1, q2, . . . for the growing Krylov subspaces K1 , K2 , . . .. Now\nwe select vectors x1, x2, . . . in those subspaces that approach the exact solution to\nAx = b. We concentrate on the conjugate gradient method for symmetric positive\ndefinite A.\nThe rule for xk in conjugate gradients is that the residual rk = b - Axk should\nbe orthogonal to all vectors in Kk. Since rk will be in Kk+1, it must be a multiple\nof Arnoldi's next vector qk+1! Each residual is therefore orthogonal to all previous\nresiduals (which are multiples of the previous q's):\nOrthogonal residuals\nri\nT rk = 0\nfor i < k .\n(15)\nThe difference between rk and qk+1 is that the q's are normalized, as in q1 = b/∥b∥.\nK\nSince rk-1 is a multiple of qk, the difference rk -rk-1 is orthogonal to each subspace\ni with i < k. Certainly xi - xi-1 lies in that Ki. So ∆r is orthogonal to earlier\n∆x's:\n(xi - xi-1)T(rk - rk-1) = 0\nfor i < k .\n(16)\nThese differences ∆x and ∆r are directly connected, because the b's cancel in ∆r:\nrk - rk-1 = (b - Axk) - (b - Axk-1) = -A(xk - xk-1) .\n(17)\nSubstituting (17) into (16), the updates in the x's are \"A-orthogonal\" or conjugate:\nConjugate updates ∆x (xi - xi-1)TA(xk - xk-1) = 0\nfor i < k .\n(18)\nNow we have all the requirements. Each conjugate gradient step will find a new\n\"search direction\" dk for the update xk - xk-1. From xk-1 it will move the right\ndistance αkdk to xk. Using (17) it will compute the new rk. The constants βk in\nthe search direction and αk in the update will be determined by (15) and (16) for\ni = k - 1. For symmetric A the orthogonality in (15) and (16) will be automatic for\ni < k - 1, as in Arnoldi. We have a \"short recurrence\" for the new xk and rk.\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nHere is one cycle of the algorithm, starting from x0 = 0 and r0 = b and β1 = 0. It\ninvolves only two new dot products and one matrix-vector multiplication Ad:\nConjugate 1 βk = rk-1rk-1/rT\nT\nk-2rk-2\n% Improvement this step\nGradient\n2 dk = rk-1 + βk dk-1\n% Next search direction\nT\nMethod\n3 αk = rk-1rk-1/dTAdk\n% Step length to next xk\nk\n4 xk = xk-1 + αk dk\n% Approximate solution\n5 rk = rk-1 - αk Adk\n% New residual from (17)\nThe formulas 1 and 3 for βk and αk are explained briefly below--and fully by Trefethen-\nBau ( ) and Shewchuk ( ) and many other good references.\nDifferent Viewpoints on Conjugate Gradients\nI want to describe the (same!) conjugate gradient method in two different ways:\n1. It solves a tridiagonal system Hy = f recursively\n2. It minimizes the energy 1 xTAx - xTb recursively.\nHow does Ax = b change to the tridiagonal Hy = f ? That uses Arnoldi's or\nthonormal columns q1, . . . , qn in Q, with QTQ = I and QTAQ = H:\nAx = b is (QTAQ)(QT x) = QTb which is Hy = f = (∥b∥, 0, . . . , 0) .\n(19)\nT\nSince q1 is b/∥b∥, the first component of f = QTb is q1 b = ∥b∥ and the other com\nponents are qTb = 0. The conjugate gradient method is implicitly computing this\ni\nsymmetric tridiagonal H and updating the solution y at each step. Here is the third\nstep:\n⎡\n⎤ ⎡\n⎤\n⎡\n⎤\nh11\nh12\n∥b∥\nH3y3 = ⎣ h21\nh22\nh23 ⎦ ⎣ y3 ⎦ = ⎣ 0 ⎦ .\n(20)\nh32\nh33\nThis is the equation Ax = b projected by Q3 onto the third Krylov subspace K3 .\nThese h's never appear in conjugate gradients. We don't want to do Arnoldi too!\nIt is the LDLT factors of H that CG is somehow computing--two new numbers at\neach step. Those give a fast update from yj-1 to yj . The corresponding xj = Qj yj\nfrom conjugate gradients approaches the exact solution xn = Qnyn which is x = A-1b.\nIf we can see conjugate gradients also as an energy minimizing algorithm, we can\nextend it to nonlinear problems and use it in optimization. For our linear equation\nAx = b, the energy is E(x) = 1 xTAx - xTb. Minimizing E(x) is the same as solving\nAx = b, when A is positive definite (the main point of Section 1.\n). The CG\niteration minimizes E(x) on the growing Krylov subspaces. On the first\nsubspace K1, the line where x is αb = αd1, this minimization produces the right\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nvalue for α1:\nbTb\nE(αb) = α2bTAb - αbTb\nis minimized at\nα1 =\n.\n(21)\nbTAb\nThat α1 is the constant chosen in step 3 of the first conjugate gradient cycle.\nThe gradient of E(x) = 2\n1 xTAx - xTb is exactly Ax - b. The steepest descent\ndirection at x1 is along the negative gradient, which is r1! This sounds like the perfect\ndirection d2 for the next move. But the great difficulty with steepest descent is that\nthis r1 can be too close to the first direction. Little progress that way. So we add\nthe right multiple β2d1, in order to make d2 = r1 + β2d1 A-orthogonal to the first\ndirection d1.\nThen we move in this conjugate direction d2 to x2 = x1 + α2d2. This explains the\nname conjugate gradients, rather than the pure gradients of steepest descent. Every\ncycle of CG chooses αj to minimize E(x) in the new search direction x = xj-1 + αdj .\nThe last cycle (if we go that far) gives the overall minimizer xn = x = A-1b.\nExample\n⎡\n⎤⎡\n⎤\n⎡\n⎤\n⎣ 1\n⎦⎣ -1 ⎦ = ⎣\nAx = b\nis\n0 ⎦ .\n\n-1\nFrom x0 = 0 and β1 = 0 and r0 = d1 = b the first cycle gives α1 = 2 and x1 = 1b =\n(2, 0, 0). The new residual is r1 = b - Ax1 = (0, -2, -2). Then the second cycle yields\n⎡\n⎤\n⎡\n⎤\nβ2 =\n,\nd2 = ⎣ -2 ⎦ ,\nα2 =\n,\nx2 = ⎣ -1 ⎦ = A-1b !\n-2\n-1\nA\nThe correct solution is reached in two steps, where normally it will take n = 3 steps. The\nreason is that this particular A has only two distinct eigenvalues 4 and 1. In that case\n-1b is a combination of b and Ab, and this best combination x2 is found at cycle 2. The\nresidual r2 is zero and the cycles stop early--very unusual.\nEnergy minimization leads in [ ] to an estimate of the convergence rate for the\n√\nerror e = x - xj in conjugate gradients, using the A-norm ∥e∥A =\neTAe:\nλ\n√\n√\n\nj\nmax -\nλmin\nλ\nError estimate\n∥x - xj ∥A ≤ 2 √\n√\n∥x - x0∥A .\n(22)\nmax +\nλmin\nλ\nThis is the best-known error estimate, although it doesn't account for any clustering of\nthe eigenvalues of A. It involves only the condition number λmax/λmin. Problem\ngives the \"optimal\" error estimate but it is not so easy to compute. That optimal\nestimate needs all the eigenvalues of A, while (22) uses only the extreme eigenvalues\nmax(A) and λmin(A)--which in practice we can bound above and below.\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nMinimum Residual Methods\nWhen A is not symmetric positive definite, conjugate gradient is not guaranteed to\nsolve Ax = b. Most likely it won't. We will follow van der Vorst [ ] in briefly\ndescribing the minimum norm residual approach, leading to MINRES and GMRES.\nThese methods choose xj in the Krylov subspace Kj so that ∥b- Axj ∥ is minimal.\nFirst we compute the orthonormal Arnoldi vectors q1, . . . , qj . They go in the columns\nof Qj , so QT\nj Qj = I. As in (19) we set xj = Qj y, to express the solution as a\ncombination of those q's. Then the norm of the residual rj using (9) is\n∥b - Axj ∥ = ∥b - AQj y∥ = ∥b - Qj+1Hj+1,j y∥ .\n(23)\nT\nT\nThese vectors are all in the Krylov space Kj+1, where rj (Qj+1Qj\nT\n+1rj ) = rj rj . This\nsays that the norm is not changed when we multiply by Qj\nT\n+1. Our problem becomes:\nChoose y to minimize ∥rj ∥ = ∥Qj\nT\n+1b - Hj+1,j y∥ = ∥f - Hy∥ .\n(24)\nThis is an ordinary least squares problem for the equation Hy = f with only j + 1\nequations and j unknowns. The right side f = QT\nj+1b is (∥r0∥, 0, . . ., 0) as in (19).\nThe matrix H = Hj+1,j is Hessenberg as in (9), with one nonzero diagonal below the\nmain diagonal. We face a completely typical problem of numerical linear algebra:\nUse the special properties of H and f to find a fast algorithm that computes y. The\ntwo favorite algorithms for this least squares problem are closely related:\nMINRES A is symmetric (probably indefinite, or we use CG) and H is tridiagonal\nGMRES\nA is not symmetric and the upper triangular part of H can be full\nIn both cases we want to clear out that nonzero diagonal below the main diagonal of\nH. The natural way to do that, one nonzero entry at a time, is by \"Givens rotations.\"\nThese plane rotations are so useful and simple (the essential part is only 2 by 2) that\nwe complete this section by explaining them.\nGivens Rotations\nThe direct approach to the least squares solution of Hy = f constructs the normal\nequations HTHy = HTf. That was the central idea in Chapter 1, but you see what\nwe lose. If H is Hessenberg, with many good zeros, HTH is full. Those zeros in H\nshould simplify and shorten the computations, so we don't want the normal equations.\nThe other approach to least squares is by Gram-Schmidt. We factor H into\northogonal times upper triangular. Since the letter Q is already used, the or\nthogonal matrix will be called G (after Givens). The upper triangular matrix is G-1H.\nThe 3 by 2 case shows how a plane rotation G-1 can clear out the subdiagonal entry\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nh21:\n⎡\n⎤ ⎡\n⎤\n⎡\n⎤\ncos θ sin θ 0\nh11\nh12\n∗\n∗\n\nG-1\n21 H = ⎣-sin θ cos θ 0 ⎦ ⎣ h21\nh22 ⎦ = ⎣ 0 ∗⎦ .\n(25)\nh32\n0 ∗\nG\nThat bold zero entry requires h11 sin θ = h21 cos θ, which determines θ. A second\n-1\nrotation G-1\n32 , in the 2-3 plane, will zero out the 3, 2 entry. Then G-1\nH is a\nsquare upper triangular matrix U above a row of zeros!\nThe Givens orthogonal matrix is G = G21G32 but there is no reason to do this mul\ntiplication. We use each Gij as it is constructed, to simplify the least squares problem.\nRotations (and all orthogonal matrices) leave the lengths of vectors unchanged:\n21 Hy -G-1G-1\nU\nF\n∥Hy -f ∥= ∥G-1G-1\nf ∥= ∥\ny -\n∥.\n(26)\ne\nThis length is what MINRES and GMRES minimize. The row of zeros below U\nmeans that the last entry e is the error--we can't reduce it. But we get all the other\nentries exactly right by solving the j by j system Uy = F (here j = 2). This gives\nthe best least squares solution y. Going back to the original problem of minimizing\n∥r∥= ∥b -Axj ∥, the best xj in the Krylov space Kj is Qj y.\nFor non-symmetric A (GMRES rather than MINRES) we don't have a short\nrecurrence. The upper triangle in H can be full, and step j becomes expensive and\npossibly inaccurate as j increases. So we may change \"full GMRES\" to GMRES(m),\nwhich restarts the algorithm every m steps. It is not so easy to choose a good m.\nProblem Set 3.6\nCreate K2D for a 4 by 4 square grid with N 2 = 32 interior mesh points (so\nn = 9). Print out its factors K = LU (or its Cholesky factor C = chol(K) for\nthe symmetrized form K = CTC). How many zeros in these triangular factors?\nAlso print out inv(K) to see that it is full.\nAs N increases, what parts of the LU factors of K2D are filled in?\nCan you answer the same question for K3D? In each case we really want an\nestimate cN p of the number of nonzeros (the most important number is p).\nUse the tic; ...; toc clocking command to compare the solution time for K2Dx =\nrandom f in ordinary MATLAB and sparse MATLAB (where K2D is defined as\na sparse matrix). Above what value of N does the sparse routine K\\f win?\nCompare ordinary vs. sparse solution times in the three-dimensional K3Dx =\nrandom f . At which N does the sparse K\\f begin to win?\nIncomplete LU\nConjugate gradients\n\n3.6. SOLVING LARGE LINEAR SYSTEMS\nDraw the next step after Figure 3.19 when the matrix has become 4 by 4 and\nthe graph has nodes 2-4-5-6. Which have minimum degree? Is there more\nfill-in?\nRedraw the right side of Figure 3.19 if row number 2 is chosen as the second\npivot row. Node 2 does not have minimum degree. Indicate new edges in the\n5-node graph and new nonzeros F in the matrix.\nT\nTo show that T -1K = I + leT\n1 in (7), with e1 = [ 1 0 . . . 0 ], we can start from\nT\nT\nK = T + e1e1 . Then T -1K = I + (T -1e1)e1 and we verify that e1 = Tl:\n⎤\n⎡\n⎤\n⎡\n⎤\n⎡\n1 -1\nN\nTl =\n⎢⎢⎣\n-1\n2 -1\n·\n·\n·\n⎢⎢⎣\n⎥⎥⎦\nN -1\n·\n⎥⎥⎦ =\n⎢⎢⎣\n·\n⎥⎥⎦ = e1 .\n-1\nSecond differences of a linear vector l are zero. Now multiply T -1K = I + leT\ntimes I -(leT\n1 )/(N + 1) to establish the inverse matrix K-1T in (7).\nArnoldi expresses each Aqk as hk+1,kqk+1 + hk,kqk + · · · + h1,k q1. Multiply by qT\ni\nT\nto find hi,k = qi Aqk . If A is symmetric you can write this as (Aqi)Tqk. Explain\nwhy (Aqi)Tqk = 0 for i < k -1 by expanding Aqi into hi+1,iqi+1 + · · · + h1,iq1.\nWe have a short recurrence if A = AT (only hk+1,k and hk,k and hk-1,k are\nnonzero)."
    },
    {
      "category": "Resource",
      "title": "am37.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/d9ad8a20efdcd46c4cccd981bc4306e5_am37.pdf",
      "content": "Partial Differential Equations\n3.7\nFour Model Examples\nThe differential equations in Chapter 1 were very ordinary. There were time deriva\ntives d/dt or space derivatives d/dx but not both:\ndu\nd2u\nd\ndu\n= -Ku or M\n+ Ku = F (t)\nor\n-\nc(x)\n= f (x) .\n(1)\ndt\ndt2\ndx\ndx\nA partial differential equation contains two or more derivatives (they have to be partial\nderivatives like ∂/∂x and ∂/∂y and ∂/∂t so we can tell them apart). The solution\nu(x, y) or u(x, t) or even u(x, y, t) is a function of those \"independent variables\" x\nand y and t.\nIt is important to distinguish different types of equations, above all the distinction\nbetween \"boundary value problems\" and initial value problems\". The time variable t\nindicates an initial value problem. The first equation in (1) starts from an initial value\nu(0). The solution u(ε) evolves?? for t > 0 by obeying the equation du/dt = Au.\nThe second equation needs also an initial value du/dt(0) for the velocity, because\nthe leading term involves d2u/dt2 . Boundary values were given at endpoints x = 0\nand x = 1. Inside the boundary (in the interior) u(x) solved the equation, with just\nenough freedom (two arbitrary constants) to satisfy the two boundary conditions. All\ngood. The third equation in (1) described a steady state u(x).\nFor partial differential equations, start with initial value problems. We will focus\non three examples. They involve first or second order derivatives in t and in x and\nu(x, t) is a scalar. The names of the equations are important too:\n∂u\n∂u\nOne way wave equation\n∂t = ∂x\n(2)\n∂u\n∂2u\nHeat equation, diffusion equation\n=\n∂t\n∂x2\n(3)\n∂2u\nWave equation (with velocity c)\n∂t2 = c 2 ∂2u\n(4)\n∂x2\nThe first two equations involve ∂/∂t (first order) so initial values u(x, 0) will\nbe given (at t = 0). We know where the solution starts, and in Figure 3.1 those\ninitial values are delta functions. Notice the difference at t = 1! In the one way\nwave equation, the delta function moved to the left. In the heat equation, the delta\nfunction diffused into a Gaussian. And it spreads out even further by the time t = 2.\n\n′\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nSince the initial value is symmetric around the centerpoint x = 0, so is the solution\nu(x, t). The heat equation doesn't notice if you change x to -x, but it sure notices if\nyou switch t to -t. The \"backward heat equation\" -∂u/∂t = ∂2u/∂x2 is impossible\nto solve. Physically, hot air can spread into a room, but time doesn't reverse and the\ndiffused heat doesn't return back to the starting point.\nThe full wave equation involves ∂2u/∂t2, so we need an initial velocity ∂u/∂t(x, 0)\nin addition to u(x, 0). In Figure 3.2a that initial velocity is zero. We see waves going in\nboth directions (symmetrically). In Figure 3.2b the initial velocity is ∂u/∂t(x, 0) = 1.\nThe wave to the left is different from the wave to the right. You might note that the\nsame word \"velocity\" was used for the number c (velocity in x - t space) and for\n∂u/∂t (velocity in u - t space).\nHow could those examples be extended? The one way wave equation could become\n∂u\n∂u\n∂u\n∂u\n∂u\n∂u\n???\n= c\nor\n= c(x)\nor even\n= c(u)\n.\n(5)\n∂t\n∂x\n∂t\n∂x\n∂t\n∂x\nThe last of those is nonlinear! It is highly important, one good application is to traffic\nflow. At a point x on the highway, the car density is u(x, t) at time t. If the density\nup ahead (one way drivers!) is greater, then cars slow down and get denser. The\nrelation depends on u itself, it is not linear. This produces the waves of stop and go\ndriving that a helicopter sees in a traffic jam.\nThe heat equation should have a \"diffusivity constant\" c, with the dimension of\n(distance)2/time. In fact this fits our framework exactly, there is a perfect analogy\nwith K = ATCA and u = -Ku:\n∂u\n∂\n∂u\n=\nc(x)\n.\n(6)\n∂t\n∂x\n∂x\nWhen c(x) is a positive constant, we can rescale time to make c = 1. That is the case\nwe can solve. (When c is a negative constant, nobody can solve the backward heat\nequation. We never allowed c < 0 in Chapters 1 and 2 either.) When c depends on\nu or ∂u/∂x, the equation becomes nonlinear and we don't expect an exact formula\n(but we can compute!).\n??? The wave equation would also look better in its symmetric form using ATCA.\nNotice also that it can be rewritten as\n\n∂u\n∂u\n∂\n∂t\n0 c\n∂\n=\n∂t\n∂t\nc ∂u\nc 0\n∂x\nc ∂u\n.\n(7)\n∂x\n∂x\nIn a sense (Problem A) this is a pair of one way wave equations!\nThose time-dependent wave and heat equations will come after we study the\nall-important equation of equilibrium: Laplace's equation. This describes a steady\nstate. The variables are x, y, z (in three space dimensions) or x and y (in two dimensions--\nwe will concentrate on this very remarkable model).\n\n3.7. FOUR MODEL EXAMPLES\nLaplace's equation has pure second derivatives ∂2u/∂x2 = uxx and ∂2u/∂y2 = uyy :\n∂2u\nd2u\nLaplace:\n+ ∂y2 = uxx + uyy = 0 in a plane region R\n(8)\n∂x2\nThis describes the steady state temperature u(x, y) over the region R, when there is\nno heat source inside (the right side of the equation is zero). The problem doesn't\nhave initial conditions, it has boundary conditions! The boundary of R is a closed\ncurve C. At every point of C we may prescribe either a fixed temperature u0 or a\nheat flux F0:\n∂u\nBoundary conditions:\nu = u0 or\n= F0\nat each point of C .\n(9)\n∂n\n∂u\nThat \"normal derivative\" ∂n is the rate of change of u in the direction perpendicular\nto the boundary. At a point where the boundary is insulated (meaning that no heat\ncan flow through) the flux is ∂u/∂n = 0.\nThis is the problem of Section 3.2: Laplace's equation (8) with boundary condi\ntions (9). It is the two-dimensional analogue, a partial differential equation, of the\nmost basic two-point value problem:\nd2u = 0 with [u(0) or u ′(0)] and [u(1) or u ′(1)] given at the endpoints . (10)\ndx2\nThis describes the displacement (or it could be the temperature) in a rod. The\nsolution to equation (10) is just u(x) = A + Bx. For Laplace's equation we will list\nan infinite family of solutions (which we need because there are infinitely many more\nboundary points!).\nEquation (10) was our simple model, with no applied force f and with a constant\ncoefficient c = 1. The more general form in Chapter 2 was\nd\ndu\ndu\n-\nc(x)\n= f(x), with boundary conditions on u or w = c\n.\n(11)\ndx\ndx\ndx\nThose possibilities for f and c are also seen in two dimensions. When there is a source\nterm f(x, y) we have Poisson's equation (pronounced Pwa-son):\n∂2u\n∂2u\nPoisson:\n+ ∂y2 = uxx + uyy = f(x, y) in the region R .\n(12)\n∂x2\nWhen the material in R is not homogeneous, the constant coefficient c = 1 becomes\na variable coefficient c(x, y):\n∂\n∂u\n∂\n∂u\nNonhomogeneous:\nc\n+\nc\n= f(x, y) in the region R\n(13)\n∂x\n∂x\n∂y\n∂y\nMaybe you can see that we are closing in on our favorite framework ATCAu = f!\nSection 3.2 sets this framework, by identifying A and AT . Those are the key\noperators of vector calculus, the gradient and the divergence. Laplace's equation,\nwith c = 1 and f = 0, is seen as div grad u = 0. Then we concentrate on solving this\nexceptional equation, by analysis or by scientific computing:\n\nCHAPTER 3. BOUNDARY VALUE PROBLEMS\nExact solution (formula and series): Section 3.2\nNumerical solution (finite differences and finite elements): Section 3.3."
    },
    {
      "category": "Resource",
      "title": "am51.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/e548f1cb5f6c6bc9a5c90ee5180a3b91_am51.pdf"
    },
    {
      "category": "Resource",
      "title": "am52.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/307c9b34baca5b281add586a56de5756_am52.pdf"
    },
    {
      "category": "Resource",
      "title": "am53.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/8eaa23367474c809cc0816f24fdacc7f_am53.pdf"
    },
    {
      "category": "Resource",
      "title": "am54.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/5db29e69494eb09a26f7224d43adc6f6_am54.pdf"
    },
    {
      "category": "Resource",
      "title": "am55.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/1d170d72359b7d27052c960197b425e5_am55.pdf"
    },
    {
      "category": "Resource",
      "title": "am56.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/3762c803fd3f27bcc47cc76341162824_am56.pdf"
    },
    {
      "category": "Resource",
      "title": "am57.pdf",
      "type": "PDF",
      "source_url": "https://ocw.mit.edu/courses/18-086-mathematical-methods-for-engineers-ii-spring-2006/49baea85bf92b8bd0c73c9a313fd3f33_am57.pdf"
    }
  ]
}