import os
import io
import re
import json
import time
import random
import logging
import requests
import unicodedata
import fitz
from bs4 import BeautifulSoup
from docx import Document
from urllib.parse import urljoin
from src.utils.logger import Logger
from constants import *

class ContentScraper:
    def __init__(self, course_url, download_dir=DEFAULT_DOWNLOAD_DIR):
        self.course_url = course_url
        self.download_url = urljoin(self.course_url, "download/")
        self.syllabus_url = urljoin(self.course_url, "pages/syllabus/")
        self.download_dir = download_dir
        self.output_path = None
        self.logger = Logger(__name__, see_time=True, console_log=True)
        self.session = requests.Session()  # Create a session for resource management
        self._ensure_dir_exists(self.download_dir)

    def _ensure_dir_exists(self, directory):
        """Ensures the specified directory exists."""
        os.makedirs(directory, exist_ok=True)

    def _generate_safe_filename(self, name):
        """Generates a safe filename from a course name."""
        # Remove invalid characters
        safe_name = re.sub(r'[<>:"/\\|?*]', '', name)
        # Replace spaces with underscores
        safe_name = re.sub(r'\s+', '_', safe_name)
        # Truncate if too long (optional, adjust length as needed)
        max_len = 100
        if len(safe_name) > max_len:
            safe_name = safe_name[:max_len]
        return f"{safe_name}.json"

    def _clean_text(self, text: str) -> str:
        """Cleans extracted text content."""
        if not isinstance(text, str):
            self.logger.log_message(f"_clean_text received non-string: {type(text)}. Returning as is.", level=logging.WARNING)
            return text
        # Avoid cleaning specific info/error messages generated by the extractor
        if text.startswith(("Error:", "Info:", "Extraction not implemented")):
             return text

        # Step 1: Replace known problematic Unicode characters
        known_unicode_map = {
            '\uf0b7': '-', '\u2022': '-', '\u2018': "'", '\u2019': "'", '\u201c': '"',
            '\u201d': '"', '\u2013': '-', '\u2014': '--', '\xa0': ' ', '\uf0a7': '*',
            '\u25e6': '*', '\u00a9': '(c)', '\u00ae': '(r)', '\u2026': '...', '\u2020': '+',
            '\u2021': '++', '\u2030': '%', '\u2212': '-', '\u221e': 'inf'
        }
        for bad_char, replacement in known_unicode_map.items():
            text = text.replace(bad_char, replacement)

        # Step 2: Normalize Unicode characters
        try:
            text = unicodedata.normalize('NFKD', text)
            text = ''.join([char for char in text if not unicodedata.combining(char)])
        except Exception as e:
            self.logger.log_message(f"Unicode normalization failed: {e}", level=logging.WARNING)

        # Step 3: Remove non-printing chars (allow newline/tab)
        text = ''.join(
            ch for ch in text
            if unicodedata.category(ch)[0] not in ('C', 'Zl', 'Zp') or ch in '\n\t'
        )

        # Step 4: Normalize whitespaces/newlines & remove page numbers
        text = re.sub(r'[ \t]+', ' ', text) # Multiple spaces/tabs to one space
        text = re.sub(r' *\n *', '\n', text) # Spaces around newlines
        lines = text.split('\n')
        cleaned_lines = [line for line in lines if not re.fullmatch(r'\s*\d+\s*', line)] # Page numbers
        text = '\n'.join(cleaned_lines)
        text = re.sub(r'\n{3,}', '\n\n', text) # Max 2 consecutive newlines

        # Step 5: Trim
        return text.strip()

    def scrape_course_metadata(self):
        """Scrapes the main course page for name, description, and topics."""
        self.logger.log_message(f"Scraping course metadata from: {self.course_url}")
        try:
            response = self.session.get(self.course_url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")
            
            title_tag = soup.find("h1")
            course_name = title_tag.text.strip() if title_tag else "Unknown Course"

            course_description = "No description found."
            description_div = soup.find("div", {"id": "course-description"})
            if description_div:
                expanded_desc = description_div.find("div", {"id": "expanded-description"})
                desc_container = expanded_desc if expanded_desc else description_div.find("div", {"id": "collapsed-description"})
                if desc_container:
                    button = desc_container.find("button")
                    if button: button.decompose()
                    course_description = desc_container.text.strip()

            topics = []
            topic_tags = soup.find_all("a", class_="course-info-topic")
            for tag in topic_tags:
                if tag.text:
                    topics.append(tag.text.strip())
            self.logger.log_message(f"Found course: {course_name}, Topics: {len(topics)}")
            return course_name, course_description, topics
        except requests.exceptions.RequestException as e:
            self.logger.log_message(f"Failed to fetch course metadata: {e}", level=logging.ERROR)
            return "Error Fetching Metadata", "", []
        except Exception as e:
            self.logger.log_message(f"Error parsing course metadata: {e}", level=logging.ERROR)
            return "Error Parsing Metadata", "", []

    def scrape_file_metadata(self):
        """Scrapes the download page for file metadata ONLY."""
        self.logger.log_message(f"Scraping file metadata from: {self.download_url}")
        file_metadata_list = []
        try:
            response = self.session.get(self.download_url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")
            resource_items = soup.find_all("div", class_="resource-item resource-list-page")
            self.logger.log_message(f"Found {len(resource_items)} potential resource items on download page.")

            for item in resource_items:
                thumbnail_link = item.find("a", class_="resource-thumbnail")
                details_section = item.find("div", class_="resource-list-item-details")

                if not thumbnail_link or not details_section or not thumbnail_link.get('href'):
                    continue

                file_href = thumbnail_link['href']
                file_href_lower = file_href.lower()
                source_url = urljoin(BASE_URL, file_href)

                # Filter for specific file types first
                if not any(file_href_lower.endswith(ext) for ext in [".pdf", ".doc", ".docx", ".py"]):
                    self.logger.log_message(f"Skipping item: Unwanted file type ({file_href_lower.split('.')[-1]})", level=logging.DEBUG) # Optional: Log skipped types
                    continue

                # Extract file type
                file_type = "unknown"
                file_type_div = thumbnail_link.find("div", class_="resource-type-thumbnail")
                if file_type_div and len(file_type_div.get('class', [])) > 1:
                    type_class = file_type_div['class'][1]
                    if type_class in ['pdf', 'doc', 'docx']:
                        file_type = type_class.upper()
                    elif type_class == 'file' and file_href_lower.endswith(".py"):
                        file_type = "PY"
                if file_type == "unknown": # Fallback
                    if file_href_lower.endswith(".pdf"): file_type = "PDF"
                    elif file_href_lower.endswith(".doc"): file_type = "DOC"
                    elif file_href_lower.endswith(".docx"): file_type = "DOCX"
                    elif file_href_lower.endswith(".py"): file_type = "PY"
                
                # If still unknown after checks, skip
                if file_type == "unknown": continue

                # Extract title and category
                title_link = details_section.find("a", class_="resource-list-title")
                resource_title = title_link.text.strip() if title_link else "Untitled Resource"
                resource_category = self._determine_category(resource_title, file_href_lower, file_type)

                file_metadata_list.append({
                    "category": resource_category,
                    "title": resource_title,
                    "type": file_type,
                    "source_url": source_url
                    # No 'content' key here
                })

            self.logger.log_message(f"Finished scraping metadata. Found {len(file_metadata_list)} target files.")
            return file_metadata_list
        except requests.exceptions.RequestException as e:
            self.logger.log_message(f"Failed to fetch download page: {e}", level=logging.ERROR)
            return []
        except Exception as e:
            self.logger.log_message(f"Error parsing download page: {e}", level=logging.ERROR)
            return []

    def _determine_category(self, title, href_lower, file_type):
        """Determines resource category based on title keywords."""
        resource_category = "Resource" # Default
        title_lower = title.lower()
        if any(term in title_lower for term in ["assignment", "problem set", "pset"]):
            resource_category = "Assignment"
        elif any(term in title_lower for term in ["lecture", "lec"]):
            resource_category = "Lecture Notes" if file_type == "PDF" else "Lecture Code"
        elif any(term in title_lower for term in ["exam", "quiz"]): resource_category = "Exam"
        elif "syllabus" in title_lower: resource_category = "Syllabus"
        return resource_category

    def _extract_content(self, url, file_type, title):
        """Downloads and extracts raw text content for a given URL and type."""
        self.logger.log_message(f"Extracting content for {file_type}: {title} from {url}")
        raw_content = None
        error_message = None

        try:
            file_response = self.session.get(url, headers=HEADERS, timeout=60)
            file_response.raise_for_status()
            file_bytes = file_response.content

            if file_type == "PDF":
                try:
                    with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                        raw_content = "\n".join(page.get_text() for page in doc)
                except Exception as e:
                    error_message = f"Error extracting PDF: {e}"
            elif file_type == "DOCX":
                try:
                    with io.BytesIO(file_bytes) as docx_stream:
                        document = Document(docx_stream)
                        raw_content = "\n".join([para.text for para in document.paragraphs])
                except Exception as e:
                    error_message = f"Error extracting DOCX: {e}"
            elif file_type == "PY":
                try:
                    raw_content = file_bytes.decode('utf-8')
                except UnicodeDecodeError:
                    try: raw_content = file_bytes.decode('latin-1')
                    except Exception as e: error_message = f"Error decoding PY (fallback): {e}"
                except Exception as e: error_message = f"Error decoding PY: {e}"
            elif file_type == "DOC":
                try:
                    raw_content = file_bytes.decode('utf-8', errors='ignore')
                    if raw_content is not None and not raw_content.strip():
                        raw_content = "Info: Basic DOC decode yielded no text (binary?)."
                except Exception as e: error_message = f"Error decoding DOC: {e}"
            else:
                error_message = f"Extraction not implemented for type {file_type}."

        except requests.exceptions.Timeout:
            error_message = f"Timeout downloading: {url}"
        except requests.exceptions.RequestException as e:
            error_message = f"Download error: {e}"
        except Exception as e:
            error_message = f"Unexpected error during extraction: {e}"

        if error_message:
            self.logger.log_message(f"Failed extraction for {title}: {error_message}", level=logging.ERROR)
            return error_message # Return error message
        elif raw_content is not None:
            self.logger.log_message(f"Successfully extracted raw content for {title}.", level=logging.DEBUG)
            return raw_content
        else:
            self.logger.log_message(f"Extraction yielded no content and no error for {title}.", level=logging.WARNING)
            return "Info: Extraction yielded no content."

    def _load_existing_data(self, filepath):
        """Loads existing JSON data from the filepath."""
        if os.path.exists(filepath):
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                self.logger.log_message(f"Error loading existing data from {filepath}: {e}. Starting fresh.", level=logging.WARNING)
        return None

    def _save_data(self, data, filepath):
        """Saves data to JSON file."""
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            self.logger.log_message(f"Data saved to {filepath}", level=logging.DEBUG) # More frequent, so DEBUG
        except IOError as e:
            self.logger.log_message(f"Failed to save data to {filepath}: {e}", level=logging.ERROR)

    def scrape_syllabus_content(self):
        """Scrapes content from the syllabus page."""
        self.logger.log_message(f"Scraping syllabus content from: {self.syllabus_url}")
        syllabus_data = {
            "content": "",
            "files": []
        }

        try:
            response = self.session.get(self.syllabus_url, headers=HEADERS, timeout=30)
            # Check if page exists
            if response.status_code == 404:
                self.logger.log_message(f"Syllabus page not found at {self.syllabus_url}", level=logging.WARNING)
                return syllabus_data
                
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")
            
            # Extract main syllabus content
            main_content = soup.find("main")
            if main_content:
                # Remove navigation elements
                for nav in main_content.find_all(["nav", "footer"]):
                    if hasattr(nav, 'decompose'):
                        nav.decompose()
                
                syllabus_data["content"] = self._clean_text(main_content.get_text(separator="\n"))
            
            # Look for downloadable files in the syllabus page
            resource_items = soup.find_all("div", class_="resource-item")
            for item in resource_items:
                link = item.find("a", href=True)
                if not link:
                    continue
                    
                file_href = link['href']
                title_element = item.find(class_="resource-list-title")
                title = title_element.text.strip() if title_element else "Untitled Resource"
                
                # Determine file type
                file_type = "unknown"
                if file_href.lower().endswith(".pdf"):
                    file_type = "PDF"
                elif file_href.lower().endswith(".docx"):
                    file_type = "DOCX"
                elif file_href.lower().endswith(".doc"):
                    file_type = "DOC"
                
                source_url = urljoin(BASE_URL, file_href)
                
                syllabus_data["files"].append({
                    "category": "Syllabus",
                    "title": title,
                    "type": file_type,
                    "source_url": source_url
                })
                
            self.logger.log_message(f"Found {len(syllabus_data['files'])} files in syllabus page.")
            return syllabus_data
        
        except requests.exceptions.RequestException as e:
            self.logger.log_message(f"Failed to fetch syllabus page: {e}", level=logging.ERROR)
            return syllabus_data
        except Exception as e:
            self.logger.log_message(f"Error parsing syllabus page: {e}", level=logging.ERROR)
            return syllabus_data

    def run(self):
        """Main execution method for the scraper."""
        self.logger.log_message("--- Content Scraper Started ---")

        # 1. Scrape course metadata
        course_name, course_description, topics = self.scrape_course_metadata()
        if course_name.startswith("Error"):
            self.logger.log_message("Aborting due to failure scraping course metadata.", level=logging.ERROR)
            return None  # Return None to indicate failure to the multi-course scraper
        
        # Generate dynamic output path *after* getting course name
        self.output_path = os.path.join(self.download_dir, self._generate_safe_filename(course_name))
        self.logger.log_message(f"Output will be saved to: {self.output_path}")

        # 2. Scrape syllabus content
        syllabus_data = self.scrape_syllabus_content()
        
        # 3. Scrape file metadata (URLs, types, etc.)
        file_metadata_list = self.scrape_file_metadata()
        
        # Combine files from syllabus with other files
        file_metadata_list.extend(syllabus_data["files"])
        
        if not file_metadata_list:
            self.logger.log_message("No file metadata found or error occurred. Saving basic course info.", level=logging.WARNING)
            final_data = {
                "course_name": course_name,
                "course_description": course_description,
                "topics": topics,
                "syllabus_content": syllabus_data["content"],
                "files": []
            }
            self._save_data(final_data, self.output_path)
            self.logger.log_message("--- Content Scraper Finished (No files processed) ---")
            return {
                "path": self.output_path,
                "content_processed": False
            }  # 返回包含路径和处理状态的字典

        # 4. Load existing data or initialize
        existing_data = self._load_existing_data(self.output_path)
        content_processed = False  # 跟踪是否处理了新内容
        
        if existing_data and isinstance(existing_data.get('files'), list):
            self.logger.log_message(f"Loaded existing data from {self.output_path}")
            final_data = existing_data
            # Update top-level metadata in case it changed
            final_data['course_name'] = course_name
            final_data['course_description'] = course_description
            final_data['topics'] = topics
            final_data['syllabus_content'] = syllabus_data["content"]
            # Update file list based on current scrape
            existing_files_map = {f['source_url']: f for f in final_data['files']}
            current_files_processed = []
            for meta_item in file_metadata_list:
                url = meta_item['source_url']
                if url in existing_files_map:
                    # Preserve existing content if present
                    existing_files_map[url].update(meta_item) # Update metadata like title/category
                    current_files_processed.append(existing_files_map[url])
                else:
                    # New file found during scrape
                    current_files_processed.append(meta_item) # Add new file, content to be extracted
                    content_processed = True  # 发现了新文件，需要处理
            final_data['files'] = current_files_processed
        else:
            self.logger.log_message("No valid existing data found. Initializing new structure.")
            final_data = {
                "course_name": course_name,
                "course_description": course_description,
                "topics": topics,
                "syllabus_content": syllabus_data["content"],
                "files": file_metadata_list # Initially no content key
            }
            content_processed = True  # 新的数据结构，需要处理
            # Save the initial structure immediately
            self._save_data(final_data, self.output_path)

        # 4. Iterate, Extract Content, and Save Progressively
        self.logger.log_message("Starting content extraction phase...")
        files_to_process = final_data.get('files', [])
        total_files = len(files_to_process)
        for index, file_entry in enumerate(files_to_process):
            self.logger.log_message(f"--- Checking file {index + 1}/{total_files}: {file_entry.get('title')} ---")
            source_url = file_entry.get('source_url')
            file_type = file_entry.get('type')
            title = file_entry.get('title', '[Unknown Title]')

            if not source_url or not file_type:
                self.logger.log_message(f"Skipping file due to missing URL or Type.", level=logging.WARNING)
                continue

            # Check if content already exists and is valid
            content = file_entry.get('content')
            has_valid_content = content and not isinstance(content, (int, float, bool)) and not content.startswith(("Error:", "Info:")) 

            if has_valid_content:
                self.logger.log_message(f"Valid content already exists for '{title}'. Skipping extraction.")
                continue
            else:
                self.logger.log_message(f"Extracting content for '{title}'...")
                # Extract content
                extracted_data = self._extract_content(source_url, file_type, title)
                # Clean the extracted content (handles error messages appropriately)
                cleaned_content = self._clean_text(extracted_data)
                # Update the specific file entry
                file_entry['content'] = cleaned_content
                # Save the entire updated data immediately after processing this file
                self._save_data(final_data, self.output_path)
                self.logger.log_message(f"Updated and saved data for '{title}'.")
                content_processed = True  # 处理了新内容
                # Add delay after successful extraction/save
                time.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))

        self.logger.log_message("--- Content Scraper Finished ---")
        return {
            "path": self.output_path,
            "content_processed": content_processed
        }  # 返回包含路径和处理状态的字典

    def cleanup(self):
        """Close the session and cleanup resources."""
        if hasattr(self, 'session'):
            self.session.close()
            self.logger.log_message("HTTP session closed.")
    
    def __del__(self):
        """Cleanup when object is destroyed."""
        self.cleanup()

